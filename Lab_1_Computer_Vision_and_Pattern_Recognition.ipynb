{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOP8hOHF9AhWzFQZwsNvyyH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/21092004Goda/data_anal/blob/main/Lab_1_Computer_Vision_and_Pattern_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Лабораторная работа 1\n",
        "**Сбор, визуализация и анализ данных по графу (cs.CV, arXiv)**\n",
        "\n",
        "Кратко:\n",
        "- Скачиваем метаданные arXiv.\n",
        "- Извлекаем ключевые слова моделью Transformers.\n",
        "- Строим граф ключевых слов и граф публикаций.\n",
        "- Анализируем сообщества и центральные узлы.\n",
        "\n",
        "*Внимание:* извлечение ключевых слов может быть долгим. Рекомендуется GPU и кеширование промежуточных результатов.\n"
      ],
      "metadata": {
        "id": "www_mTav2gQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install feedparser"
      ],
      "metadata": {
        "id": "eYFALvcWH6d-"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import feedparser\n",
        "import urllib.request as libreq\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "B5ltoBcHH71T"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Сбор данных с arXiv"
      ],
      "metadata": {
        "id": "Awsii-w1pUGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data collection\n",
        "\n",
        "with libreq.urlopen('http://export.arxiv.org/api/query?search_query=all:cs.CV&start=0&max_results=1000') as url:\n",
        "  r = url.read()\n",
        "print(r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXgzYztPH7xm",
        "outputId": "4b449499-f0ae-406e-e4f8-4001bc9f2c55"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\\n  <link href=\"http://arxiv.org/api/query?search_query%3Dall%3Acs.CV%26id_list%3D%26start%3D0%26max_results%3D1000\" rel=\"self\" type=\"application/atom+xml\"/>\\n  <title type=\"html\">ArXiv Query: search_query=all:cs.CV&amp;id_list=&amp;start=0&amp;max_results=1000</title>\\n  <id>http://arxiv.org/api/Us4TmgaiaHBxlhlMNGOB3sxrnVQ</id>\\n  <updated>2025-09-22T00:00:00-04:00</updated>\\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">167528</opensearch:totalResults>\\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1000</opensearch:itemsPerPage>\\n  <entry>\\n    <id>http://arxiv.org/abs/0903.0134v2</id>\\n    <updated>2010-01-08T10:32:52Z</updated>\\n    <published>2009-03-01T11:10:27Z</published>\\n    <title>Recognition of Regular Shapes in Satelite Images</title>\\n    <summary>  This paper has been withdrawn by the author ali pourmohammad.\\n</summary>\\n    <author>\\n      <name>Ahmad Reza Eskandari</name>\\n    </author>\\n    <author>\\n      <name>Ali Pourmohammad</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This paper has been withdrawn</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/0903.0134v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0903.0134v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1010.0422v1</id>\\n    <updated>2010-10-03T16:55:56Z</updated>\\n    <published>2010-10-03T16:55:56Z</published>\\n    <title>Convolutional Matching Pursuit and Dictionary Training</title>\\n    <summary>  Matching pursuit and K-SVD is demonstrated in the translation invariant\\nsetting\\n</summary>\\n    <author>\\n      <name>Arthur Szlam</name>\\n    </author>\\n    <author>\\n      <name>Koray Kavukcuoglu</name>\\n    </author>\\n    <author>\\n      <name>Yann LeCun</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1010.0422v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1010.0422v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1406.7120v1</id>\\n    <updated>2014-06-27T09:18:44Z</updated>\\n    <published>2014-06-27T09:18:44Z</published>\\n    <title>Template Matching based Object Detection Using HOG Feature Pyramid</title>\\n    <summary>  This article provides a step by step development of designing a Object\\nDetection scheme using the HOG based Feature Pyramid aligned with the concept\\nof Template Matching.\\n</summary>\\n    <author>\\n      <name>Anish Acharya</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1406.7120v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1406.7120v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1707.01243v1</id>\\n    <updated>2017-07-05T07:43:00Z</updated>\\n    <published>2017-07-05T07:43:00Z</published>\\n    <title>Exploration of object recognition from 3D point cloud</title>\\n    <summary>  We present our latest experiment results of object recognition from 3D point\\ncloud data collected through moving car.\\n</summary>\\n    <author>\\n      <name>Lin Duan</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1707.01243v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1707.01243v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.11643v1</id>\\n    <updated>2018-07-31T02:51:21Z</updated>\\n    <published>2018-07-31T02:51:21Z</published>\\n    <title>Brain MRI Image Super Resolution using Phase Stretch Transform and\\n  Transfer Learning</title>\\n    <summary>  A hallucination-free and computationally efficient algorithm for enhancing\\nthe resolution of brain MRI images is demonstrated.\\n</summary>\\n    <author>\\n      <name>Sifeng He</name>\\n    </author>\\n    <author>\\n      <name>Bahram Jalali</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1807.11643v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.11643v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.10788v1</id>\\n    <updated>2019-01-30T12:47:44Z</updated>\\n    <published>2019-01-30T12:47:44Z</published>\\n    <title>Blurred Images Lead to Bad Local Minima</title>\\n    <summary>  Blurred Images Lead to Bad Local Minima\\n</summary>\\n    <author>\\n      <name>Gal Katzhendler</name>\\n    </author>\\n    <author>\\n      <name>Daphna Weinshall</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1901.10788v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.10788v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1909.07245v1</id>\\n    <updated>2019-09-16T14:44:19Z</updated>\\n    <published>2019-09-16T14:44:19Z</published>\\n    <title>BMVC 2019: Workshop on Interpretable and Explainable Machine Vision</title>\\n    <summary>  Proceedings of the BMVC 2019 Workshop on Interpretable and Explainable\\nMachine Vision, Cardiff, UK, September 12, 2019.\\n</summary>\\n    <author>\\n      <name>Alun Preece</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1909.07245v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1909.07245v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2010.09016v1</id>\\n    <updated>2020-10-18T16:16:50Z</updated>\\n    <published>2020-10-18T16:16:50Z</published>\\n    <title>Covapixels</title>\\n    <summary>  We propose and discuss the summarization of superpixel-type image\\ntiles/patches using mean and covariance information. We refer to the resulting\\nobjects as covapixels.\\n</summary>\\n    <author>\\n      <name>Jeffrey Uhlmann</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2010.09016v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2010.09016v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.05219v1</id>\\n    <updated>2018-12-13T01:36:36Z</updated>\\n    <published>2018-12-13T01:36:36Z</published>\\n    <title>Advances of Scene Text Datasets</title>\\n    <summary>  This article introduces publicly available datasets in scene text detection\\nand recognition. The information is as of 2017.\\n</summary>\\n    <author>\\n      <name>Masakazu Iwamura</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1812.05219v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.05219v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2110.13806v1</id>\\n    <updated>2021-10-25T15:51:35Z</updated>\\n    <published>2021-10-25T15:51:35Z</published>\\n    <title>Detecting speaking persons in video</title>\\n    <summary>  We present a novel method for detecting speaking persons in video, by\\nextracting facial landmarks with a neural network and analysing these landmarks\\nstatistically over time\\n</summary>\\n    <author>\\n      <name>Hannes Fassold</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted for MMSP 2021</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2110.13806v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2110.13806v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2205.10556v1</id>\\n    <updated>2022-05-21T10:25:00Z</updated>\\n    <published>2022-05-21T10:25:00Z</published>\\n    <title>Cycle-GAN for eye-tracking</title>\\n    <summary>  This manuscript presents a not typical implementation of the cycle generative\\nadversarial networks (Cycle-GAN) method for eye-tracking tasks.\\n</summary>\\n    <author>\\n      <name>Ildar Rakhmatulin</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages, 11 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2205.10556v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2205.10556v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0406047v1</id>\\n    <updated>2004-06-24T13:14:58Z</updated>\\n    <published>2004-06-24T13:14:58Z</published>\\n    <title>Self-organizing neural networks in classification and image recognition</title>\\n    <summary>  Self-organizing neural networks are used for brick finding in OPERA\\nexperiment. Self-organizing neural networks and wavelet analysis used for\\nrecognition and extraction of car numbers from images.\\n</summary>\\n    <author>\\n      <name>G. A. Ososkov</name>\\n    </author>\\n    <author>\\n      <name>S. G. Dmitrievskiy</name>\\n    </author>\\n    <author>\\n      <name>A. V. Stadnik</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/cs/0406047v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0406047v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0902.2788v2</id>\\n    <updated>2010-01-08T10:23:17Z</updated>\\n    <published>2009-02-16T21:13:35Z</published>\\n    <title>Using SLP Neural Network to Persian Handwritten Digits Recognition</title>\\n    <summary>  This paper has been withdrawn by the author ali pourmohammad.\\n</summary>\\n    <author>\\n      <name>Ali Pourmohammad</name>\\n    </author>\\n    <author>\\n      <name>Seyed Mohammad Ahadi</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This paper has been withdrawn</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/0902.2788v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0902.2788v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0906.5120v1</id>\\n    <updated>2009-06-28T08:10:33Z</updated>\\n    <published>2009-06-28T08:10:33Z</published>\\n    <title>Comments on \"A new combination of evidence based on compromise\" by K.\\n  Yamada</title>\\n    <summary>  Comments on ``A new combination of evidence based on compromise\\'\\' by K.\\nYamada\\n</summary>\\n    <author>\\n      <name>Jean Dezert</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">ONERA</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Arnaud Martin</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">E3I2</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Florentin Smarandache</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">UNM</arxiv:affiliation>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Fuzzy Sets and Systems 160, 6 (2009) 853-855</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/0906.5120v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0906.5120v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4; I.5\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1004.4793v1</id>\\n    <updated>2010-04-27T13:22:36Z</updated>\\n    <published>2010-04-27T13:22:36Z</published>\\n    <title>Logical methods of object recognition on satellite images using spatial\\n  constraints</title>\\n    <summary>  A logical approach to object recognition on image is proposed. The main idea\\nof the approach is to perform the object recognition as a logical inference on\\na set of rules describing an object shape.\\n</summary>\\n    <author>\\n      <name>R. K. Fedorov</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1004.4793v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1004.4793v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4.8; I.5.1\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1107.2085v1</id>\\n    <updated>2011-07-11T18:44:17Z</updated>\\n    <published>2011-07-11T18:44:17Z</published>\\n    <title>Kunchenko\\'s Polynomials for Template Matching</title>\\n    <summary>  This paper reviews Kunchenko\\'s polynomials using as template matching method\\nto recognize template in one-dimensional input signal. Kunchenko\\'s polynomials\\nmethod is compared with classical methods - cross-correlation and sum of\\nsquared differences according to numerical statistical example.\\n</summary>\\n    <author>\\n      <name>Oleg Chertov</name>\\n    </author>\\n    <author>\\n      <name>Taras Slipets</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">3 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1107.2085v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1107.2085v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.5.4\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1111.0466v1</id>\\n    <updated>2011-11-02T11:42:37Z</updated>\\n    <published>2011-11-02T11:42:37Z</published>\\n    <title>Kernel diff-hash</title>\\n    <summary>  This paper presents a kernel formulation of the recently introduced diff-hash\\nalgorithm for the construction of similarity-sensitive hash functions. Our\\nkernel diff-hash algorithm that shows superior performance on the problem of\\nimage feature descriptor matching.\\n</summary>\\n    <author>\\n      <name>Michael M Bronstein</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1111.0466v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1111.0466v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1204.1615v1</id>\\n    <updated>2012-04-07T09:28:19Z</updated>\\n    <published>2012-04-07T09:28:19Z</published>\\n    <title>Discrimination between Arabic and Latin from bilingual documents</title>\\n    <summary>  2011 International Conference on Communications, Computing and Control\\nApplications (CCCA)\\n</summary>\\n    <author>\\n      <name>Sofiene Haboubi</name>\\n    </author>\\n    <author>\\n      <name>Samia Maddouri</name>\\n    </author>\\n    <author>\\n      <name>Hamid Amiri</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/CCCA.2011.6031496</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/CCCA.2011.6031496\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1204.1615v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1204.1615v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1212.0819v1</id>\\n    <updated>2012-12-04T18:39:14Z</updated>\\n    <published>2012-12-04T18:39:14Z</published>\\n    <title>A Topological Code for Plane Images</title>\\n    <summary>  It is proposed a new code for contours of plane images. This code was applied\\nfor optical character recognition of printed and handwritten characters. One\\ncan apply it to recognition of any visual images.\\n</summary>\\n    <author>\\n      <name>Evgeny Shchepin</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1212.0819v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1212.0819v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.GT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1303.1829v1</id>\\n    <updated>2013-03-07T21:15:29Z</updated>\\n    <published>2013-03-07T21:15:29Z</published>\\n    <title>Watersheds on edge or node weighted graphs \"par l\\'exemple\"</title>\\n    <summary>  Watersheds have been defined both for node and edge weighted graphs. We show\\nthat they are identical: for each edge (resp.\\\\ node) weighted graph exists a\\nnode (resp. edge) weighted graph with the same minima and catchment basin.\\n</summary>\\n    <author>\\n      <name>Fernand Meyer</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">21 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1303.1829v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1303.1829v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1304.7948v2</id>\\n    <updated>2013-06-02T20:32:05Z</updated>\\n    <published>2013-04-30T10:41:26Z</published>\\n    <title>Convolutional Neural Networks learn compact local image descriptors</title>\\n    <summary>  A standard deep convolutional neural network paired with a suitable loss\\nfunction learns compact local image descriptors that perform comparably to\\nstate-of-the art approaches.\\n</summary>\\n    <author>\\n      <name>Christian Osendorfer</name>\\n    </author>\\n    <author>\\n      <name>Justin Bayer</name>\\n    </author>\\n    <author>\\n      <name>Patrick van der Smagt</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1304.7948v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1304.7948v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1305.5905v1</id>\\n    <updated>2013-05-25T09:30:49Z</updated>\\n    <published>2013-05-25T09:30:49Z</published>\\n    <title>\\xc3\\x96AGM/AAPR 2013 - The 37th Annual Workshop of the Austrian Association\\n  for Pattern Recognition</title>\\n    <summary>  In this editorial, the organizers summarize facts and background about the\\nevent.\\n</summary>\\n    <author>\\n      <name>Justus Piater</name>\\n    </author>\\n    <author>\\n      <name>Antonio J. Rodr\\xc3\\xadguez S\\xc3\\xa1nchez</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Part of the OAGM/AAPR 2013 proceedings (arXiv:1304.1876)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1305.5905v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1305.5905v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1309.0261v1</id>\\n    <updated>2013-09-01T20:35:17Z</updated>\\n    <published>2013-09-01T20:35:17Z</published>\\n    <title>Multi-Column Deep Neural Networks for Offline Handwritten Chinese\\n  Character Classification</title>\\n    <summary>  Our Multi-Column Deep Neural Networks achieve best known recognition rates on\\nChinese characters from the ICDAR 2011 and 2013 offline handwriting\\ncompetitions, approaching human performance.\\n</summary>\\n    <author>\\n      <name>Dan Cire\\xc5\\x9fan</name>\\n    </author>\\n    <author>\\n      <name>J\\xc3\\xbcrgen Schmidhuber</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages, 1 figure, IDSIA tech report</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1309.0261v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1309.0261v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1310.0365v1</id>\\n    <updated>2013-10-01T16:06:00Z</updated>\\n    <published>2013-10-01T16:06:00Z</published>\\n    <title>The complex-valued encoding for dicision-making based on aliasing data</title>\\n    <summary>  It is proposed a complex valued channel encoding for multidimensional data.\\nThe basic approach contains overlapping of complex nonlinear mappings. Its\\ndevelopment leads to sparse representation of multi-channel data, increasing\\ntheir dimensions and the distance between the images.\\n</summary>\\n    <author>\\n      <name>P. A. Golovinski</name>\\n    </author>\\n    <author>\\n      <name>V. A. Astapenko</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1310.0365v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1310.0365v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1403.5590v1</id>\\n    <updated>2014-03-21T23:26:47Z</updated>\\n    <published>2014-03-21T23:26:47Z</published>\\n    <title>Continuous Optimization for Fields of Experts Denoising Works</title>\\n    <summary>  Several recent papers use image denoising with a Fields of Experts prior to\\nbenchmark discrete optimization methods. We show that a non-linear least\\nsquares solver significantly outperforms all known discrete methods on this\\nproblem.\\n</summary>\\n    <author>\\n      <name>Petter Strandmark</name>\\n    </author>\\n    <author>\\n      <name>Sameer Agarwal</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1403.5590v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1403.5590v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1411.1446v1</id>\\n    <updated>2014-11-05T23:14:10Z</updated>\\n    <published>2014-11-05T23:14:10Z</published>\\n    <title>Electrocardiography Separation of Mother and Baby</title>\\n    <summary>  Extraction of Electrocardiography (ECG or EKG) signals of mother and baby is\\na challenging task, because one single device is used and it receives a mixture\\nof multiple heart beats. In this paper, we would like to design a filter to\\nseparate the signals from each other.\\n</summary>\\n    <author>\\n      <name>Wei Wang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1411.1446v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1411.1446v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1412.5796v1</id>\\n    <updated>2014-12-18T10:32:57Z</updated>\\n    <published>2014-12-18T10:32:57Z</published>\\n    <title>Image Enhancement Using a Generalization of Homographic Function</title>\\n    <summary>  This paper presents a new method of gray level image enhancement, based on\\npoint transforms. In order to define the transform function, it was used a\\ngeneralization of the homographic function.\\n</summary>\\n    <author>\\n      <name>Vasile Patrascu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">The IEEE International Conference COMMUNICATIONS 2002, pp. 429-434,\\n  December 5-7, 2002, Bucharest, Romania</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1412.5796v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1412.5796v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1601.07021v1</id>\\n    <updated>2016-01-26T13:41:48Z</updated>\\n    <published>2016-01-26T13:41:48Z</published>\\n    <title>Polyhedron Volume-Ratio-based Classification for Image Recognition</title>\\n    <summary>  In this paper, a novel method, called polyhedron volume ratio classification\\n(PVRC) is proposed for image recognition\\n</summary>\\n    <author>\\n      <name>Qingxiang Feng</name>\\n    </author>\\n    <author>\\n      <name>Jeng-Shyang Pan</name>\\n    </author>\\n    <author>\\n      <name>Jar-Ferr Yang</name>\\n    </author>\\n    <author>\\n      <name>Yang-Ting Chou</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1601.07021v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1601.07021v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1608.05518v2</id>\\n    <updated>2020-11-12T06:45:06Z</updated>\\n    <published>2016-08-19T07:30:33Z</published>\\n    <title>On the Existence of a Projective Reconstruction</title>\\n    <summary>  In this note we study the connection between the existence of a projective\\nreconstruction and the existence of a fundamental matrix satisfying the\\nepipolar constraints.\\n</summary>\\n    <author>\\n      <name>Hon-Leung Lee</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages, 1 figure</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1608.05518v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1608.05518v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1701.08380v1</id>\\n    <updated>2017-01-29T13:42:14Z</updated>\\n    <published>2017-01-29T13:42:14Z</published>\\n    <title>The HASYv2 dataset</title>\\n    <summary>  This paper describes the HASYv2 dataset. HASY is a publicly available, free\\nof charge dataset of single symbols similar to MNIST. It contains 168233\\ninstances of 369 classes. HASY contains two challenges: A classification\\nchallenge with 10 pre-defined folds for 10-fold cross-validation and a\\nverification challenge.\\n</summary>\\n    <author>\\n      <name>Martin Thoma</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1701.08380v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1701.08380v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1702.01507v1</id>\\n    <updated>2017-02-06T06:32:14Z</updated>\\n    <published>2017-02-06T06:32:14Z</published>\\n    <title>Challenge of Multi-Camera Tracking</title>\\n    <summary>  Multi-camera tracking is quite different from single camera tracking, and it\\nfaces new technology and system architecture challenges. By analyzing the\\ncorresponding characteristics and disadvantages of the existing algorithms,\\nproblems in multi-camera tracking are summarized and some new directions for\\nfuture work are also generalized.\\n</summary>\\n    <author>\\n      <name>Yong Wang</name>\\n    </author>\\n    <author>\\n      <name>Ke Lu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1702.01507v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1702.01507v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1703.00523v1</id>\\n    <updated>2017-03-01T21:41:58Z</updated>\\n    <published>2017-03-01T21:41:58Z</published>\\n    <title>ISIC 2017 - Skin Lesion Analysis Towards Melanoma Detection</title>\\n    <summary>  Our system addresses Part 1, Lesion Segmentation and Part 3, Lesion\\nClassification of the ISIC 2017 challenge. Both algorithms make use of deep\\nconvolutional networks to achieve the challenge objective.\\n</summary>\\n    <author>\\n      <name>Matt Berseth</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">ISIC2017</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1703.00523v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1703.00523v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1703.05165v2</id>\\n    <updated>2017-09-28T02:26:32Z</updated>\\n    <published>2017-03-15T14:18:23Z</published>\\n    <title>Automatic skin lesion segmentation with fully\\n  convolutional-deconvolutional networks</title>\\n    <summary>  This paper summarizes our method and validation results for the ISBI\\nChallenge 2017 - Skin Lesion Analysis Towards Melanoma Detection - Part I:\\nLesion Segmentation\\n</summary>\\n    <author>\\n      <name>Yading Yuan</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/JBHI.2017.2787487</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/JBHI.2017.2787487\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">ISIC2017 challenge, 4 pages</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IEEE Journal of Biomedical and Health Informatics, 2018</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1703.05165v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1703.05165v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1706.06942v1</id>\\n    <updated>2017-06-21T14:54:15Z</updated>\\n    <published>2017-06-21T14:54:15Z</published>\\n    <title>Graphcut Texture Synthesis for Single-Image Superresolution</title>\\n    <summary>  Texture synthesis has proven successful at imitating a wide variety of\\ntextures. Adding additional constraints (in the form of a low-resolution\\nversion of the texture to be synthesized) makes it possible to use texture\\nsynthesis methods for texture superresolution.\\n</summary>\\n    <author>\\n      <name>Douglas Summers-Stay</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">NYU Master\\'s Thesis from 2006</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1706.06942v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1706.06942v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1707.03088v2</id>\\n    <updated>2017-07-12T04:38:13Z</updated>\\n    <published>2017-07-11T00:28:23Z</published>\\n    <title>Online Handwritten Mathematical Expressions Recognition System Using\\n  Fuzzy Neural Network</title>\\n    <summary>  The article describes developed information technology for online recognition\\nof handwritten mathematical expressions that based on proposed approaches to\\nhandwritten symbols recognition and structural analysis.\\n</summary>\\n    <author>\\n      <name>E. Naderan</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">in Russian</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">ITHEA, Information Content and Processing, 2014, 1 (3) , 262-268</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1707.03088v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1707.03088v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.2.7\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1712.02890v1</id>\\n    <updated>2017-12-07T23:35:11Z</updated>\\n    <published>2017-12-07T23:35:11Z</published>\\n    <title>Network Analysis for Explanation</title>\\n    <summary>  Safety critical systems strongly require the quality aspects of artificial\\nintelligence including explainability. In this paper, we analyzed a trained\\nnetwork to extract features which mainly contribute the inference. Based on the\\nanalysis, we developed a simple solution to generate explanations of the\\ninference processes.\\n</summary>\\n    <author>\\n      <name>Hiroshi Kuwajima</name>\\n    </author>\\n    <author>\\n      <name>Masayuki Tanaka</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1712.02890v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1712.02890v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1803.10864v1</id>\\n    <updated>2018-03-28T21:44:21Z</updated>\\n    <published>2018-03-28T21:44:21Z</published>\\n    <title>Human Emotional Facial Expression Recognition</title>\\n    <summary>  An automatic Facial Expression Recognition (FER) model with Adaboost face\\ndetector, feature selection based on manifold learning and synergetic prototype\\nbased classifier has been proposed. Improved feature selection method and\\nproposed classifier can achieve favorable effectiveness to performance FER in\\nreasonable processing time.\\n</summary>\\n    <author>\\n      <name>Chendi Wang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1803.10864v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1803.10864v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.00487v1</id>\\n    <updated>2018-07-02T06:47:47Z</updated>\\n    <published>2018-07-02T06:47:47Z</published>\\n    <title>An initial study on estimating area of a leaf using image processing</title>\\n    <summary>  Calculating leaf area is very important. Computer aided image processing can\\nmake this faster and more accurate. This include scanning the leaf , converting\\nit to binary image and calculation of number of pixels covered. Later this is\\nconverted to mm2.\\n</summary>\\n    <author>\\n      <name>G. D. Illeperuma</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1807.00487v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.00487v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.06466v1</id>\\n    <updated>2018-07-17T14:24:37Z</updated>\\n    <published>2018-07-17T14:24:37Z</published>\\n    <title>Automatic Skin Lesion Segmentation Using Deep Fully Convolutional\\n  Networks</title>\\n    <summary>  This paper summarizes our method and validation results for the ISIC\\nChallenge 2018 - Skin Lesion Analysis Towards Melanoma Detection - Task 1:\\nLesion Segmentation\\n</summary>\\n    <author>\\n      <name>Hongming Xu</name>\\n    </author>\\n    <author>\\n      <name>Tae Hyun Hwang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, ISIC Challenge 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1807.06466v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.06466v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.07502v3</id>\\n    <updated>2020-02-20T01:38:30Z</updated>\\n    <published>2018-11-19T05:07:50Z</published>\\n    <title>Fast Efficient Object Detection Using Selective Attention</title>\\n    <summary>  Retraction due to significant oversight\\n</summary>\\n    <author>\\n      <name>Shivanthan Yohanandan</name>\\n    </author>\\n    <author>\\n      <name>Andy Song</name>\\n    </author>\\n    <author>\\n      <name>Adrian G. Dyer</name>\\n    </author>\\n    <author>\\n      <name>Angela Faragasso</name>\\n    </author>\\n    <author>\\n      <name>Subhrajit Roy</name>\\n    </author>\\n    <author>\\n      <name>Dacheng Tao</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Retraction due to significant oversight</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1811.07502v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.07502v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.12797v1</id>\\n    <updated>2018-11-30T14:05:37Z</updated>\\n    <published>2018-11-30T14:05:37Z</published>\\n    <title>Structure and Motion from Multiframes</title>\\n    <summary>  The paper gives an overview of the problems and methods of recovery of\\nstructure and motion parameters of rigid bodies from multiframes.\\n</summary>\\n    <author>\\n      <name>Mieczys\\xc5\\x82aw A. K\\xc5\\x82opotek</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 figures, 20 pages</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">M.A. K{\\\\l}opotek: Structure and Motion from Multiframes. Machine\\n  Graphics and Vision , Vol. 7, nos 1/2, 1998,pp. 383-396</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1811.12797v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.12797v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.03068v1</id>\\n    <updated>2019-01-10T09:13:16Z</updated>\\n    <published>2019-01-10T09:13:16Z</published>\\n    <title>New Radon Transform Based Texture Features of Handwritten Document</title>\\n    <summary>  In this paper, we present some new features describing the handwritten\\ndocument as a texture. These features are based on the Radon transform. All\\nvalues can be obtained easily and suit for the coarse classification of\\ndocuments.\\n</summary>\\n    <author>\\n      <name>Rustam Latypov</name>\\n    </author>\\n    <author>\\n      <name>Evgeni Stolov</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1901.03068v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.03068v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1904.05712v1</id>\\n    <updated>2019-04-11T14:17:52Z</updated>\\n    <published>2019-04-11T14:17:52Z</published>\\n    <title>Reconstructing Network Inputs with Additive Perturbation Signatures</title>\\n    <summary>  In this work, we present preliminary results demonstrating the ability to\\nrecover a significant amount of information about secret model inputs given\\nonly very limited access to model outputs and the ability evaluate the model on\\nadditive perturbations to the input.\\n</summary>\\n    <author>\\n      <name>Nick Moran</name>\\n    </author>\\n    <author>\\n      <name>Chiraag Juvekar</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1904.05712v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1904.05712v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2005.13734v1</id>\\n    <updated>2020-05-28T01:53:57Z</updated>\\n    <published>2020-05-28T01:53:57Z</published>\\n    <title>Anomaly Detection Based on Deep Learning Using Video for Prevention of\\n  Industrial Accidents</title>\\n    <summary>  This paper proposes an anomaly detection method for the prevention of\\nindustrial accidents using machine learning technology.\\n</summary>\\n    <author>\\n      <name>Satoshi Hashimoto</name>\\n    </author>\\n    <author>\\n      <name>Yonghoon Ji</name>\\n    </author>\\n    <author>\\n      <name>Kenichi Kudo</name>\\n    </author>\\n    <author>\\n      <name>Takayuki Takahashi</name>\\n    </author>\\n    <author>\\n      <name>Kazunori Umeda</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2005.13734v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2005.13734v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2005.14386v1</id>\\n    <updated>2020-05-29T05:03:15Z</updated>\\n    <published>2020-05-29T05:03:15Z</published>\\n    <title>Controlling Length in Image Captioning</title>\\n    <summary>  We develop and evaluate captioning models that allow control of caption\\nlength. Our models can leverage this control to generate captions of different\\nstyle and descriptiveness.\\n</summary>\\n    <author>\\n      <name>Ruotian Luo</name>\\n    </author>\\n    <author>\\n      <name>Greg Shakhnarovich</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2005.14386v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2005.14386v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2112.15483v1</id>\\n    <updated>2021-12-23T07:25:19Z</updated>\\n    <published>2021-12-23T07:25:19Z</published>\\n    <title>Cloud Removal from Satellite Images</title>\\n    <summary>  In this report, we have analyzed available cloud detection technique using\\nsentinel hub. We have also implemented spatial attention generative adversarial\\nnetwork and improved quality of generated image compared to previous solution\\n[7].\\n</summary>\\n    <author>\\n      <name>Rutvik Chauhan</name>\\n    </author>\\n    <author>\\n      <name>Antarpuneet Singh</name>\\n    </author>\\n    <author>\\n      <name>Sujoy Saha</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2112.15483v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2112.15483v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0908.1369v1</id>\\n    <updated>2009-08-10T18:33:51Z</updated>\\n    <published>2009-08-10T18:33:51Z</published>\\n    <title>Segmentation for radar images based on active contour</title>\\n    <summary>  We exam various geometric active contour methods for radar image\\nsegmentation. Due to special properties of radar images, we propose our new\\nmodel based on modified Chan-Vese functional. Our method is efficient in\\nseparating non-meteorological noises from meteorological images.\\n</summary>\\n    <author>\\n      <name>Meijun Zhu</name>\\n    </author>\\n    <author>\\n      <name>Pengfei Zhang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/0908.1369v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0908.1369v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1511.01994v1</id>\\n    <updated>2015-11-06T07:18:56Z</updated>\\n    <published>2015-11-06T07:18:56Z</published>\\n    <title>Next Generation Multicuts for Semi-Planar Graphs</title>\\n    <summary>  We study the problem of multicut segmentation. We introduce modified versions\\nof the Semi-PlanarCC based on bounding Lagrange multipliers. We apply our work\\nto natural image segmentation.\\n</summary>\\n    <author>\\n      <name>Julian Yarkony</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1511.01994v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1511.01994v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DS\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1511.07963v1</id>\\n    <updated>2015-11-25T06:09:42Z</updated>\\n    <published>2015-11-25T06:09:42Z</published>\\n    <title>Calculate distance to object in the area where car, using video analysis</title>\\n    <summary>  The method of using video cameras installed on the car, to calculate the\\ndistance to the object in its area of movement.\\n</summary>\\n    <author>\\n      <name>Elena Legchekova</name>\\n    </author>\\n    <author>\\n      <name>Oleg Titov</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages, in Russian</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1511.07963v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1511.07963v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.00503v1</id>\\n    <updated>2018-04-30T03:49:22Z</updated>\\n    <published>2018-04-30T03:49:22Z</published>\\n    <title>Machine Learning for Exam Triage</title>\\n    <summary>  In this project, we extend the state-of-the-art CheXNet (Rajpurkar et al.\\n[2017]) by making use of the additional non-image features in the dataset. Our\\nmodel produced better AUROC scores than the original CheXNet.\\n</summary>\\n    <author>\\n      <name>Xinyu Guan</name>\\n    </author>\\n    <author>\\n      <name>Jessica Lee</name>\\n    </author>\\n    <author>\\n      <name>Peter Wu</name>\\n    </author>\\n    <author>\\n      <name>Yue Wu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1805.00503v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.00503v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.00638v2</id>\\n    <updated>2019-06-25T05:11:09Z</updated>\\n    <published>2018-05-02T06:03:47Z</published>\\n    <title>A Deep Network for Arousal-Valence Emotion Prediction with\\n  Acoustic-Visual Cues</title>\\n    <summary>  In this paper, we comprehensively describe the methodology of our submissions\\nto the One-Minute Gradual-Emotion Behavior Challenge 2018.\\n</summary>\\n    <author>\\n      <name>Songyou Peng</name>\\n    </author>\\n    <author>\\n      <name>Le Zhang</name>\\n    </author>\\n    <author>\\n      <name>Yutong Ban</name>\\n    </author>\\n    <author>\\n      <name>Meng Fang</name>\\n    </author>\\n    <author>\\n      <name>Stefan Winkler</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1805.00638v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.00638v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.08174v1</id>\\n    <updated>2018-05-21T16:50:55Z</updated>\\n    <published>2018-05-21T16:50:55Z</published>\\n    <title>Reproducibility Report for \"Learning To Count Objects In Natural Images\\n  For Visual Question Answering\"</title>\\n    <summary>  This is the reproducibility report for the paper \"Learning To Count Objects\\nIn Natural Images For Visual QuestionAnswering\"\\n</summary>\\n    <author>\\n      <name>Shagun Sodhani</name>\\n    </author>\\n    <author>\\n      <name>Vardaan Pahuja</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Submitted to Reproducibility in ML Workshop, ICML\\'18</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1805.08174v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.08174v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1907.06483v1</id>\\n    <updated>2019-07-15T13:04:31Z</updated>\\n    <published>2019-07-15T13:04:31Z</published>\\n    <title>Color Cerberus</title>\\n    <summary>  Simple convolutional neural network was able to win ISISPA color constancy\\ncompetition. Partial reimplementation of (Bianco, 2017) neural architecture\\nwould have shown even better results in this setup.\\n</summary>\\n    <author>\\n      <name>A. ~Savchik</name>\\n    </author>\\n    <author>\\n      <name>E. ~Ershov</name>\\n    </author>\\n    <author>\\n      <name>S. ~Karpenko</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/ISPA.2019.8868425</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/ISPA.2019.8868425\" rel=\"related\"/>\\n    <link href=\"http://arxiv.org/abs/1907.06483v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1907.06483v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2008.00947v1</id>\\n    <updated>2020-07-27T15:31:27Z</updated>\\n    <published>2020-07-27T15:31:27Z</published>\\n    <title>Pre-training for Video Captioning Challenge 2020 Summary</title>\\n    <summary>  The Pre-training for Video Captioning Challenge 2020 Summary: results and\\nchallenge participants\\' technical reports.\\n</summary>\\n    <author>\\n      <name>Yingwei Pan</name>\\n    </author>\\n    <author>\\n      <name>Jun Xu</name>\\n    </author>\\n    <author>\\n      <name>Yehao Li</name>\\n    </author>\\n    <author>\\n      <name>Ting Yao</name>\\n    </author>\\n    <author>\\n      <name>Tao Mei</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2008.00947v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2008.00947v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2009.05388v1</id>\\n    <updated>2020-09-02T17:14:05Z</updated>\\n    <published>2020-09-02T17:14:05Z</published>\\n    <title>Automatic cinematography for 360 video</title>\\n    <summary>  We describe our method for automatic generation of a visually interesting\\ncamera path (automatic cinematography)from a 360 video. Based on the\\ninformation from the scene objects, multiple shot hypotheses for different shot\\ntypes are constructed and the best one is rendered.\\n</summary>\\n    <author>\\n      <name>Hannes Fassold</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted as demo paper for IEEE MMSP 2020</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2009.05388v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2009.05388v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2104.02651v1</id>\\n    <updated>2021-04-04T04:23:36Z</updated>\\n    <published>2021-04-04T04:23:36Z</published>\\n    <title>A Modified Convolutional Network for Auto-encoding based on Pattern\\n  Theory Growth Function</title>\\n    <summary>  This brief paper reports the shortcoming of a variant of convolutional neural\\nnetwork whose components are developed based on the pattern theory framework.\\n</summary>\\n    <author>\\n      <name>Erico Tjoa</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2104.02651v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2104.02651v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2104.08657v1</id>\\n    <updated>2021-04-17T22:59:15Z</updated>\\n    <published>2021-04-17T22:59:15Z</published>\\n    <title>IUPUI Driving Videos and Images in All Weather and Illumination\\n  Conditions</title>\\n    <summary>  This document describes an image and video dataset of driving views captured\\nin all weather and illumination conditions. The data set has been submitted to\\nCDVL.\\n</summary>\\n    <author>\\n      <name>Jiang Yu Zheng</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages, 5 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2104.08657v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2104.08657v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2108.04466v2</id>\\n    <updated>2021-08-11T02:35:11Z</updated>\\n    <published>2021-08-10T06:21:42Z</published>\\n    <title>Method Towards CVPR 2021 SimLocMatch Challenge</title>\\n    <summary>  This report describes Megvii-3D team\\'s approach towards SimLocMatch Challenge\\n@ CVPR 2021 Image Matching Workshop.\\n</summary>\\n    <author>\\n      <name>Xiaopeng Bi</name>\\n    </author>\\n    <author>\\n      <name>Ran Yan</name>\\n    </author>\\n    <author>\\n      <name>Zheng Chai</name>\\n    </author>\\n    <author>\\n      <name>Haotian Zhang</name>\\n    </author>\\n    <author>\\n      <name>Xiao Liu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2108.04466v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2108.04466v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2109.07843v2</id>\\n    <updated>2021-09-18T06:00:56Z</updated>\\n    <published>2021-09-16T10:11:58Z</published>\\n    <title>Label Assignment Distillation for Object Detection</title>\\n    <summary>  This article has been removed by arXiv administrators due to a claim of\\ncopyright infringement\\n</summary>\\n    <author>\\n      <name>Hailun Zhang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This article has been removed by arXiv administrators due to a claim\\n  of copyright infringement. Author list truncated to the submitter</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2109.07843v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2109.07843v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2201.00975v1</id>\\n    <updated>2022-01-04T04:44:05Z</updated>\\n    <published>2022-01-04T04:44:05Z</published>\\n    <title>StyleM: Stylized Metrics for Image Captioning Built with Contrastive\\n  N-grams</title>\\n    <summary>  In this paper, we build two automatic evaluation metrics for evaluating the\\nassociation between a machine-generated caption and a ground truth stylized\\ncaption: OnlyStyle and StyleCIDEr.\\n</summary>\\n    <author>\\n      <name>Chengxi Li</name>\\n    </author>\\n    <author>\\n      <name>Brent Harrison</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2201.00975v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2201.00975v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2201.09388v1</id>\\n    <updated>2022-01-23T22:44:45Z</updated>\\n    <published>2022-01-23T22:44:45Z</published>\\n    <title>A Survey on Patients Privacy Protection with Stganography and Visual\\n  Encryption</title>\\n    <summary>  In this survey, thirty models for steganography and visual encryption methods\\nhave been discussed to provide patients privacy protection.\\n</summary>\\n    <author>\\n      <name>Hussein K. Alzubaidy</name>\\n    </author>\\n    <author>\\n      <name>Dhiah Al-Shammary</name>\\n    </author>\\n    <author>\\n      <name>Mohammed Hamzah Abed</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2201.09388v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2201.09388v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.MM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2204.12318v2</id>\\n    <updated>2022-04-27T08:40:43Z</updated>\\n    <published>2022-04-26T13:53:40Z</published>\\n    <title>Evaluating the Quality of a Synthesized Motion with the Fr\\xc3\\xa9chet Motion\\n  Distance</title>\\n    <summary>  Evaluating the Quality of a Synthesized Motion with the Fr\\\\\\'echet Motion\\nDistance\\n</summary>\\n    <author>\\n      <name>Antoine Maiorca</name>\\n    </author>\\n    <author>\\n      <name>Youngwoo Yoon</name>\\n    </author>\\n    <author>\\n      <name>Thierry Dutoit</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">2 pages, 2 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2204.12318v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2204.12318v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2207.04639v1</id>\\n    <updated>2022-07-11T05:47:27Z</updated>\\n    <published>2022-07-11T05:47:27Z</published>\\n    <title>A Dual-Polarization Information Guided Network for SAR Ship\\n  Classification</title>\\n    <summary>  How to fully utilize polarization to enhance synthetic aperture radar (SAR)\\nship classification remains an unresolved issue. Thus, we propose a\\ndual-polarization information guided network (DPIG-Net) to solve it.\\n</summary>\\n    <author>\\n      <name>Tianwen Zhang</name>\\n    </author>\\n    <author>\\n      <name>Xiaoling Zhang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2207.04639v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2207.04639v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2207.07604v1</id>\\n    <updated>2022-07-15T17:02:55Z</updated>\\n    <published>2022-07-15T17:02:55Z</published>\\n    <title>Image and Texture Independent Deep Learning Noise Estimation using\\n  Multiple Frames</title>\\n    <summary>  In this study, a novel multiple-frame based image and texture independent\\nconvolutional Neural Network (CNN) noise estimator is introduced. The estimator\\nworks.\\n</summary>\\n    <author>\\n      <name>Hikmet Kirmizitas</name>\\n    </author>\\n    <author>\\n      <name>Nurettin Besli</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.5755/j02.eie.30586</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.5755/j02.eie.30586\" rel=\"related\"/>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Elektronika Ir Elektrotechnika 2022 28(6) (42-47)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/2207.07604v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2207.07604v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2211.04998v1</id>\\n    <updated>2022-11-09T16:22:34Z</updated>\\n    <published>2022-11-09T16:22:34Z</published>\\n    <title>Similarity among the 2D-shapes and the analysis of dissimilarity scores</title>\\n    <summary>  We present a conceptually simple and intuitive method to calculate and to\\nmeasure the dissimilarities among 2D shapes. Several methods to interpret and\\nto visualize the resulting dissimilarity matrix are presented and compared.\\n</summary>\\n    <author>\\n      <name>Karel Zimmermann</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2211.04998v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2211.04998v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"G.1; I.4; I.5\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2303.17361v1</id>\\n    <updated>2023-03-30T13:19:07Z</updated>\\n    <published>2023-03-30T13:19:07Z</published>\\n    <title>Invertible Convolution with Symmetric Paddings</title>\\n    <summary>  We show that symmetrically padded convolution can be analytically inverted\\nvia DFT. We comprehensively analyze several different symmetric and\\nanti-symmetric padding modes and show that multiple cases exist where the\\ninversion can be achieved. The implementation is available at\\n\\\\url{https://github.com/prclibo/iconv_dft}.\\n</summary>\\n    <author>\\n      <name>Bo Li</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2303.17361v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2303.17361v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2308.03340v1</id>\\n    <updated>2023-08-07T06:47:36Z</updated>\\n    <published>2023-08-07T06:47:36Z</published>\\n    <title>A Hybrid CNN-Transformer Architecture with Frequency Domain Contrastive\\n  Learning for Image Deraining</title>\\n    <summary>  Image deraining is a challenging task that involves restoring degraded images\\naffected by rain streaks.\\n</summary>\\n    <author>\\n      <name>Cheng Wang</name>\\n    </author>\\n    <author>\\n      <name>Wei Li</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">21 pages,6 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2308.03340v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2308.03340v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4.4\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2401.01134v1</id>\\n    <updated>2024-01-02T10:22:06Z</updated>\\n    <published>2024-01-02T10:22:06Z</published>\\n    <title>Hybrid Pooling and Convolutional Network for Improving Accuracy and\\n  Training Convergence Speed in Object Detection</title>\\n    <summary>  This paper introduces HPC-Net, a high-precision and rapidly convergent object\\ndetection network.\\n</summary>\\n    <author>\\n      <name>Shiwen Zhao</name>\\n    </author>\\n    <author>\\n      <name>Wei Wang</name>\\n    </author>\\n    <author>\\n      <name>Junhui Hou</name>\\n    </author>\\n    <author>\\n      <name>Hai Wu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages,5 figures, conference</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2401.01134v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2401.01134v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2407.18682v1</id>\\n    <updated>2024-07-26T11:56:23Z</updated>\\n    <published>2024-07-26T11:56:23Z</published>\\n    <title>Rapid Object Annotation</title>\\n    <summary>  In this report we consider the problem of rapidly annotating a video with\\nbounding boxes for a novel object. We describe a UI and associated workflow\\ndesigned to make this process fast for an arbitrary novel target.\\n</summary>\\n    <author>\\n      <name>Misha Denil</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2407.18682v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2407.18682v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2412.18136v1</id>\\n    <updated>2024-12-24T03:44:26Z</updated>\\n    <published>2024-12-24T03:44:26Z</published>\\n    <title>ERVD: An Efficient and Robust ViT-Based Distillation Framework for\\n  Remote Sensing Image Retrieval</title>\\n    <summary>  ERVD: An Efficient and Robust ViT-Based Distillation Framework for Remote\\nSensing Image Retrieval\\n</summary>\\n    <author>\\n      <name>Le Dong</name>\\n    </author>\\n    <author>\\n      <name>Qixuan Cao</name>\\n    </author>\\n    <author>\\n      <name>Lei Pu</name>\\n    </author>\\n    <author>\\n      <name>Fangfang Wu</name>\\n    </author>\\n    <author>\\n      <name>Weisheng Dong</name>\\n    </author>\\n    <author>\\n      <name>Xin Li</name>\\n    </author>\\n    <author>\\n      <name>Guangming Shi</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2412.18136v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2412.18136v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2505.11424v1</id>\\n    <updated>2025-05-16T16:38:01Z</updated>\\n    <published>2025-05-16T16:38:01Z</published>\\n    <title>Improving Object Detection Performance through YOLOv8: A Comprehensive\\n  Training and Evaluation Study</title>\\n    <summary>  This study evaluated the performance of a YOLOv8-based segmentation model for\\ndetecting and segmenting wrinkles in facial images.\\n</summary>\\n    <author>\\n      <name>Rana Poureskandar</name>\\n    </author>\\n    <author>\\n      <name>Shiva Razzagzadeh</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2505.11424v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2505.11424v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2506.04081v1</id>\\n    <updated>2025-06-04T15:44:03Z</updated>\\n    <published>2025-06-04T15:44:03Z</published>\\n    <title>Point Cloud Quality Assessment Using the Perceptual Clustering Weighted\\n  Graph (PCW-Graph) and Attention Fusion Network</title>\\n    <summary>  No-Reference Point Cloud Quality Assessment (NR-PCQA) is critical for\\nevaluating 3D content in real-world applications where reference models are\\nunavailable.\\n</summary>\\n    <author>\\n      <name>Abdelouahed Laazoufi</name>\\n    </author>\\n    <author>\\n      <name>Mohammed El Hassouni</name>\\n    </author>\\n    <author>\\n      <name>Hocine Cherifi</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2506.04081v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2506.04081v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/9810003v1</id>\\n    <updated>1998-10-02T03:34:38Z</updated>\\n    <published>1998-10-02T03:34:38Z</published>\\n    <title>A Linear Shift Invariant Multiscale Transform</title>\\n    <summary>  This paper presents a multiscale decomposition algorithm. Unlike standard\\nwavelet transforms, the proposed operator is both linear and shift invariant.\\nThe central idea is to obtain shift invariance by averaging the aligned wavelet\\ntransform projections over all circular shifts of the signal. It is shown how\\nthe same transform can be obtained by a linear filter bank.\\n</summary>\\n    <author>\\n      <name>Andreas Siebert</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, 5 figures</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proceedings 1998 International Conference on Image Processing,\\n  Chicago, 4-7 October 1998</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/cs/9810003v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/9810003v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4.3\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/9810017v1</id>\\n    <updated>1998-10-19T20:46:16Z</updated>\\n    <published>1998-10-19T20:46:16Z</published>\\n    <title>General Theory of Image Normalization</title>\\n    <summary>  We give a systematic, abstract formulation of the image normalization method\\nas applied to a general group of image transformations, and then illustrate the\\nabstract analysis by applying it to the hierarchy of viewing transformations of\\na planar object.\\n</summary>\\n    <author>\\n      <name>Stephen L. Adler</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">33 pages, plain tex, no figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/cs/9810017v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/9810017v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.2.10, I.4.7, I.4.8\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/9908017v1</id>\\n    <updated>1999-08-26T17:18:49Z</updated>\\n    <published>1999-08-26T17:18:49Z</published>\\n    <title>A Differential Invariant for Zooming</title>\\n    <summary>  This paper presents an invariant under scaling and linear brightness change.\\nThe invariant is based on differentials and therefore is a local feature.\\nRotationally invariant 2-d differential Gaussian operators up to third order\\nare proposed for the implementation of the invariant. The performance is\\nanalyzed by simulating a camera zoom-out.\\n</summary>\\n    <author>\\n      <name>Andreas Siebert</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages, 7 figures</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proceedings 1999 International Conference on Image Processing,\\n  Kobe, 25-28 October 1999</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/cs/9908017v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/9908017v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4.7\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0001024v1</id>\\n    <updated>2000-01-25T16:09:37Z</updated>\\n    <published>2000-01-25T16:09:37Z</published>\\n    <title>A Parallel Algorithm for Dilated Contour Extraction from Bilevel Images</title>\\n    <summary>  We describe a simple, but efficient algorithm for the generation of dilated\\ncontours from bilevel images. The initial part of the contour extraction is\\nexplained to be a good candidate for parallel computer code generation. The\\nremainder of the algorithm is of linear nature.\\n</summary>\\n    <author>\\n      <name>B. R. Schlei</name>\\n    </author>\\n    <author>\\n      <name>L. Prasad</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages, including 3 figures. For additional detail check\\n  http://www.nis.lanl.gov/~bschlei/labvis/index.html</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/cs/0001024v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0001024v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.2.10, D.1.3, G.1.2\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0003079v1</id>\\n    <updated>2000-03-26T23:18:43Z</updated>\\n    <published>2000-03-26T23:18:43Z</published>\\n    <title>Differential Invariants under Gamma Correction</title>\\n    <summary>  This paper presents invariants under gamma correction and similarity\\ntransformations. The invariants are local features based on differentials which\\nare implemented using derivatives of the Gaussian. The use of the proposed\\ninvariant representation is shown to yield improved correlation results in a\\ntemplate matching scenario.\\n</summary>\\n    <author>\\n      <name>Andreas Siebert</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, 12 figures</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Vision Interface 2000, Montreal, 2000</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/cs/0003079v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0003079v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4.7\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0006001v1</id>\\n    <updated>2000-05-31T23:37:48Z</updated>\\n    <published>2000-05-31T23:37:48Z</published>\\n    <title>Boosting the Differences: A fast Bayesian classifier neural network</title>\\n    <summary>  A Bayesian classifier that up-weights the differences in the attribute values\\nis discussed. Using four popular datasets from the UCI repository, some\\ninteresting features of the network are illustrated. The network is suitable\\nfor classification problems.\\n</summary>\\n    <author>\\n      <name>Ninan Sajeeth Philip</name>\\n    </author>\\n    <author>\\n      <name>K. Babu Joseph</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">latex 18pages no figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/cs/0006001v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0006001v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I1.2;F.1.1;F1.2;C1.3\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0006002v1</id>\\n    <updated>2000-05-31T23:52:31Z</updated>\\n    <published>2000-05-31T23:52:31Z</published>\\n    <title>Distorted English Alphabet Identification : An application of Difference\\n  Boosting Algorithm</title>\\n    <summary>  The difference-boosting algorithm is used on letters dataset from the UCI\\nrepository to classify distorted raster images of English alphabets. In\\ncontrast to rather complex networks, the difference-boosting is found to\\nproduce comparable or better classification efficiency on this complex problem.\\n</summary>\\n    <author>\\n      <name>Ninan Sajeeth Philip</name>\\n    </author>\\n    <author>\\n      <name>K. Babu Joseph</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">latex 14pages no figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/cs/0006002v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0006002v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I1.2;F.1.1;F1.2;C1.3\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0006040v1</id>\\n    <updated>2000-06-28T18:34:14Z</updated>\\n    <published>2000-06-28T18:34:14Z</published>\\n    <title>Correlation over Decomposed Signals: A Non-Linear Approach to Fast and\\n  Effective Sequences Comparison</title>\\n    <summary>  A novel non-linear approach to fast and effective comparison of sequences is\\npresented, compared to the traditional cross-correlation operator, and\\nillustrated with respect to DNA sequences.\\n</summary>\\n    <author>\\n      <name>Luciano da Fontoura Costa</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages, 1 figure</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/cs/0006040v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0006040v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DS\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-bio\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.5.4; F.2.2; I.5.4; J.3\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0301001v1</id>\\n    <updated>2003-01-01T19:58:03Z</updated>\\n    <published>2003-01-01T19:58:03Z</published>\\n    <title>Least squares fitting of circles and lines</title>\\n    <summary>  We study theoretical and computational aspects of the least squares fit (LSF)\\nof circles and circular arcs. First we discuss the existence and uniqueness of\\nLSF and various parametrization schemes. Then we evaluate several popular\\ncircle fitting algorithms and propose a new one that surpasses the existing\\nmethods in reliability. We also discuss and compare direct (algebraic) circle\\nfits.\\n</summary>\\n    <author>\\n      <name>N. Chernov</name>\\n    </author>\\n    <author>\\n      <name>C. Lesort</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">26 pages, 14 figures, submitted</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/cs/0301001v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0301001v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4.8; I.5.1; I.2.10; G.1.2; G.3\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0308034v1</id>\\n    <updated>2003-08-21T10:47:27Z</updated>\\n    <published>2003-08-21T10:47:27Z</published>\\n    <title>Fingerprint based bio-starter and bio-access</title>\\n    <summary>  In the paper will be presented a safety and security system based on\\nfingerprint technology. The results suggest a new scenario where the new cars\\ncan use a fingerprint sensor integrated in car handle to allow access and in\\nthe dashboard as starter button.\\n</summary>\\n    <author>\\n      <name>G. Iovane</name>\\n    </author>\\n    <author>\\n      <name>P. Giordano</name>\\n    </author>\\n    <author>\\n      <name>C. Iovane</name>\\n    </author>\\n    <author>\\n      <name>F. Rotulo</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, Proceeding of Automotive 2003, Turin (Italy)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/cs/0308034v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0308034v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.2.10, I.4, I.5\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0308035v1</id>\\n    <updated>2003-08-21T10:52:53Z</updated>\\n    <published>2003-08-21T10:52:53Z</published>\\n    <title>IS (Iris Security)</title>\\n    <summary>  In the paper will be presented a safety system based on iridology. The\\nresults suggest a new scenario where the security problem in supervised and\\nunsupervised areas can be treat with the present system and the iris image\\nrecognition.\\n</summary>\\n    <author>\\n      <name>G. Iovane</name>\\n    </author>\\n    <author>\\n      <name>F. S. Tortoriello</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages, Proceeding of NIDays 2003 (Sponsored by National\\n  Instruments), Rome (Italy)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/cs/0308035v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0308035v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.5, I.5\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0401018v1</id>\\n    <updated>2004-01-22T05:53:30Z</updated>\\n    <published>2004-01-22T05:53:30Z</published>\\n    <title>Factor Temporal Prognosis of Tick-Borne Encephalitis Foci Functioning on\\n  the South of Russian Far East</title>\\n    <summary>  A method of temporal factor prognosis of TE (tick-borne encephalitis)\\ninfection has been developed. The high precision of the prognosis results for a\\nnumber of geographical regions of Primorsky Krai has been achieved. The method\\ncan be applied not only to epidemiological research but also to others.\\n</summary>\\n    <author>\\n      <name>E. I. Bolotin</name>\\n    </author>\\n    <author>\\n      <name>G. Sh. Tsitsiashvili</name>\\n    </author>\\n    <author>\\n      <name>I. V. Golycheva</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/cs/0401018v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0401018v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"B.1.3\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0608073v1</id>\\n    <updated>2006-08-18T08:28:23Z</updated>\\n    <published>2006-08-18T08:28:23Z</published>\\n    <title>Parametrical Neural Networks and Some Other Similar Architectures</title>\\n    <summary>  A review of works on associative neural networks accomplished during last\\nfour years in the Institute of Optical Neural Technologies RAS is given. The\\npresentation is based on description of parametrical neural networks (PNN). For\\ntoday PNN have record recognizing characteristics (storage capacity, noise\\nimmunity and speed of operation). Presentation of basic ideas and principles is\\naccentuated.\\n</summary>\\n    <author>\\n      <name>Leonid B. Litinskii</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">15 pages, 2 figures, accepted for publication in \"Optical Memory &amp;\\n  Neural Networks\" (2006)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/cs/0608073v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0608073v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0609164v1</id>\\n    <updated>2006-09-29T13:48:35Z</updated>\\n    <published>2006-09-29T13:48:35Z</published>\\n    <title>Conditional Expressions for Blind Deconvolution: Multi-point form</title>\\n    <summary>  We present conditional expression (CE) for finding blurs convolved in given\\nimages. The CE is given in terms of the zero-values of the blurs evaluated at\\nmulti-point. The CE can detect multiple blur all at once. We illustrate the\\nmultiple blur-detection by using a test image.\\n</summary>\\n    <author>\\n      <name>S. Aogaki</name>\\n    </author>\\n    <author>\\n      <name>I. Moritani</name>\\n    </author>\\n    <author>\\n      <name>T. Sugai</name>\\n    </author>\\n    <author>\\n      <name>F. Takeutchi</name>\\n    </author>\\n    <author>\\n      <name>F. M. Toyama</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, 3 figures, conference</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/cs/0609164v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0609164v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0609165v1</id>\\n    <updated>2006-09-29T13:50:12Z</updated>\\n    <published>2006-09-29T13:50:12Z</published>\\n    <title>Simple method to eliminate blur based on Lane and Bates algorithm</title>\\n    <summary>  A simple search method for finding a blur convolved in a given image is\\npresented. The method can be easily extended to a large blur. The method has\\nbeen experimentally tested with a model blurred image.\\n</summary>\\n    <author>\\n      <name>S. Aogaki</name>\\n    </author>\\n    <author>\\n      <name>I. Moritani</name>\\n    </author>\\n    <author>\\n      <name>T. Sugai</name>\\n    </author>\\n    <author>\\n      <name>F. Takeutchi</name>\\n    </author>\\n    <author>\\n      <name>F. M. Toyama</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">3 pages, 2 figures, conference</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/cs/0609165v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0609165v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0705.0214v1</id>\\n    <updated>2007-05-02T07:32:58Z</updated>\\n    <published>2007-05-02T07:32:58Z</published>\\n    <title>Riemannian level-set methods for tensor-valued data</title>\\n    <summary>  We present a novel approach for the derivation of PDE modeling\\ncurvature-driven flows for matrix-valued data. This approach is based on the\\nRiemannian geometry of the manifold of Symmetric Positive Definite Matrices\\nPos(n).\\n</summary>\\n    <author>\\n      <name>Mourad Zerai</name>\\n    </author>\\n    <author>\\n      <name>Maher Moakher</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">11 pages, 03 figures, to be published in the proceedings of SSVM\\n  2007, LNCS Springer</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/0705.0214v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0705.0214v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4.3\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0706.0300v1</id>\\n    <updated>2007-06-03T05:17:38Z</updated>\\n    <published>2007-06-03T05:17:38Z</published>\\n    <title>Automatic Detection of Pulmonary Embolism using Computational\\n  Intelligence</title>\\n    <summary>  This article describes the implementation of a system designed to\\nautomatically detect the presence of pulmonary embolism in lung scans. These\\nimages are firstly segmented, before alignment and feature extraction using\\nPCA. The neural network was trained using the Hybrid Monte Carlo method,\\nresulting in a committee of 250 neural networks and good results are obtained.\\n</summary>\\n    <author>\\n      <name>Simon Scurrell</name>\\n    </author>\\n    <author>\\n      <name>Tshilidzi Marwala</name>\\n    </author>\\n    <author>\\n      <name>David Rubin</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/0706.0300v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0706.0300v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0712.0131v1</id>\\n    <updated>2007-12-02T10:02:01Z</updated>\\n    <published>2007-12-02T10:02:01Z</published>\\n    <title>Learning Similarity for Character Recognition and 3D Object Recognition</title>\\n    <summary>  I describe an approach to similarity motivated by Bayesian methods. This\\nyields a similarity function that is learnable using a standard Bayesian\\nmethods. The relationship of the approach to variable kernel and variable\\nmetric methods is discussed. The approach is related to variable kernel\\nExperimental results on character recognition and 3D object recognition are\\npresented..\\n</summary>\\n    <author>\\n      <name>Thomas M. Breuel</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/0712.0131v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0712.0131v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0712.2923v1</id>\\n    <updated>2007-12-18T10:43:23Z</updated>\\n    <published>2007-12-18T10:43:23Z</published>\\n    <title>A Class of LULU Operators on Multi-Dimensional Arrays</title>\\n    <summary>  The LULU operators for sequences are extended to multi-dimensional arrays via\\nthe morphological concept of connection in a way which preserves their\\nessential properties, e.g. they are separators and form a four element fully\\nordered semi-group. The power of the operators is demonstrated by deriving a\\ntotal variation preserving discrete pulse decomposition of images.\\n</summary>\\n    <author>\\n      <name>Roumen Anguelov</name>\\n    </author>\\n    <author>\\n      <name>Inger Plaskitt</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/0712.2923v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0712.2923v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0802.1258v1</id>\\n    <updated>2008-02-09T12:22:47Z</updated>\\n    <published>2008-02-09T12:22:47Z</published>\\n    <title>Bayesian Nonlinear Principal Component Analysis Using Random Fields</title>\\n    <summary>  We propose a novel model for nonlinear dimension reduction motivated by the\\nprobabilistic formulation of principal component analysis. Nonlinearity is\\nachieved by specifying different transformation matrices at different locations\\nof the latent space and smoothing the transformation using a Markov random\\nfield type prior. The computation is made feasible by the recent advances in\\nsampling from von Mises-Fisher distributions.\\n</summary>\\n    <author>\\n      <name>Heng Lian</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/0802.1258v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0802.1258v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0805.2690v1</id>\\n    <updated>2008-05-17T17:15:26Z</updated>\\n    <published>2008-05-17T17:15:26Z</published>\\n    <title>Increasing Linear Dynamic Range of Commercial Digital Photocamera Used\\n  in Imaging Systems with Optical Coding</title>\\n    <summary>  Methods of increasing linear optical dynamic range of commercial photocamera\\nfor optical-digital imaging systems are described. Use of such methods allows\\nto use commercial photocameras for optical measurements. Experimental results\\nare reported.\\n</summary>\\n    <author>\\n      <name>M. V. Konnik</name>\\n    </author>\\n    <author>\\n      <name>E. A. Manykin</name>\\n    </author>\\n    <author>\\n      <name>S. N. Starikov</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">unnecessary figures were removed; typos corrected</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/0805.2690v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0805.2690v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0806.3885v1</id>\\n    <updated>2008-06-24T13:43:06Z</updated>\\n    <published>2008-06-24T13:43:06Z</published>\\n    <title>Conceptualization of seeded region growing by pixels aggregation. Part\\n  1: the framework</title>\\n    <summary>  Adams and Bishop have proposed in 1994 a novel region growing algorithm\\ncalled seeded region growing by pixels aggregation (SRGPA). This paper\\nintroduces a framework to implement an algorithm using SRGPA. This framework is\\nbuilt around two concepts: localization and organization of applied action.\\nThis conceptualization gives a quick implementation of algorithms, a direct\\ntranslation between the mathematical idea and the numerical implementation, and\\nan improvement of algorithms efficiency.\\n</summary>\\n    <author>\\n      <name>Vincent Tariel</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/0806.3885v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0806.3885v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0808.2227v1</id>\\n    <updated>2008-08-16T01:34:48Z</updated>\\n    <published>2008-08-16T01:34:48Z</published>\\n    <title>Higher Order Moments Generation by Mellin Transform for Compound Models\\n  of Clutter</title>\\n    <summary>  The compound models of clutter statistics are found suitable to describe the\\nnonstationary nature of radar backscattering from high-resolution observations.\\nIn this letter, we show that the properties of Mellin transform can be utilized\\nto generate higher order moments of simple and compound models of clutter\\nstatistics in a compact manner.\\n</summary>\\n    <author>\\n      <name>C Bhattacharya</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4pages, 1 figure</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/0808.2227v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0808.2227v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0811.4699v2</id>\\n    <updated>2009-03-03T09:49:07Z</updated>\\n    <published>2008-11-28T12:11:21Z</published>\\n    <title>Mapping Images with the Coherence Length Diagrams</title>\\n    <summary>  Statistical pattern recognition methods based on the Coherence Length Diagram\\n(CLD) have been proposed for medical image analyses, such as quantitative\\ncharacterisation of human skin textures, and for polarized light microscopy of\\nliquid crystal textures. Further investigations are made on image maps\\noriginated from such diagram and some examples related to irregularity of\\nmicrostructures are shown.\\n</summary>\\n    <author>\\n      <name>A. Sparavigna</name>\\n    </author>\\n    <author>\\n      <name>R. Marazzato</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">International Journal of Software Engineering and Computing, pp.\\n  53-57, 2009, Vol. 1</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/0811.4699v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0811.4699v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0812.0340v2</id>\\n    <updated>2009-10-01T18:44:03Z</updated>\\n    <published>2008-12-01T18:59:52Z</published>\\n    <title>A Matlab Implementation of a Flat Norm Motivated Polygonal Edge Matching\\n  Method using a Decomposition of Boundary into Four 1-Dimensional Currents</title>\\n    <summary>  We describe and provide code and examples for a polygonal edge matching\\nmethod.\\n</summary>\\n    <author>\\n      <name>Simon P. Morgan</name>\\n    </author>\\n    <author>\\n      <name>Wotao Yin</name>\\n    </author>\\n    <author>\\n      <name>Kevin R. Vixie</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Contains Matlab code and 4 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/0812.0340v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0812.0340v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0812.1340v2</id>\\n    <updated>2009-02-03T16:31:46Z</updated>\\n    <published>2008-12-07T11:42:41Z</published>\\n    <title>Obtaining Depth Maps From Color Images By Region Based Stereo Matching\\n  Algorithms</title>\\n    <summary>  In the paper, region based stereo matching algorithms are developed for\\nextraction depth information from two color stereo image pair. A filter\\neliminating unreliable disparity estimation was used for increasing reliability\\nof the disparity map. Obtained results by algorithms were represented and\\ncompared.\\n</summary>\\n    <author>\\n      <name>B. Baykant Alagoz</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">New figures were added</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">OncuBilim Algorithm And Systems Labs. Vol.08, Art.No:04,(2008)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/0812.1340v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0812.1340v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0902.4073v1</id>\\n    <updated>2009-02-24T20:34:43Z</updated>\\n    <published>2009-02-24T20:34:43Z</published>\\n    <title>Dipole and Quadrupole Moments in Image Processing</title>\\n    <summary>  This paper proposes an algorithm for image processing, obtained by adapting\\nto image maps the definitions of two well-known physical quantities. These\\nquantities are the dipole and quadrupole moments of a charge distribution. We\\nwill see how it is possible to define dipole and quadrupole moments for the\\ngray-tone maps and apply them in the development of algorithms for edge\\ndetection.\\n</summary>\\n    <author>\\n      <name>Amelia Sparavigna</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/0902.4073v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0902.4073v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0902.4663v1</id>\\n    <updated>2009-02-26T18:42:30Z</updated>\\n    <published>2009-02-26T18:42:30Z</published>\\n    <title>Dipole Vectors in Images Processing</title>\\n    <summary>  Instead of evaluating the gradient field of the brightness map of an image,\\nwe propose the use of dipole vectors. This approach is obtained by adapting to\\nthe image gray-tone distribution the definition of the dipole moment of charge\\ndistributions. We will show how to evaluate the dipoles and obtain a vector\\nfield, which can be a good alternative to the gradient field in pattern\\nrecognition.\\n</summary>\\n    <author>\\n      <name>Amelia Sparavigna</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/0902.4663v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0902.4663v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0909.1608v1</id>\\n    <updated>2009-09-09T02:12:22Z</updated>\\n    <published>2009-09-09T02:12:22Z</published>\\n    <title>Motion Segmentation by SCC on the Hopkins 155 Database</title>\\n    <summary>  We apply the Spectral Curvature Clustering (SCC) algorithm to a benchmark\\ndatabase of 155 motion sequences, and show that it outperforms all other\\nstate-of-the-art methods. The average misclassification rate by SCC is 1.41%\\nfor sequences having two motions and 4.85% for three motions.\\n</summary>\\n    <author>\\n      <name>G. Chen</name>\\n    </author>\\n    <author>\\n      <name>G. Lerman</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/ICCVW.2009.5457626</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/ICCVW.2009.5457626\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted to 2009 ICCV Workshop on Dynamical Vision</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Computer Vision Workshops (ICCV Workshops), 2009 IEEE 12th\\n  International Conference on, 2009, pp. 759 - 764</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/0909.1608v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0909.1608v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1003.0776v1</id>\\n    <updated>2010-03-03T10:58:20Z</updated>\\n    <published>2010-03-03T10:58:20Z</published>\\n    <title>Properties of the Discrete Pulse Transform for Multi-Dimensional Arrays</title>\\n    <summary>  This report presents properties of the Discrete Pulse Transform on\\nmulti-dimensional arrays introduced by the authors two or so years ago. The\\nmain result given here in Lemma 2.1 is also formulated in a paper to appear in\\nIEEE Transactions on Image Processing. However, the proof, being too technical,\\nwas omitted there and hence it appears in full in this publication.\\n</summary>\\n    <author>\\n      <name>Roumen Anguelov</name>\\n    </author>\\n    <author>\\n      <name>Inger Fabris-Rotelli</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1003.0776v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1003.0776v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1003.5249v1</id>\\n    <updated>2010-03-27T00:17:19Z</updated>\\n    <published>2010-03-27T00:17:19Z</published>\\n    <title>Active Testing for Face Detection and Localization</title>\\n    <summary>  We provide a novel search technique, which uses a hierarchical model and a\\nmutual information gain heuristic to efficiently prune the search space when\\nlocalizing faces in images. We show exponential gains in computation over\\ntraditional sliding window approaches, while keeping similar performance\\nlevels.\\n</summary>\\n    <author>\\n      <name>Raphael Sznitman</name>\\n    </author>\\n    <author>\\n      <name>Bruno Jedynak</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/TPAMI.2010.106</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/TPAMI.2010.106\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">16 pages, 5 figures, accepted in IEEE Transactions on Pattern\\n  Analysis and Machine Intelligence (TPAMI), 2010</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1003.5249v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1003.5249v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1006.2368v1</id>\\n    <updated>2010-06-11T19:05:05Z</updated>\\n    <published>2010-06-11T19:05:05Z</published>\\n    <title>L2-optimal image interpolation and its applications to medical imaging</title>\\n    <summary>  Digital medical images are always displayed scaled to fit particular view.\\nInterpolation is responsible for this scaling, and if not done properly, can\\nsignificantly degrade diagnostic image quality. However, theoretically-optimal\\ninterpolation algorithms may also be the most time-consuming and impractical.\\nWe propose a new approach, adapted to the needs of digital medical imaging, to\\ncombine high interpolation speed and superior L2-optimal image quality.\\n</summary>\\n    <author>\\n      <name>Oleg Pianykh</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1006.2368v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1006.2368v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.GR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1007.1016v1</id>\\n    <updated>2010-07-06T23:25:39Z</updated>\\n    <published>2010-07-06T23:25:39Z</published>\\n    <title>Bilateral filters: what they can and cannot do</title>\\n    <summary>  Nonlinear bilateral filters (BF) deliver a fine blend of computational\\nsimplicity and blur-free denoising. However, little is known about their\\nnature, noise-suppressing properties, and optimal choices of filter parameters.\\nOur study is meant to fill this gap-explaining the underlying mechanism of\\nbilateral filtering and providing the methodology for optimal filter selection.\\nPractical application to CT image denoising is discussed to illustrate our\\nresults.\\n</summary>\\n    <author>\\n      <name>Oleg S. Pianykh</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1007.1016v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1007.1016v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1101.5766v1</id>\\n    <updated>2011-01-30T12:05:12Z</updated>\\n    <published>2011-01-30T12:05:12Z</published>\\n    <title>Geometric Models with Co-occurrence Groups</title>\\n    <summary>  A geometric model of sparse signal representations is introduced for classes\\nof signals. It is computed by optimizing co-occurrence groups with a maximum\\nlikelihood estimate calculated with a Bernoulli mixture model. Applications to\\nface image compression and MNIST digit classification illustrate the\\napplicability of this model.\\n</summary>\\n    <author>\\n      <name>Joan Bruna</name>\\n    </author>\\n    <author>\\n      <name>St\\xc3\\xa9phane Mallat</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages, ESANN 2010</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1101.5766v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1101.5766v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1104.0582v1</id>\\n    <updated>2011-04-04T14:18:51Z</updated>\\n    <published>2011-04-04T14:18:51Z</published>\\n    <title>Visual Concept Detection and Real Time Object Detection</title>\\n    <summary>  Bag-of-words model is implemented and tried on 10-class visual concept\\ndetection problem. The experimental results show that \"DURF+ERT+SVM\"\\noutperforms \"SIFT+ERT+SVM\" both in detection performance and computation\\nefficiency. Besides, combining DURF and SIFT results in even better detection\\nperformance. Real-time object detection using SIFT and RANSAC is also tried on\\nsimple objects, e.g. drink can, and good result is achieved.\\n</summary>\\n    <author>\\n      <name>Ran Tao</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1104.0582v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1104.0582v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1104.2059v1</id>\\n    <updated>2011-04-11T20:32:54Z</updated>\\n    <published>2011-04-11T20:32:54Z</published>\\n    <title>Template-based matching using weight maps</title>\\n    <summary>  Template matching is one of the most prevalent pattern recognition methods\\nworldwide. It has found uses in most visual concept detection fields. In this\\nwork, we investigate methods for improving template matching by adjusting the\\nweights of different regions of the template. We compare several weight maps\\nand test the methods using the FERET face test set in the context of human eye\\ndetection.\\n</summary>\\n    <author>\\n      <name>Kwie Min Wong</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1104.2059v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1104.2059v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1104.4989v6</id>\\n    <updated>2011-08-11T09:00:58Z</updated>\\n    <published>2011-04-26T18:38:01Z</published>\\n    <title>Preprocessing: A Step in Automating Early Detection of Cervical Cancer</title>\\n    <summary>  This paper has been withdrawn\\n</summary>\\n    <author>\\n      <name>Abhishek Das</name>\\n    </author>\\n    <author>\\n      <name>Avijit Kar</name>\\n    </author>\\n    <author>\\n      <name>Debasis Bhattacharyya</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">wrong conference name mentioned (This paper has been withdrawn)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1104.4989v6\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1104.4989v6\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1107.2807v1</id>\\n    <updated>2011-07-14T12:51:10Z</updated>\\n    <published>2011-07-14T12:51:10Z</published>\\n    <title>Modelling Distributed Shape Priors by Gibbs Random Fields of Second\\n  Order</title>\\n    <summary>  We analyse the potential of Gibbs Random Fields for shape prior modelling. We\\nshow that the expressive power of second order GRFs is already sufficient to\\nexpress simple shapes and spatial relations between them simultaneously. This\\nallows to model and recognise complex shapes as spatial compositions of simpler\\nparts.\\n</summary>\\n    <author>\\n      <name>Boris Flach</name>\\n    </author>\\n    <author>\\n      <name>Dmitrij Schlesinger</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">17 pages, 8 figures</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Control Systems and Computers, (2) 2011, pp 14-24</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1107.2807v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1107.2807v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1111.1461v1</id>\\n    <updated>2011-11-07T00:28:37Z</updated>\\n    <published>2011-11-07T00:28:37Z</published>\\n    <title>Multimodal diff-hash</title>\\n    <summary>  Many applications require comparing multimodal data with different structure\\nand dimensionality that cannot be compared directly. Recently, there has been\\nincreasing interest in methods for learning and efficiently representing such\\nmultimodal similarity. In this paper, we present a simple algorithm for\\nmultimodal similarity-preserving hashing, trying to map multimodal data into\\nthe Hamming space while preserving the intra- and inter-modal similarities. We\\nshow that our method significantly outperforms the state-of-the-art method in\\nthe field.\\n</summary>\\n    <author>\\n      <name>Michael M. Bronstein</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1111.1461v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1111.1461v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1111.6030v2</id>\\n    <updated>2011-12-06T13:21:25Z</updated>\\n    <published>2011-11-25T15:46:37Z</published>\\n    <title>An image processing of a Raphael\\'s portrait of Leonardo</title>\\n    <summary>  In one of his paintings, the School of Athens, Raphael is depicting Leonardo\\nda Vinci as the philosopher Plato. Some image processing tools can help us in\\ncomparing this portrait with two Leonardo\\'s portraits, considered as\\nself-portraits.\\n</summary>\\n    <author>\\n      <name>Amelia Carolina Sparavigna</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Image processing. Portrait. Self-portrait. Leonardo da Vinci.\\n  Raphael. Raffaello Sanzio Images revised using a high-quality image of\\n  Raphael\\'s Plato</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1111.6030v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1111.6030v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1202.0216v1</id>\\n    <updated>2012-02-01T17:00:45Z</updated>\\n    <published>2012-02-01T17:00:45Z</published>\\n    <title>The watershed concept and its use in segmentation : a brief history</title>\\n    <summary>  The watershed is one of the most used tools in image segmentation. We present\\nhow its concept is born and developed over time. Its implementation as an\\nalgorithm or a hardwired device evolved together with the technology which\\nallowed it. We present also how it is used in practice, first together with\\nmarkers, and later introduced in a multiscale framework, in order to produce\\nnot a unique partition but a complete hierarchy.\\n</summary>\\n    <author>\\n      <name>Fernand Meyer</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1202.0216v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1202.0216v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68U10, 05C85\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1204.1634v1</id>\\n    <updated>2012-04-07T13:46:24Z</updated>\\n    <published>2012-04-07T13:46:24Z</published>\\n    <title>Automatic liver segmentation method in CT images</title>\\n    <summary>  The aim of this work is to develop a method for automatic segmentation of the\\nliver based on a priori knowledge of the image, such as location and shape of\\nthe liver.\\n</summary>\\n    <author>\\n      <name>Oussema zayane</name>\\n    </author>\\n    <author>\\n      <name>besma jouini</name>\\n    </author>\\n    <author>\\n      <name>Mohamed Ali Mahjoub</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Canadian Journal on Image Processing &amp; Computer Vision Vol. 2, No.\\n  8, 1923-1717 December 2011</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1204.1634v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1204.1634v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1204.2994v1</id>\\n    <updated>2012-04-13T13:48:27Z</updated>\\n    <published>2012-04-13T13:48:27Z</published>\\n    <title>Image Restoration with Signal-dependent Camera Noise</title>\\n    <summary>  This article describes a fast iterative algorithm for image denoising and\\ndeconvolution with signal-dependent observation noise. We use an optimization\\nstrategy based on variable splitting that adapts traditional Gaussian\\nnoise-based restoration algorithms to account for the observed image being\\ncorrupted by mixed Poisson-Gaussian noise and quantization errors.\\n</summary>\\n    <author>\\n      <name>Ayan Chakrabarti</name>\\n    </author>\\n    <author>\\n      <name>Todd Zickler</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages, 3 figures, 2 tables</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1204.2994v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1204.2994v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.AP\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1205.3766v1</id>\\n    <updated>2012-05-16T19:11:51Z</updated>\\n    <published>2012-05-16T19:11:51Z</published>\\n    <title>Efficient Topology-Controlled Sampling of Implicit Shapes</title>\\n    <summary>  Sampling from distributions of implicitly defined shapes enables analysis of\\nvarious energy functionals used for image segmentation. Recent work describes a\\ncomputationally efficient Metropolis-Hastings method for accomplishing this\\ntask. Here, we extend that framework so that samples are accepted at every\\niteration of the sampler, achieving an order of magnitude speed up in\\nconvergence. Additionally, we show how to incorporate topological constraints.\\n</summary>\\n    <author>\\n      <name>Jason Chang</name>\\n    </author>\\n    <author>\\n      <name>John W. Fisher III</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1205.3766v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1205.3766v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1207.7244v1</id>\\n    <updated>2012-06-29T15:07:26Z</updated>\\n    <published>2012-06-29T15:07:26Z</published>\\n    <title>Visual Vocabulary Learning and Its Application to 3D and Mobile Visual\\n  Search</title>\\n    <summary>  In this technical report, we review related works and recent trends in visual\\nvocabulary based web image search, object recognition, mobile visual search,\\nand 3D object retrieval. Especial focuses would be also given for the recent\\ntrends in supervised/unsupervised vocabulary optimization, compact descriptor\\nfor visual search, as well as in multi-view based 3D object representation.\\n</summary>\\n    <author>\\n      <name>Liujuan Cao</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1207.7244v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1207.7244v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1210.5653v1</id>\\n    <updated>2012-10-20T20:37:22Z</updated>\\n    <published>2012-10-20T20:37:22Z</published>\\n    <title>Identifications of concealed weapon in a Human Body</title>\\n    <summary>  The detection of weapons concealed underneath a person cloths is very much\\nimportant to the improvement of the security of the public as well as the\\nsafety of public assets like airports, buildings and railway stations etc.\\n</summary>\\n    <author>\\n      <name>Prof. Samir K. Bandyopadhyay</name>\\n    </author>\\n    <author>\\n      <name>Biswajita Datta</name>\\n    </author>\\n    <author>\\n      <name>Sudipta Roy</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages, International Journal of Scientific &amp; Engineering Research\\n  (ISSN 2229-5518) 2012</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1210.5653v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1210.5653v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1211.1127v1</id>\\n    <updated>2012-11-06T07:26:49Z</updated>\\n    <published>2012-11-06T07:26:49Z</published>\\n    <title>Visual Transfer Learning: Informal Introduction and Literature Overview</title>\\n    <summary>  Transfer learning techniques are important to handle small training sets and\\nto allow for quick generalization even from only a few examples. The following\\npaper is the introduction as well as the literature overview part of my thesis\\nrelated to the topic of transfer learning for visual recognition problems.\\n</summary>\\n    <author>\\n      <name>Erik Rodner</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">part of my PhD thesis</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1211.1127v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1211.1127v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1211.5712v1</id>\\n    <updated>2012-11-24T23:08:15Z</updated>\\n    <published>2012-11-24T23:08:15Z</published>\\n    <title>Detection of elliptical shapes via cross-entropy clustering</title>\\n    <summary>  The problem of finding elliptical shapes in an image will be considered. We\\ndiscuss the solution which uses cross-entropy clustering. The proposed method\\nallows the search for ellipses with predefined sizes and position in the space.\\nMoreover, it works well for search of ellipsoids in higher dimensions.\\n</summary>\\n    <author>\\n      <name>Jacek Tabor</name>\\n    </author>\\n    <author>\\n      <name>Krzysztof Misztal</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1007/978-3-642-38628-2_78</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1007/978-3-642-38628-2_78\" rel=\"related\"/>\\n    <link href=\"http://arxiv.org/abs/1211.5712v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1211.5712v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1212.4527v1</id>\\n    <updated>2012-12-18T22:30:23Z</updated>\\n    <published>2012-12-18T22:30:23Z</published>\\n    <title>GMM-Based Hidden Markov Random Field for Color Image and 3D Volume\\n  Segmentation</title>\\n    <summary>  In this project, we first study the Gaussian-based hidden Markov random field\\n(HMRF) model and its expectation-maximization (EM) algorithm. Then we\\ngeneralize it to Gaussian mixture model-based hidden Markov random field. The\\nalgorithm is implemented in MATLAB. We also apply this algorithm to color image\\nsegmentation problems and 3D volume segmentation problems.\\n</summary>\\n    <author>\\n      <name>Quan Wang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1212.4527v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1212.4527v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1302.2575v1</id>\\n    <updated>2013-02-04T22:52:13Z</updated>\\n    <published>2013-02-04T22:52:13Z</published>\\n    <title>Coded aperture compressive temporal imaging</title>\\n    <summary>  We use mechanical translation of a coded aperture for code division multiple\\naccess compression of video. We present experimental results for reconstruction\\nat 148 frames per coded snapshot.\\n</summary>\\n    <author>\\n      <name>Patrick Llull</name>\\n    </author>\\n    <author>\\n      <name>Xuejun Liao</name>\\n    </author>\\n    <author>\\n      <name>Xin Yuan</name>\\n    </author>\\n    <author>\\n      <name>Jianbo Yang</name>\\n    </author>\\n    <author>\\n      <name>David Kittle</name>\\n    </author>\\n    <author>\\n      <name>Lawrence Carin</name>\\n    </author>\\n    <author>\\n      <name>Guillermo Sapiro</name>\\n    </author>\\n    <author>\\n      <name>David J. Brady</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1364/OE.21.010526</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1364/OE.21.010526\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">19 pages (when compiled with Optics Express\\' TEX template), 15\\n  figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1302.2575v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1302.2575v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1302.5985v1</id>\\n    <updated>2013-02-25T03:12:12Z</updated>\\n    <published>2013-02-25T03:12:12Z</published>\\n    <title>A Meta-Theory of Boundary Detection Benchmarks</title>\\n    <summary>  Human labeled datasets, along with their corresponding evaluation algorithms,\\nplay an important role in boundary detection. We here present a psychophysical\\nexperiment that addresses the reliability of such benchmarks. To find better\\nremedies to evaluate the performance of any boundary detection algorithm, we\\npropose a computational framework to remove inappropriate human labels and\\nestimate the intrinsic properties of boundaries.\\n</summary>\\n    <author>\\n      <name>Xiaodi Hou</name>\\n    </author>\\n    <author>\\n      <name>Alan Yuille</name>\\n    </author>\\n    <author>\\n      <name>Christof Koch</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">NIPS 2012 Workshop on Human Computation for Science and Computational\\n  Sustainability</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1302.5985v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1302.5985v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1303.2844v1</id>\\n    <updated>2013-03-12T11:23:47Z</updated>\\n    <published>2013-03-12T11:23:47Z</published>\\n    <title>A Stochastic Grammar for Natural Shapes</title>\\n    <summary>  We consider object detection using a generic model for natural shapes. A\\ncommon approach for object recognition involves matching object models directly\\nto images. Another approach involves building intermediate representations via\\na generic grouping processes. We argue that these two processes (model-based\\nrecognition and grouping) may use similar computational mechanisms. By defining\\na generic model for shapes we can use model-based techniques to implement a\\nmid-level vision grouping process.\\n</summary>\\n    <author>\\n      <name>Pedro F. Felzenszwalb</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1007/978-1-4471-5195-1_21</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1007/978-1-4471-5195-1_21\" rel=\"related\"/>\\n    <link href=\"http://arxiv.org/abs/1303.2844v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1303.2844v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1304.0421v1</id>\\n    <updated>2013-04-01T19:14:27Z</updated>\\n    <published>2013-04-01T19:14:27Z</published>\\n    <title>Stroke-Based Cursive Character Recognition</title>\\n    <summary>  Human eye can see and read what is written or displayed either in natural\\nhandwriting or in printed format. The same work in case the machine does is\\ncalled handwriting recognition. Handwriting recognition can be broken down into\\ntwo categories: off-line and on-line. ...\\n</summary>\\n    <author>\\n      <name>K. C. Santosh</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">LORIA</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>E. Iwata</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Advances in Character Recognition INTECH (Ed.) (2012) 175-192</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1304.0421v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1304.0421v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1304.1876v3</id>\\n    <updated>2013-05-28T05:24:25Z</updated>\\n    <published>2013-04-06T10:36:25Z</published>\\n    <title>Proceedings of the 37th Annual Workshop of the Austrian Association for\\n  Pattern Recognition (\\xc3\\x96AGM/AAPR), 2013</title>\\n    <summary>  This volume represents the proceedings of the 37th Annual Workshop of the\\nAustrian Association for Pattern Recognition (\\\\\"OAGM/AAPR), held May 23-24,\\n2013, in Innsbruck, Austria.\\n</summary>\\n    <author>\\n      <name>Justus Piater</name>\\n    </author>\\n    <author>\\n      <name>Antonio Rodr\\xc3\\xadguez-S\\xc3\\xa1nchez</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Contributed papers presented at \\\\\"OAGM/AAPR 2013</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1304.1876v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1304.1876v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4; I.5; I.2.10\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1304.1972v1</id>\\n    <updated>2013-04-07T09:43:47Z</updated>\\n    <published>2013-04-07T09:43:47Z</published>\\n    <title>Facial transformations of ancient portraits: the face of Caesar</title>\\n    <summary>  Some software solutions used to obtain the facial transformations can help\\ninvestigating the artistic metamorphosis of the ancient portraits of the same\\nperson. An analysis with a freely available software of portraitures of Julius\\nCaesar is proposed, showing his several \"morphs\". The software helps enhancing\\nthe mood the artist added to a portrait.\\n</summary>\\n    <author>\\n      <name>Amelia Carolina Sparavigna</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Image processing, Facial transformation, Morphing, Portraits, Julius\\n  Caesar, Arles bust, Tusculum bust</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1304.1972v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1304.1972v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1304.2743v1</id>\\n    <updated>2013-03-27T19:48:54Z</updated>\\n    <published>2013-03-27T19:48:54Z</published>\\n    <title>Comparisons of Reasoning Mechanisms for Computer Vision</title>\\n    <summary>  An evidential reasoning mechanism based on the Dempster-Shafer theory of\\nevidence is introduced. Its performance in real-world image analysis is\\ncompared with other mechanisms based on the Bayesian formalism and a simple\\nweight combination method.\\n</summary>\\n    <author>\\n      <name>Ze-Nian Li</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Appears in Proceedings of the Third Conference on Uncertainty in\\n  Artificial Intelligence (UAI1987)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1304.2743v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1304.2743v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1304.2749v1</id>\\n    <updated>2013-03-27T19:49:25Z</updated>\\n    <published>2013-03-27T19:49:25Z</published>\\n    <title>Evidential Reasoning in Image Understanding</title>\\n    <summary>  In this paper, we present some results of evidential reasoning in\\nunderstanding multispectral images of remote sensing systems. The\\nDempster-Shafer approach of combination of evidences is pursued to yield\\ncontextual classification results, which are compared with previous results of\\nthe Bayesian context free classification, contextual classifications of dynamic\\nprogramming and stochastic relaxation approaches.\\n</summary>\\n    <author>\\n      <name>Minchuan Zhang</name>\\n    </author>\\n    <author>\\n      <name>Su-shing Chen</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Appears in Proceedings of the Third Conference on Uncertainty in\\n  Artificial Intelligence (UAI1987)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1304.2749v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1304.2749v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1304.3447v1</id>\\n    <updated>2013-03-27T19:58:23Z</updated>\\n    <published>2013-03-27T19:58:23Z</published>\\n    <title>Developing and Analyzing Boundary Detection Operators Using\\n  Probabilistic Models</title>\\n    <summary>  Most feature detectors such as edge detectors or circle finders are\\nstatistical, in the sense that they decide at each point in an image about the\\npresence of a feature, this paper describes the use of Bayesian feature\\ndetectors.\\n</summary>\\n    <author>\\n      <name>David Sher</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Appears in Proceedings of the First Conference on Uncertainty in\\n  Artificial Intelligence (UAI1985)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1304.3447v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1304.3447v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1307.1303v1</id>\\n    <updated>2013-07-02T15:25:09Z</updated>\\n    <published>2013-07-02T15:25:09Z</published>\\n    <title>Submodularity of a Set Label Disagreement Function</title>\\n    <summary>  A set label disagreement function is defined over the number of variables\\nthat deviates from the dominant label. The dominant label is the value assumed\\nby the largest number of variables within a set of binary variables. The\\nsubmodularity of a certain family of set label disagreement function is\\ndiscussed in this manuscript. Such disagreement function could be utilized as a\\ncost function in combinatorial optimization approaches for problems defined\\nover hypergraphs.\\n</summary>\\n    <author>\\n      <name>Toufiq Parag</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Janelia Farm Research Campus-HHMI</arxiv:affiliation>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1307.1303v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1307.1303v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1307.3759v1</id>\\n    <updated>2013-07-14T17:37:36Z</updated>\\n    <published>2013-07-14T17:37:36Z</published>\\n    <title>A Minimal Six-Point Auto-Calibration Algorithm</title>\\n    <summary>  A non-iterative auto-calibration algorithm is presented. It deals with a\\nminimal set of six scene points in three views taken by a camera with fixed but\\nunknown intrinsic parameters. Calibration is based on the image correspondences\\nonly. The algorithm is implemented and validated on synthetic image data.\\n</summary>\\n    <author>\\n      <name>Evgeniy Martyushev</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages, 4 figures</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proceedings of the 23rd International Conference on Computer\\n  Graphics and Vision, September 16-20, 2013 Vladivostok, Russia</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1307.3759v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1307.3759v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1308.0890v1</id>\\n    <updated>2013-08-05T05:17:26Z</updated>\\n    <published>2013-08-05T05:17:26Z</published>\\n    <title>Head Gesture Recognition using Optical Flow based Classification with\\n  Reinforcement of GMM based Background Subtraction</title>\\n    <summary>  This paper describes a technique of real time head gesture recognition\\nsystem. The method includes Gaussian mixture model (GMM) accompanied by optical\\nflow algorithm which provided us the required information regarding head\\nmovement. The proposed model can be implemented in various control system. We\\nare also presenting the result and implementation of both mentioned method.\\n</summary>\\n    <author>\\n      <name>Parimita Saikia</name>\\n    </author>\\n    <author>\\n      <name>Karen Das</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1308.0890v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1308.0890v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1310.0319v3</id>\\n    <updated>2013-11-03T08:37:46Z</updated>\\n    <published>2013-10-01T14:26:29Z</published>\\n    <title>Second Croatian Computer Vision Workshop (CCVW 2013)</title>\\n    <summary>  Proceedings of the Second Croatian Computer Vision Workshop (CCVW 2013,\\nhttp://www.fer.unizg.hr/crv/ccvw2013) held September 19, 2013, in Zagreb,\\nCroatia. Workshop was organized by the Center of Excellence for Computer Vision\\nof the University of Zagreb.\\n</summary>\\n    <author>\\n      <name>Sven Lon\\xc4\\x8dari\\xc4\\x87</name>\\n    </author>\\n    <author>\\n      <name>Sini\\xc5\\xa1a \\xc5\\xa0egvi\\xc4\\x87</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Papers presented at the Second Croatian Computer Vision Workshop CCVW\\n  2013</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1310.0319v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1310.0319v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1311.6500v1</id>\\n    <updated>2013-11-11T20:32:50Z</updated>\\n    <published>2013-11-11T20:32:50Z</published>\\n    <title>Stitched Panoramas from Toy Airborne Video Cameras</title>\\n    <summary>  Effective panoramic photographs are taken from vantage points that are high.\\nHigh vantage points have recently become easier to reach as the cost of\\nquadrotor helicopters has dropped to nearly disposable levels. Although cameras\\ncarried by such aircraft weigh only a few grams, their low-quality video can be\\nconverted into panoramas of high quality and high resolution. Also, the small\\nsize of these aircraft vastly reduces the risks inherent to flight.\\n</summary>\\n    <author>\\n      <name>Camille Goudeseune</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages, 9 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1311.6500v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1311.6500v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.3.3; I.4; J.5\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1312.3035v1</id>\\n    <updated>2013-12-11T04:59:49Z</updated>\\n    <published>2013-12-11T04:59:49Z</published>\\n    <title>Heat kernel coupling for multiple graph analysis</title>\\n    <summary>  In this paper, we introduce heat kernel coupling (HKC) as a method of\\nconstructing multimodal spectral geometry on weighted graphs of different size\\nwithout vertex-wise bijective correspondence. We show that Laplacian averaging\\ncan be derived as a limit case of HKC, and demonstrate its applications on\\nseveral problems from the manifold learning and pattern recognition domain.\\n</summary>\\n    <author>\\n      <name>Michael M. Bronstein</name>\\n    </author>\\n    <author>\\n      <name>Klaus Glashoff</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1312.3035v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1312.3035v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1312.3724v1</id>\\n    <updated>2013-12-13T07:54:22Z</updated>\\n    <published>2013-12-13T07:54:22Z</published>\\n    <title>ARIANNA: pAth Recognition for Indoor Assisted NavigatioN with Augmented\\n  perception</title>\\n    <summary>  ARIANNA stands for pAth Recognition for Indoor Assisted Navigation with\\nAugmented perception. It is a flexible and low cost navigation system for vi-\\nsually impaired people. Arianna permits to navigate colored paths painted or\\nsticked on the floor revealing their directions through vibrational feedback on\\ncommercial smartphones.\\n</summary>\\n    <author>\\n      <name>Pierluigi Gallo</name>\\n    </author>\\n    <author>\\n      <name>Ilenia Tinnirello</name>\\n    </author>\\n    <author>\\n      <name>Laura Giarr\\xc3\\xa9</name>\\n    </author>\\n    <author>\\n      <name>Domenico Garlisi</name>\\n    </author>\\n    <author>\\n      <name>Daniele Croce</name>\\n    </author>\\n    <author>\\n      <name>Adriano Fagiolini</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1312.3724v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1312.3724v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1403.2031v1</id>\\n    <updated>2014-03-09T06:53:13Z</updated>\\n    <published>2014-03-09T06:53:13Z</published>\\n    <title>Texture Defect Detection in Gradient Space</title>\\n    <summary>  In this paper, we propose a machine vision algorithm for automatically\\ndetecting defects in patterned textures with the help of gradient space and its\\nenergy. Experiments on real fabric images with defects show that the proposed\\nmethod can be used for automatic detection of fabric defects in textile\\nindustries.\\n</summary>\\n    <author>\\n      <name>V. Asha</name>\\n    </author>\\n    <author>\\n      <name>N. U. Bhajantri</name>\\n    </author>\\n    <author>\\n      <name>P. Nagabhushan</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, ICFoCS-2011</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1403.2031v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1403.2031v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"11K70, 39A14, 39A70, 47A30, 62H30, 68M20\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1403.3964v1</id>\\n    <updated>2014-03-16T22:03:45Z</updated>\\n    <published>2014-03-16T22:03:45Z</published>\\n    <title>Image processing using miniKanren</title>\\n    <summary>  An integral image is one of the most efficient optimization technique for\\nimage processing. However an integral image is only a special case of delayed\\nstream or memoization. This research discusses generalizing concept of integral\\nimage optimization technique, and how to generate an integral image optimized\\nprogram code automatically from abstracted image processing algorithm. In oder\\nto abstruct algorithms, we forces to miniKanren.\\n</summary>\\n    <author>\\n      <name>Hirotaka Niitsuma</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1403.3964v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1403.3964v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.PL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1404.7748v1</id>\\n    <updated>2014-04-30T14:54:19Z</updated>\\n    <published>2014-04-30T14:54:19Z</published>\\n    <title>A graph-based mathematical morphology reader</title>\\n    <summary>  This survey paper aims at providing a \"literary\" anthology of mathematical\\nmorphology on graphs. It describes in the English language many ideas stemming\\nfrom a large number of different papers, hence providing a unified view of an\\nactive and diverse field of research.\\n</summary>\\n    <author>\\n      <name>Laurent Najman</name>\\n    </author>\\n    <author>\\n      <name>Jean Cousty</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1016/j.patrec.2014.05.007</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1016/j.patrec.2014.05.007\" rel=\"related\"/>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Pattern Recognition Letters 47 (2014) 3-17</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1404.7748v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1404.7748v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1405.1678v3</id>\\n    <updated>2015-03-12T12:38:13Z</updated>\\n    <published>2014-05-07T17:51:52Z</published>\\n    <title>RPCA-KFE: Key Frame Extraction for Consumer Video based Robust Principal\\n  Component Analysis</title>\\n    <summary>  Key frame extraction algorithms consider the problem of selecting a subset of\\nthe most informative frames from a video to summarize its content.\\n</summary>\\n    <author>\\n      <name>Chinh Dang</name>\\n    </author>\\n    <author>\\n      <name>Abdolreza Moghadam</name>\\n    </author>\\n    <author>\\n      <name>Hayder Radha</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/TIP.2015.2445572</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/TIP.2015.2445572\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This paper has been withdrawn by the author due to a crucial sign\\n  error in equation 1</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1405.1678v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1405.1678v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1405.6132v1</id>\\n    <updated>2014-02-05T00:46:18Z</updated>\\n    <published>2014-02-05T00:46:18Z</published>\\n    <title>Comparative analysis of common edge detection techniques in context of\\n  object extraction</title>\\n    <summary>  Edges characterize boundaries and are therefore a problem of practical\\nimportance in remote sensing.In this paper a comparative study of various edge\\ndetection techniques and band wise analysis of these algorithms in the context\\nof object extraction with regard to remote sensing satellite images from the\\nIndian Remote Sensing Satellite (IRS) sensors LISS 3, LISS 4 and Cartosat1 as\\nwell as Google Earth is presented.\\n</summary>\\n    <author>\\n      <name>S. K. Katiyar</name>\\n    </author>\\n    <author>\\n      <name>P. V. Arun</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1405.6132v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1405.6132v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1405.6133v1</id>\\n    <updated>2014-02-05T14:49:36Z</updated>\\n    <published>2014-02-05T14:49:36Z</published>\\n    <title>A review over the applicability of image entropy in analyses of remote\\n  sensing datasets</title>\\n    <summary>  Entropy is the measure of uncertainty in any data and is adopted for\\nmaximisation of mutual information in many remote sensing operations. The\\navailability of wide entropy variations motivated us for an investigation over\\nthe suitability preference of these versions to specific operations.\\n</summary>\\n    <author>\\n      <name>S. K. Katiyar</name>\\n    </author>\\n    <author>\\n      <name>P. V. Arun</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">arXiv admin note: substantial text overlap with arXiv:1303.6926</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1405.6133v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1405.6133v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1407.3673v2</id>\\n    <updated>2016-10-21T16:07:15Z</updated>\\n    <published>2014-07-03T10:55:52Z</published>\\n    <title>Enhanced EZW Technique for Compression of Image by Setting Detail\\n  Retaining Pass Number</title>\\n    <summary>  This submission has been withdrawn by arXiv administrators because it\\ncontains excessive and unattributed reuse of content from other authors.\\n</summary>\\n    <author>\\n      <name>Isha Tyagi</name>\\n    </author>\\n    <author>\\n      <name>Ashish Nautiyal</name>\\n    </author>\\n    <author>\\n      <name>Vishwanath Bijalwan</name>\\n    </author>\\n    <author>\\n      <name>Meenu Balodhi</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This submission has been withdrawn by arXiv administrators because it\\n  contains excessive and unattributed reuse of content from other authors</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1407.3673v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1407.3673v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1407.6492v2</id>\\n    <updated>2014-08-14T11:28:48Z</updated>\\n    <published>2014-07-24T08:53:00Z</published>\\n    <title>Recognition of Handwritten Persian/Arabic Numerals Based on Robust\\n  Feature Set and K-NN Classifier</title>\\n    <summary>  This paper has been withdrawn by the author due to a crucial sign error in\\nequation 2 and some mistake in Table 1 information. please let me for changing\\nthis information and updating this paper.\\n</summary>\\n    <author>\\n      <name>Reza Azad</name>\\n    </author>\\n    <author>\\n      <name>Fatemeh Davami</name>\\n    </author>\\n    <author>\\n      <name>Hamid Reza Shayegh</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This paper has been withdrawn by the main author due to the Table 1\\n  and equation 2 errors</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1407.6492v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1407.6492v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1408.1986v1</id>\\n    <updated>2014-08-08T21:24:59Z</updated>\\n    <published>2014-08-08T21:24:59Z</published>\\n    <title>Gabor-like Image Filtering using a Neural Microcircuit</title>\\n    <summary>  In this letter, we present an implementation of a neural microcircuit for\\nimage processing employing Hebbian-adaptive learning. The neuronal circuit\\nutilizes only excitatory synapses to correlate action potentials, extracting\\nthe uncorrelated ones, which contain significant image information. This\\ncircuit is capable of approximating Gabor-like image filtering and other image\\nprocessing functions\\n</summary>\\n    <author>\\n      <name>C. Mayr</name>\\n    </author>\\n    <author>\\n      <name>A. Heittmann</name>\\n    </author>\\n    <author>\\n      <name>R. Sch\\xc3\\xbcffny</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/TNN.2007.891687</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/TNN.2007.891687\" rel=\"related\"/>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IEEE Transactions on Neural Networks, vol. 18, pages 955-959, 2007</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1408.1986v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1408.1986v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.ET\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-bio.NC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1410.2474v1</id>\\n    <updated>2014-10-09T14:08:46Z</updated>\\n    <published>2014-10-09T14:08:46Z</published>\\n    <title>Genetic Stereo Matching Algorithm with Fuzzy Fitness</title>\\n    <summary>  This paper presents a genetic stereo matching algorithm with fuzzy evaluation\\nfunction. The proposed algorithm presents a new encoding scheme in which a\\nchromosome is represented by a disparity matrix. Evolution is controlled by a\\nfuzzy fitness function able to deal with noise and uncertain camera\\nmeasurements, and uses classical evolutionary operators. The result of the\\nalgorithm is accurate dense disparity maps obtained in a reasonable\\ncomputational time suitable for real-time applications as shown in experimental\\nresults.\\n</summary>\\n    <author>\\n      <name>Haythem Ghazouani</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1410.2474v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1410.2474v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1410.2663v1</id>\\n    <updated>2014-10-10T02:48:08Z</updated>\\n    <published>2014-10-10T02:48:08Z</published>\\n    <title>Challenge IEEE-ISBI/TCB : Application of Covariance matrices and wavelet\\n  marginals</title>\\n    <summary>  This short memo aims at explaining our approach for the challenge IEEE-ISBI\\non Bone Texture Characterization. In this work, we focus on the use of\\ncovariance matrices and wavelet marginals in an SVM classifier.\\n</summary>\\n    <author>\\n      <name>Florian Yger</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages, 4 Figues, 2 Tables, Challenge IEEE-ISBI : Bone Texture\\n  Characterization</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1410.2663v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1410.2663v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1411.1442v1</id>\\n    <updated>2014-11-05T22:56:10Z</updated>\\n    <published>2014-11-05T22:56:10Z</published>\\n    <title>Optical Character Recognition, Using K-Nearest Neighbors</title>\\n    <summary>  The problem of optical character recognition, OCR, has been widely discussed\\nin the literature. Having a hand-written text, the program aims at recognizing\\nthe text. Even though there are several approaches to this issue, it is still\\nan open problem. In this paper we would like to propose an approach that uses\\nK-nearest neighbors algorithm, and has the accuracy of more than 90%. The\\ntraining and run time is also very short.\\n</summary>\\n    <author>\\n      <name>Wei Wang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1411.1442v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1411.1442v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1411.6850v1</id>\\n    <updated>2014-11-25T13:13:47Z</updated>\\n    <published>2014-11-25T13:13:47Z</published>\\n    <title>Similarity- based approach for outlier detection</title>\\n    <summary>  This paper presents a new approach for detecting outliers by introducing the\\nnotion of object\\'s proximity. The main idea is that normal point has similar\\ncharacteristics with several neighbors. So the point in not an outlier if it\\nhas a high degree of proximity and its neighbors are several. The performance\\nof this approach is illustrated through real datasets\\n</summary>\\n    <author>\\n      <name>Amina Dik</name>\\n    </author>\\n    <author>\\n      <name>Khalid Jebari</name>\\n    </author>\\n    <author>\\n      <name>Abdelaziz Bouroumi</name>\\n    </author>\\n    <author>\\n      <name>Aziz Ettouhami</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">International Journal of Computer Science Issues 2014</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1411.6850v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1411.6850v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1411.7855v1</id>\\n    <updated>2014-11-28T13:18:22Z</updated>\\n    <published>2014-11-28T13:18:22Z</published>\\n    <title>V-variable image compression</title>\\n    <summary>  V-variable fractals, where $V$ is a positive integer, are intuitively\\nfractals with at most $V$ different \"forms\" or \"shapes\" at all levels of\\nmagnification. In this paper we describe how V-variable fractals can be used\\nfor the purpose of image compression.\\n</summary>\\n    <author>\\n      <name>Franklin Mendivil</name>\\n    </author>\\n    <author>\\n      <name>\\xc3\\x96rjan Stenflo</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1142/S0218348X15500073</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1142/S0218348X15500073\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">15 pages, 22 figures</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Fractals, 23, no 02, (2015)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1411.7855v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1411.7855v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"28A80, 68U10, 94A08\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1412.5334v1</id>\\n    <updated>2014-12-17T10:58:46Z</updated>\\n    <published>2014-12-17T10:58:46Z</published>\\n    <title>The Affine Transforms for Image Enhancement in the Context of\\n  Logarithmic Models</title>\\n    <summary>  The logarithmic model offers new tools for image processing. An efficient\\nmethod for image enhancement is to use an affine transformation with the\\nlogarithmic operations: addition and scalar multiplication. We define some\\ncriteria for automatically determining the parameters of the processing and\\nthis is done via mean and variance computed by logarithmic operations.\\n</summary>\\n    <author>\\n      <name>Vasile Patrascu</name>\\n    </author>\\n    <author>\\n      <name>Vasile Buzuloiu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">International Conference on Computer Vision and Graphics, ICCVG2002,\\n  25-29 September, 2002, Zakopane, Poland</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1412.5334v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1412.5334v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1412.5769v1</id>\\n    <updated>2014-12-18T09:19:50Z</updated>\\n    <published>2014-12-18T09:19:50Z</published>\\n    <title>Gray level image enhancement using the Bernstein polynomials</title>\\n    <summary>  This paper presents a method for enhancing the gray level images. This\\npresented method takes part from the category of point operations and it is\\nbased on piecewise linear functions. The interpolation nodes of these functions\\nare calculated using the Bernstein polynomials.\\n</summary>\\n    <author>\\n      <name>Vasile Patrascu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Scientific Bulletin of the Politechnica, University of\\n  Timisoara,Transactions on Electronics and Communications, Vol. 47 (61), No:\\n  2,pp.121-126, June 2002</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1412.5769v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1412.5769v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1412.6061v1</id>\\n    <updated>2014-12-15T06:55:28Z</updated>\\n    <published>2014-12-15T06:55:28Z</published>\\n    <title>CITlab ARGUS for Arabic Handwriting</title>\\n    <summary>  In the recent years it turned out that multidimensional recurrent neural\\nnetworks (MDRNN) perform very well for offline handwriting recognition tasks\\nlike the OpenHaRT 2013 evaluation DIR. With suitable writing preprocessing and\\ndictionary lookup, our ARGUS software completed this task with an error rate of\\n26.27% in its primary setup.\\n</summary>\\n    <author>\\n      <name>Gundram Leifert</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">University of Rostock - CITlab</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Roger Labahn</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">University of Rostock - CITlab</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Tobias Strau\\xc3\\x9f</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">University of Rostock - CITlab</arxiv:affiliation>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">http://www.nist.gov/itl/iad/mig/upload/OpenHaRT2013_SysDesc_CITLAB.pdf</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1412.6061v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1412.6061v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68T10, 68T05\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1412.6154v1</id>\\n    <updated>2014-10-06T11:45:07Z</updated>\\n    <published>2014-10-06T11:45:07Z</published>\\n    <title>Effective persistent homology of digital images</title>\\n    <summary>  In this paper, three Computational Topology methods (namely effective\\nhomology, persistent homology and discrete vector fields) are mixed together to\\nproduce algorithms for homological digital image processing. The algorithms\\nhave been implemented as extensions of the Kenzo system and have shown a good\\nperformance when applied on some actual images extracted from a public dataset.\\n</summary>\\n    <author>\\n      <name>Ana Romero</name>\\n    </author>\\n    <author>\\n      <name>Julio Rubio</name>\\n    </author>\\n    <author>\\n      <name>Francis Sergeraert</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1412.6154v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1412.6154v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1501.07692v1</id>\\n    <updated>2015-01-30T08:12:48Z</updated>\\n    <published>2015-01-30T08:12:48Z</published>\\n    <title>Blob indentation identification via curvature measurement</title>\\n    <summary>  This paper presents a novel method for identifying indentations on the\\nboundary of solid 2D shape. It uses the signed curvature at a set of points\\nalong the boundary to identify indentations and provides one parameter for\\ntuning the selection mechanism for discriminating indentations from other\\nboundary irregularities. An efficient implementation is described based on the\\nFourier transform for calculating curvature from a sequence of points obtained\\nfrom the boundary of a binary blob.\\n</summary>\\n    <author>\\n      <name>Matthew Sottile</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1501.07692v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1501.07692v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1502.05565v1</id>\\n    <updated>2015-02-19T13:35:06Z</updated>\\n    <published>2015-02-19T13:35:06Z</published>\\n    <title>Multi-valued Color Representation Based on Frank t-norm Properties</title>\\n    <summary>  In this paper two knowledge representation models are proposed, FP4 and FP6.\\nBoth combine ideas from fuzzy sets and four-valued and hexa-valued logics. Both\\nrepresent imprecise properties whose accomplished degree is unknown or\\ncontradictory for some objects. A possible application in the color analysis\\nand color image processing is discussed.\\n</summary>\\n    <author>\\n      <name>Vasile Patrascu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">12th International Conference Information Processing and Management\\n  of Uncertainty for Knowledge-Based Systems, IPMU\\'2008, pp. 1215-1222, June\\n  22-27, 2008, Malaga, Spain</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1502.05565v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1502.05565v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1502.06556v1</id>\\n    <updated>2015-02-23T19:11:12Z</updated>\\n    <published>2015-02-23T19:11:12Z</published>\\n    <title>Shannon, Tsallis and Kaniadakis entropies in bi-level image thresholding</title>\\n    <summary>  The maximum entropy principle is often used for bi-level or multi-level\\nthresholding of images. For this purpose, some methods are available based on\\nShannon and Tsallis entropies. In this paper, we discuss them and propose a\\nmethod based on Kaniadakis entropy.\\n</summary>\\n    <author>\\n      <name>Amelia Carolina Sparavigna</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.18483/ijSci.626</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.18483/ijSci.626\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Keywords: Kaniadakis Entropy, Image Processing, Image Segmentation,\\n  Image Thresholding, Texture Transitions</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">International Journal of Sciences 4(2), 35-43, 2015</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1502.06556v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1502.06556v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1503.01557v3</id>\\n    <updated>2015-04-19T02:42:16Z</updated>\\n    <published>2015-03-05T07:06:02Z</published>\\n    <title>Supervised Discrete Hashing</title>\\n    <summary>  This paper has been withdrawn by the authour.\\n</summary>\\n    <author>\\n      <name>Fumin Shen</name>\\n    </author>\\n    <author>\\n      <name>Chunhua Shen</name>\\n    </author>\\n    <author>\\n      <name>Wei Liu</name>\\n    </author>\\n    <author>\\n      <name>Heng Tao Shen</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This paper has been withdrawn by the authour since the algorithm is\\n  being used for patent application</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1503.01557v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1503.01557v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1505.01065v1</id>\\n    <updated>2015-04-30T10:10:16Z</updated>\\n    <published>2015-04-30T10:10:16Z</published>\\n    <title>Proceedings of The 39th Annual Workshop of the Austrian Association for\\n  Pattern Recognition (OAGM), 2015</title>\\n    <summary>  The 39th annual workshop of the Austrian Association for Pattern Recognition\\n(OAGM/AAPR) provides a platform for presentation and discussion of research\\nprogress as well as research projects within the OAGM/AAPR community.\\n</summary>\\n    <author>\\n      <name>Sebastian Hegenbart</name>\\n    </author>\\n    <author>\\n      <name>Roland Kwitt</name>\\n    </author>\\n    <author>\\n      <name>Andreas Uhl</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Index submitted before individual papers</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1505.01065v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1505.01065v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1505.02890v2</id>\\n    <updated>2015-08-25T15:12:37Z</updated>\\n    <published>2015-05-12T07:30:22Z</published>\\n    <title>Sparse 3D convolutional neural networks</title>\\n    <summary>  We have implemented a convolutional neural network designed for processing\\nsparse three-dimensional input data. The world we live in is three dimensional\\nso there are a large number of potential applications including 3D object\\nrecognition and analysis of space-time objects. In the quest for efficiency, we\\nexperiment with CNNs on the 2D triangular-lattice and 3D tetrahedral-lattice.\\n</summary>\\n    <author>\\n      <name>Ben Graham</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">BMVC 2015</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1505.02890v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1505.02890v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1505.03795v1</id>\\n    <updated>2015-05-14T16:43:07Z</updated>\\n    <published>2015-05-14T16:43:07Z</published>\\n    <title>Fast and numerically stable circle fit</title>\\n    <summary>  We develop a new algorithm for fitting circles that does not have drawbacks\\ncommonly found in existing circle fits. Our fit achieves ultimate accuracy (to\\nmachine precision), avoids divergence, and is numerically stable even when\\nfitting circles get arbitrary large. Lastly, our algorithm takes less than 10\\niterations to converge, on average.\\n</summary>\\n    <author>\\n      <name>Houssam Abdul-Rahman</name>\\n    </author>\\n    <author>\\n      <name>Nikolai Chernov</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1007/s10851-013-0461-4</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1007/s10851-013-0461-4\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">16 pages</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Journal of Mathematical Imaging and Vision June 2014, Volume 49,\\n  Issue 2, pp 289-295</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1505.03795v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1505.03795v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1505.05240v1</id>\\n    <updated>2015-05-20T04:09:47Z</updated>\\n    <published>2015-05-20T04:09:47Z</published>\\n    <title>Benchmarking KAZE and MCM for Multiclass Classification</title>\\n    <summary>  In this paper, we propose a novel approach for feature generation by\\nappropriately fusing KAZE and SIFT features. We then use this feature set along\\nwith Minimal Complexity Machine(MCM) for object classification. We show that\\nKAZE and SIFT features are complementary. Experimental results indicate that an\\nelementary integration of these techniques can outperform the state-of-the-art\\napproaches.\\n</summary>\\n    <author>\\n      <name>Siddharth Srivastava</name>\\n    </author>\\n    <author>\\n      <name>Prerana Mukherjee</name>\\n    </author>\\n    <author>\\n      <name>Brejesh Lall</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1505.05240v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1505.05240v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4.7; I.5.4; I.4.8; I.4.9; I.5.2\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1505.06769v1</id>\\n    <updated>2015-05-25T22:18:36Z</updated>\\n    <published>2015-05-25T22:18:36Z</published>\\n    <title>VeinPLUS: A Transillumination and Reflection-based Hand Vein Database</title>\\n    <summary>  This paper gives a short summary of work related to the creation of a\\ndepartment-hosted hand vein database. After the introducing section, special\\nproperties of the hand vein acquisition are explained, followed by a comparison\\ntable, which shows key differences to existing well-known hand vein databases.\\nAt the end, the ROI extraction process is described and sample images and ROIs\\nare presented.\\n</summary>\\n    <author>\\n      <name>Alexander Gruschina</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Presented at OAGM Workshop, 2015 (arXiv:1505.01065)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1505.06769v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1505.06769v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1507.05053v1</id>\\n    <updated>2015-07-17T17:48:49Z</updated>\\n    <published>2015-07-17T17:48:49Z</published>\\n    <title>Massively Deep Artificial Neural Networks for Handwritten Digit\\n  Recognition</title>\\n    <summary>  Greedy Restrictive Boltzmann Machines yield an fairly low 0.72% error rate on\\nthe famous MNIST database of handwritten digits. All that was required to\\nachieve this result was a high number of hidden layers consisting of many\\nneurons, and a graphics card to greatly speed up the rate of learning.\\n</summary>\\n    <author>\\n      <name>Keiron O\\'Shea</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">2 pages, 1 figure</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1507.05053v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1507.05053v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1507.05244v1</id>\\n    <updated>2015-07-19T03:14:56Z</updated>\\n    <published>2015-07-19T03:14:56Z</published>\\n    <title>Handwriting Recognition</title>\\n    <summary>  This paper describes the method to recognize offline handwritten characters.\\nA robust algorithm for handwriting segmentation is described here with the help\\nof which individual characters can be segmented from a selected word from a\\nparagraph of handwritten text image which is given as input.\\n</summary>\\n    <author>\\n      <name>Jayati Ghosh Dastidar</name>\\n    </author>\\n    <author>\\n      <name>Surabhi Sarkar</name>\\n    </author>\\n    <author>\\n      <name>Rick Punyadyuti Sinha</name>\\n    </author>\\n    <author>\\n      <name>Kasturi Basu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1507.05244v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1507.05244v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1508.02246v1</id>\\n    <updated>2015-08-10T13:51:35Z</updated>\\n    <published>2015-08-10T13:51:35Z</published>\\n    <title>Feature Learning for Interaction Activity Recognition in RGBD Videos</title>\\n    <summary>  This paper proposes a human activity recognition method which is based on\\nfeatures learned from 3D video data without incorporating domain knowledge. The\\nexperiments on data collected by RGBD cameras produce results outperforming\\nother techniques. Our feature encoding method follows the bag-of-visual-word\\nmodel, then we use a SVM classifier to recognise the activities. We do not use\\nskeleton or tracking information and the same technique is applied on color and\\ndepth data.\\n</summary>\\n    <author>\\n      <name>Ngu Nguyen</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1508.02246v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1508.02246v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1509.04232v1</id>\\n    <updated>2015-09-14T18:30:05Z</updated>\\n    <published>2015-09-14T18:30:05Z</published>\\n    <title>gSLICr: SLIC superpixels at over 250Hz</title>\\n    <summary>  We introduce a parallel GPU implementation of the Simple Linear Iterative\\nClustering (SLIC) superpixel segmentation. Using a single graphic card, our\\nimplementation achieves speedups of up to $83\\\\times$ from the standard\\nsequential implementation. Our implementation is fully compatible with the\\nstandard sequential implementation and the software is now available online and\\nis open source.\\n</summary>\\n    <author>\\n      <name>Carl Yuheng Ren</name>\\n    </author>\\n    <author>\\n      <name>Victor Adrian Prisacariu</name>\\n    </author>\\n    <author>\\n      <name>Ian D Reid</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1509.04232v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1509.04232v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1509.05054v1</id>\\n    <updated>2015-09-16T20:23:06Z</updated>\\n    <published>2015-09-16T20:23:06Z</published>\\n    <title>Overcomplete Dictionary Learning with Jacobi Atom Updates</title>\\n    <summary>  Dictionary learning for sparse representations is traditionally approached\\nwith sequential atom updates, in which an optimized atom is used immediately\\nfor the optimization of the next atoms. We propose instead a Jacobi version, in\\nwhich groups of atoms are updated independently, in parallel. Extensive\\nnumerical evidence for sparse image representation shows that the parallel\\nalgorithms, especially when all atoms are updated simultaneously, give better\\ndictionaries than their sequential counterparts.\\n</summary>\\n    <author>\\n      <name>Paul Irofti</name>\\n    </author>\\n    <author>\\n      <name>Bogdan Dumitrescu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1509.05054v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1509.05054v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1512.01533v1</id>\\n    <updated>2015-12-04T20:28:27Z</updated>\\n    <published>2015-12-04T20:28:27Z</published>\\n    <title>Motion trails from time-lapse video</title>\\n    <summary>  From an image sequence captured by a stationary camera, background\\nsubtraction can detect moving foreground objects in the scene. Distinguishing\\nforeground from background is further improved by various heuristics. Then each\\nobject\\'s motion can be emphasized by duplicating its positions as a motion\\ntrail. These trails clarify the objects\\' spatial relationships. Also, adding\\nmotion trails to a video before previewing it at high speed reduces the risk of\\noverlooking transient events.\\n</summary>\\n    <author>\\n      <name>Camille Goudeseune</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages, 6 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1512.01533v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1512.01533v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.3.3; I.4.6\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1512.02329v2</id>\\n    <updated>2015-12-15T06:18:35Z</updated>\\n    <published>2015-12-08T05:04:57Z</published>\\n    <title>Computational Models for Multiview Dense Depth Maps of Dynamic Scene</title>\\n    <summary>  This paper reviews the recent progresses of the depth map generation for\\ndynamic scene and its corresponding computational models. This paper mainly\\ncovers the homogeneous ambiguity models in depth sensing, resolution models in\\ndepth processing, and consistency models in depth optimization. We also\\nsummarize the future work in the depth map generation.\\n</summary>\\n    <author>\\n      <name>Qifei Wang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, IEEE COMSOC MMTC E-Letter 2015</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1512.02329v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1512.02329v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1512.02357v1</id>\\n    <updated>2015-12-08T07:25:29Z</updated>\\n    <published>2015-12-08T07:25:29Z</published>\\n    <title>Towards the Application of Linear Programming Methods For Multi-Camera\\n  Pose Estimation</title>\\n    <summary>  We presented a separation based optimization algorithm which, rather than\\noptimization the entire variables altogether, This would allow us to employ: 1)\\na class of nonlinear functions with three variables and 2) a convex quadratic\\nmultivariable polynomial, for minimization of reprojection error. Neglecting\\nthe inversion required to minimize the nonlinear functions, in this paper we\\ndemonstrate how separation allows eradication of matrix inversion.\\n</summary>\\n    <author>\\n      <name>Masoud Aghamohamadian-Sharbaf</name>\\n    </author>\\n    <author>\\n      <name>Ahmadreza Heravi</name>\\n    </author>\\n    <author>\\n      <name>Hamidreza Pourreza</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1512.02357v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1512.02357v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1512.06014v2</id>\\n    <updated>2016-12-04T20:42:48Z</updated>\\n    <published>2015-12-18T16:17:35Z</published>\\n    <title>Multiclass Classification of Cervical Cancer Tissues by Hidden Markov\\n  Model</title>\\n    <summary>  In this paper, we report a hidden Markov model based multiclass\\nclassification of cervical cancer tissues. This model has been validated\\ndirectly over time series generated by the medium refractive index fluctuations\\nextracted from differential interference contrast images of healthy and\\ndifferent stages of cancer tissues. The method shows promising results for\\nmulticlass classification with higher accuracy.\\n</summary>\\n    <author>\\n      <name>Sabyasachi Mukhopadhyay</name>\\n    </author>\\n    <author>\\n      <name>Sanket Nandan</name>\\n    </author>\\n    <author>\\n      <name>Indrajit Kurmi</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1512.06014v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1512.06014v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1601.00396v1</id>\\n    <updated>2016-01-04T07:28:53Z</updated>\\n    <published>2016-01-04T07:28:53Z</published>\\n    <title>Automatic Detection and Decoding of Photogrammetric Coded Targets</title>\\n    <summary>  Close-range Photogrammetry is widely used in many industries because of the\\ncost effectiveness and efficiency of the technique. In this research, we\\nintroduce an automated coded target detection method which can be used to\\nenhance the efficiency of the Photogrammetry.\\n</summary>\\n    <author>\\n      <name>Udaya Wijenayake</name>\\n    </author>\\n    <author>\\n      <name>Sung-In Choi</name>\\n    </author>\\n    <author>\\n      <name>Soon-Yong Park</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/ELINFOCOM.2014.6914413</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/ELINFOCOM.2014.6914413\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">3 pages, 4 figures, Electronics, Information and Communications\\n  (ICEIC), 2014 International Conference on</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1601.00396v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1601.00396v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1601.04568v1</id>\\n    <updated>2016-01-18T15:22:48Z</updated>\\n    <published>2016-01-18T15:22:48Z</published>\\n    <title>Content Aware Neural Style Transfer</title>\\n    <summary>  This paper presents a content-aware style transfer algorithm for paintings\\nand photos of similar content using pre-trained neural network, obtaining\\nbetter results than the previous work. In addition, the numerical experiments\\nshow that the style pattern and the content information is not completely\\nseparated by neural network.\\n</summary>\\n    <author>\\n      <name>Rujie Yin</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1601.04568v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1601.04568v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68T10\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4.10; I.5.2; I.5.4\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1601.08003v1</id>\\n    <updated>2016-01-29T08:54:22Z</updated>\\n    <published>2016-01-29T08:54:22Z</published>\\n    <title>Efficient Robust Mean Value Calculation of 1D Features</title>\\n    <summary>  A robust mean value is often a good alternative to the standard mean value\\nwhen dealing with data containing many outliers. An efficient method for\\nsamples of one-dimensional features and the truncated quadratic error norm is\\npresented and compared to the method of channel averaging (soft histograms).\\n</summary>\\n    <author>\\n      <name>Erik Jonsson</name>\\n    </author>\\n    <author>\\n      <name>Michael Felsberg</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Presented at the SSBA Symposium 2005, Malm\\\\\"o, Sweden</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1601.08003v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1601.08003v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1603.01772v1</id>\\n    <updated>2016-03-06T00:30:34Z</updated>\\n    <published>2016-03-06T00:30:34Z</published>\\n    <title>Fast calculation of correlations in recognition systems</title>\\n    <summary>  Computationally efficient classification system architecture is proposed. It\\nutilizes fast tensor-vector multiplication algorithm to apply linear operators\\nupon input signals . The approach is applicable to wide variety of recognition\\nsystem architectures ranging from single stage matched filter bank classifiers\\nto complex neural networks with unlimited number of hidden layers.\\n</summary>\\n    <author>\\n      <name>Pavel Dourbal</name>\\n    </author>\\n    <author>\\n      <name>Mikhail Pekker</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1603.01772v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1603.01772v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"62H30, 65F05, 65F10, 65F30, 65F50, 68T05, 68T10, 94A11, 94A12,&#10;  94A13, 94A15\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"F.2.1; G.1.0; G.1.3; G.4; H.4.2; I.1.2; I.2.2; I.5.2; I.5.4\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1603.06433v1</id>\\n    <updated>2016-03-21T14:23:00Z</updated>\\n    <published>2016-03-21T14:23:00Z</published>\\n    <title>Illumination-invariant image mosaic calculation based on logarithmic\\n  search</title>\\n    <summary>  This technical report describes an improved image mosaicking algorithm. It is\\nbased on Jain\\'s logarithmic search algorithm [Jain 1981] which is coupled to\\nthe method of Kourogi (1999} for matching images in a video sequence.\\nLogarithmic search has a better invariance against illumination changes than\\nthe original optical-flow-based method of Kourogi.\\n</summary>\\n    <author>\\n      <name>Wolfgang Konen</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, 1 figure</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1603.06433v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1603.06433v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1603.09037v1</id>\\n    <updated>2016-03-30T04:40:31Z</updated>\\n    <published>2016-03-30T04:40:31Z</published>\\n    <title>Vector Quantization for Machine Vision</title>\\n    <summary>  This paper shows how to reduce the computational cost for a variety of common\\nmachine vision tasks by operating directly in the compressed domain,\\nparticularly in the context of hardware acceleration. Pyramid Vector\\nQuantization (PVQ) is the compression technique of choice and its properties\\nare exploited to simplify Support Vector Machines (SVM), Convolutional Neural\\nNetworks(CNNs), Histogram of Oriented Gradients (HOG) features, interest points\\nmatching and other algorithms.\\n</summary>\\n    <author>\\n      <name>Vincenzo Liguori</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages, 7 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1603.09037v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1603.09037v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1603.09129v1</id>\\n    <updated>2016-03-30T11:11:29Z</updated>\\n    <published>2016-03-30T11:11:29Z</published>\\n    <title>Exploiting Facial Landmarks for Emotion Recognition in the Wild</title>\\n    <summary>  In this paper, we describe an entry to the third Emotion Recognition in the\\nWild Challenge, EmotiW2015. We detail the associated experiments and show that,\\nthrough more accurately locating the facial landmarks, and considering only the\\ndistances between them, we can achieve a surprising level of performance. The\\nresulting system is not only more accurate than the challenge baseline, but\\nalso much simpler.\\n</summary>\\n    <author>\\n      <name>Matthew Day</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, ICMI 2015</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1603.09129v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1603.09129v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.2.10\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1604.04926v1</id>\\n    <updated>2016-04-17T20:48:49Z</updated>\\n    <published>2016-04-17T20:48:49Z</published>\\n    <title>Some medical applications of example-based super-resolution</title>\\n    <summary>  Example-based super-resolution (EBSR) reconstructs a high-resolution image\\nfrom a low-resolution image, given a training set of high-resolution images. In\\nthis note I propose some applications of EBSR to medical imaging. A particular\\ninteresting application, which I call \"x-ray voxelization\", approximates the\\nresult of a CT scan from an x-ray image.\\n</summary>\\n    <author>\\n      <name>Ramin Zabih</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1604.04926v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1604.04926v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1605.04250v2</id>\\n    <updated>2016-08-01T13:25:06Z</updated>\\n    <published>2016-05-13T16:56:10Z</published>\\n    <title>Color Homography</title>\\n    <summary>  We show the surprising result that colors across a change in viewing\\ncondition (changing light color, shading and camera) are related by a\\nhomography. Our homography color correction application delivers improved color\\nfidelity compared with the linear least-square.\\n</summary>\\n    <author>\\n      <name>Graham D. Finlayson</name>\\n    </author>\\n    <author>\\n      <name>Han Gong</name>\\n    </author>\\n    <author>\\n      <name>Robert B. Fisher</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted by Progress in Colour Studies 2016</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1605.04250v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1605.04250v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1605.04785v1</id>\\n    <updated>2016-05-16T14:42:34Z</updated>\\n    <published>2016-05-16T14:42:34Z</published>\\n    <title>An Alternative Matting Laplacian</title>\\n    <summary>  Cutting out and object and estimate its transparency mask is a key task in\\nmany applications. We take on the work on closed-form matting by Levin et al.,\\nthat is used at the core of many matting techniques, and propose an alternative\\nformulation that offers more flexible controls over the matting priors. We also\\nshow that this new approach is efficient at upscaling transparency maps from\\ncoarse estimates.\\n</summary>\\n    <author>\\n      <name>Fran\\xc3\\xa7ois Piti\\xc3\\xa9</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">ICIP 2016</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1605.04785v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1605.04785v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1606.03473v1</id>\\n    <updated>2016-06-10T20:34:39Z</updated>\\n    <published>2016-06-10T20:34:39Z</published>\\n    <title>Face Detection with the Faster R-CNN</title>\\n    <summary>  The Faster R-CNN has recently demonstrated impressive results on various\\nobject detection benchmarks. By training a Faster R-CNN model on the large\\nscale WIDER face dataset, we report state-of-the-art results on two widely used\\nface detection benchmarks, FDDB and the recently released IJB-A.\\n</summary>\\n    <author>\\n      <name>Huaizu Jiang</name>\\n    </author>\\n    <author>\\n      <name>Erik Learned-Miller</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">technical report</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1606.03473v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1606.03473v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1606.08315v1</id>\\n    <updated>2016-06-27T15:23:04Z</updated>\\n    <published>2016-06-27T15:23:04Z</published>\\n    <title>Depth Estimation from Single Image using Sparse Representations</title>\\n    <summary>  Monocular depth estimation is an interesting and challenging problem as there\\nis no analytic mapping known between an intensity image and its depth map.\\nRecently there has been a lot of data accumulated through depth-sensing\\ncameras, in parallel to that researchers started to tackle this task using\\nvarious learning algorithms. In this paper, a deep sparse coding method is\\nproposed for monocular depth estimation along with an approach for\\ndeterministic dictionary initialization.\\n</summary>\\n    <author>\\n      <name>Yigit Oktar</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.13140/RG.2.1.5059.0323</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.13140/RG.2.1.5059.0323\" rel=\"related\"/>\\n    <link href=\"http://arxiv.org/abs/1606.08315v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1606.08315v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1608.03617v1</id>\\n    <updated>2016-08-11T20:53:48Z</updated>\\n    <published>2016-08-11T20:53:48Z</published>\\n    <title>Automatic detection of moving objects in video surveillance</title>\\n    <summary>  This work is in the field of video surveillance including motion detection.\\nThe video surveillance is one of essential techniques for automatic video\\nanalysis to extract crucial information or relevant scenes in video\\nsurveillance systems. The aim of our work is to propose solutions for the\\nautomatic detection of moving objects in real time with a surveillance camera.\\nThe detected objects are objects that have some geometric shape (circle,\\nellipse, square, and rectangle).\\n</summary>\\n    <author>\\n      <name>Larbi Guezouli</name>\\n    </author>\\n    <author>\\n      <name>Hanane Belhani</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1608.03617v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1608.03617v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1608.03832v1</id>\\n    <updated>2016-08-12T15:55:12Z</updated>\\n    <published>2016-08-12T15:55:12Z</published>\\n    <title>On Minimal Accuracy Algorithm Selection in Computer Vision and\\n  Intelligent Systems</title>\\n    <summary>  In this paper we discuss certain theoretical properties of algorithm\\nselection approach to image processing and to intelligent system in general. We\\nanalyze the theoretical limits of algorithm selection with respect to the\\nalgorithm selection accuracy. We show the theoretical formulation of a crisp\\nbound on the algorithm selector precision guaranteeing to always obtain better\\nthan the best available algorithm result.\\n</summary>\\n    <author>\\n      <name>Martin Lukac</name>\\n    </author>\\n    <author>\\n      <name>Kamila Abdiyeva</name>\\n    </author>\\n    <author>\\n      <name>Michitaka Kameyama</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1608.03832v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1608.03832v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1609.02271v2</id>\\n    <updated>2016-09-09T18:37:03Z</updated>\\n    <published>2016-09-08T04:49:31Z</published>\\n    <title>Ashwin: Plug-and-Play System for Machine-Human Image Annotation</title>\\n    <summary>  We present an end-to-end machine-human image annotation system where each\\ncomponent can be attached in a plug-and-play fashion. These components include\\nFeature Extraction, Machine Classifier, Task Sampling and Crowd Consensus.\\n</summary>\\n    <author>\\n      <name>Anand Sriraman</name>\\n    </author>\\n    <author>\\n      <name>Mandar Kulkarni</name>\\n    </author>\\n    <author>\\n      <name>Rahul Kumar</name>\\n    </author>\\n    <author>\\n      <name>Kanika Kalra</name>\\n    </author>\\n    <author>\\n      <name>Purushotam Radadia</name>\\n    </author>\\n    <author>\\n      <name>Shirish Karande</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">HCOMP 2016 Demonstrations Track</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1609.02271v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1609.02271v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1609.03529v1</id>\\n    <updated>2016-09-12T19:00:24Z</updated>\\n    <published>2016-09-12T19:00:24Z</published>\\n    <title>Examining Representational Similarity in ConvNets and the Primate Visual\\n  Cortex</title>\\n    <summary>  We compare several ConvNets with different depth and regularization\\ntechniques with multi-unit macaque IT cortex recordings and assess the impact\\nof the same on representational similarity with the primate visual cortex. We\\nfind that with increasing depth and validation performance, ConvNet features\\nare closer to cortical IT representations.\\n</summary>\\n    <author>\\n      <name>Abhimanyu Dubey</name>\\n    </author>\\n    <author>\\n      <name> Jayadeva</name>\\n    </author>\\n    <author>\\n      <name>Sumeet Agarwal</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, short abstract, Accepted to the Workshop on Biological and\\n  Artificial Vision, ECCV, 2016</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1609.03529v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1609.03529v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-bio.NC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1609.05001v1</id>\\n    <updated>2016-09-16T11:20:07Z</updated>\\n    <published>2016-09-16T11:20:07Z</published>\\n    <title>Stamp processing with examplar features</title>\\n    <summary>  Document digitization is becoming increasingly crucial. In this work, we\\npropose a shape based approach for automatic stamp verification/detection in\\ndocument images using an unsupervised feature learning. Given a small set of\\ntraining images, our algorithm learns an appropriate shape representation using\\nan unsupervised clustering. Experimental results demonstrate the effectiveness\\nof our framework in challenging scenarios.\\n</summary>\\n    <author>\\n      <name>Yash Bhalgat</name>\\n    </author>\\n    <author>\\n      <name>Mandar Kulkarni</name>\\n    </author>\\n    <author>\\n      <name>Shirish Karande</name>\\n    </author>\\n    <author>\\n      <name>Sachin Lodha</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1609.05001v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1609.05001v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1609.07597v1</id>\\n    <updated>2016-09-24T10:32:30Z</updated>\\n    <published>2016-09-24T10:32:30Z</published>\\n    <title>DimensionApp : android app to estimate object dimensions</title>\\n    <summary>  In this project, we develop an android app that uses on computer vision\\ntechniques to estimate an object dimension present in field of view. The app\\nwhile having compact size, is accurate upto +/- 5 mm and robust towards touch\\ninputs. We use single-view metrology to compute accurate measurement. Unlike\\nprevious approaches, our technique does not rely on line detection and can be\\ngeneralize to any object shape easily.\\n</summary>\\n    <author>\\n      <name>Suriya Singh</name>\\n    </author>\\n    <author>\\n      <name>Vijay Kumar</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Project Report 2014</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1609.07597v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1609.07597v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1610.04575v1</id>\\n    <updated>2016-04-19T03:16:14Z</updated>\\n    <published>2016-04-19T03:16:14Z</published>\\n    <title>Comparing Face Detection and Recognition Techniques</title>\\n    <summary>  This paper implements and compares different techniques for face detection\\nand recognition. One is find where the face is located in the images that is\\nface detection and second is face recognition that is identifying the person.\\nWe study three techniques in this paper: Face detection using self organizing\\nmap (SOM), Face recognition by projection and nearest neighbor and Face\\nrecognition using SVM.\\n</summary>\\n    <author>\\n      <name>Jyothi Korra</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1610.04575v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1610.04575v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1610.09520v1</id>\\n    <updated>2016-10-29T14:54:28Z</updated>\\n    <published>2016-10-29T14:54:28Z</published>\\n    <title>Multi-Camera Occlusion and Sudden-Appearance-Change Detection Using\\n  Hidden Markovian Chains</title>\\n    <summary>  This paper was originally submitted to Xinova as a response to a Request for\\nInvention (RFI) on new event monitoring methods. In this paper, a new object\\ntracking algorithm using multiple cameras for surveillance applications is\\nproposed. The proposed system can detect sudden-appearance-changes and\\nocclusions using a hidden Markovian statistical model. The experimental results\\nconfirm that our system detect the sudden-appearance changes and occlusions\\nreliably.\\n</summary>\\n    <author>\\n      <name>Xudong Ma</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1610.09520v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1610.09520v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1611.03873v1</id>\\n    <updated>2016-11-11T21:00:58Z</updated>\\n    <published>2016-11-11T21:00:58Z</published>\\n    <title>Effective sparse representation of X-Ray medical images</title>\\n    <summary>  Effective sparse representation of X-Ray medical images within the context of\\ndata reduction is considered. The proposed framework is shown to render an\\nenormous reduction in the cardinality of the data set required to represent\\nthis class of images at very good quality. The particularity of the approach is\\nthat it can be implemented at very competitive processing time and low memory\\nrequirements\\n</summary>\\n    <author>\\n      <name>Laura Rebollo-Neira</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Routines for implementing the approach are available on\\n  http://www.nonlinear-approx.info/examples/node06.html</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1611.03873v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1611.03873v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1701.01035v1</id>\\n    <updated>2017-01-04T14:53:07Z</updated>\\n    <published>2017-01-04T14:53:07Z</published>\\n    <title>Path-following based Point Matching using Similarity Transformation</title>\\n    <summary>  To address the problem of 3D point matching where the poses of two point sets\\nare unknown, we adapt a recently proposed path following based method to use\\nsimilarity transformation instead of the original affine transformation. The\\nreduced number of transformation parameters leads to more constrained and\\ndesirable matching results. Experimental results demonstrate better robustness\\nof the proposed method over state-of-the-art methods.\\n</summary>\\n    <author>\\n      <name>Wei Lian</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1701.01035v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1701.01035v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1701.01885v1</id>\\n    <updated>2017-01-07T21:12:24Z</updated>\\n    <published>2017-01-07T21:12:24Z</published>\\n    <title>Group Visual Sentiment Analysis</title>\\n    <summary>  In this paper, we introduce a framework for classifying images according to\\nhigh-level sentiment. We subdivide the task into three primary problems:\\nemotion classification on faces, human pose estimation, and 3D estimation and\\nclustering of groups of people. We introduce novel algorithms for matching body\\nparts to a common individual and clustering people in images based on physical\\nlocation and orientation. Our results outperform several baseline approaches.\\n</summary>\\n    <author>\\n      <name>Zeshan Hussain</name>\\n    </author>\\n    <author>\\n      <name>Tariq Patanam</name>\\n    </author>\\n    <author>\\n      <name>Hardie Cate</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1701.01885v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1701.01885v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1701.07354v1</id>\\n    <updated>2017-01-25T15:35:09Z</updated>\\n    <published>2017-01-25T15:35:09Z</published>\\n    <title>Photographic dataset: playing cards</title>\\n    <summary>  This is a photographic dataset collected for testing image processing\\nalgorithms. The idea is to have images that can exploit the properties of total\\nvariation, therefore a set of playing cards was distributed on the scene. The\\ndataset is made available at www.fips.fi/photographic_dataset2.php\\n</summary>\\n    <author>\\n      <name>David Villacis</name>\\n    </author>\\n    <author>\\n      <name>Santeri Kaupinm\\xc3\\xa4ki</name>\\n    </author>\\n    <author>\\n      <name>Samuli Siltanen</name>\\n    </author>\\n    <author>\\n      <name>Teemu Helenius</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages, 12 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1701.07354v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1701.07354v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.data-an\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1702.00723v1</id>\\n    <updated>2017-02-01T18:32:12Z</updated>\\n    <published>2017-02-01T18:32:12Z</published>\\n    <title>Handwritten Recognition Using SVM, KNN and Neural Network</title>\\n    <summary>  Handwritten recognition (HWR) is the ability of a computer to receive and\\ninterpret intelligible handwritten input from source such as paper documents,\\nphotographs, touch-screens and other devices. In this paper we will using three\\n(3) classification t o re cognize the handwritten which is SVM, KNN and Neural\\nNetwork.\\n</summary>\\n    <author>\\n      <name>Norhidayu Abdul Hamid</name>\\n    </author>\\n    <author>\\n      <name>Nilam Nur Amir Sjarif</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">11 pages ; 22 Figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1702.00723v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1702.00723v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68Txx\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1702.07006v1</id>\\n    <updated>2017-02-22T21:03:49Z</updated>\\n    <published>2017-02-22T21:03:49Z</published>\\n    <title>Synthesising Dynamic Textures using Convolutional Neural Networks</title>\\n    <summary>  Here we present a parametric model for dynamic textures. The model is based\\non spatiotemporal summary statistics computed from the feature representations\\nof a Convolutional Neural Network (CNN) trained on object recognition. We\\ndemonstrate how the model can be used to synthesise new samples of dynamic\\ntextures and to predict motion in simple movies.\\n</summary>\\n    <author>\\n      <name>Christina M. Funke</name>\\n    </author>\\n    <author>\\n      <name>Leon A. Gatys</name>\\n    </author>\\n    <author>\\n      <name>Alexander S. Ecker</name>\\n    </author>\\n    <author>\\n      <name>Matthias Bethge</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1702.07006v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1702.07006v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1702.07963v1</id>\\n    <updated>2017-02-26T00:56:25Z</updated>\\n    <published>2017-02-26T00:56:25Z</published>\\n    <title>Spatially Aware Melanoma Segmentation Using Hybrid Deep Learning\\n  Techniques</title>\\n    <summary>  In this paper, we proposed using a hybrid method that utilises deep\\nconvolutional and recurrent neural networks for accurate delineation of skin\\nlesion of images supplied with ISBI 2017 lesion segmentation challenge. The\\nproposed method was trained using 1800 images and tested on 150 images from\\nISBI 2017 challenge.\\n</summary>\\n    <author>\\n      <name>M. Attia</name>\\n    </author>\\n    <author>\\n      <name>M. Hossny</name>\\n    </author>\\n    <author>\\n      <name>S. Nahavandi</name>\\n    </author>\\n    <author>\\n      <name>A. Yazdabadi</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">ISIC2017</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1702.07963v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1702.07963v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1703.01402v1</id>\\n    <updated>2017-03-04T06:32:15Z</updated>\\n    <published>2017-03-04T06:32:15Z</published>\\n    <title>Skin Lesion Classification Using Deep Multi-scale Convolutional Neural\\n  Networks</title>\\n    <summary>  We present a deep learning approach to the ISIC 2017 Skin Lesion\\nClassification Challenge using a multi-scale convolutional neural network. Our\\napproach utilizes an Inception-v3 network pre-trained on the ImageNet dataset,\\nwhich is fine-tuned for skin lesion classification using two different scales\\nof input images.\\n</summary>\\n    <author>\\n      <name>Terrance DeVries</name>\\n    </author>\\n    <author>\\n      <name>Dhanesh Ramachandram</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1703.01402v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1703.01402v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1703.03108v1</id>\\n    <updated>2017-03-09T02:35:59Z</updated>\\n    <published>2017-03-09T02:35:59Z</published>\\n    <title>Image Classification of Melanoma, Nevus and Seborrheic Keratosis by Deep\\n  Neural Network Ensemble</title>\\n    <summary>  This short paper reports the method and the evaluation results of Casio and\\nShinshu University joint team for the ISBI Challenge 2017 - Skin Lesion\\nAnalysis Towards Melanoma Detection - Part 3: Lesion Classification hosted by\\nISIC. Our online validation score was 0.958 with melanoma classifier AUC 0.924\\nand seborrheic keratosis classifier AUC 0.993.\\n</summary>\\n    <author>\\n      <name>Kazuhisa Matsunaga</name>\\n    </author>\\n    <author>\\n      <name>Akira Hamada</name>\\n    </author>\\n    <author>\\n      <name>Akane Minagawa</name>\\n    </author>\\n    <author>\\n      <name>Hiroshi Koga</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages. 3 figures. ISIC2017</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1703.03108v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1703.03108v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1703.03186v1</id>\\n    <updated>2017-03-09T09:14:40Z</updated>\\n    <published>2017-03-09T09:14:40Z</published>\\n    <title>Segmenting Dermoscopic Images</title>\\n    <summary>  We propose an automatic algorithm, named SDI, for the segmentation of skin\\nlesions in dermoscopic images, articulated into three main steps: selection of\\nthe image ROI, selection of the segmentation band, and segmentation. We present\\nextensive experimental results achieved by the SDI algorithm on the lesion\\nsegmentation dataset made available for the ISIC 2017 challenge on Skin Lesion\\nAnalysis Towards Melanoma Detection, highlighting its advantages and\\ndisadvantages.\\n</summary>\\n    <author>\\n      <name>Mario Rosario Guarracino</name>\\n    </author>\\n    <author>\\n      <name>Lucia Maddalena</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1703.03186v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1703.03186v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1703.03888v1</id>\\n    <updated>2017-03-11T01:18:14Z</updated>\\n    <published>2017-03-11T01:18:14Z</published>\\n    <title>Segmentation of skin lesions based on fuzzy classification of pixels and\\n  histogram thresholding</title>\\n    <summary>  This paper proposes an innovative method for segmentation of skin lesions in\\ndermoscopy images developed by the authors, based on fuzzy classification of\\npixels and histogram thresholding.\\n</summary>\\n    <author>\\n      <name>Jose Luis Garcia-Arroyo</name>\\n    </author>\\n    <author>\\n      <name>Begonya Garcia-Zapirain</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages, 6 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1703.03888v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1703.03888v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1703.08366v1</id>\\n    <updated>2017-03-24T11:39:26Z</updated>\\n    <published>2017-03-24T11:39:26Z</published>\\n    <title>A Hybrid Deep Learning Approach for Texture Analysis</title>\\n    <summary>  Texture classification is a problem that has various applications such as\\nremote sensing and forest species recognition. Solutions tend to be custom fit\\nto the dataset used but fails to generalize. The Convolutional Neural Network\\n(CNN) in combination with Support Vector Machine (SVM) form a robust selection\\nbetween powerful invariant feature extractor and accurate classifier. The\\nfusion of experts provides stability in classification rates among different\\ndatasets.\\n</summary>\\n    <author>\\n      <name>Hussein Adly</name>\\n    </author>\\n    <author>\\n      <name>Mohamed Moustafa</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1703.08366v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1703.08366v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1704.02956v1</id>\\n    <updated>2017-04-10T17:13:00Z</updated>\\n    <published>2017-04-10T17:13:00Z</published>\\n    <title>Surface Normals in the Wild</title>\\n    <summary>  We study the problem of single-image depth estimation for images in the wild.\\nWe collect human annotated surface normals and use them to train a neural\\nnetwork that directly predicts pixel-wise depth. We propose two novel loss\\nfunctions for training with surface normal annotations. Experiments on NYU\\nDepth and our own dataset demonstrate that our approach can significantly\\nimprove the quality of depth estimation in the wild.\\n</summary>\\n    <author>\\n      <name>Weifeng Chen</name>\\n    </author>\\n    <author>\\n      <name>Donglai Xiang</name>\\n    </author>\\n    <author>\\n      <name>Jia Deng</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1704.02956v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1704.02956v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1704.03966v1</id>\\n    <updated>2017-04-13T01:41:30Z</updated>\\n    <published>2017-04-13T01:41:30Z</published>\\n    <title>Collaborative Low-Rank Subspace Clustering</title>\\n    <summary>  In this paper we present Collaborative Low-Rank Subspace Clustering. Given\\nmultiple observations of a phenomenon we learn a unified representation matrix.\\nThis unified matrix incorporates the features from all the observations, thus\\nincreasing the discriminative power compared with learning the representation\\nmatrix on each observation separately. Experimental evaluation shows that our\\nmethod outperforms subspace clustering on separate observations and the state\\nof the art collaborative learning algorithm.\\n</summary>\\n    <author>\\n      <name>Stephen Tierney</name>\\n    </author>\\n    <author>\\n      <name>Yi Guo</name>\\n    </author>\\n    <author>\\n      <name>Junbin Gao</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1704.03966v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1704.03966v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1705.01148v1</id>\\n    <updated>2017-05-02T19:21:51Z</updated>\\n    <published>2017-05-02T19:21:51Z</published>\\n    <title>Recovery of structure of looped jointed objects from multiframes</title>\\n    <summary>  A method to recover structural parameters of looped jointed objects from\\nmultiframes is being developed. Each rigid part of the jointed body needs only\\nto be traced at two (that is at junction) points.\\n  This method has been linearized for 4-part loops, with recovery from at least\\n19 frames.\\n</summary>\\n    <author>\\n      <name>Mieczys\\xc5\\x82aw K\\xc5\\x82opotek</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">a preliminary version for Machine Graphics and Vision, Vol. 3 No.\\n  4, pp. 645-656, 1995</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1705.01148v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1705.01148v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1705.01809v1</id>\\n    <updated>2017-05-04T12:20:56Z</updated>\\n    <published>2017-05-04T12:20:56Z</published>\\n    <title>Pixel Normalization from Numeric Data as Input to Neural Networks</title>\\n    <summary>  Text to image transformation for input to neural networks requires\\nintermediate steps. This paper attempts to present a new approach to pixel\\nnormalization so as to convert textual data into image, suitable as input for\\nneural networks. This method can be further improved by its Graphics Processing\\nUnit (GPU) implementation to provide significant speedup in computational time.\\n</summary>\\n    <author>\\n      <name>Parth Sane</name>\\n    </author>\\n    <author>\\n      <name>Ravindra Agrawal</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IEEE WiSPNET 2017 conference in Chennai</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1705.01809v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1705.01809v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.SE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1705.04272v1</id>\\n    <updated>2017-04-29T11:14:16Z</updated>\\n    <published>2017-04-29T11:14:16Z</published>\\n    <title>Improved underwater image enhancement algorithms based on partial\\n  differential equations (PDEs)</title>\\n    <summary>  The experimental results of improved underwater image enhancement algorithms\\nbased on partial differential equations (PDEs) are presented in this report.\\nThis second work extends the study of previous work and incorporating several\\nimprovements into the revised algorithm. Experiments show the evidence of the\\nimprovements when compared to previously proposed approaches and other\\nconventional algorithms found in the literature.\\n</summary>\\n    <author>\\n      <name>U. A. Nnolim</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">22 pages, 6 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1705.04272v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1705.04272v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1706.00083v1</id>\\n    <updated>2017-05-31T20:44:55Z</updated>\\n    <published>2017-05-31T20:44:55Z</published>\\n    <title>Blood capillaries and vessels segmentation in optical coherence\\n  tomography angiogram using fuzzy C-means and Curvelet transform</title>\\n    <summary>  This paper has been removed from arXiv as the submitter did not have\\nownership of the data presented in this work.\\n</summary>\\n    <author>\\n      <name>Fariborz Taherkhani</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">arXiv admin note: This paper has been removed from arXiv as the\\n  submitter did not have ownership of the data presented in this work</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1706.00083v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1706.00083v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1706.03497v1</id>\\n    <updated>2017-06-12T08:04:42Z</updated>\\n    <published>2017-06-12T08:04:42Z</published>\\n    <title>A filter based approach for inbetweening</title>\\n    <summary>  We present a filter based approach for inbetweening. We train a convolutional\\nneural network to generate intermediate frames. This network aim to generate\\nsmooth animation of line drawings. Our method can process scanned images\\ndirectly. Our method does not need to compute correspondence of lines and\\ntopological changes explicitly. We experiment our method with real animation\\nproduction data. The results show that our method can generate intermediate\\nframes partially.\\n</summary>\\n    <author>\\n      <name>Yuichi Yagi</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages, in Japanese</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1706.03497v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1706.03497v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.GR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1706.05534v1</id>\\n    <updated>2017-06-17T13:33:29Z</updated>\\n    <published>2017-06-17T13:33:29Z</published>\\n    <title>Rotation Invariance Neural Network</title>\\n    <summary>  Rotation invariance and translation invariance have great values in image\\nrecognition tasks. In this paper, we bring a new architecture in convolutional\\nneural network (CNN) named cyclic convolutional layer to achieve rotation\\ninvariance in 2-D symbol recognition. We can also get the position and\\norientation of the 2-D symbol by the network to achieve detection purpose for\\nmultiple non-overlap target. Last but not least, this architecture can achieve\\none-shot learning in some cases using those invariance.\\n</summary>\\n    <author>\\n      <name>Shiyuan Li</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages, 4 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1706.05534v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1706.05534v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1706.06230v1</id>\\n    <updated>2017-06-20T00:43:22Z</updated>\\n    <published>2017-06-20T00:43:22Z</published>\\n    <title>A Bayesian algorithm for detecting identity matches and fraud in image\\n  databases</title>\\n    <summary>  A statistical algorithm for categorizing different types of matches and fraud\\nin image databases is presented. The approach is based on a generative model of\\na graph representing images and connections between pairs of identities,\\ntrained using properties of a matching algorithm between images.\\n</summary>\\n    <author>\\n      <name>Gaurav Thakur</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1706.06230v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1706.06230v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1706.07757v1</id>\\n    <updated>2017-04-14T14:54:14Z</updated>\\n    <published>2017-04-14T14:54:14Z</published>\\n    <title>Improved Human Emotion Recognition Using Symmetry of Facial Key Points\\n  with Dihedral Group</title>\\n    <summary>  This article describes how to deploy dihedral group theory to detect Facial\\nKey Points (FKP) symmetry to recognize emotions. The method can be applied in\\nmany other areas which those have the same data texture.\\n</summary>\\n    <author>\\n      <name>Mehdi Ghayoumi</name>\\n    </author>\\n    <author>\\n      <name>Arvind Bansal</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IJASCSE Volume 6 Issue 01 2017</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1706.07757v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1706.07757v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1707.01159v1</id>\\n    <updated>2017-07-04T21:34:08Z</updated>\\n    <published>2017-07-04T21:34:08Z</published>\\n    <title>UPSET and ANGRI : Breaking High Performance Image Classifiers</title>\\n    <summary>  In this paper, targeted fooling of high performance image classifiers is\\nachieved by developing two novel attack methods. The first method generates\\nuniversal perturbations for target classes and the second generates image\\nspecific perturbations. Extensive experiments are conducted on MNIST and\\nCIFAR10 datasets to provide insights about the proposed algorithms and show\\ntheir effectiveness.\\n</summary>\\n    <author>\\n      <name>Sayantan Sarkar</name>\\n    </author>\\n    <author>\\n      <name>Ankan Bansal</name>\\n    </author>\\n    <author>\\n      <name>Upal Mahbub</name>\\n    </author>\\n    <author>\\n      <name>Rama Chellappa</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1707.01159v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1707.01159v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1707.02051v1</id>\\n    <updated>2017-07-07T06:27:54Z</updated>\\n    <published>2017-07-07T06:27:54Z</published>\\n    <title>Image Segmentation Algorithms Overview</title>\\n    <summary>  The technology of image segmentation is widely used in medical image\\nprocessing, face recognition pedestrian detection, etc. The current image\\nsegmentation techniques include region-based segmentation, edge detection\\nsegmentation, segmentation based on clustering, segmentation based on\\nweakly-supervised learning in CNN, etc. This paper analyzes and summarizes\\nthese algorithms of image segmentation, and compares the advantages and\\ndisadvantages of different algorithms. Finally, we make a prediction of the\\ndevelopment trend of image segmentation with the combination of these\\nalgorithms.\\n</summary>\\n    <author>\\n      <name>Song Yuheng</name>\\n    </author>\\n    <author>\\n      <name>Yan Hao</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1707.02051v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1707.02051v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1707.06825v1</id>\\n    <updated>2017-07-21T10:17:33Z</updated>\\n    <published>2017-07-21T10:17:33Z</published>\\n    <title>Evaluation of Hashing Methods Performance on Binary Feature Descriptors</title>\\n    <summary>  In this paper we evaluate performance of data-dependent hashing methods on\\nbinary data. The goal is to find a hashing method that can effectively produce\\nlower dimensional binary representation of 512-bit FREAK descriptors. A\\nrepresentative sample of recent unsupervised, semi-supervised and supervised\\nhashing methods was experimentally evaluated on large datasets of labelled\\nbinary FREAK feature descriptors.\\n</summary>\\n    <author>\\n      <name>Jacek Komorowski</name>\\n    </author>\\n    <author>\\n      <name>Tomasz Trzcinski</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1707.06825v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1707.06825v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1707.08722v1</id>\\n    <updated>2017-07-27T07:00:02Z</updated>\\n    <published>2017-07-27T07:00:02Z</published>\\n    <title>Algebraic Relations and Triangulation of Unlabeled Image Points</title>\\n    <summary>  In multiview geometry when correspondences among multiple views are unknown\\nthe image points can be understood as being unlabeled. This is a common problem\\nin computer vision. We give a novel approach to handle such a situation by\\nregarding unlabeled point configurations as points on the Chow variety\\n$\\\\text{Sym}_m(\\\\mathbb{P}^2)$. For two unlabeled points we design an algorithm\\nthat solves the triangulation problem with unknown correspondences. Further the\\nunlabeled multiview variety $\\\\text{Sym}_m(V_A)$ is studied.\\n</summary>\\n    <author>\\n      <name>Andr\\xc3\\xa9 Wagner</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">17 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1707.08722v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1707.08722v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1707.09869v1</id>\\n    <updated>2017-07-23T01:35:55Z</updated>\\n    <published>2017-07-23T01:35:55Z</published>\\n    <title>A comment on the paper Prediction of Kidney Function from Biopsy Images\\n  using Convolutional Neural Networks</title>\\n    <summary>  This letter presente a comment on the paper Prediction of Kidney Function\\nfrom Biopsy Images using Convolutional Neural Networks by Ledbetter et al.\\n(2017)\\n</summary>\\n    <author>\\n      <name>Washington LC dos-Santos</name>\\n    </author>\\n    <author>\\n      <name>Angelo A Duarte</name>\\n    </author>\\n    <author>\\n      <name>Luiz AR de Freitas</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">2 pages, 1 figure</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1707.09869v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1707.09869v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1709.05867v1</id>\\n    <updated>2017-09-18T11:28:15Z</updated>\\n    <published>2017-09-18T11:28:15Z</published>\\n    <title>Combinational neural network using Gabor filters for the classification\\n  of handwritten digits</title>\\n    <summary>  A classification algorithm that combines the components of k-nearest\\nneighbours and multilayer neural networks has been designed and tested. With\\nthis method the computational time required for training the dataset has been\\nreduced substancially. Gabor filters were used for the feature extraction to\\nensure a better performance. This algorithm is tested with MNIST dataset and it\\nwill be integrated as a module in the object recognition software which is\\ncurrently under development.\\n</summary>\\n    <author>\\n      <name>N. Joshi</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1709.05867v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1709.05867v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1710.01462v1</id>\\n    <updated>2017-10-04T05:20:41Z</updated>\\n    <published>2017-10-04T05:20:41Z</published>\\n    <title>Secrets in Computing Optical Flow by Convolutional Networks</title>\\n    <summary>  Convolutional neural networks (CNNs) have been widely used over many areas in\\ncompute vision. Especially in classification. Recently, FlowNet and several\\nworks on opti- cal estimation using CNNs shows the potential ability of CNNs in\\ndoing per-pixel regression. We proposed several CNNs network architectures that\\ncan estimate optical flow, and fully unveiled the intrinsic different between\\nthese structures.\\n</summary>\\n    <author>\\n      <name>Junxuan Li</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1710.01462v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1710.01462v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1710.08011v1</id>\\n    <updated>2017-10-22T20:48:49Z</updated>\\n    <published>2017-10-22T20:48:49Z</published>\\n    <title>ActivityNet Challenge 2017 Summary</title>\\n    <summary>  The ActivityNet Large Scale Activity Recognition Challenge 2017 Summary:\\nresults and challenge participants papers.\\n</summary>\\n    <author>\\n      <name>Bernard Ghanem</name>\\n    </author>\\n    <author>\\n      <name>Juan Carlos Niebles</name>\\n    </author>\\n    <author>\\n      <name>Cees Snoek</name>\\n    </author>\\n    <author>\\n      <name>Fabian Caba Heilbron</name>\\n    </author>\\n    <author>\\n      <name>Humam Alwassel</name>\\n    </author>\\n    <author>\\n      <name>Ranjay Khrisna</name>\\n    </author>\\n    <author>\\n      <name>Victor Escorcia</name>\\n    </author>\\n    <author>\\n      <name>Kenji Hata</name>\\n    </author>\\n    <author>\\n      <name>Shyamal Buch</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">76 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1710.08011v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1710.08011v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1712.07286v4</id>\\n    <updated>2018-11-19T02:16:17Z</updated>\\n    <published>2017-12-20T01:38:53Z</published>\\n    <title>LVreID: Person Re-Identification with Long Sequence Videos</title>\\n    <summary>  This paper mainly establishes a large-scale Long sequence Video database for\\nperson re-IDentification (LVreID).\\n</summary>\\n    <author>\\n      <name>Jianing Li</name>\\n    </author>\\n    <author>\\n      <name>Shiliang Zhang</name>\\n    </author>\\n    <author>\\n      <name>Jingdong Wang</name>\\n    </author>\\n    <author>\\n      <name>Wen Gao</name>\\n    </author>\\n    <author>\\n      <name>Qi Tian</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">There is experimental error in secction 5.7</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1712.07286v4\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1712.07286v4\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1801.06104v2</id>\\n    <updated>2018-05-09T12:18:09Z</updated>\\n    <published>2018-01-18T15:53:00Z</published>\\n    <title>Invariants of multidimensional time series based on their\\n  iterated-integral signature</title>\\n    <summary>  We introduce a novel class of features for multidimensional time series, that\\nare invariant with respect to transformations of the ambient space. The general\\nlinear group, the group of rotations and the group of permutations of the axes\\nare considered. The starting point for their construction is Chen\\'s\\niterated-integral signature.\\n</summary>\\n    <author>\\n      <name>Joscha Diehl</name>\\n    </author>\\n    <author>\\n      <name>Jeremy Reizenstein</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">complete rewrite of Section 3.3</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1801.06104v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1801.06104v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.RT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1801.06504v2</id>\\n    <updated>2018-01-24T16:04:15Z</updated>\\n    <published>2018-01-19T17:41:12Z</published>\\n    <title>Detecting and counting tiny faces</title>\\n    <summary>  Finding Tiny Faces (by Hu and Ramanan) proposes a novel approach to find\\nsmall objects in an image. Our contribution consists in deeply understanding\\nthe choices of the paper together with applying and extending a similar method\\nto a real world subject which is the counting of people in a public\\ndemonstration.\\n</summary>\\n    <author>\\n      <name>Alexandre Attia</name>\\n    </author>\\n    <author>\\n      <name>Sharone Dayan</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, 10 figures, 2 appendix page</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1801.06504v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1801.06504v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1801.06694v1</id>\\n    <updated>2018-01-20T16:08:12Z</updated>\\n    <published>2018-01-20T16:08:12Z</published>\\n    <title>Determination of Digital Straight Segments Using the Slope</title>\\n    <summary>  We present a new method for the recognition of digital straight lines based\\non the slope. This method combines the Freeman\\'s chain coding scheme and new\\ndiscovered properties of the digital slope introduced in this paper. We also\\npresent the efficiency of our method from a testbed.\\n</summary>\\n    <author>\\n      <name>Alejandro Cartas</name>\\n    </author>\\n    <author>\\n      <name>Mar\\xc3\\xada Elena Algorri</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1801.06694v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1801.06694v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1803.05779v1</id>\\n    <updated>2018-01-19T06:30:59Z</updated>\\n    <published>2018-01-19T06:30:59Z</published>\\n    <title>A predictor-corrector method for the training of deep neural networks</title>\\n    <summary>  The training of deep neural nets is expensive. We present a predictor-\\ncorrector method for the training of deep neural nets. It alternates a\\npredictor pass with a corrector pass using stochastic gradient descent with\\nbackpropagation such that there is no loss in validation accuracy. No special\\nmodifications to SGD with backpropagation is required by this methodology. Our\\nexperiments showed a time improvement of 9% on the CIFAR-10 dataset.\\n</summary>\\n    <author>\\n      <name>Yatin Saraiya</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages, 2 figures, 2 tables</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1803.05779v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1803.05779v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1803.05785v1</id>\\n    <updated>2018-03-15T14:48:47Z</updated>\\n    <published>2018-03-15T14:48:47Z</published>\\n    <title>Aggregated Sparse Attention for Steering Angle Prediction</title>\\n    <summary>  In this paper, we apply the attention mechanism to autonomous driving for\\nsteering angle prediction. We propose the first model, applying the recently\\nintroduced sparse attention mechanism to visual domain, as well as the\\naggregated extension for this model. We show the improvement of the proposed\\nmethod, comparing to no attention as well as to different types of attention.\\n</summary>\\n    <author>\\n      <name>Sen He</name>\\n    </author>\\n    <author>\\n      <name>Dmitry Kangin</name>\\n    </author>\\n    <author>\\n      <name>Yang Mi</name>\\n    </author>\\n    <author>\\n      <name>Nicolas Pugeault</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1803.05785v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1803.05785v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1803.07608v1</id>\\n    <updated>2018-03-20T19:12:05Z</updated>\\n    <published>2018-03-20T19:12:05Z</published>\\n    <title>A Survey of Deep Learning Techniques for Mobile Robot Applications</title>\\n    <summary>  Advancements in deep learning over the years have attracted research into how\\ndeep artificial neural networks can be used in robotic systems. This research\\nsurvey will present a summarization of the current research with a specific\\nfocus on the gains and obstacles for deep learning to be applied to mobile\\nrobotics.\\n</summary>\\n    <author>\\n      <name>Jahanzaib Shabbir</name>\\n    </author>\\n    <author>\\n      <name>Tarique Anwer</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1803.07608v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1803.07608v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1804.02543v1</id>\\n    <updated>2018-04-07T10:24:04Z</updated>\\n    <published>2018-04-07T10:24:04Z</published>\\n    <title>Not quite unreasonable effectiveness of machine learning algorithms</title>\\n    <summary>  State-of-the-art machine learning algorithms demonstrate close to absolute\\nperformance in selected challenges. We provide arguments that the reason can be\\nin low variability of the samples and high effectiveness in learning typical\\npatterns. Due to this fact, standard performance metrics do not reveal model\\ncapacity and new metrics are required for the better understanding of\\nstate-of-the-art.\\n</summary>\\n    <author>\\n      <name>Egor Illarionov</name>\\n    </author>\\n    <author>\\n      <name>Roman Khudorozhkov</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1804.02543v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1804.02543v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1804.03286v1</id>\\n    <updated>2018-04-10T04:54:29Z</updated>\\n    <published>2018-04-10T04:54:29Z</published>\\n    <title>On the Robustness of the CVPR 2018 White-Box Adversarial Example\\n  Defenses</title>\\n    <summary>  Neural networks are known to be vulnerable to adversarial examples. In this\\nnote, we evaluate the two white-box defenses that appeared at CVPR 2018 and\\nfind they are ineffective: when applying existing techniques, we can reduce the\\naccuracy of the defended models to 0%.\\n</summary>\\n    <author>\\n      <name>Anish Athalye</name>\\n    </author>\\n    <author>\\n      <name>Nicholas Carlini</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1804.03286v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1804.03286v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1806.05233v1</id>\\n    <updated>2018-06-13T19:23:51Z</updated>\\n    <published>2018-06-13T19:23:51Z</published>\\n    <title>End-to-End Parkinson Disease Diagnosis using Brain MR-Images by 3D-CNN</title>\\n    <summary>  In this work, we use a deep learning framework for simultaneous\\nclassification and regression of Parkinson disease diagnosis based on MR-Images\\nand personal information (i.e. age, gender). We intend to facilitate and\\nincrease the confidence in Parkinson disease diagnosis through our deep\\nlearning framework.\\n</summary>\\n    <author>\\n      <name>Soheil Esmaeilzadeh</name>\\n    </author>\\n    <author>\\n      <name>Yao Yang</name>\\n    </author>\\n    <author>\\n      <name>Ehsan Adeli</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1806.05233v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1806.05233v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.00273v1</id>\\n    <updated>2018-07-01T05:28:27Z</updated>\\n    <published>2018-07-01T05:28:27Z</published>\\n    <title>Photorealistic Style Transfer for Videos</title>\\n    <summary>  Photorealistic style transfer is a technique which transfers colour from one\\nreference domain to another domain by using deep learning and optimization\\ntechniques. Here, we present a technique which we use to transfer style and\\ncolour from a reference image to a video.\\n</summary>\\n    <author>\\n      <name>Michael Honke</name>\\n    </author>\\n    <author>\\n      <name>Rahul Iyer</name>\\n    </author>\\n    <author>\\n      <name>Dishant Mittal</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1807.00273v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.00273v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.00686v1</id>\\n    <updated>2018-06-29T07:49:08Z</updated>\\n    <published>2018-06-29T07:49:08Z</published>\\n    <title>YH Technologies at ActivityNet Challenge 2018</title>\\n    <summary>  This notebook paper presents an overview and comparative analysis of our\\nsystems designed for the following five tasks in ActivityNet Challenge 2018:\\ntemporal action proposals, temporal action localization, dense-captioning\\nevents in videos, trimmed action recognition, and spatio-temporal action\\nlocalization.\\n</summary>\\n    <author>\\n      <name>Ting Yao</name>\\n    </author>\\n    <author>\\n      <name>Xue Li</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Rank 2 in both Temporal Activity Detection Task &amp; Kinetics Task @\\n  ActivityNet 2018. arXiv admin note: substantial text overlap with\\n  arXiv:1710.08011 by other authors</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1807.00686v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.00686v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.06644v1</id>\\n    <updated>2018-07-17T20:03:19Z</updated>\\n    <published>2018-07-17T20:03:19Z</published>\\n    <title>A Framework for Moment Invariants</title>\\n    <summary>  For more than half a century, moments have attracted lot ot interest in the\\npattern recognition community.The moments of a distribution (an object) provide\\nseveral of its characteristics as center of gravity, orientation, disparity,\\nvolume. Moments can be used to define invariant characteristics to some\\ntransformations that an object can undergo, commonly called moment invariants.\\nThis work provides a simple and systematic formalism to compute geometric\\nmoment invariants in n-dimensional space.\\n</summary>\\n    <author>\\n      <name>Omar Tahri</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1807.06644v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.06644v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.08332v1</id>\\n    <updated>2018-07-22T18:07:50Z</updated>\\n    <published>2018-07-22T18:07:50Z</published>\\n    <title>Skin Lesion Analysis Towards Melanoma Detection via End-to-end Deep\\n  Learning of Convolutional Neural Networks</title>\\n    <summary>  This article presents the design, experiments and results of our solution\\nsubmitted to the 2018 ISIC challenge: Skin Lesion Analysis Towards Melanoma\\nDetection. We design a pipeline using state-of-the-art Convolutional Neural\\nNetwork (CNN) models for a Lesion Boundary Segmentation task and a Lesion\\nDiagnosis task.\\n</summary>\\n    <author>\\n      <name>Katherine M. Li</name>\\n    </author>\\n    <author>\\n      <name>Evelyn C. Li</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1807.08332v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.08332v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.08471v2</id>\\n    <updated>2018-07-25T09:31:46Z</updated>\\n    <published>2018-07-23T08:14:36Z</published>\\n    <title>Deep attention-guided fusion network for lesion segmentation</title>\\n    <summary>  We participated the Task 1: Lesion Segmentation. The paper describes our\\nalgorithm and the final result of validation set for the ISIC Challenge 2018 -\\nSkin Lesion Analysis Towards Melanoma Detection.\\n</summary>\\n    <author>\\n      <name>Hengliang Zhu</name>\\n    </author>\\n    <author>\\n      <name>Yangyang Hao</name>\\n    </author>\\n    <author>\\n      <name>Lizhuang Ma</name>\\n    </author>\\n    <author>\\n      <name>Ruixing Li</name>\\n    </author>\\n    <author>\\n      <name>Hua Wang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1807.08471v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.08471v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.09083v1</id>\\n    <updated>2018-07-24T13:17:55Z</updated>\\n    <published>2018-07-24T13:17:55Z</published>\\n    <title>ISIC 2017 Skin Lesion Segmentation Using Deep Encoder-Decoder Network</title>\\n    <summary>  This paper summarizes our method and validation results for part 1 of the\\nISBI Challenge 2018. Our algorithm makes use of deep encoder-decoder network\\nand novel skin lesion data augmentation to segment the challenge objective.\\nBesides, we also propose an effective testing strategy by applying multi-model\\ncomparison.\\n</summary>\\n    <author>\\n      <name>Ngoc-Quang Nguyen</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">ISIC 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1807.09083v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.09083v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.09163v1</id>\\n    <updated>2018-07-24T14:48:57Z</updated>\\n    <published>2018-07-24T14:48:57Z</published>\\n    <title>Skin disease identification from dermoscopy images using deep\\n  convolutional neural network</title>\\n    <summary>  In this paper, a deep neural network based ensemble method is experimented\\nfor automatic identification of skin disease from dermoscopic images. The\\ndeveloped algorithm is applied on the task3 of the ISIC 2018 challenge dataset\\n(Skin Lesion Analysis Towards Melanoma Detection).\\n</summary>\\n    <author>\\n      <name>Anabik Pal</name>\\n    </author>\\n    <author>\\n      <name>Sounak Ray</name>\\n    </author>\\n    <author>\\n      <name>Utpal Garain</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Challenge Participation in ISIC 2018: Skin Lesion Analysis Towards\\n  Melanoma Detection</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1807.09163v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.09163v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.09312v1</id>\\n    <updated>2018-07-24T19:14:29Z</updated>\\n    <published>2018-07-24T19:14:29Z</published>\\n    <title>A Simple Probabilistic Model for Uncertainty Estimation</title>\\n    <summary>  The article focuses on determining the predictive uncertainty of a model on\\nthe example of atrial fibrillation detection problem by a single-lead ECG\\nsignal. To this end, the model predicts parameters of the beta distribution\\nover class probabilities instead of these probabilities themselves. It was\\nshown that the described approach allows to detect atypical recordings and\\nsignificantly improve the quality of the algorithm on confident predictions.\\n</summary>\\n    <author>\\n      <name>Alexander Kuvaev</name>\\n    </author>\\n    <author>\\n      <name>Roman Khudorozhkov</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1807.09312v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.09312v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1808.02373v2</id>\\n    <updated>2018-08-14T01:59:29Z</updated>\\n    <published>2018-08-04T11:57:27Z</published>\\n    <title>Troy: Give Attention to Saliency and for Saliency</title>\\n    <summary>  In addition, our work has text overlap with arXiv:1804.06242,\\narXiv:1705.00938 by other authors. We want to rewrite this paper for avoiding\\nthis fact.\\n</summary>\\n    <author>\\n      <name>Pingping Zhang</name>\\n    </author>\\n    <author>\\n      <name>Huchuan Lu</name>\\n    </author>\\n    <author>\\n      <name>Chunhua Shen</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">All of authors agree to withdrawal this paper because we have noticed\\n  several important errors</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1808.02373v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1808.02373v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1809.05076v1</id>\\n    <updated>2018-09-13T17:33:18Z</updated>\\n    <published>2018-09-13T17:33:18Z</published>\\n    <title>Computer Vision-aided Atom Tracking in STEM Imaging</title>\\n    <summary>  To address the SMC\\'17 data challenge -- \"Data mining atomically resolved\\nimages for material properties\", we first used the classic \"blob detection\"\\nalgorithms developed in computer vision to identify all atom centers in each\\nSTEM image frame. With the help of nearest neighbor analysis, we then found and\\nlabeled every atom center common to all the STEM frames and tracked their\\nmovements through the given time interval for both Molybdenum or Selenium\\natoms.\\n</summary>\\n    <author>\\n      <name>Yawei Hui</name>\\n    </author>\\n    <author>\\n      <name>Yaohua Liu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1809.05076v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1809.05076v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1809.06079v1</id>\\n    <updated>2018-09-17T08:59:22Z</updated>\\n    <published>2018-09-17T08:59:22Z</published>\\n    <title>An Integral Pose Regression System for the ECCV2018 PoseTrack Challenge</title>\\n    <summary>  For the ECCV 2018 PoseTrack Challenge, we present a 3D human pose estimation\\nsystem based mainly on the integral human pose regression method. We show a\\ncomprehensive ablation study to examine the key performance factors of the\\nproposed system. Our system obtains 47mm MPJPE on the CHALL_H80K test dataset,\\nplacing second in the ECCV2018 3D human pose estimation challenge. Code will be\\nreleased to facilitate future work.\\n</summary>\\n    <author>\\n      <name>Xiao Sun</name>\\n    </author>\\n    <author>\\n      <name>Chuankang Li</name>\\n    </author>\\n    <author>\\n      <name>Stephen Lin</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1809.06079v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1809.06079v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1809.11066v1</id>\\n    <updated>2018-09-28T14:42:29Z</updated>\\n    <published>2018-09-28T14:42:29Z</published>\\n    <title>Camera Pose Estimation from Sequence of Calibrated Images</title>\\n    <summary>  In this paper a method for camera pose estimation from a sequence of images\\nis presented. The method assumes camera is calibrated (intrinsic parameters are\\nknown) which allows to decrease a number of required pairs of corresponding\\npoints compared to uncalibrated case. Our algorithm can be used as a first\\nstage in a structure from motion stereo reconstruction system.\\n</summary>\\n    <author>\\n      <name>Jacek Komorowski</name>\\n    </author>\\n    <author>\\n      <name>Przemyslaw Rokita</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1809.11066v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1809.11066v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1810.00965v1</id>\\n    <updated>2018-10-01T20:37:07Z</updated>\\n    <published>2018-10-01T20:37:07Z</published>\\n    <title>Natural measures of alignment</title>\\n    <summary>  Natural coordinate system will be proposed. In this coordinate system\\nalignment procedure of a device and a detector can be easily performed. This\\napproach is generalization of previous specific formulas in the field of\\ncalibration and provide top level description of the procedure. A basic example\\napplication to linac therapy plan is also provided.\\n</summary>\\n    <author>\\n      <name>R. A. Kycia</name>\\n    </author>\\n    <author>\\n      <name>Z. Tabor</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">12 pages, 1 figure</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1810.00965v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1810.00965v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.GR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.06287v1</id>\\n    <updated>2018-11-15T10:44:35Z</updated>\\n    <published>2018-11-15T10:44:35Z</published>\\n    <title>Sketch based Reduced Memory Hough Transform</title>\\n    <summary>  This paper proposes using sketch algorithms to represent the votes in Hough\\ntransforms. Replacing the accumulator array with a sketch (Sketch Hough\\nTransform - SHT) significantly reduces the memory needed to compute a Hough\\ntransform. We also present a new sketch, Count Median Update, which works\\nbetter than known sketch methods for replacing the accumulator array in the\\nHough Transform.\\n</summary>\\n    <author>\\n      <name>Levi Offen</name>\\n    </author>\\n    <author>\\n      <name>Michael Werman</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">2018 25th IEEE International Conference on Image Processing (ICIP)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1811.06287v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.06287v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"1\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.10355v1</id>\\n    <updated>2018-11-26T13:22:17Z</updated>\\n    <published>2018-11-26T13:22:17Z</published>\\n    <title>Unsupervised learning with sparse space-and-time autoencoders</title>\\n    <summary>  We use spatially-sparse two, three and four dimensional convolutional\\nautoencoder networks to model sparse structures in 2D space, 3D space, and\\n3+1=4 dimensional space-time. We evaluate the resulting latent spaces by\\ntesting their usefulness for downstream tasks. Applications are to handwriting\\nrecognition in 2D, segmentation for parts in 3D objects, segmentation for\\nobjects in 3D scenes, and body-part segmentation for 4D wire-frame models\\ngenerated from motion capture data.\\n</summary>\\n    <author>\\n      <name>Benjamin Graham</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1811.10355v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.10355v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.11314v1</id>\\n    <updated>2018-11-27T23:45:58Z</updated>\\n    <published>2018-11-27T23:45:58Z</published>\\n    <title>Skin lesion segmentation using U-Net and good training strategies</title>\\n    <summary>  In this paper we approach the problem of skin lesion segmentation using a\\nconvolutional neural network based on the U-Net architecture. We present a set\\nof training strategies that had a significant impact on the performance of this\\nmodel. We evaluated this method on the ISIC Challenge 2018 - Skin Lesion\\nAnalysis Towards Melanoma Detection, obtaining threshold Jaccard index of\\n77.5%.\\n</summary>\\n    <author>\\n      <name>Fred Guth</name>\\n    </author>\\n    <author>\\n      <name>Teofilo E. deCampos</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1811.11314v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.11314v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68T45\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.2.10\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.03706v5</id>\\n    <updated>2019-05-17T06:39:19Z</updated>\\n    <published>2019-01-13T12:42:42Z</published>\\n    <title>Generating Adversarial Perturbation with Root Mean Square Gradient</title>\\n    <summary>  We focus our attention on the problem of generating adversarial perturbations\\nbased on the gradient in image classification domain\\n</summary>\\n    <author>\\n      <name>Yatie Xiao</name>\\n    </author>\\n    <author>\\n      <name>Chi-Man Pun</name>\\n    </author>\\n    <author>\\n      <name>Jizhe Zhou</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">The formula in Algorithm 1 lacks important representations</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1901.03706v5\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.03706v5\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.05259v1</id>\\n    <updated>2019-01-16T12:31:08Z</updated>\\n    <published>2019-01-16T12:31:08Z</published>\\n    <title>MRI to CT Translation with GANs</title>\\n    <summary>  We present a detailed description and reference implementation of\\npreprocessing steps necessary to prepare the public Retrospective Image\\nRegistration Evaluation (RIRE) dataset for the task of magnetic resonance\\nimaging (MRI) to X-ray computed tomography (CT) translation. Furthermore we\\ndescribe and implement three state of the art convolutional neural network\\n(CNN) and generative adversarial network (GAN) models where we report\\nstatistics and visual results of two of them.\\n</summary>\\n    <author>\\n      <name>Bodo Kaiser</name>\\n    </author>\\n    <author>\\n      <name>Shadi Albarqouni</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">22 pages, 12 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1901.05259v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.05259v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.05531v1</id>\\n    <updated>2019-01-16T21:27:57Z</updated>\\n    <published>2019-01-16T21:27:57Z</published>\\n    <title>Response to \"Visual Dialogue without Vision or Dialogue\" (Massiceti et\\n  al., 2018)</title>\\n    <summary>  In a recent workshop paper, Massiceti et al. presented a baseline model and\\nsubsequent critique of Visual Dialog (Das et al., CVPR 2017) that raises what\\nwe believe to be unfounded concerns about the dataset and evaluation. This\\narticle intends to rebut the critique and clarify potential confusions for\\npractitioners and future participants in the Visual Dialog challenge.\\n</summary>\\n    <author>\\n      <name>Abhishek Das</name>\\n    </author>\\n    <author>\\n      <name>Devi Parikh</name>\\n    </author>\\n    <author>\\n      <name>Dhruv Batra</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1901.05531v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.05531v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.09156v1</id>\\n    <updated>2019-01-26T04:22:30Z</updated>\\n    <published>2019-01-26T04:22:30Z</published>\\n    <title>Human Pose Estimation using Motion Priors and Ensemble Models</title>\\n    <summary>  Human pose estimation in images and videos is one of key technologies for\\nrealizing a variety of human activity recognition tasks (e.g., human-computer\\ninteraction, gesture recognition, surveillance, and video summarization). This\\npaper presents two types of human pose estimation methodologies; 1) 3D human\\npose tracking using motion priors and 2) 2D human pose estimation with ensemble\\nmodeling.\\n</summary>\\n    <author>\\n      <name>Norimichi Ukita</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Presented at the 2017 International Conference on Advanced\\n  Computer Science and Information Systems (ICACSIS)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1901.09156v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.09156v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1902.02831v1</id>\\n    <updated>2019-02-07T20:22:15Z</updated>\\n    <published>2019-02-07T20:22:15Z</published>\\n    <title>Evaluating Crowd Density Estimators via Their Uncertainty Bounds</title>\\n    <summary>  In this work, we use the Belief Function Theory which extends the\\nprobabilistic framework in order to provide uncertainty bounds to different\\ncategories of crowd density estimators. Our method allows us to compare the\\nmulti-scale performance of the estimators, and also to characterize their\\nreliability for crowd monitoring applications requiring varying degrees of\\nprudence.\\n</summary>\\n    <author>\\n      <name>Jennifer Vandoni</name>\\n    </author>\\n    <author>\\n      <name>Emanuel Aldea</name>\\n    </author>\\n    <author>\\n      <name>Sylvie Le H\\xc3\\xa9garat-Mascle</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1902.02831v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1902.02831v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1902.03091v1</id>\\n    <updated>2019-02-08T14:24:36Z</updated>\\n    <published>2019-02-08T14:24:36Z</published>\\n    <title>FocusNet: An attention-based Fully Convolutional Network for Medical\\n  Image Segmentation</title>\\n    <summary>  We propose a novel technique to incorporate attention within convolutional\\nneural networks using feature maps generated by a separate convolutional\\nautoencoder. Our attention architecture is well suited for incorporation with\\ndeep convolutional networks. We evaluate our model on benchmark segmentation\\ndatasets in skin cancer segmentation and lung lesion segmentation. Results show\\nhighly competitive performance when compared with U-Net and it\\'s residual\\nvariant.\\n</summary>\\n    <author>\\n      <name>Chaitanya Kaul</name>\\n    </author>\\n    <author>\\n      <name>Suresh Manandhar</name>\\n    </author>\\n    <author>\\n      <name>Nick Pears</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1902.03091v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1902.03091v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1902.03601v1</id>\\n    <updated>2019-02-10T13:51:47Z</updated>\\n    <published>2019-02-10T13:51:47Z</published>\\n    <title>Vulnerable road user detection: state-of-the-art and open challenges</title>\\n    <summary>  Correctly identifying vulnerable road users (VRUs), e.g. cyclists and\\npedestrians, remains one of the most challenging environment perception tasks\\nfor autonomous vehicles (AVs). This work surveys the current state-of-the-art\\nin VRU detection, covering topics such as benchmarks and datasets, object\\ndetection techniques and relevant machine learning algorithms. The article\\nconcludes with a discussion of remaining open challenges and promising future\\nresearch directions for this domain.\\n</summary>\\n    <author>\\n      <name>Patrick Mannion</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1902.03601v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1902.03601v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1902.05429v1</id>\\n    <updated>2019-02-13T09:09:52Z</updated>\\n    <published>2019-02-13T09:09:52Z</published>\\n    <title>Structured Bayesian Compression for Deep models in mobile enabled\\n  devices for connected healthcare</title>\\n    <summary>  Deep Models, typically Deep neural networks, have millions of parameters,\\nanalyze medical data accurately, yet in a time-consuming method. However,\\nenergy cost effectiveness and computational efficiency are important for\\nprerequisites developing and deploying mobile-enabled devices, the mainstream\\ntrend in connected healthcare.\\n</summary>\\n    <author>\\n      <name>Sijia Chen</name>\\n    </author>\\n    <author>\\n      <name>Bin Song</name>\\n    </author>\\n    <author>\\n      <name>Xiaojiang Du</name>\\n    </author>\\n    <author>\\n      <name>Nadra Guizani</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1902.05429v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1902.05429v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1904.06577v2</id>\\n    <updated>2020-05-30T11:18:01Z</updated>\\n    <published>2019-04-13T17:50:28Z</published>\\n    <title>Direct Sparse Mapping</title>\\n    <summary>  Photometric bundle adjustment, PBA, accurately estimates geometry from video.\\nHowever, current PBA systems have a temporary map that cannot manage scene\\nreobservations. We present, DSM, a full monocular visual SLAM based on PBA. Its\\npersistent map handles reobservations, yielding the most accurate results up to\\ndate on EuRoC for a direct method.\\n</summary>\\n    <author>\\n      <name>Jon Zubizarreta</name>\\n    </author>\\n    <author>\\n      <name>Iker Aguinaga</name>\\n    </author>\\n    <author>\\n      <name>J. M. M. Montiel</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/TRO.2020.2991614</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/TRO.2020.2991614\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted for publication in IEEE Transactions on Robotics</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1904.06577v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1904.06577v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1905.04093v1</id>\\n    <updated>2019-05-10T11:58:10Z</updated>\\n    <published>2019-05-10T11:58:10Z</published>\\n    <title>Towards Unsupervised Familiar Scene Recognition in Egocentric Videos</title>\\n    <summary>  Nowadays, there is an upsurge of interest in using lifelogging devices. Such\\ndevices generate huge amounts of image data; consequently, the need for\\nautomatic methods for analyzing and summarizing these data is drastically\\nincreasing. We present a new method for familiar scene recognition in\\negocentric videos, based on background pattern detection through automatically\\nconfigurable COSFIRE filters. We present some experiments over egocentric data\\nacquired with the Narrative Clip.\\n</summary>\\n    <author>\\n      <name>Estefania Talavera</name>\\n    </author>\\n    <author>\\n      <name>Nicolai Petkov</name>\\n    </author>\\n    <author>\\n      <name>Petia Radeva</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1905.04093v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1905.04093v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1905.04740v1</id>\\n    <updated>2019-05-12T16:38:19Z</updated>\\n    <published>2019-05-12T16:38:19Z</published>\\n    <title>Object Detection in Specific Traffic Scenes using YOLOv2</title>\\n    <summary>  object detection framework plays crucial role in autonomous driving. In this\\npaper, we introduce the real-time object detection framework called You Only\\nLook Once (YOLOv1) and the related improvements of YOLOv2. We further explore\\nthe capability of YOLOv2 by implementing its pre-trained model to do the object\\ndetecting tasks in some specific traffic scenes. The four artificially designed\\ntraffic scenes include single-car, single-person, frontperson-rearcar and\\nfrontcar-rearperson.\\n</summary>\\n    <author>\\n      <name>Shouyu Wang</name>\\n    </author>\\n    <author>\\n      <name>Weitao Tang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1905.04740v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1905.04740v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1905.06540v2</id>\\n    <updated>2019-05-27T07:11:10Z</updated>\\n    <published>2019-05-16T05:55:38Z</published>\\n    <title>Title Redacted</title>\\n    <summary>  arXiv admin note: This version removed by arXiv administrators as the\\nsubmitter did not have the right to agree to the license at the time of\\nsubmission\\n</summary>\\n    <author>\\n      <name>Shivang Bharadwaj</name>\\n    </author>\\n    <author>\\n      <name>Bhupendra Niranjan</name>\\n    </author>\\n    <author>\\n      <name>Anant Kumar</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This paper has been withdrawn as it is the proprietary property of an\\n  organization. A revision might or might not be uploaded in the future after\\n  further internal reviews and revisions. arXiv admin note: Title redacted</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1905.06540v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1905.06540v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1905.13302v1</id>\\n    <updated>2019-05-26T15:47:28Z</updated>\\n    <published>2019-05-26T15:47:28Z</published>\\n    <title>A Survey on Biomedical Image Captioning</title>\\n    <summary>  Image captioning applied to biomedical images can assist and accelerate the\\ndiagnosis process followed by clinicians. This article is the first survey of\\nbiomedical image captioning, discussing datasets, evaluation measures, and\\nstate of the art methods. Additionally, we suggest two baselines, a weak and a\\nstronger one; the latter outperforms all current state of the art systems on\\none of the datasets.\\n</summary>\\n    <author>\\n      <name>Vasiliki Kougia</name>\\n    </author>\\n    <author>\\n      <name>John Pavlopoulos</name>\\n    </author>\\n    <author>\\n      <name>Ion Androutsopoulos</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">SiVL 2019</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1905.13302v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1905.13302v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1906.05893v1</id>\\n    <updated>2019-06-13T18:59:29Z</updated>\\n    <published>2019-06-13T18:59:29Z</published>\\n    <title>IntrinSeqNet: Learning to Estimate the Reflectance from Varying\\n  Illumination</title>\\n    <summary>  This article has been removed by arXiv administrators because the submitter\\ndid not have the rights to agree to the license at the time of submission\\n</summary>\\n    <author>\\n      <name>Gr\\xc3\\xa9goire Nieto</name>\\n    </author>\\n    <author>\\n      <name>Mohammad Rouhani</name>\\n    </author>\\n    <author>\\n      <name>Philippe Robert</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This article has been removed by arXiv administrators because the\\n  submitter did not have the rights to agree to the license at the time of\\n  submission</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1906.05893v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1906.05893v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1906.07016v1</id>\\n    <updated>2019-06-14T08:39:52Z</updated>\\n    <published>2019-06-14T08:39:52Z</published>\\n    <title>Trimmed Action Recognition, Dense-Captioning Events in Videos, and\\n  Spatio-temporal Action Localization with Focus on ActivityNet Challenge 2019</title>\\n    <summary>  This notebook paper presents an overview and comparative analysis of our\\nsystems designed for the following three tasks in ActivityNet Challenge 2019:\\ntrimmed action recognition, dense-captioning events in videos, and\\nspatio-temporal action localization.\\n</summary>\\n    <author>\\n      <name>Zhaofan Qiu</name>\\n    </author>\\n    <author>\\n      <name>Dong Li</name>\\n    </author>\\n    <author>\\n      <name>Yehao Li</name>\\n    </author>\\n    <author>\\n      <name>Qi Cai</name>\\n    </author>\\n    <author>\\n      <name>Yingwei Pan</name>\\n    </author>\\n    <author>\\n      <name>Ting Yao</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">arXiv admin note: substantial text overlap with arXiv:1807.00686,\\n  arXiv:1710.08011</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1906.07016v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1906.07016v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1909.08866v1</id>\\n    <updated>2019-09-19T08:54:51Z</updated>\\n    <published>2019-09-19T08:54:51Z</published>\\n    <title>Challenging deep image descriptors for retrieval in heterogeneous\\n  iconographic collections</title>\\n    <summary>  This article proposes to study the behavior of recent and efficient\\nstate-of-the-art deep-learning based image descriptors for content-based image\\nretrieval, facing a panel of complex variations appearing in heterogeneous\\nimage datasets, in particular in cultural collections that may involve\\nmulti-source, multi-date and multi-view Permission to make digital\\n</summary>\\n    <author>\\n      <name>Dimitri Gominski</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">LaSTIG</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Martyna Poreba</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">LaSTIG</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Val\\xc3\\xa9rie Gouet-Brunet</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">LaSTIG</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Liming Chen</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">LaSTIG</arxiv:affiliation>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1145/3347317.3357246</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1145/3347317.3357246\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">SUMAC \\'19, 2019</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1909.08866v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1909.08866v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1910.03903v1</id>\\n    <updated>2019-10-09T11:22:59Z</updated>\\n    <published>2019-10-09T11:22:59Z</published>\\n    <title>MixMatch Domain Adaptaion: Prize-winning solution for both tracks of\\n  VisDA 2019 challenge</title>\\n    <summary>  We present a domain adaptation (DA) system that can be used in multi-source\\nand semi-supervised settings. Using the proposed method we achieved 2nd place\\non multi-source track and 3rd place on semi-supervised track of the VisDA 2019\\nchallenge (http://ai.bu.edu/visda-2019/). The source code of the method is\\navailable at https://github.com/filaPro/visda2019.\\n</summary>\\n    <author>\\n      <name>Danila Rukhovich</name>\\n    </author>\\n    <author>\\n      <name>Danil Galeev</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">accepted at TASK-CV 2019 at ICCV</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1910.03903v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1910.03903v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1910.11100v1</id>\\n    <updated>2019-10-18T08:09:23Z</updated>\\n    <published>2019-10-18T08:09:23Z</published>\\n    <title>Development of a hand pose recognition system on an embedded computer\\n  using CNNs</title>\\n    <summary>  Demand of hand pose recognition systems are growing in the last years in\\ntechnologies like human-machine interfaces. This work suggests an approach for\\nhand pose recognition in embedded computers using hand tracking and CNNs.\\nResults show a fast time response with an accuracy of 94.50% and low power\\nconsumption.\\n</summary>\\n    <author>\\n      <name>Dennis N\\xc3\\xba\\xc3\\xb1ez Fern\\xc3\\xa1ndez</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">LatinX in AI Research at NeurIPS 2019</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1910.11100v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1910.11100v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1910.11534v1</id>\\n    <updated>2019-10-25T05:28:36Z</updated>\\n    <published>2019-10-25T05:28:36Z</published>\\n    <title>Team PFDet\\'s Methods for Open Images Challenge 2019</title>\\n    <summary>  We present the instance segmentation and the object detection method used by\\nteam PFDet for Open Images Challenge 2019. We tackle a massive dataset size,\\nhuge class imbalance and federated annotations. Using this method, the team\\nPFDet achieved 3rd and 4th place in the instance segmentation and the object\\ndetection track, respectively.\\n</summary>\\n    <author>\\n      <name>Yusuke Niitani</name>\\n    </author>\\n    <author>\\n      <name>Toru Ogawa</name>\\n    </author>\\n    <author>\\n      <name>Shuji Suzuki</name>\\n    </author>\\n    <author>\\n      <name>Takuya Akiba</name>\\n    </author>\\n    <author>\\n      <name>Tommi Kerola</name>\\n    </author>\\n    <author>\\n      <name>Kohei Ozaki</name>\\n    </author>\\n    <author>\\n      <name>Shotaro Sano</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1910.11534v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1910.11534v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1911.07938v1</id>\\n    <updated>2019-10-28T03:00:28Z</updated>\\n    <published>2019-10-28T03:00:28Z</published>\\n    <title>Towards Good Practices for Multi-Person Pose Estimation</title>\\n    <summary>  Multi-Person Pose Estimation is an interesting yet challenging task in\\ncomputer vision. In this paper, we conduct a series of refinements with the\\nMSPN and PoseFix Networks, and empirically evaluate their impact on the final\\nmodel performance through ablation studies. By taking all the refinements, we\\nachieve 78.7 on the COCO test-dev dataset and 76.3 on the COCO test-challenge\\ndataset.\\n</summary>\\n    <author>\\n      <name>Dongdong Yu</name>\\n    </author>\\n    <author>\\n      <name>Kai Su</name>\\n    </author>\\n    <author>\\n      <name>Changhu Wang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1911.07938v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1911.07938v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1911.07939v1</id>\\n    <updated>2019-10-28T03:03:48Z</updated>\\n    <published>2019-10-28T03:03:48Z</published>\\n    <title>Towards Good Practices for Instance Segmentation</title>\\n    <summary>  Instance Segmentation is an interesting yet challenging task in computer\\nvision. In this paper, we conduct a series of refinements with the Hybrid Task\\nCascade (HTC) Network, and empirically evaluate their impact on the final model\\nperformance through ablation studies. By taking all the refinements, we achieve\\n0.47 on the COCO test-dev dataset and 0.47 on the COCO test-challenge dataset.\\n</summary>\\n    <author>\\n      <name>Dongdong Yu</name>\\n    </author>\\n    <author>\\n      <name>Zehuan Yuan</name>\\n    </author>\\n    <author>\\n      <name>Jinlai Liu</name>\\n    </author>\\n    <author>\\n      <name>Kun Yuan</name>\\n    </author>\\n    <author>\\n      <name>Changhu Wang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1911.07939v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1911.07939v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1911.08169v1</id>\\n    <updated>2019-11-19T09:25:59Z</updated>\\n    <published>2019-11-19T09:25:59Z</published>\\n    <title>Dense Fusion Classmate Network for Land Cover Classification</title>\\n    <summary>  Recently, FCNs based methods have made great progress in semantic\\nsegmentation. Different with ordinary scenes, satellite image owns specific\\ncharacteristics, which elements always extend to large scope and no regular or\\nclear boundaries. Therefore, effective mid-level structure information\\nextremely missing, precise pixel-level classification becomes tough issues. In\\nthis paper, a Dense Fusion Classmate Network (DFCNet) is proposed to adopt in\\nland cover classification.\\n</summary>\\n    <author>\\n      <name>Chao Tian</name>\\n    </author>\\n    <author>\\n      <name>Cong Li</name>\\n    </author>\\n    <author>\\n      <name>Jianping Shi</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1911.08169v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1911.08169v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1911.10636v1</id>\\n    <updated>2019-11-24T23:03:19Z</updated>\\n    <published>2019-11-24T23:03:19Z</published>\\n    <title>Pyramid Vector Quantization and Bit Level Sparsity in Weights for\\n  Efficient Neural Networks Inference</title>\\n    <summary>  This paper discusses three basic blocks for the inference of convolutional\\nneural networks (CNNs). Pyramid Vector Quantization (PVQ) is discussed as an\\neffective quantizer for CNNs weights resulting in highly sparse and\\ncompressible networks. Properties of PVQ are exploited for the elimination of\\nmultipliers during inference while maintaining high performance. The result is\\nthen extended to any other quantized weights. The Tiny Yolo v3 CNN is used to\\ncompare such basic blocks.\\n</summary>\\n    <author>\\n      <name>Vincenzo Liguori</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1911.10636v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1911.10636v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1911.12249v1</id>\\n    <updated>2019-11-27T16:14:14Z</updated>\\n    <published>2019-11-27T16:14:14Z</published>\\n    <title>Literature Review of Action Recognition in the Wild</title>\\n    <summary>  The literature review presented below on Action Recognition in the wild is\\nthe in-depth study of Research Papers. Action Recognition problem in the\\nuntrimmed videos is a challenging task and most of the papers have tackled this\\nproblem using hand-crafted features with shallow learning techniques and\\nsophisticated end-to-end deep learning techniques.\\n</summary>\\n    <author>\\n      <name>Asket Kaur</name>\\n    </author>\\n    <author>\\n      <name>Navya Rao</name>\\n    </author>\\n    <author>\\n      <name>Tanya Joon</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1911.12249v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1911.12249v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1911.12706v1</id>\\n    <updated>2019-11-28T13:40:06Z</updated>\\n    <published>2019-11-28T13:40:06Z</published>\\n    <title>Cameras Viewing Cameras Geometry</title>\\n    <summary>  A basic problem in computer vision is to understand the structure of a\\nreal-world scene given several images of it. Here we study several theoretical\\naspects of the intra multi-view geometry of calibrated cameras when all that\\nthey can reliably recognize is each other. With the proliferation of wearable\\ncameras, autonomous vehicles and drones, the geometry of these multiple cameras\\nis a timely and relevant problem to study.\\n</summary>\\n    <author>\\n      <name>Danail Brezov</name>\\n    </author>\\n    <author>\\n      <name>Michael Werman</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1911.12706v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1911.12706v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1912.02202v1</id>\\n    <updated>2019-12-04T19:00:15Z</updated>\\n    <published>2019-12-04T19:00:15Z</published>\\n    <title>MORPHOLO C++ Library for glasses-free multi-view stereo vision and\\n  streaming of live 3D video</title>\\n    <summary>  The MORPHOLO C++ extended Library allows to convert a specific stereoscopic\\nsnapshot into a Native multi-view image through morphing algorithms taking into\\naccount display calibration data for specific slanted lenticular 3D monitors.\\nMORPHOLO can also be implemented for glasses-free live applicatons of 3D video\\nstreaming, and for diverse innovative scientific, engineering and 3D video game\\napplications -see http://www.morpholo.it\\n</summary>\\n    <author>\\n      <name>Enrique Canessa</name>\\n    </author>\\n    <author>\\n      <name>Livio Tenze</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">28 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1912.02202v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1912.02202v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1912.04376v1</id>\\n    <updated>2019-12-09T21:06:15Z</updated>\\n    <published>2019-12-09T21:06:15Z</published>\\n    <title>Modular Multimodal Architecture for Document Classification</title>\\n    <summary>  Page classification is a crucial component to any document analysis system,\\nallowing for complex branching control flows for different components of a\\ngiven document. Utilizing both the visual and textual content of a page, the\\nproposed method exceeds the current state-of-the-art performance on the\\nRVL-CDIP benchmark at 93.03% test accuracy.\\n</summary>\\n    <author>\\n      <name>Tyler Dauphinee</name>\\n    </author>\\n    <author>\\n      <name>Nikunj Patel</name>\\n    </author>\\n    <author>\\n      <name>Mohammad Rashidi</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1912.04376v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1912.04376v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1912.12167v1</id>\\n    <updated>2019-12-18T19:23:29Z</updated>\\n    <published>2019-12-18T19:23:29Z</published>\\n    <title>Design Considerations for Efficient Deep Neural Networks on\\n  Processing-in-Memory Accelerators</title>\\n    <summary>  This paper describes various design considerations for deep neural networks\\nthat enable them to operate efficiently and accurately on processing-in-memory\\naccelerators. We highlight important properties of these accelerators and the\\nresulting design considerations using experiments conducted on various\\nstate-of-the-art deep neural networks with the large-scale ImageNet dataset.\\n</summary>\\n    <author>\\n      <name>Tien-Ju Yang</name>\\n    </author>\\n    <author>\\n      <name>Vivienne Sze</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted by IEDM 2019</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1912.12167v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1912.12167v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.ET\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2001.05841v1</id>\\n    <updated>2020-01-13T08:36:10Z</updated>\\n    <published>2020-01-13T08:36:10Z</published>\\n    <title>Predicting population neural activity in the Algonauts challenge using\\n  end-to-end trained Siamese networks and group convolutions</title>\\n    <summary>  The Algonauts challenge is about predicting the object representations in the\\nform of Representational Dissimilarity Matrices (RDMS) derived from visual\\nbrain regions. We used a customized deep learning model using the concept of\\nSiamese networks and group convolutions to predict neural distances\\ncorresponding to a pair of images. Training data was best explained by\\ndistances computed over the last layer.\\n</summary>\\n    <author>\\n      <name>Georgin Jacob</name>\\n    </author>\\n    <author>\\n      <name>Harish Katti</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2001.05841v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2001.05841v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2001.05865v1</id>\\n    <updated>2020-01-15T08:20:54Z</updated>\\n    <published>2020-01-15T08:20:54Z</published>\\n    <title>Ensemble based discriminative models for Visual Dialog Challenge 2018</title>\\n    <summary>  This manuscript describes our approach for the Visual Dialog Challenge 2018.\\nWe use an ensemble of three discriminative models with different encoders and\\ndecoders for our final submission. Our best performing model on \\'test-std\\'\\nsplit achieves the NDCG score of 55.46 and the MRR value of 63.77, securing\\nthird position in the challenge.\\n</summary>\\n    <author>\\n      <name>Shubham Agarwal</name>\\n    </author>\\n    <author>\\n      <name>Raghav Goyal</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Rankings: https://visualdialog.org/challenge/2018#winners</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2001.05865v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2001.05865v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2002.05107v3</id>\\n    <updated>2020-08-16T13:29:19Z</updated>\\n    <published>2020-02-12T17:32:18Z</published>\\n    <title>Analysis of Dutch Master Paintings with Convolutional Neural Networks</title>\\n    <summary>  Trained on the works of an artist under study and visually comparable works\\nof other artists, convolutional neural networks can identify forgeries and\\nprovide attributions. They can also assign classification probabilities within\\na painting, revealing mixed authorship and identifying regions painted by\\ndifferent hands.\\n</summary>\\n    <author>\\n      <name>Steven J. Frank</name>\\n    </author>\\n    <author>\\n      <name>Andrea M. Frank</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2002.05107v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2002.05107v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2002.05447v1</id>\\n    <updated>2020-02-13T11:29:46Z</updated>\\n    <published>2020-02-13T11:29:46Z</published>\\n    <title>Emotion Recognition for In-the-wild Videos</title>\\n    <summary>  This paper is a brief introduction to our submission to the seven basic\\nexpression classification track of Affective Behavior Analysis in-the-wild\\nCompetition held in conjunction with the IEEE International Conference on\\nAutomatic Face and Gesture Recognition (FG) 2020. Our method combines Deep\\nResidual Network (ResNet) and Bidirectional Long Short-Term Memory Network\\n(BLSTM), achieving 64.3% accuracy and 43.4% final metric on the validation set.\\n</summary>\\n    <author>\\n      <name>Hanyu Liu</name>\\n    </author>\\n    <author>\\n      <name>Jiabei Zeng</name>\\n    </author>\\n    <author>\\n      <name>Shiguang Shan</name>\\n    </author>\\n    <author>\\n      <name>Xilin Chen</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2002.05447v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2002.05447v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2002.10363v1</id>\\n    <updated>2020-02-24T16:48:30Z</updated>\\n    <published>2020-02-24T16:48:30Z</published>\\n    <title>Joint Learning of Assignment and Representation for Biometric Group\\n  Membership</title>\\n    <summary>  This paper proposes a framework for group membership protocols preventing the\\ncurious but honest server from reconstructing the enrolled biometric signatures\\nand inferring the identity of querying clients. This framework learns the\\nembedding parameters, group representations and assignments simultaneously.\\nExperiments show the trade-off between security/privacy and\\nverification/identification performances.\\n</summary>\\n    <author>\\n      <name>Marzieh Gheisari</name>\\n    </author>\\n    <author>\\n      <name>Teddy Furon</name>\\n    </author>\\n    <author>\\n      <name>Laurent Amsaleg</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2002.10363v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2002.10363v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2004.03339v1</id>\\n    <updated>2020-03-27T23:34:01Z</updated>\\n    <published>2020-03-27T23:34:01Z</published>\\n    <title>Automatic Generation of Chinese Handwriting via Fonts Style\\n  Representation Learning</title>\\n    <summary>  In this paper, we propose and end-to-end deep Chinese font generation system.\\nThis system can generate new style fonts by interpolation of latent\\nstyle-related embeding variables that could achieve smooth transition between\\ndifferent style. Our method is simpler and more effective than other methods,\\nwhich will help to improve the font design efficiency\\n</summary>\\n    <author>\\n      <name>Fenxi Xiao</name>\\n    </author>\\n    <author>\\n      <name>Bo Huang</name>\\n    </author>\\n    <author>\\n      <name>Xia Wu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2004.03339v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2004.03339v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2004.09430v1</id>\\n    <updated>2020-04-20T16:36:01Z</updated>\\n    <published>2020-04-20T16:36:01Z</published>\\n    <title>Improving correlation method with convolutional neural networks</title>\\n    <summary>  We present a convolutional neural network for the classification of\\ncorrelation responses obtained by correlation filters. The proposed approach\\ncan improve the accuracy of classification, as well as achieve invariance to\\nthe image classes and parameters.\\n</summary>\\n    <author>\\n      <name>Dmitriy Goncharov</name>\\n    </author>\\n    <author>\\n      <name>Rostislav Starikov</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, 3 figures, 2 tables, 1 formula</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2004.09430v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2004.09430v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2005.08645v1</id>\\n    <updated>2020-05-09T12:13:43Z</updated>\\n    <published>2020-05-09T12:13:43Z</published>\\n    <title>Multi-Task Learning in Histo-pathology for Widely Generalizable Model</title>\\n    <summary>  In this work we show preliminary results of deep multi-task learning in the\\narea of computational pathology. We combine 11 tasks ranging from patch-wise\\noral cancer classification, one of the most prevalent cancers in the developing\\nworld, to multi-tissue nuclei instance segmentation and classification.\\n</summary>\\n    <author>\\n      <name>Jevgenij Gamper</name>\\n    </author>\\n    <author>\\n      <name>Navid Alemi Kooohbanani</name>\\n    </author>\\n    <author>\\n      <name>Nasir Rajpoot</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">AI4CC ICLR 2020 workshop</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/2005.08645v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2005.08645v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2007.08170v1</id>\\n    <updated>2020-07-16T08:10:42Z</updated>\\n    <published>2020-07-16T08:10:42Z</published>\\n    <title>VIPriors Object Detection Challenge</title>\\n    <summary>  This paper is a brief report to our submission to the VIPriors Object\\nDetection Challenge. Object Detection has attracted many researchers\\' attention\\nfor its full application, but it is still a challenging task. In this paper, we\\nstudy analysis the characteristics of the data, and an effective data\\nenhancement method is proposed. We carefully choose the model which is more\\nsuitable for training from scratch. We benefit a lot from using softnms and\\nmodel fusion skillfully.\\n</summary>\\n    <author>\\n      <name>Zhipeng Luo</name>\\n    </author>\\n    <author>\\n      <name>Lixuan Che</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2007.08170v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2007.08170v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2007.10232v1</id>\\n    <updated>2020-07-17T09:09:41Z</updated>\\n    <published>2020-07-17T09:09:41Z</published>\\n    <title>The Effect of Top-Down Attention in Occluded Object Recognition</title>\\n    <summary>  This study is concerned with the top-down visual processing benefit in the\\ntask of occluded object recognition. To this end, a psychophysical experiment\\nis designed and carried out which aimed at investigating the effect of\\nconsistency of contextual information on the recognition of objects which are\\npartially occluded. The results demonstrate the facilitative impact of\\nconsistent contextual clues on the task of object recognition in presence of\\nocclusion.\\n</summary>\\n    <author>\\n      <name>Zahra Sadeghi</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, 2 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2007.10232v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2007.10232v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2010.05131v1</id>\\n    <updated>2020-10-11T01:20:55Z</updated>\\n    <published>2020-10-11T01:20:55Z</published>\\n    <title>Segmenting Epipolar Line</title>\\n    <summary>  Identifying feature correspondence between two images is a fundamental\\nprocedure in three-dimensional computer vision. Usually the feature search\\nspace is confined by the epipolar line. Using the cheirality constraint, this\\npaper finds that the feature search space can be restrained to one of two or\\nthree segments of the epipolar line that are defined by the epipole and a\\nso-called virtual infinity point.\\n</summary>\\n    <author>\\n      <name>Shengjie Li</name>\\n    </author>\\n    <author>\\n      <name>Qi Cai</name>\\n    </author>\\n    <author>\\n      <name>Yuanxin Wu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages, 6 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2010.05131v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2010.05131v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2010.06454v2</id>\\n    <updated>2020-11-05T05:52:08Z</updated>\\n    <published>2020-09-21T04:19:27Z</published>\\n    <title>The DongNiao International Birds 10000 Dataset</title>\\n    <summary>  DongNiao International Birds 10000 (DIB-10K) is a challenging image dataset\\nwhich has more than 10 thousand different types of birds. It was created to\\nenable the study of machine learning and also ornithology research. DIB-10K\\ndoes not own the copyright of these images. It only provides thumbnails of\\nimages, in a way similar to ImageNet.\\n</summary>\\n    <author>\\n      <name>Jian Mei</name>\\n    </author>\\n    <author>\\n      <name>Hao Dong</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2010.06454v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2010.06454v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2010.11339v1</id>\\n    <updated>2020-10-21T22:42:19Z</updated>\\n    <published>2020-10-21T22:42:19Z</published>\\n    <title>Voronoi Convolutional Neural Networks</title>\\n    <summary>  In this technical report, we investigate extending convolutional neural\\nnetworks to the setting where functions are not sampled in a grid pattern. We\\nshow that by treating the samples as the average of a function within a cell,\\nwe can find a natural equivalent of most layers used in CNN. We also present an\\nalgorithm for running inference for these models exactly using standard convex\\ngeometry algorithms.\\n</summary>\\n    <author>\\n      <name>Soroosh Yazdani</name>\\n    </author>\\n    <author>\\n      <name>Andrea Tagliasacchi</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Technical report</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2010.11339v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2010.11339v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4.0\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2010.15250v1</id>\\n    <updated>2020-10-28T21:42:16Z</updated>\\n    <published>2020-10-28T21:42:16Z</published>\\n    <title>Semantic video segmentation for autonomous driving</title>\\n    <summary>  We aim to solve semantic video segmentation in autonomous driving, namely\\nroad detection in real time video, using techniques discussed in (Shelhamer et\\nal., 2016a). While fully convolutional network gives good result, we show that\\nthe speed can be halved while preserving the accuracy. The test dataset being\\nused is KITTI, which consists of real footage from Germany\\'s streets.\\n</summary>\\n    <author>\\n      <name>Minh Triet Chau</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This work was done around 2017. Some minor changes were added</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2010.15250v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2010.15250v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2011.07230v1</id>\\n    <updated>2020-11-14T06:55:31Z</updated>\\n    <published>2020-11-14T06:55:31Z</published>\\n    <title>TDAsweep: A Novel Dimensionality Reduction Method for Image\\n  Classification Tasks</title>\\n    <summary>  One of the most celebrated achievements of modern machine learning technology\\nis automatic classification of images. However, success is typically achieved\\nonly with major computational costs. Here we introduce TDAsweep, a machine\\nlearning tool aimed at improving the efficiency of automatic classification of\\nimages.\\n</summary>\\n    <author>\\n      <name>Yu-Shih Chen</name>\\n    </author>\\n    <author>\\n      <name>Melissa Goh</name>\\n    </author>\\n    <author>\\n      <name>Norm Matloff</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2011.07230v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2011.07230v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2011.10759v1</id>\\n    <updated>2020-11-21T10:27:21Z</updated>\\n    <published>2020-11-21T10:27:21Z</published>\\n    <title>Visual Recognition of Great Ape Behaviours in the Wild</title>\\n    <summary>  We propose a first great ape-specific visual behaviour recognition system\\nutilising deep learning that is capable of detecting nine core ape behaviours.\\n</summary>\\n    <author>\\n      <name>Faizaan Sakib</name>\\n    </author>\\n    <author>\\n      <name>Tilo Burghardt</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, 4 figures, to be published in the proceedings of ICPR 2020\\n  at the Visual observation and analysis of Vertebrate And Insect Behaviour\\n  (VAIB) workshop</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2011.10759v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2011.10759v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2011.13698v1</id>\\n    <updated>2020-11-27T12:25:24Z</updated>\\n    <published>2020-11-27T12:25:24Z</published>\\n    <title>Lightweight U-Net for High-Resolution Breast Imaging</title>\\n    <summary>  We study the fully convolutional neural networks in the context of malignancy\\ndetection for breast cancer screening. We work on a supervised segmentation\\ntask looking for an acceptable compromise between the precision of the network\\nand the computational complexity.\\n</summary>\\n    <author>\\n      <name>Mickael Tardy</name>\\n    </author>\\n    <author>\\n      <name>Diana Mateus</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">in Proceedings of iTWIST\\'20, Paper-ID: 30, Nantes, France, December,\\n  2-4, 2020</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2011.13698v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2011.13698v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2011.14761v1</id>\\n    <updated>2020-11-30T13:15:51Z</updated>\\n    <published>2020-11-30T13:15:51Z</published>\\n    <title>How Good MVSNets Are at Depth Fusion</title>\\n    <summary>  We study the effects of the additional input to deep multi-view stereo\\nmethods in the form of low-quality sensor depth. We modify two state-of-the-art\\ndeep multi-view stereo methods for using with the input depth. We show that the\\nadditional input depth may improve the quality of deep multi-view stereo.\\n</summary>\\n    <author>\\n      <name>Oleg Voynov</name>\\n    </author>\\n    <author>\\n      <name>Aleksandr Safin</name>\\n    </author>\\n    <author>\\n      <name>Savva Ignatyev</name>\\n    </author>\\n    <author>\\n      <name>Evgeny Burnaev</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages, 6 figures, 1 table. Accepted to ICMV 2020</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2011.14761v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2011.14761v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2101.00350v1</id>\\n    <updated>2021-01-02T01:51:38Z</updated>\\n    <published>2021-01-02T01:51:38Z</published>\\n    <title>Multi-Image Steganography Using Deep Neural Networks</title>\\n    <summary>  Steganography is the science of hiding a secret message within an ordinary\\npublic message. Over the years, steganography has been used to encode a lower\\nresolution image into a higher resolution image by simple methods like LSB\\nmanipulation. We aim to utilize deep neural networks for the encoding and\\ndecoding of multiple secret images inside a single cover image of the same\\nresolution.\\n</summary>\\n    <author>\\n      <name>Abhishek Das</name>\\n    </author>\\n    <author>\\n      <name>Japsimar Singh Wahi</name>\\n    </author>\\n    <author>\\n      <name>Mansi Anand</name>\\n    </author>\\n    <author>\\n      <name>Yugant Rana</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2101.00350v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2101.00350v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2101.06022v1</id>\\n    <updated>2021-01-15T09:14:10Z</updated>\\n    <published>2021-01-15T09:14:10Z</published>\\n    <title>Motion-Based Handwriting Recognition</title>\\n    <summary>  We attempt to overcome the restriction of requiring a writing surface for\\nhandwriting recognition. In this study, we design a prototype of a stylus\\nequipped with motion sensor, and utilizes gyroscopic and acceleration sensor\\nreading to perform written letter classification using various deep learning\\ntechniques such as CNN and RNNs. We also explore various data augmentation\\ntechniques and their effects, reaching up to 86% accuracy.\\n</summary>\\n    <author>\\n      <name>Junshen Kevin Chen</name>\\n    </author>\\n    <author>\\n      <name>Wanze Xie</name>\\n    </author>\\n    <author>\\n      <name>Yutong He</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2101.06022v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2101.06022v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2101.06025v1</id>\\n    <updated>2021-01-15T09:24:04Z</updated>\\n    <published>2021-01-15T09:24:04Z</published>\\n    <title>Motion-Based Handwriting Recognition and Word Reconstruction</title>\\n    <summary>  In this project, we leverage a trained single-letter classifier to predict\\nthe written word from a continuously written word sequence, by designing a word\\nreconstruction pipeline consisting of a dynamic-programming algorithm and an\\nauto-correction model. We conduct experiments to optimize models in this\\npipeline, then employ domain adaptation to explore using this pipeline on\\nunseen data distributions.\\n</summary>\\n    <author>\\n      <name>Junshen Kevin Chen</name>\\n    </author>\\n    <author>\\n      <name>Wanze Xie</name>\\n    </author>\\n    <author>\\n      <name>Yutong He</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2101.06025v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2101.06025v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2102.06793v1</id>\\n    <updated>2021-01-25T17:56:15Z</updated>\\n    <published>2021-01-25T17:56:15Z</published>\\n    <title>Unanswerable Questions about Images and Texts</title>\\n    <summary>  Questions about a text or an image that cannot be answered raise distinctive\\nissues for an AI. This note discusses the problem of unanswerable questions in\\nVQA (visual question answering), in QA (visual question answering), and in AI\\ngenerally.\\n</summary>\\n    <author>\\n      <name>Ernest Davis</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.3389/frai.2020.00051</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.3389/frai.2020.00051\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">15 pages, 4 figures</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Frontiers in Artificial Intelligence: Language and Computation.\\n  July 2020</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/2102.06793v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2102.06793v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2102.07455v1</id>\\n    <updated>2021-02-15T11:06:46Z</updated>\\n    <published>2021-02-15T11:06:46Z</published>\\n    <title>Video Analytics on IoT devices</title>\\n    <summary>  Deep Learning (DL) combined with advanced model optimization methods such as\\nRC-NN and Edge2Train has enabled offline execution of large networks on the IoT\\ndevices. In this paper, we compare the modern Deep Learning (DL) based video\\nanalytics approaches with the standard Computer Vision (CV) based approaches\\nand finally, discuss the best-suited approach for video analytics on IoT\\ndevices.\\n</summary>\\n    <author>\\n      <name>Sree Premkumar</name>\\n    </author>\\n    <author>\\n      <name>Vimal Premkumar</name>\\n    </author>\\n    <author>\\n      <name>Rakesh Dhakshinamurthy</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2102.07455v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2102.07455v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2103.09475v1</id>\\n    <updated>2021-03-17T07:11:38Z</updated>\\n    <published>2021-03-17T07:11:38Z</published>\\n    <title>Virtual Dress Swap Using Landmark Detection</title>\\n    <summary>  Online shopping has gained popularity recently. This paper addresses one\\ncrucial problem of buying dress online, which has not been solved yet. This\\nresearch tries to implement the idea of clothes swapping with the help of\\nDeepFashion dataset where 6,223 images with eight landmarks each used. Deep\\nConvolutional Neural Network has been built for Landmark detection.\\n</summary>\\n    <author>\\n      <name>Odar Zeynal</name>\\n    </author>\\n    <author>\\n      <name>Saber Malekzadeh</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2103.09475v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2103.09475v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2105.02964v1</id>\\n    <updated>2021-05-02T00:05:17Z</updated>\\n    <published>2021-05-02T00:05:17Z</published>\\n    <title>Object detection for crabs in top-view seabed imagery</title>\\n    <summary>  This report presents the application of object detection on a database of\\nunderwater images of different species of crabs, as well as aerial images of\\nsea lions and finally the Pascal VOC dataset. The model is an end-to-end object\\ndetection neural network based on a convolutional network base and a Long\\nShort-Term Memory detector.\\n</summary>\\n    <author>\\n      <name>Vlad Velici</name>\\n    </author>\\n    <author>\\n      <name>Adam Pr\\xc3\\xbcgel-Bennett</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2105.02964v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2105.02964v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2105.04093v1</id>\\n    <updated>2021-05-10T03:48:55Z</updated>\\n    <published>2021-05-10T03:48:55Z</published>\\n    <title>Elastic Weight Consolidation (EWC): Nuts and Bolts</title>\\n    <summary>  In this report, we present a theoretical support of the continual learning\\nmethod \\\\textbf{Elastic Weight Consolidation}, introduced in paper titled\\n`Overcoming catastrophic forgetting in neural networks\\'. Being one of the most\\ncited paper in regularized methods for continual learning, this report\\ndisentangles the underlying concept of the proposed objective function. We\\nassume that the reader is aware of the basic terminologies of continual\\nlearning.\\n</summary>\\n    <author>\\n      <name>Abhishek Aich</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2105.04093v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2105.04093v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2105.08796v1</id>\\n    <updated>2021-05-18T19:33:17Z</updated>\\n    <published>2021-05-18T19:33:17Z</published>\\n    <title>Analyzing the effectiveness of image augmentations for face recognition\\n  from limited data</title>\\n    <summary>  This work presents an analysis of the efficiency of image augmentations for\\nthe face recognition problem from limited data. We considered basic\\nmanipulations, generative methods, and their combinations for augmentations.\\nOur results show that augmentations, in general, can considerably improve the\\nquality of face recognition systems and the combination of generative and basic\\napproaches performs better than the other tested techniques.\\n</summary>\\n    <author>\\n      <name>Aleksei Zhuchkov</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2105.08796v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2105.08796v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4.9\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2105.10063v1</id>\\n    <updated>2021-05-20T23:11:36Z</updated>\\n    <published>2021-05-20T23:11:36Z</published>\\n    <title>Uma implementa\\xc3\\xa7\\xc3\\xa3o do jogo Pedra, Papel e Tesoura utilizando Visao\\n  Computacional</title>\\n    <summary>  This paper presents a game, controlled by computer vision, in identification\\nof hand gestures (hand-tracking). The proposed work is based on image\\nsegmentation and construction of a convex hull with Jarvis Algorithm , and\\ndetermination of the pattern based on the extraction of area characteristics in\\nthe convex hull.\\n</summary>\\n    <author>\\n      <name>Ezequiel Fran\\xc3\\xa7a dos Santos</name>\\n    </author>\\n    <author>\\n      <name>Gabriel Fontenelle</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">14 pages, in Portuguese</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2105.10063v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2105.10063v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2105.13264v1</id>\\n    <updated>2021-05-27T16:02:40Z</updated>\\n    <published>2021-05-27T16:02:40Z</published>\\n    <title>How saccadic vision might help with theinterpretability of deep networks</title>\\n    <summary>  We describe how some problems (interpretability,lack of object-orientedness)\\nof modern deep networks potentiallycould be solved by adapting a biologically\\nplausible saccadicmechanism of perception. A sketch of such a saccadic\\nvisionmodel is proposed. Proof of concept experimental results areprovided to\\nsupport the proposed approach.\\n</summary>\\n    <author>\\n      <name>Iana Sereda</name>\\n    </author>\\n    <author>\\n      <name>Grigory Osipov</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2105.13264v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2105.13264v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2107.05186v1</id>\\n    <updated>2021-07-12T04:02:57Z</updated>\\n    <published>2021-07-12T04:02:57Z</published>\\n    <title>Early warning of pedestrians and cyclists</title>\\n    <summary>  State-of-the-art motor vehicles are able to break for pedestrians in an\\nemergency. We investigate what it would take to issue an early warning to the\\ndriver so he/she has time to react. We have identified that predicting the\\nintention of a pedestrian reliably by position is a particularly hard\\nchallenge. This paper describes an early pedestrian warning demonstration\\nsystem.\\n</summary>\\n    <author>\\n      <name>Joerg Christian Wolf</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2107.05186v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2107.05186v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2107.06780v1</id>\\n    <updated>2021-07-03T09:41:53Z</updated>\\n    <published>2021-07-03T09:41:53Z</published>\\n    <title>Person-MinkUNet: 3D Person Detection with LiDAR Point Cloud</title>\\n    <summary>  In this preliminary work we attempt to apply submanifold sparse convolution\\nto the task of 3D person detection. In particular, we present Person-MinkUNet,\\na single-stage 3D person detection network based on Minkowski Engine with U-Net\\narchitecture. The network achieves a 76.4% average precision (AP) on the JRDB\\n3D detection benchmark.\\n</summary>\\n    <author>\\n      <name>Dan Jia</name>\\n    </author>\\n    <author>\\n      <name>Bastian Leibe</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">accepted as an extended abstract in JRDB-ACT Workshop at CVPR21</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2107.06780v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2107.06780v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2112.02297v1</id>\\n    <updated>2021-12-04T09:59:01Z</updated>\\n    <published>2021-12-04T09:59:01Z</published>\\n    <title>Ablation study of self-supervised learning for image classification</title>\\n    <summary>  This project focuses on the self-supervised training of convolutional neural\\nnetworks (CNNs) and transformer networks for the task of image recognition. A\\nsimple siamese network with different backbones is used in order to maximize\\nthe similarity of two augmented transformed images from the same source image.\\nIn this way, the backbone is able to learn visual information without\\nsupervision. Finally, the method is evaluated on three image recognition\\ndatasets.\\n</summary>\\n    <author>\\n      <name>Ilias Papastratis</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2112.02297v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2112.02297v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2112.05576v1</id>\\n    <updated>2021-12-10T14:43:24Z</updated>\\n    <published>2021-12-10T14:43:24Z</published>\\n    <title>GPU-accelerated image alignment for object detection in industrial\\n  applications</title>\\n    <summary>  This research proposes a practical method for detecting featureless objects\\nby using image alignment approach with a robust similarity measure in\\nindustrial applications. This similarity measure is robust against occlusion,\\nillumination changes and background clutter. The performance of the proposed\\nGPU (Graphics Processing Unit) accelerated algorithm is deemed successful in\\nexperiments of comparison between both CPU and GPU implementations\\n</summary>\\n    <author>\\n      <name>Trung-Son Le</name>\\n    </author>\\n    <author>\\n      <name>Chyi-Yeu Lin</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/ARIS.2017.8297173</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/ARIS.2017.8297173\" rel=\"related\"/>\\n    <link href=\"http://arxiv.org/abs/2112.05576v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2112.05576v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2112.13707v1</id>\\n    <updated>2021-12-27T14:31:24Z</updated>\\n    <published>2021-12-27T14:31:24Z</published>\\n    <title>Visual Place Representation and Recognition from Depth Images</title>\\n    <summary>  This work proposes a new method for place recognition based on the scene\\narchitecture. From depth video, we compute the 3D model and we derive and\\ndescribe geometrically the 2D map from which the scene descriptor is deduced to\\nconstitute the core of the proposed algorithm. The obtained results show the\\nefficiency and the robustness of the propounded descriptor to scene appearance\\nchanges and light variations.\\n</summary>\\n    <author>\\n      <name>Farah Ibelaiden</name>\\n    </author>\\n    <author>\\n      <name>Slimane Larabi</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1016/j.ijleo.2022.169109</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1016/j.ijleo.2022.169109\" rel=\"related\"/>\\n    <link href=\"http://arxiv.org/abs/2112.13707v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2112.13707v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2206.02002v1</id>\\n    <updated>2022-06-04T14:55:24Z</updated>\\n    <published>2022-06-04T14:55:24Z</published>\\n    <title>CVNets: High Performance Library for Computer Vision</title>\\n    <summary>  We introduce CVNets, a high-performance open-source library for training deep\\nneural networks for visual recognition tasks, including classification,\\ndetection, and segmentation. CVNets supports image and video understanding\\ntools, including data loading, data transformations, novel data sampling\\nmethods, and implementations of several standard networks with similar or\\nbetter performance than previous studies.\\n  Our source code is available at: \\\\url{https://github.com/apple/ml-cvnets}.\\n</summary>\\n    <author>\\n      <name>Sachin Mehta</name>\\n    </author>\\n    <author>\\n      <name>Farzad Abdolhosseini</name>\\n    </author>\\n    <author>\\n      <name>Mohammad Rastegari</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Technical report</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2206.02002v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2206.02002v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2208.08863v1</id>\\n    <updated>2022-08-03T01:42:49Z</updated>\\n    <published>2022-08-03T01:42:49Z</published>\\n    <title>Compressive Self-localization Using Relative Attribute Embedding</title>\\n    <summary>  The use of relative attribute (e.g., beautiful, safe, convenient) -based\\nimage embeddings in visual place recognition, as a domain-adaptive compact\\nimage descriptor that is orthogonal to the typical approach of absolute\\nattribute (e.g., color, shape, texture) -based image embeddings, is explored in\\nthis paper.\\n</summary>\\n    <author>\\n      <name>Ryogo Yamamoto</name>\\n    </author>\\n    <author>\\n      <name>Kanji Tanaka</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">3 pages, 4 figures, An extended abstract version of a manuscript\\n  submitted to an international conference</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2208.08863v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2208.08863v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2209.10497v2</id>\\n    <updated>2023-01-26T07:26:02Z</updated>\\n    <published>2022-09-21T16:53:04Z</published>\\n    <title>Animating Still Images</title>\\n    <summary>  We present a method for imparting motion to a still 2D image. Our method uses\\ndeep learning to segment a section of the image denoted as subject, then uses\\nin-painting to complete the background, and finally adds animation to the\\nsubject by embedding the image in a triangle mesh, while preserving the rest of\\nthe image.\\n</summary>\\n    <author>\\n      <name>Kushagr Batra</name>\\n    </author>\\n    <author>\\n      <name>Mridul Kavidayal</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2209.10497v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2209.10497v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2209.12330v1</id>\\n    <updated>2022-09-25T22:03:39Z</updated>\\n    <published>2022-09-25T22:03:39Z</published>\\n    <title>Personalizing Text-to-Image Generation via Aesthetic Gradients</title>\\n    <summary>  This work proposes aesthetic gradients, a method to personalize a\\nCLIP-conditioned diffusion model by guiding the generative process towards\\ncustom aesthetics defined by the user from a set of images. The approach is\\nvalidated with qualitative and quantitative experiments, using the recent\\nstable diffusion model and several aesthetically-filtered datasets. Code is\\nreleased at https://github.com/vicgalle/stable-diffusion-aesthetic-gradients\\n</summary>\\n    <author>\\n      <name>Victor Gallego</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Submitted to NeurIPS 2022 Machine Learning for Creativity and Design\\n  Workshop</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2209.12330v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2209.12330v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0809.3352v1</id>\\n    <updated>2008-09-19T11:02:39Z</updated>\\n    <published>2008-09-19T11:02:39Z</published>\\n    <title>Generalized Prediction Intervals for Arbitrary Distributed\\n  High-Dimensional Data</title>\\n    <summary>  This paper generalizes the traditional statistical concept of prediction\\nintervals for arbitrary probability density functions in high-dimensional\\nfeature spaces by introducing significance level distributions, which provides\\ninterval-independent probabilities for continuous random variables. The\\nadvantage of the transformation of a probability density function into a\\nsignificance level distribution is that it enables one-class classification or\\noutlier detection in a direct manner.\\n</summary>\\n    <author>\\n      <name>Steffen Kuehn</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">13 pages, 3 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/0809.3352v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0809.3352v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0809.4501v1</id>\\n    <updated>2008-09-25T20:54:29Z</updated>\\n    <published>2008-09-25T20:54:29Z</published>\\n    <title>Audio Classification from Time-Frequency Texture</title>\\n    <summary>  Time-frequency representations of audio signals often resemble texture\\nimages. This paper derives a simple audio classification algorithm based on\\ntreating sound spectrograms as texture images. The algorithm is inspired by an\\nearlier visual classification scheme particularly efficient at classifying\\ntextures. While solely based on time-frequency texture features, the algorithm\\nachieves surprisingly good performance in musical instrument classification\\nexperiments.\\n</summary>\\n    <author>\\n      <name>Guoshen Yu</name>\\n    </author>\\n    <author>\\n      <name>Jean-Jacques Slotine</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/0809.4501v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0809.4501v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1506.06274v1</id>\\n    <updated>2015-06-20T17:55:49Z</updated>\\n    <published>2015-06-20T17:55:49Z</published>\\n    <title>Pose Estimation Based on 3D Models</title>\\n    <summary>  In this paper, we proposed a pose estimation system based on rendered image\\ntraining set, which predicts the pose of objects in real image, with knowledge\\nof object category and tight bounding box. We developed a patch-based\\nmulti-class classification algorithm, and an iterative approach to improve the\\naccuracy. We achieved state-of-the-art performance on pose estimation task.\\n</summary>\\n    <author>\\n      <name>Chuiwen Ma</name>\\n    </author>\\n    <author>\\n      <name>Hao Su</name>\\n    </author>\\n    <author>\\n      <name>Liang Shi</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1506.06274v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1506.06274v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1506.08959v2</id>\\n    <updated>2015-09-24T09:04:24Z</updated>\\n    <published>2015-06-30T06:47:50Z</published>\\n    <title>A Large-Scale Car Dataset for Fine-Grained Categorization and\\n  Verification</title>\\n    <summary>  Updated on 24/09/2015: This update provides preliminary experiment results\\nfor fine-grained classification on the surveillance data of CompCars. The\\ntrain/test splits are provided in the updated dataset. See details in Section\\n6.\\n</summary>\\n    <author>\\n      <name>Linjie Yang</name>\\n    </author>\\n    <author>\\n      <name>Ping Luo</name>\\n    </author>\\n    <author>\\n      <name>Chen Change Loy</name>\\n    </author>\\n    <author>\\n      <name>Xiaoou Tang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">An extension to our conference paper in CVPR 2015</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1506.08959v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1506.08959v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1511.03650v3</id>\\n    <updated>2015-12-07T14:20:01Z</updated>\\n    <published>2015-11-11T20:54:28Z</published>\\n    <title>Piecewise Linear Activation Functions For More Efficient Deep Networks</title>\\n    <summary>  This submission has been withdrawn by arXiv administrators because it is\\nintentionally incomplete, which is in violation of our policies.\\n</summary>\\n    <author>\\n      <name>Cheng-Yang Fu</name>\\n    </author>\\n    <author>\\n      <name>Alexander C. Berg</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Withdrawn by arXiv admins</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1511.03650v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1511.03650v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1511.06489v2</id>\\n    <updated>2018-10-23T22:04:42Z</updated>\\n    <published>2015-11-20T04:56:47Z</published>\\n    <title>A Simple Hierarchical Pooling Data Structure for Loop Closure</title>\\n    <summary>  We propose a data structure obtained by hierarchically averaging bag-of-word\\ndescriptors during a sequence of views that achieves average speedups in\\nlarge-scale loop closure applications ranging from 4 to 20 times on benchmark\\ndatasets. Although simple, the method works as well as sophisticated\\nagglomerative schemes at a fraction of the cost with minimal loss of\\nperformance.\\n</summary>\\n    <author>\\n      <name>Xiaohan Fei</name>\\n    </author>\\n    <author>\\n      <name>Konstantine Tsotsos</name>\\n    </author>\\n    <author>\\n      <name>Stefano Soatto</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1511.06489v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1511.06489v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1511.07347v1</id>\\n    <updated>2015-11-23T18:15:13Z</updated>\\n    <published>2015-11-23T18:15:13Z</published>\\n    <title>Node Specificity in Convolutional Deep Nets Depends on Receptive Field\\n  Position and Size</title>\\n    <summary>  In convolutional deep neural networks, receptive field (RF) size increases\\nwith hierarchical depth. When RF size approaches full coverage of the input\\nimage, different RF positions result in RFs with different specificity, as\\nportions of the RF fall out of the input space. This leads to a departure from\\nthe convolutional concept of positional invariance and opens the possibility\\nfor complex forms of context specificity.\\n</summary>\\n    <author>\\n      <name>Karl Zipser</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1511.07347v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1511.07347v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.08416v1</id>\\n    <updated>2018-05-22T06:11:00Z</updated>\\n    <published>2018-05-22T06:11:00Z</published>\\n    <title>Training Convolutional Networks with Web Images</title>\\n    <summary>  In this thesis we investigate the effect of using web images to build a large\\nscale database to be used along a deep learning method for a classification\\ntask. We replicate the ImageNet large scale database (ILSVRC-2012) from images\\ncollected from the web using 4 different download strategies varying: the\\nsearch engine, the query and the image resolution. As a deep learning method,\\nwe will choose the Convolutional Neural Network that was very successful with\\nrecognition tasks; the AlexNet.\\n</summary>\\n    <author>\\n      <name>Nizar Massouh</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1805.08416v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.08416v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.09421v1</id>\\n    <updated>2018-05-23T20:57:31Z</updated>\\n    <published>2018-05-23T20:57:31Z</published>\\n    <title>Use of symmetric kernels for convolutional neural networks</title>\\n    <summary>  At this work we introduce horizontally symmetric convolutional kernels for\\nCNNs which make the network output invariant to horizontal flips of the image.\\nWe also study other types of symmetric kernels which lead to vertical flip\\ninvariance, and approximate rotational invariance. We show that usage of such\\nkernels acts as regularizer, and improves generalization of the convolutional\\nneural networks at the cost of more complicated training process.\\n</summary>\\n    <author>\\n      <name>Viacheslav Dudar</name>\\n    </author>\\n    <author>\\n      <name>Vladimir Semenov</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">ICDSIAI 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1805.09421v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.09421v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.00877v1</id>\\n    <updated>2018-11-23T22:36:36Z</updated>\\n    <published>2018-11-23T22:36:36Z</published>\\n    <title>Automatic lesion boundary detection in dermoscopy</title>\\n    <summary>  This manuscript addresses the problem of the automatic lesion boundary\\ndetection in dermoscopy, using deep neural networks. An approach is based on\\nthe adaptation of the U-net convolutional neural network with skip connections\\nfor lesion boundary segmentation task. I hope this paper could serve, to some\\nextent, as an experiment of using deep convolutional networks in biomedical\\nsegmentation task and as a guideline of the boundary detection benchmark,\\ninspiring further attempts and researches.\\n</summary>\\n    <author>\\n      <name>Glib Kechyn</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1812.00877v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.00877v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.02542v1</id>\\n    <updated>2018-12-06T14:16:56Z</updated>\\n    <published>2018-12-06T14:16:56Z</published>\\n    <title>Computer Vision for Autonomous Vehicles</title>\\n    <summary>  In this work, we try to implement Image Processing techniques in the area of\\nautonomous vehicles, both indoor and outdoor. The challenges for both are\\ndifferent and the ways to tackle them vary too. We also showed deep learning\\nmakes things easier and precise. We also made base models for all the problems\\nwe tackle while building an autonomous car for Indian Institute of Space\\nscience and Technology.\\n</summary>\\n    <author>\\n      <name>Rohit Gandikota</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1812.02542v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.02542v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.10915v1</id>\\n    <updated>2018-12-28T07:51:08Z</updated>\\n    <published>2018-12-28T07:51:08Z</published>\\n    <title>Spatiotemporal Data Fusion for Precipitation Nowcasting</title>\\n    <summary>  Precipitation nowcasting using neural networks and ground-based radars has\\nbecome one of the key components of modern weather prediction services, but it\\nis limited to the regions covered by ground-based radars. Truly global\\nprecipitation nowcasting requires fusion of radar and satellite observations.\\nWe propose the data fusion pipeline based on computer vision techniques,\\nincluding novel inpainting algorithm with soft masking.\\n</summary>\\n    <author>\\n      <name>Vladimir Ivashkin</name>\\n    </author>\\n    <author>\\n      <name>Vadim Lebedev</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1812.10915v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.10915v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.00117v2</id>\\n    <updated>2019-03-13T14:07:45Z</updated>\\n    <published>2019-03-01T01:13:30Z</published>\\n    <title>A Sketch Based 3D Shape Retrieval Approach Based on Efficient Deep\\n  Point-to-Subspace Metric Learning</title>\\n    <summary>  A sketch based 3D shape retrieval\\n</summary>\\n    <author>\\n      <name>Yinjie Lei</name>\\n    </author>\\n    <author>\\n      <name>Ziqin Zhou</name>\\n    </author>\\n    <author>\\n      <name>Pingping Zhang</name>\\n    </author>\\n    <author>\\n      <name>Yulan Guo</name>\\n    </author>\\n    <author>\\n      <name>Zijun Ma</name>\\n    </author>\\n    <author>\\n      <name>Lingqiao Liu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">The first author wants to withdraw this paper. He has noticed several\\n  setting errors in experiment parts</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1903.00117v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.00117v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.01814v1</id>\\n    <updated>2019-03-05T13:32:03Z</updated>\\n    <published>2019-03-05T13:32:03Z</published>\\n    <title>HexagDLy - Processing hexagonally sampled data with CNNs in PyTorch</title>\\n    <summary>  HexagDLy is a Python-library extending the PyTorch deep learning framework\\nwith convolution and pooling operations on hexagonal grids. It aims to ease the\\naccess to convolutional neural networks for applications that rely on\\nhexagonally sampled data as, for example, commonly found in ground-based\\nastroparticle physics experiments.\\n</summary>\\n    <author>\\n      <name>Constantin Steppa</name>\\n    </author>\\n    <author>\\n      <name>Tim Lukas Holch</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1016/j.softx.2019.02.010</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1016/j.softx.2019.02.010\" rel=\"related\"/>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">SoftwareX, 9, 193-198, 2019</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1903.01814v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.01814v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"astro-ph.IM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.11421v1</id>\\n    <updated>2019-03-27T13:41:17Z</updated>\\n    <published>2019-03-27T13:41:17Z</published>\\n    <title>Social Behavioral Phenotyping of Drosophila with a2D-3D Hybrid CNN\\n  Framework</title>\\n    <summary>  Behavioural phenotyping of Drosophila is an important means in biological and\\nmedical research to identify genetic, pathologic or psychologic impact on\\nanimal behaviour.\\n</summary>\\n    <author>\\n      <name>Ziping Jiang</name>\\n    </author>\\n    <author>\\n      <name>Paul L. Chazot</name>\\n    </author>\\n    <author>\\n      <name>M. Emre Celebi</name>\\n    </author>\\n    <author>\\n      <name>Danny Crookes</name>\\n    </author>\\n    <author>\\n      <name>Richard Jiang</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IEEE Access 2019</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1903.11421v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.11421v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.ET\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1907.09233v1</id>\\n    <updated>2019-07-22T11:12:35Z</updated>\\n    <published>2019-07-22T11:12:35Z</published>\\n    <title>Adapting Computer Vision Algorithms for Omnidirectional Video</title>\\n    <summary>  Omnidirectional (360{\\\\deg}) video has got quite popular because it provides a\\nhighly immersive viewing experience. For computer vision algorithms, it poses\\nseveral challenges, like the special (equirectangular) projection commonly\\nemployed and the huge image size. In this work, we give a high-level overview\\nof these challenges and outline strategies how to adapt computer vision\\nalgorithm for the specifics of omnidirectional video.\\n</summary>\\n    <author>\\n      <name>Hannes Fassold</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted for 27th ACM International Conference on Multimedia (ACMM MM\\n  2019, Nice, France)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1907.09233v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1907.09233v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1908.04386v1</id>\\n    <updated>2019-08-04T11:31:26Z</updated>\\n    <published>2019-08-04T11:31:26Z</published>\\n    <title>Detection of the Group of Traffic Signs with Central Slice Theorem</title>\\n    <summary>  Our sensor system consists of a combination of Photonic Mixer Device - PMD\\nand Mono optical cameras. Some traffic signs have stripes at 45{deg}. These\\ntraffic signs cancel different restrictions on the road. We detect this class\\nof signs with Radon transformation. Here the Radon transformation is calculated\\nusing Central Slice Theorem. We approximate the slice of spectrum by the\\nDiscrete Cosine Transformation (DCT).\\n</summary>\\n    <author>\\n      <name>Koba Natroshvili</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1908.04386v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1908.04386v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1908.10585v1</id>\\n    <updated>2019-08-28T07:41:53Z</updated>\\n    <published>2019-08-28T07:41:53Z</published>\\n    <title>Attention-based Fusion for Outfit Recommendation</title>\\n    <summary>  This paper describes an attention-based fusion method for outfit\\nrecommendation which fuses the information in the product image and description\\nto capture the most important, fine-grained product features into the item\\nrepresentation. We experiment with different kinds of attention mechanisms and\\ndemonstrate that the attention-based fusion improves item understanding. We\\noutperform state-of-the-art outfit recommendation results on three benchmark\\ndatasets.\\n</summary>\\n    <author>\\n      <name>Katrien Laenen</name>\\n    </author>\\n    <author>\\n      <name>Marie-Francine Moens</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1908.10585v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1908.10585v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2003.08756v1</id>\\n    <updated>2020-03-04T15:02:05Z</updated>\\n    <published>2020-03-04T15:02:05Z</published>\\n    <title>Deep Neural Network Perception Models and Robust Autonomous Driving\\n  Systems</title>\\n    <summary>  This paper analyzes the robustness of deep learning models in autonomous\\ndriving applications and discusses the practical solutions to address that.\\n</summary>\\n    <author>\\n      <name>Mohammad Javad Shafiee</name>\\n    </author>\\n    <author>\\n      <name>Ahmadreza Jeddi</name>\\n    </author>\\n    <author>\\n      <name>Amir Nazemi</name>\\n    </author>\\n    <author>\\n      <name>Paul Fieguth</name>\\n    </author>\\n    <author>\\n      <name>Alexander Wong</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2003.08756v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2003.08756v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2003.09234v1</id>\\n    <updated>2020-03-11T13:20:42Z</updated>\\n    <published>2020-03-11T13:20:42Z</published>\\n    <title>DeepFake Detection: Current Challenges and Next Steps</title>\\n    <summary>  High quality fake videos and audios generated by AI-algorithms (the deep\\nfakes) have started to challenge the status of videos and audios as definitive\\nevidence of events. In this paper, we highlight a few of these challenges and\\ndiscuss the research opportunities in this direction.\\n</summary>\\n    <author>\\n      <name>Siwei Lyu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">arXiv admin note: text overlap with arXiv:1909.12962</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2003.09234v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2003.09234v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2003.09971v2</id>\\n    <updated>2020-05-10T21:59:36Z</updated>\\n    <published>2020-03-22T19:04:25Z</published>\\n    <title>A Better Variant of Self-Critical Sequence Training</title>\\n    <summary>  In this work, we present a simple yet better variant of Self-Critical\\nSequence Training. We make a simple change in the choice of baseline function\\nin REINFORCE algorithm. The new baseline can bring better performance with no\\nextra cost, compared to the greedy decoding baseline.\\n</summary>\\n    <author>\\n      <name>Ruotian Luo</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2003.09971v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2003.09971v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2006.02536v1</id>\\n    <updated>2020-06-03T21:13:05Z</updated>\\n    <published>2020-06-03T21:13:05Z</published>\\n    <title>Phasic dopamine release identification using ensemble of AlexNet</title>\\n    <summary>  Dopamine (DA) is an organic chemical that influences several parts of\\nbehaviour and physical functions. Fast-scan cyclic voltammetry (FSCV) is a\\ntechnique used for in vivo phasic dopamine release measurements. The analysis\\nof such measurements, though, requires notable effort. In this paper, we\\npresent the use of convolutional neural networks (CNNs) for the identification\\nof phasic dopamine releases.\\n</summary>\\n    <author>\\n      <name>Luca Patarnello</name>\\n    </author>\\n    <author>\\n      <name>Marco Celin</name>\\n    </author>\\n    <author>\\n      <name>Loris Nanni</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2006.02536v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2006.02536v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2006.02692v2</id>\\n    <updated>2020-06-05T07:51:53Z</updated>\\n    <published>2020-06-04T08:20:30Z</published>\\n    <title>Problems of dataset creation for light source estimation</title>\\n    <summary>  The paper describes our experience collecting a new dataset for the light\\nsource estimation problem in a single image. The analysis of existing color\\ntargets is presented along with various technical and scientific aspects\\nessential for data collection. The paper also contains an announcement of an\\nupcoming 2-nd International Illumination Estimation Challenge (IEC 2020).\\n</summary>\\n    <author>\\n      <name>E. I. Ershov</name>\\n    </author>\\n    <author>\\n      <name>A. V. Belokopytov</name>\\n    </author>\\n    <author>\\n      <name>A. V. Savchik</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2006.02692v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2006.02692v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2006.05927v1</id>\\n    <updated>2020-06-10T16:25:28Z</updated>\\n    <published>2020-06-10T16:25:28Z</published>\\n    <title>Recent Advances in 3D Object and Hand Pose Estimation</title>\\n    <summary>  3D object and hand pose estimation have huge potentials for Augmented\\nReality, to enable tangible interfaces, natural interfaces, and blurring the\\nboundaries between the real and virtual worlds. In this chapter, we present the\\nrecent developments for 3D object and hand pose estimation using cameras, and\\ndiscuss their abilities and limitations and the possible future development of\\nthe field.\\n</summary>\\n    <author>\\n      <name>Vincent Lepetit</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2006.05927v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2006.05927v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2006.10923v1</id>\\n    <updated>2020-06-19T01:49:37Z</updated>\\n    <published>2020-06-19T01:49:37Z</published>\\n    <title>Hyperparameter Analysis for Image Captioning</title>\\n    <summary>  In this paper, we perform a thorough sensitivity analysis on state-of-the-art\\nimage captioning approaches using two different architectures: CNN+LSTM and\\nCNN+Transformer. Experiments were carried out using the Flickr8k dataset. The\\nbiggest takeaway from the experiments is that fine-tuning the CNN encoder\\noutperforms the baseline and all other experiments carried out for both\\narchitectures.\\n</summary>\\n    <author>\\n      <name>Amish Patel</name>\\n    </author>\\n    <author>\\n      <name>Aravind Varier</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages, 9 figures, and 7 tables</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2006.10923v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2006.10923v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2006.14319v1</id>\\n    <updated>2020-06-25T11:50:35Z</updated>\\n    <published>2020-06-25T11:50:35Z</published>\\n    <title>Deep Learning for Cornea Microscopy Blind Deblurring</title>\\n    <summary>  The goal of this project is to build a deep-learning solution that deblurs\\ncornea scans, used for medical examination. The spherical shape of the eye\\nprevents ophtamologist from having completely sharp image. Provided with a\\nstack of corneas from confocal images, our approach is to build a model that\\nperforms an upscaling of the images using an SR (Super Resolution) Network.\\n</summary>\\n    <author>\\n      <name>Toussain Cardot</name>\\n    </author>\\n    <author>\\n      <name>Pilar Marxer</name>\\n    </author>\\n    <author>\\n      <name>Ivan Snozzi</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2006.14319v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2006.14319v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2008.00168v2</id>\\n    <updated>2020-10-21T11:10:31Z</updated>\\n    <published>2020-08-01T04:31:11Z</published>\\n    <title>Land Cover Classification from Remote Sensing Images Based on\\n  Multi-Scale Fully Convolutional Network</title>\\n    <summary>  In this paper, a Multi-Scale Fully Convolutional Network (MSFCN) with\\nmulti-scale convolutional kernel is proposed to exploit discriminative\\nrepresentations from two-dimensional (2D) satellite images.\\n</summary>\\n    <author>\\n      <name>Rui Li</name>\\n    </author>\\n    <author>\\n      <name>Shunyi Zheng</name>\\n    </author>\\n    <author>\\n      <name>Chenxi Duan</name>\\n    </author>\\n    <author>\\n      <name>Ce Zhang</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1080/10095020.2021.2017237</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1080/10095020.2021.2017237\" rel=\"related\"/>\\n    <link href=\"http://arxiv.org/abs/2008.00168v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2008.00168v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2008.05336v1</id>\\n    <updated>2020-08-12T14:13:53Z</updated>\\n    <published>2020-08-12T14:13:53Z</published>\\n    <title>Image-based Portrait Engraving</title>\\n    <summary>  This paper describes a simple image-based method that applies engraving\\nstylisation to portraits using ordered dithering. Face detection is used to\\nestimate a rough proxy geometry of the head consisting of a cylinder, which is\\nused to warp the dither matrix, causing the engraving lines to curve around the\\nface for better stylisation. Finally, an application of the approach to colour\\nengraving is demonstrated.\\n</summary>\\n    <author>\\n      <name>Paul L. Rosin</name>\\n    </author>\\n    <author>\\n      <name>Yu-Kun Lai</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages, 8 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2008.05336v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2008.05336v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2008.10236v1</id>\\n    <updated>2020-08-24T07:37:12Z</updated>\\n    <published>2020-08-24T07:37:12Z</published>\\n    <title>Strawberry Detection using Mixed Training on Simulated and Real Data</title>\\n    <summary>  This paper demonstrates how simulated images can be useful for object\\ndetection tasks in the agricultural sector, where labeled data can be scarce\\nand costly to collect. We consider training on mixed datasets with real and\\nsimulated data for strawberry detection in real images. Our results show that\\nusing the real dataset augmented by the simulated dataset resulted in slightly\\nhigher accuracy.\\n</summary>\\n    <author>\\n      <name>Sunny Goondram</name>\\n    </author>\\n    <author>\\n      <name>Akansel Cosgun</name>\\n    </author>\\n    <author>\\n      <name>Dana Kulic</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">DICTA 2020 Short Paper Track</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2008.10236v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2008.10236v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2009.05132v1</id>\\n    <updated>2020-08-24T05:45:20Z</updated>\\n    <published>2020-08-24T05:45:20Z</published>\\n    <title>1st Place Solution to Google Landmark Retrieval 2020</title>\\n    <summary>  This paper presents the 1st place solution to the Google Landmark Retrieval\\n2020 Competition on Kaggle. The solution is based on metric learning to\\nclassify numerous landmark classes, and uses transfer learning with two train\\ndatasets, fine-tuning on bigger images, adjusting loss weight for cleaner\\nsamples, and esemble to enhance the model\\'s performance further. Finally, it\\nscored 0.38677 mAP@100 on the private leaderboard.\\n</summary>\\n    <author>\\n      <name>SeungKee Jeon</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">3 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2009.05132v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2009.05132v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2009.09703v1</id>\\n    <updated>2020-09-21T09:18:19Z</updated>\\n    <published>2020-09-21T09:18:19Z</published>\\n    <title>The High-Quality Wide Multi-Channel Attack (HQ-WMCA) database</title>\\n    <summary>  The High-Quality Wide Multi-Channel Attack database (HQ-WMCA) database\\nextends the previous Wide Multi-Channel Attack database(WMCA), with more\\nchannels including color, depth, thermal, infrared (spectra), and short-wave\\ninfrared (spectra), and also a wide variety of attacks.\\n</summary>\\n    <author>\\n      <name>Zohreh Mostaani</name>\\n    </author>\\n    <author>\\n      <name>Anjith George</name>\\n    </author>\\n    <author>\\n      <name>Guillaume Heusch</name>\\n    </author>\\n    <author>\\n      <name>David Geissbuhler</name>\\n    </author>\\n    <author>\\n      <name>Sebastien Marcel</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2009.09703v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2009.09703v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2009.10115v1</id>\\n    <updated>2020-09-21T18:19:23Z</updated>\\n    <published>2020-09-21T18:19:23Z</published>\\n    <title>Extreme compression of grayscale images</title>\\n    <summary>  Given an grayscale digital image, and a positive integer $n$, how well can we\\nstore the image at a compression ratio of $n:1$?\\n  In this paper we address the above question in extreme cases when $n&gt;&gt;50$\\nusing \"$\\\\mathbf{V}$-variable image compression\".\\n</summary>\\n    <author>\\n      <name>Franklin Mendivil</name>\\n    </author>\\n    <author>\\n      <name>\\xc3\\x96rjan Stenflo</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1016/j.cnsns.2020.105546</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1016/j.cnsns.2020.105546\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">16 pages, 12 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2009.10115v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2009.10115v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"28A80, 68U10, 94A08\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2009.13793v1</id>\\n    <updated>2020-09-29T05:58:31Z</updated>\\n    <published>2020-09-29T05:58:31Z</published>\\n    <title>A comparison of classical and variational autoencoders for anomaly\\n  detection</title>\\n    <summary>  This paper analyzes and compares a classical and a variational autoencoder in\\nthe context of anomaly detection. To better understand their architecture and\\nfunctioning, describe their properties and compare their performance, it\\nexplores how they address a simple problem: reconstructing a line with a slope.\\n</summary>\\n    <author>\\n      <name>Fabrizio Patuzzo</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2009.13793v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2009.13793v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2104.01622v1</id>\\n    <updated>2021-04-04T15:02:32Z</updated>\\n    <published>2021-04-04T15:02:32Z</published>\\n    <title>OnTarget: An Electronic Archery Scoring</title>\\n    <summary>  There are several challenges in creating an electronic archery scoring system\\nusing computer vision techniques. Variability of light, reconstruction of the\\ntarget from several images, variability of target configuration, and filtering\\nnoise were significant challenges during the creation of this scoring system.\\nThis paper discusses the approach used to determine where an arrow hits a\\ntarget, for any possible single or set of targets and provides an algorithm\\nthat balances the difficulty of robust arrow detection while retaining the\\nrequired accuracy.\\n</summary>\\n    <author>\\n      <name>Andreea Danielescu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2104.01622v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2104.01622v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2106.02222v1</id>\\n    <updated>2021-06-04T02:55:46Z</updated>\\n    <published>2021-06-04T02:55:46Z</published>\\n    <title>History Encoding Representation Design for Human Intention Inference</title>\\n    <summary>  In this extended abstract, we investigate the design of learning\\nrepresentation for human intention inference. In our designed human intention\\nprediction task, we propose a history encoding representation that is both\\ninterpretable and effective for prediction. Through extensive experiments, we\\nshow our prediction framework with a history encoding representation design is\\nsuccessful on the human intention prediction problem.\\n</summary>\\n    <author>\\n      <name>Zhuo Xu</name>\\n    </author>\\n    <author>\\n      <name>Masayoshi Tomizuka</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2106.02222v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2106.02222v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2106.04104v1</id>\\n    <updated>2021-06-08T05:06:51Z</updated>\\n    <published>2021-06-08T05:06:51Z</published>\\n    <title>Design of Low-Artifact Interpolation Kernels by Means of Computer\\n  Algebra</title>\\n    <summary>  We present a number of new piecewise-polynomial kernels for image\\ninterpolation. The kernels are constructed by optimizing a measure of\\ninterpolation quality based on the magnitude of anisotropic artifacts. The\\nkernel design process is performed symbolically using Mathematica computer\\nalgebra system. Experimental evaluation involving 14 image quality assessment\\nmethods demonstrates that our results compare favorably with the existing\\nlinear interpolators.\\n</summary>\\n    <author>\\n      <name>Peter Karpov</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">22 pages, 6 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2106.04104v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2106.04104v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.SC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2106.08042v1</id>\\n    <updated>2021-06-15T10:52:07Z</updated>\\n    <published>2021-06-15T10:52:07Z</published>\\n    <title>Hotel Recognition via Latent Image Embedding</title>\\n    <summary>  We approach the problem of hotel recognition with deep metric learning. We\\noverview the existing approaches and propose a modification to Contrastive loss\\ncalled Contrastive-Triplet loss. We construct a robust pipeline for\\nbenchmarking metric learning models and perform experiments on Hotels-50K and\\nCUB200 datasets. Contrastive-Triplet loss is shown to achieve better retrieval\\non Hotels-50k. We open-source our code.\\n</summary>\\n    <author>\\n      <name>Boris Tseytlin</name>\\n    </author>\\n    <author>\\n      <name>Ilya Makarov</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IWANN 2021</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2106.08042v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2106.08042v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2106.08366v1</id>\\n    <updated>2021-06-15T18:36:54Z</updated>\\n    <published>2021-06-15T18:36:54Z</published>\\n    <title>Explaining decision of model from its prediction</title>\\n    <summary>  This document summarizes different visual explanations methods such as CAM,\\nGrad-CAM, Localization using Multiple Instance Learning - Saliency-based\\nmethods, Saliency-driven Class-Impressions, Muting pixels in input image -\\nAdversarial methods and Activation visualization, Convolution filter\\nvisualization - Feature-based methods. We have also shown the results produced\\nby different methods and a comparison between CAM, GradCAM, and Guided\\nBackpropagation.\\n</summary>\\n    <author>\\n      <name>Dipesh Tamboli</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Literature review</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2106.08366v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2106.08366v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2106.11467v1</id>\\n    <updated>2021-06-22T01:02:52Z</updated>\\n    <published>2021-06-22T01:02:52Z</published>\\n    <title>Multimodal trajectory forecasting based on discrete heat map</title>\\n    <summary>  In Argoverse motion forecasting competition, the task is to predict the\\nprobabilistic future trajectory distribution for the interested targets in the\\ntraffic scene. We use vectorized lane map and 2 s targets\\' history trajectories\\nas input. Then the model outputs 6 forecasted trajectories with probability for\\neach target.\\n</summary>\\n    <author>\\n      <name>Jingni Yuan</name>\\n    </author>\\n    <author>\\n      <name>Jianyun Xu</name>\\n    </author>\\n    <author>\\n      <name>Yushi Zhu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2106.11467v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2106.11467v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2106.12016v1</id>\\n    <updated>2021-06-22T18:42:44Z</updated>\\n    <published>2021-06-22T18:42:44Z</published>\\n    <title>On Matrix Factorizations in Subspace Clustering</title>\\n    <summary>  This article explores subspace clustering algorithms using CUR\\ndecompositions, and examines the effect of various hyperparameters in these\\nalgorithms on clustering performance on two real-world benchmark datasets, the\\nHopkins155 motion segmentation dataset and the Yale face dataset. Extensive\\nexperiments are done for a variety of sampling methods and oversampling\\nparameters for these datasets, and some guidelines for parameter choices are\\ngiven for practical applications.\\n</summary>\\n    <author>\\n      <name>Reeshad Arian</name>\\n    </author>\\n    <author>\\n      <name>Keaton Hamm</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">13 pages plus 4 pages of tables</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2106.12016v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2106.12016v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68P99, 68T10, 62H30\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2106.15179v1</id>\\n    <updated>2021-06-29T08:51:23Z</updated>\\n    <published>2021-06-29T08:51:23Z</published>\\n    <title>Wrong Colored Vermeer: Color-Symmetric Image Distortion</title>\\n    <summary>  Color symmetry implies that the colors of geometrical objects are assigned\\naccording to their symmetry properties. It is defined by associating the\\nelements of the symmetry group with a color permutation. I use this concept for\\ngenerative art and apply symmetry-consistent color distortions to images of\\npaintings by Johannes Vermeer. The color permutations are realized as mappings\\nof the HSV color space onto itself.\\n</summary>\\n    <author>\\n      <name>Hendrik Richter</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2106.15179v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2106.15179v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.GR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2106.15306v1</id>\\n    <updated>2021-06-08T14:57:25Z</updated>\\n    <published>2021-06-08T14:57:25Z</published>\\n    <title>Artificial Intelligence in Minimally Invasive Interventional Treatment</title>\\n    <summary>  Minimally invasive image guided treatment procedures often employ advanced\\nimage processing algorithms. The recent developments of artificial intelligence\\nalgorithms harbor potential to further enhance this domain. In this article we\\nexplore several application areas within the minimally invasive treatment space\\nand discuss the deployment of artificial intelligence within these areas.\\n</summary>\\n    <author>\\n      <name>Daniel Ruijters</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2106.15306v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2106.15306v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.GR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.2.1; I.2.10; I.4.9\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2108.04453v2</id>\\n    <updated>2021-08-11T02:36:26Z</updated>\\n    <published>2021-08-10T05:25:59Z</published>\\n    <title>Method Towards CVPR 2021 Image Matching Challenge</title>\\n    <summary>  This report describes Megvii-3D team\\'s approach towards CVPR 2021 Image\\nMatching Workshop.\\n</summary>\\n    <author>\\n      <name>Xiaopeng Bi</name>\\n    </author>\\n    <author>\\n      <name>Yu Chen</name>\\n    </author>\\n    <author>\\n      <name>Xinyang Liu</name>\\n    </author>\\n    <author>\\n      <name>Dehao Zhang</name>\\n    </author>\\n    <author>\\n      <name>Ran Yan</name>\\n    </author>\\n    <author>\\n      <name>Zheng Chai</name>\\n    </author>\\n    <author>\\n      <name>Haotian Zhang</name>\\n    </author>\\n    <author>\\n      <name>Xiao Liu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2108.04453v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2108.04453v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2109.00816v1</id>\\n    <updated>2021-09-02T09:47:43Z</updated>\\n    <published>2021-09-02T09:47:43Z</published>\\n    <title>Deep Learning-based mitosis detection in breast cancer histologic\\n  samples</title>\\n    <summary>  This is the submission for mitosis detection in the context of the MIDOG 2021\\nchallenge. It is based on the two-stage objection model Faster RCNN as well as\\nDenseNet as a backbone for the neural network architecture. It achieves a\\nF1-score of 0.6645 on the Preliminary Test Phase Leaderboard.\\n</summary>\\n    <author>\\n      <name>Michel Halmes</name>\\n    </author>\\n    <author>\\n      <name>Hippolyte Heuberger</name>\\n    </author>\\n    <author>\\n      <name>Sylvain Berlemont</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2109.00816v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2109.00816v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2109.13126v1</id>\\n    <updated>2021-09-27T15:39:27Z</updated>\\n    <published>2021-09-27T15:39:27Z</published>\\n    <title>GANiry: Bald-to-Hairy Translation Using CycleGAN</title>\\n    <summary>  This work presents our computer vision course project called bald\\nmen-to-hairy men translation using CycleGAN. On top of CycleGAN architecture,\\nwe utilize perceptual loss in order to achieve more realistic results. We also\\nintegrate conditional constrains to obtain different stylized and colored hairs\\non bald men. We conducted extensive experiments and present qualitative results\\nin this paper. Our code and models are available at\\nhttps://github.com/fidansamet/GANiry.\\n</summary>\\n    <author>\\n      <name>Fidan Samet</name>\\n    </author>\\n    <author>\\n      <name>Oguz Bakir</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2109.13126v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2109.13126v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2110.07201v1</id>\\n    <updated>2021-10-14T07:54:36Z</updated>\\n    <published>2021-10-14T07:54:36Z</published>\\n    <title>Coarse to Fine: Video Retrieval before Moment Localization</title>\\n    <summary>  The current state-of-the-art methods for video corpus moment retrieval (VCMR)\\noften use similarity-based feature alignment approach for the sake of\\nconvenience and speed. However, late fusion methods like cosine similarity\\nalignment are unable to make full use of the information from both query texts\\nand videos. In this paper, we combine feature alignment with feature fusion to\\npromote the performance on VCMR.\\n</summary>\\n    <author>\\n      <name>Zijian Gao</name>\\n    </author>\\n    <author>\\n      <name>Huanyu Liu</name>\\n    </author>\\n    <author>\\n      <name>Jingyu Liu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2110.07201v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2110.07201v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2110.14830v1</id>\\n    <updated>2021-10-28T00:35:32Z</updated>\\n    <published>2021-10-28T00:35:32Z</published>\\n    <title>ODMTCNet: An Interpretable Multi-view Deep Neural Network Architecture\\n  for Image Feature Representation</title>\\n    <summary>  This work proposes an interpretable multi-view deep neural network\\narchitecture, namely optimal discriminant multi-view tensor convolutional\\nnetwork (ODMTCNet), by integrating statistical machine learning (SML)\\nprinciples with the deep neural network (DNN) architecture.\\n</summary>\\n    <author>\\n      <name>Lei Gao</name>\\n    </author>\\n    <author>\\n      <name>Zheng Guo</name>\\n    </author>\\n    <author>\\n      <name>Ling Guan</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Submitted to IEEE TPAMI</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2110.14830v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2110.14830v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2111.04839v1</id>\\n    <updated>2021-11-08T21:23:37Z</updated>\\n    <published>2021-11-08T21:23:37Z</published>\\n    <title>Evolving Evocative 2D Views of Generated 3D Objects</title>\\n    <summary>  We present a method for jointly generating 3D models of objects and 2D\\nrenders at different viewing angles, with the process guided by ImageNet and\\nCLIP -based models. Our results indicate that it can generate anamorphic\\nobjects, with renders that both evoke the target caption and look visually\\nappealing.\\n</summary>\\n    <author>\\n      <name>Eric Chu</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">NeurIPS 2021 Workshop on Machine Learning for Creativity and\\n  Design</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/2111.04839v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2111.04839v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2111.05471v1</id>\\n    <updated>2021-11-10T00:56:45Z</updated>\\n    <published>2021-11-10T00:56:45Z</published>\\n    <title>Analysis of PDE-based binarization model for degraded document images</title>\\n    <summary>  This report presents the results of a PDE-based binarization model for\\ndegraded document images. The model utilizes an edge and binary source term in\\nits formulation. Results indicate effectiveness for document images with\\nbleed-through and faded text and stains to a lesser extent.\\n</summary>\\n    <author>\\n      <name>Uche A. Nnolim</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">11 pages, 6 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2111.05471v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2111.05471v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2111.06662v1</id>\\n    <updated>2021-11-12T11:08:15Z</updated>\\n    <published>2021-11-12T11:08:15Z</published>\\n    <title>A comprehensive study of clustering a class of 2D shapes</title>\\n    <summary>  The paper concerns clustering with respect to the shape and size of 2D\\ncontours that are boundaries of cross-sections of 3D objects of revolution. We\\npropose a number of similarity measures based on combined disparate Procrustes\\nanalysis (PA) and Dynamic Time Warping (DTW) distances. Motivation and the main\\napplication for this study comes from archaeology. The performed computational\\nexperiments refer to the clustering of archaeological pottery.\\n</summary>\\n    <author>\\n      <name>Agnieszka Kaliszewska</name>\\n    </author>\\n    <author>\\n      <name>Monika Syga</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2111.06662v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2111.06662v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2111.09113v1</id>\\n    <updated>2021-11-15T11:51:36Z</updated>\\n    <published>2021-11-15T11:51:36Z</published>\\n    <title>2nd Place Solution to Facebook AI Image Similarity Challenge Matching\\n  Track</title>\\n    <summary>  This paper presents the 2nd place solution to the Facebook AI Image\\nSimilarity Challenge : Matching Track on DrivenData. The solution is based on\\nself-supervised learning, and Vision Transformer(ViT). The main breaktrough\\ncomes from concatenating query and reference image to form as one image and\\nasking ViT to directly predict from the image if query image used reference\\nimage. The solution scored 0.8291 Micro-average Precision on the private\\nleaderboard.\\n</summary>\\n    <author>\\n      <name>SeungKee Jeon</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2111.09113v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2111.09113v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2201.08122v1</id>\\n    <updated>2022-01-20T12:01:06Z</updated>\\n    <published>2022-01-20T12:01:06Z</published>\\n    <title>A Computational Model for Machine Thinking</title>\\n    <summary>  A machine thinking model is proposed in this report based on recent advances\\nof computer vision and the recent results of neuroscience devoted to brain\\nunderstanding. We deliver the result of machine thinking in the form of\\nsentences of natural-language or drawn sketches either informative or\\ndecisional. This result is obtained from a reasoning performed on new acquired\\ndata and memorized data.\\n</summary>\\n    <author>\\n      <name>Slimane Larabi</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Internal report, RIIMA Laboratory</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2201.08122v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2201.08122v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2201.10522v1</id>\\n    <updated>2022-01-22T08:21:12Z</updated>\\n    <published>2022-01-22T08:21:12Z</published>\\n    <title>Blind Image Deblurring: a Review</title>\\n    <summary>  This is a review on blind image deblurring. First, we formulate the blind\\nimage deblurring problem and explain why it is challenging. Next, we bring some\\npsychological and cognitive studies on the way our human vision system deblurs.\\nThen, relying on several previous reviews, we discuss the topic of metrics and\\ndatasets, which is non-trivial to blind deblurring. Finally, we introduce some\\ntypical optimization-based methods and learning-based methods.\\n</summary>\\n    <author>\\n      <name>Zhengrong Xue</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2201.10522v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2201.10522v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2202.07437v3</id>\\n    <updated>2023-03-19T13:11:59Z</updated>\\n    <published>2022-02-09T01:24:36Z</published>\\n    <title>Mathematical Cookbook for Snapshot Compressive Imaging</title>\\n    <summary>  The author intends to provide you with a beautiful, elegant, user-friendly\\ncookbook for mathematics in Snapshot Compressive Imaging (SCI). Currently, the\\ncookbook is composed of introduction, conventional optimization, and deep\\nequilibrium models. The latest releases are strongly recommended! For any other\\nquestions, suggestions, or comments, feel free to email the author.\\n</summary>\\n    <author>\\n      <name>Yaping Zhao</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">15 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2202.07437v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2202.07437v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2202.07572v3</id>\\n    <updated>2025-08-10T15:24:18Z</updated>\\n    <published>2022-02-15T16:58:49Z</published>\\n    <title>On Representation Learning with Feedback</title>\\n    <summary>  This note complements the author\\'s recent paper \"Robust representation\\nlearning with feedback for single image deraining\" by providing heuristically\\ntheoretical explanations on the mechanism of representation learning with\\nfeedback, namely an essential merit of the works presented in this recent\\narticle. This note facilitates understanding of key points in the mechanism of\\nrepresentation learning with feedback.\\n</summary>\\n    <author>\\n      <name>Hao Li</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2202.07572v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2202.07572v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2202.13837v1</id>\\n    <updated>2022-02-28T14:48:51Z</updated>\\n    <published>2022-02-28T14:48:51Z</published>\\n    <title>Fuse Local and Global Semantics in Representation Learning</title>\\n    <summary>  We propose Fuse Local and Global Semantics in Representation Learning (FLAGS)\\nto generate richer representations. FLAGS aims at extract both global and local\\nsemantics from images to benefit various downstream tasks. It shows promising\\nresults under common linear evaluation protocol. We also conduct detection and\\nsegmentation on PASCAL VOC and COCO to show the representations extracted by\\nFLAGS are transferable.\\n</summary>\\n    <author>\\n      <name>Yuchi Zhao</name>\\n    </author>\\n    <author>\\n      <name>Yuhao Zhou</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2202.13837v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2202.13837v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2203.06890v2</id>\\n    <updated>2022-03-21T15:34:03Z</updated>\\n    <published>2022-03-14T07:11:20Z</published>\\n    <title>Attention based Memory video portrait matting</title>\\n    <summary>  We proposed a novel trimap free video matting method based on the attention\\nmechanism. By the nature of the problem, most existing approaches use either\\nmultiple computational expansive modules or complex algorithms to exploit\\ntemporal information fully. We designed a temporal aggregation module to\\ncompute the temporal coherence between the current frame and its two previous\\nframes.\\n</summary>\\n    <author>\\n      <name>Shufeng Song</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">12 pages, 6 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2203.06890v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2203.06890v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2203.13185v1</id>\\n    <updated>2022-03-24T17:02:43Z</updated>\\n    <published>2022-03-24T17:02:43Z</published>\\n    <title>Quantum Motion Segmentation</title>\\n    <summary>  Motion segmentation is a challenging problem that seeks to identify\\nindependent motions in two or several input images. This paper introduces the\\nfirst algorithm for motion segmentation that relies on adiabatic quantum\\noptimization of the objective function. The proposed method achieves on-par\\nperformance with the state of the art on problem instances which can be mapped\\nto modern quantum annealers.\\n</summary>\\n    <author>\\n      <name>Federica Arrigoni</name>\\n    </author>\\n    <author>\\n      <name>Willi Menapace</name>\\n    </author>\\n    <author>\\n      <name>Marcel Seelbach Benkner</name>\\n    </author>\\n    <author>\\n      <name>Elisa Ricci</name>\\n    </author>\\n    <author>\\n      <name>Vladislav Golyanik</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2203.13185v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2203.13185v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2204.01081v1</id>\\n    <updated>2022-04-03T14:28:16Z</updated>\\n    <published>2022-04-03T14:28:16Z</published>\\n    <title>Faces: AI Blitz XIII Solutions</title>\\n    <summary>  AI Blitz XIII Faces challenge hosted on www.aicrowd.com platform consisted of\\nfive problems: Sentiment Classification, Age Prediction, Mask Prediction, Face\\nRecognition, and Face De-Blurring. Our team GLaDOS took second place. Here we\\npresent our solutions and results. Code implementation:\\nhttps://github.com/ndrwmlnk/ai-blitz-xiii\\n</summary>\\n    <author>\\n      <name>Andrew Melnik</name>\\n    </author>\\n    <author>\\n      <name>Eren Akbulut</name>\\n    </author>\\n    <author>\\n      <name>Jannik Sheikh</name>\\n    </author>\\n    <author>\\n      <name>Kira Loos</name>\\n    </author>\\n    <author>\\n      <name>Michael Buettner</name>\\n    </author>\\n    <author>\\n      <name>Tobias Lenze</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2204.01081v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2204.01081v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2204.06120v1</id>\\n    <updated>2022-04-13T00:11:45Z</updated>\\n    <published>2022-04-13T00:11:45Z</published>\\n    <title>Baseline Computation for Attribution Methods Based on Interpolated\\n  Inputs</title>\\n    <summary>  We discuss a way to find a well behaved baseline for attribution methods that\\nwork by feeding a neural network with a sequence of interpolated inputs between\\ntwo given inputs. Then, we test it with our novel Riemann-Stieltjes Integrated\\nGradient-weighted Class Activation Mapping (RSI-Grad-CAM) attribution method.\\n</summary>\\n    <author>\\n      <name>Miguel Lerma</name>\\n    </author>\\n    <author>\\n      <name>Mirtha Lucas</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages, 3 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2204.06120v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2204.06120v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68T07\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"K.3.2\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2205.00934v1</id>\\n    <updated>2022-05-02T14:32:59Z</updated>\\n    <published>2022-05-02T14:32:59Z</published>\\n    <title>Assessing unconstrained surgical cuttings in VR using CNNs</title>\\n    <summary>  We present a Convolutional Neural Network (CNN) suitable to assess\\nunconstrained surgical cuttings, trained on a dataset created with a data\\naugmentation technique.\\n</summary>\\n    <author>\\n      <name>Ilias Chrysovergis</name>\\n    </author>\\n    <author>\\n      <name>Manos Kamarianakis</name>\\n    </author>\\n    <author>\\n      <name>Mike Kentros</name>\\n    </author>\\n    <author>\\n      <name>Dimitris Angelis</name>\\n    </author>\\n    <author>\\n      <name>Antonis Protopsaltis</name>\\n    </author>\\n    <author>\\n      <name>George Papagiannakis</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">2 pages, 2 figures, Submitted to the Siggraph \\'22 Poster Session\\n  (Vancouver, 8-11 Aug 2022)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2205.00934v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2205.00934v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.GR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2205.06873v1</id>\\n    <updated>2022-05-13T20:12:12Z</updated>\\n    <published>2022-05-13T20:12:12Z</published>\\n    <title>Using Augmented Face Images to Improve Facial Recognition Tasks</title>\\n    <summary>  We present a framework that uses GAN-augmented images to complement certain\\nspecific attributes, usually underrepresented, for machine learning model\\ntraining. This allows us to improve inference quality over those attributes for\\nthe facial recognition tasks.\\n</summary>\\n    <author>\\n      <name>Shuo Cheng</name>\\n    </author>\\n    <author>\\n      <name>Guoxian Song</name>\\n    </author>\\n    <author>\\n      <name>Wan-Chun Ma</name>\\n    </author>\\n    <author>\\n      <name>Chao Wang</name>\\n    </author>\\n    <author>\\n      <name>Linjie Luo</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">CHI 2022 Workshop: AI-Generated Characters: Putting Deepfakes to Good\\n  Use</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2205.06873v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2205.06873v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.GR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2207.03843v1</id>\\n    <updated>2022-07-08T11:54:37Z</updated>\\n    <published>2022-07-08T11:54:37Z</published>\\n    <title>Continuous Methods : Hamiltonian Domain Translation</title>\\n    <summary>  This paper proposes a novel approach to domain translation. Leveraging\\nestablished parallels between generative models and dynamical systems, we\\npropose a reformulation of the Cycle-GAN architecture. By embedding our model\\nwith a Hamiltonian structure, we obtain a continuous, expressive and most\\nimportantly invertible generative model for domain translation.\\n</summary>\\n    <author>\\n      <name>Emmanuel Menier</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">LISN, Inria, IRT SystemX</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Michele Alessandro Bucci</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Inria</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Mouadh Yagoubi</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IRT SystemX</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Lionel Mathelin</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">LISN</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Marc Schoenauer</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Inria, LISN</arxiv:affiliation>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2207.03843v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2207.03843v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2207.06553v1</id>\\n    <updated>2022-07-13T23:25:30Z</updated>\\n    <published>2022-07-13T23:25:30Z</published>\\n    <title>QML for Argoverse 2 Motion Forecasting Challenge</title>\\n    <summary>  To safely navigate in various complex traffic scenarios, autonomous driving\\nsystems are generally equipped with a motion forecasting module to provide\\nvital information for the downstream planning module. For the real-world\\nonboard applications, both accuracy and latency of a motion forecasting model\\nare essential. In this report, we present an effective and efficient solution,\\nwhich ranks the 3rd place in the Argoverse 2 Motion Forecasting Challenge 2022.\\n</summary>\\n    <author>\\n      <name>Tong Su</name>\\n    </author>\\n    <author>\\n      <name>Xishun Wang</name>\\n    </author>\\n    <author>\\n      <name>Xiaodong Yang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2207.06553v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2207.06553v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2207.10201v1</id>\\n    <updated>2022-07-20T21:38:47Z</updated>\\n    <published>2022-07-20T21:38:47Z</published>\\n    <title>Hybrid CNN-Transformer Model For Facial Affect Recognition In the ABAW4\\n  Challenge</title>\\n    <summary>  This paper describes our submission to the fourth Affective Behavior Analysis\\n(ABAW) competition. We proposed a hybrid CNN-Transformer model for the\\nMulti-Task-Learning (MTL) and Learning from Synthetic Data (LSD) task.\\nExperimental results on validation dataset shows that our method achieves\\nbetter performance than baseline model, which verifies that the effectiveness\\nof proposed network.\\n</summary>\\n    <author>\\n      <name>Lingfeng Wang</name>\\n    </author>\\n    <author>\\n      <name>Haocheng Li</name>\\n    </author>\\n    <author>\\n      <name>Chunyin Liu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2207.10201v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2207.10201v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2207.11329v1</id>\\n    <updated>2022-07-22T20:45:05Z</updated>\\n    <published>2022-07-22T20:45:05Z</published>\\n    <title>Video Swin Transformers for Egocentric Video Understanding @ Ego4D\\n  Challenges 2022</title>\\n    <summary>  We implemented Video Swin Transformer as a base architecture for the tasks of\\nPoint-of-No-Return temporal localization and Object State Change\\nClassification. Our method achieved competitive performance on both challenges.\\n</summary>\\n    <author>\\n      <name>Maria Escobar</name>\\n    </author>\\n    <author>\\n      <name>Laura Daza</name>\\n    </author>\\n    <author>\\n      <name>Cristina Gonz\\xc3\\xa1lez</name>\\n    </author>\\n    <author>\\n      <name>Jordi Pont-Tuset</name>\\n    </author>\\n    <author>\\n      <name>Pablo Arbel\\xc3\\xa1ez</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2207.11329v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2207.11329v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2210.09296v1</id>\\n    <updated>2022-10-14T12:26:13Z</updated>\\n    <published>2022-10-14T12:26:13Z</published>\\n    <title>3rd Place Solution for Google Universal Image Embedding</title>\\n    <summary>  This paper presents the 3rd place solution to the Google Universal Image\\nEmbedding Competition on Kaggle. We use ViT-H/14 from OpenCLIP for the backbone\\nof ArcFace, and trained in 2 stage. 1st stage is done with freezed backbone,\\nand 2nd stage is whole model training. We achieve 0.692 mean Precision @5 on\\nprivate leaderboard. Code available at\\nhttps://github.com/YasumasaNamba/google-universal-image-embedding\\n</summary>\\n    <author>\\n      <name>Nobuaki Aoki</name>\\n    </author>\\n    <author>\\n      <name>Yasumasa Namba</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">3 pages, 5 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2210.09296v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2210.09296v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2210.10594v1</id>\\n    <updated>2022-10-16T06:36:10Z</updated>\\n    <published>2022-10-16T06:36:10Z</published>\\n    <title>Motion-Based Weak Supervision for Video Parsing with Application to\\n  Colonoscopy</title>\\n    <summary>  We propose a two-stage unsupervised approach for parsing videos into phases.\\nWe use motion cues to divide the video into coarse segments. Noisy segment\\nlabels are then used to weakly supervise an appearance-based classifier. We\\nshow the effectiveness of the method for phase detection in colonoscopy videos.\\n</summary>\\n    <author>\\n      <name>Ori Kelner</name>\\n    </author>\\n    <author>\\n      <name>Or Weinstein</name>\\n    </author>\\n    <author>\\n      <name>Ehud Rivlin</name>\\n    </author>\\n    <author>\\n      <name>Roman Goldenberg</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2210.10594v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2210.10594v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2210.12417v2</id>\\n    <updated>2022-11-10T09:06:27Z</updated>\\n    <published>2022-10-22T11:17:30Z</published>\\n    <title>SLAMs: Semantic Learning based Activation Map for Weakly Supervised\\n  Semantic Segmentation</title>\\n    <summary>  Recent mainstream weakly-supervised semantic segmentation (WSSS) approaches\\nmainly relies on image-level classification learning, which has limited\\nrepresentation capacity. In this paper, we propose a novel semantic learning\\nbased framework, named SLAMs (Semantic Learning based Activation Map), for\\nWSSS.\\n</summary>\\n    <author>\\n      <name>Junliang Chen</name>\\n    </author>\\n    <author>\\n      <name>Xiaodong Zhao</name>\\n    </author>\\n    <author>\\n      <name>Minmin Liu</name>\\n    </author>\\n    <author>\\n      <name>Linlin Shen</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2210.12417v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2210.12417v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2211.15286v1</id>\\n    <updated>2022-11-18T16:05:15Z</updated>\\n    <published>2022-11-18T16:05:15Z</published>\\n    <title>Masked Autoencoders for Egocentric Video Understanding @ Ego4D Challenge\\n  2022</title>\\n    <summary>  In this report, we present our approach and empirical results of applying\\nmasked autoencoders in two egocentric video understanding tasks, namely, Object\\nState Change Classification and PNR Temporal Localization, of Ego4D Challenge\\n2022. As team TheSSVL, we ranked 2nd place in both tasks. Our code will be made\\navailable.\\n</summary>\\n    <author>\\n      <name>Jiachen Lei</name>\\n    </author>\\n    <author>\\n      <name>Shuang Ma</name>\\n    </author>\\n    <author>\\n      <name>Zhongjie Ba</name>\\n    </author>\\n    <author>\\n      <name>Sai Vemprala</name>\\n    </author>\\n    <author>\\n      <name>Ashish Kapoor</name>\\n    </author>\\n    <author>\\n      <name>Kui Ren</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2211.15286v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2211.15286v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2212.09027v1</id>\\n    <updated>2022-12-18T07:36:32Z</updated>\\n    <published>2022-12-18T07:36:32Z</published>\\n    <title>2D Pose Estimation based Child Action Recognition</title>\\n    <summary>  We present a graph convolutional network with 2D pose estimation for the\\nfirst time on child action recognition task achieving on par results with an\\nRGB modality based model on a novel benchmark dataset containing unconstrained\\nenvironment based videos.\\n</summary>\\n    <author>\\n      <name>Sanka Mohottala</name>\\n    </author>\\n    <author>\\n      <name>Sandun Abeygunawardana</name>\\n    </author>\\n    <author>\\n      <name>Pradeepa Samarasinghe</name>\\n    </author>\\n    <author>\\n      <name>Dharshana Kasthurirathna</name>\\n    </author>\\n    <author>\\n      <name>Charith Abhayaratne</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Paper Accepted for the IEEE TENCON Conference (2022). 7 pages, 5\\n  figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2212.09027v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2212.09027v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2212.13810v1</id>\\n    <updated>2022-12-28T13:06:30Z</updated>\\n    <published>2022-12-28T13:06:30Z</published>\\n    <title>All\\'s well that FID\\'s well? Result quality and metric scores in GAN\\n  models for lip-sychronization tasks</title>\\n    <summary>  We test the performance of GAN models for lip-synchronization. For this, we\\nreimplement LipGAN in Pytorch, train it on the dataset GRID and compare it to\\nour own variation, L1WGAN-GP, adapted to the LipGAN architecture and also\\ntrained on GRID.\\n</summary>\\n    <author>\\n      <name>Carina Geldhauser</name>\\n    </author>\\n    <author>\\n      <name>Johan Liljegren</name>\\n    </author>\\n    <author>\\n      <name>Pontus Nordqvist</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2212.13810v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2212.13810v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68T07\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2301.06936v1</id>\\n    <updated>2022-12-12T21:24:03Z</updated>\\n    <published>2022-12-12T21:24:03Z</published>\\n    <title>The use of Octree in point cloud analysis with application to cultural\\n  heritage</title>\\n    <summary>  In this article we present the effects of our work on the subject of the\\ntechnical approach to the 3D point cloud data analysis through the use of the\\nOctree method to compress, analyse and compute the initial data.\\n</summary>\\n    <author>\\n      <name>Rafa\\xc5\\x82 Bie\\xc5\\x84kowski</name>\\n    </author>\\n    <author>\\n      <name>Krzysztof E. Rutkowski</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages, 12 figures, 7 citations</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2301.06936v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2301.06936v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"E.1; E.2; J.2\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2301.07947v1</id>\\n    <updated>2023-01-19T08:47:31Z</updated>\\n    <published>2023-01-19T08:47:31Z</published>\\n    <title>Point Cloud Data Simulation and Modelling with Aize Workspace</title>\\n    <summary>  This work takes a look at data models often used in digital twins and\\npresents preliminary results specifically from surface reconstruction and\\nsemantic segmentation models trained using simulated data. This work is\\nexpected to serve as a ground work for future endeavours in data\\ncontextualisation inside a digital twin.\\n</summary>\\n    <author>\\n      <name>Boris Mocialov</name>\\n    </author>\\n    <author>\\n      <name>Eirik Eythorsson</name>\\n    </author>\\n    <author>\\n      <name>Reza Parseh</name>\\n    </author>\\n    <author>\\n      <name>Hoang Tran</name>\\n    </author>\\n    <author>\\n      <name>Vegard Flovik</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Extended abstract, Northern Lights Deep Learning Conference, 2023</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2301.07947v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2301.07947v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2302.12185v1</id>\\n    <updated>2023-02-02T19:19:10Z</updated>\\n    <published>2023-02-02T19:19:10Z</published>\\n    <title>Scaling Up Computer Vision Neural Networks Using Fast Fourier Transform</title>\\n    <summary>  Deep Learning-based Computer Vision field has recently been trying to explore\\nlarger kernels for convolution to effectively scale up Convolutional Neural\\nNetworks. Simultaneously, new paradigm of models such as Vision Transformers\\nfind it difficult to scale up to larger higher resolution images due to their\\nquadratic complexity in terms of input sequence. In this report, Fast Fourier\\nTransform is utilised in various ways to provide some solutions to these\\nissues.\\n</summary>\\n    <author>\\n      <name>Siddharth Agrawal</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2302.12185v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2302.12185v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2303.04208v1</id>\\n    <updated>2023-03-07T19:58:57Z</updated>\\n    <published>2023-03-07T19:58:57Z</published>\\n    <title>EscherNet 101</title>\\n    <summary>  A deep learning model, EscherNet 101, is constructed to categorize images of\\n2D periodic patterns into their respective 17 wallpaper groups. Beyond\\nevaluating EscherNet 101 performance by classification rates, at a micro-level\\nwe investigate the filters learned at different layers in the network, capable\\nof capturing second-order invariants beyond edge and curvature.\\n</summary>\\n    <author>\\n      <name>Christopher Funk</name>\\n    </author>\\n    <author>\\n      <name>Yanxi Liu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">16 page, 12 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2303.04208v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2303.04208v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"20-08 Computational methods for problems pertaining to group theory\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2303.12727v1</id>\\n    <updated>2023-03-13T11:31:35Z</updated>\\n    <published>2023-03-13T11:31:35Z</published>\\n    <title>A XGBoost Algorithm-based Fatigue Recognition Model Using Face Detection</title>\\n    <summary>  As fatigue is normally revealed in the eyes and mouth of a person\\'s face,\\nthis paper tried to construct a XGBoost Algorithm-Based fatigue recognition\\nmodel using the two indicators, EAR (Eye Aspect Ratio) and MAR(Mouth Aspect\\nRatio). With an accuracy rate of 87.37% and sensitivity rate of 89.14%, the\\nmodel was proved to be efficient and valid for further applications.\\n</summary>\\n    <author>\\n      <name>Xinrui Chen</name>\\n    </author>\\n    <author>\\n      <name>Bingquan Zhang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages;2 fiqures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2303.12727v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2303.12727v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2303.12946v1</id>\\n    <updated>2023-03-01T22:36:54Z</updated>\\n    <published>2023-03-01T22:36:54Z</published>\\n    <title>Underwater Camouflage Object Detection Dataset</title>\\n    <summary>  We have made a dataset of camouflage object detection mainly for complex\\nseabed scenes, and named it UnderWater RGB&amp;Sonar,or UW-RS for short. The UW-RS\\ndataset contains a total of 1972 image data. The dataset mainly consists of two\\nparts, namely underwater optical data part (UW-R dataset) and underwater sonar\\ndata part (UW-S dataset).\\n</summary>\\n    <author>\\n      <name>Feng Dong</name>\\n    </author>\\n    <author>\\n      <name>Jinchao Zhu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2303.12946v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2303.12946v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2304.02766v1</id>\\n    <updated>2023-04-05T22:14:53Z</updated>\\n    <published>2023-04-05T22:14:53Z</published>\\n    <title>Shape complexity estimation using VAE</title>\\n    <summary>  In this paper, we compare methods for estimating the complexity of\\ntwo-dimensional shapes and introduce a method that exploits reconstruction loss\\nof Variational Autoencoders with different sizes of latent vectors. Although\\ncomplexity of a shape is not a well defined attribute, different aspects of it\\ncan be estimated. We demonstrate that our methods captures some aspects of\\nshape complexity. Code and training details will be publicly available.\\n</summary>\\n    <author>\\n      <name>Markus Rothgaenger</name>\\n    </author>\\n    <author>\\n      <name>Andrew Melnik</name>\\n    </author>\\n    <author>\\n      <name>Helge Ritter</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2304.02766v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2304.02766v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2304.11923v2</id>\\n    <updated>2023-09-18T12:30:10Z</updated>\\n    <published>2023-04-24T09:06:06Z</published>\\n    <title>Improving Knowledge Distillation via Transferring Learning Ability</title>\\n    <summary>  Existing knowledge distillation methods generally use a teacher-student\\napproach, where the student network solely learns from a well-trained teacher.\\nHowever, this approach overlooks the inherent differences in learning abilities\\nbetween the teacher and student networks, thus causing the capacity-gap\\nproblem. To address this limitation, we propose a novel method called SLKD.\\n</summary>\\n    <author>\\n      <name>Long Liu</name>\\n    </author>\\n    <author>\\n      <name>Tong Li</name>\\n    </author>\\n    <author>\\n      <name>Hui Cheng</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2304.11923v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2304.11923v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2304.14485v1</id>\\n    <updated>2023-03-10T12:58:16Z</updated>\\n    <published>2023-03-10T12:58:16Z</published>\\n    <title>Inter-sphere consistency-based method for camera-projector pair\\n  calibration</title>\\n    <summary>  We construct constraints from consistency between estimated parameters from\\ndifferent spheres, termed inter-sphere consistency. It facilitates more\\nflexible calibration using only two spheres, which has been considered a\\nchallenging and not well addressed ill-posed problem.\\n</summary>\\n    <author>\\n      <name>Zhaoshuai Qi</name>\\n    </author>\\n    <author>\\n      <name>Jingqi Pang</name>\\n    </author>\\n    <author>\\n      <name>Yifeng Hao</name>\\n    </author>\\n    <author>\\n      <name>Yanning Zhang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">3 pages,1 figure</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2304.14485v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2304.14485v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2305.00204v1</id>\\n    <updated>2023-04-29T09:10:32Z</updated>\\n    <published>2023-04-29T09:10:32Z</published>\\n    <title>CARLA-BSP: a simulated dataset with pedestrians</title>\\n    <summary>  We present a sample dataset featuring pedestrians generated using the ARCANE\\nframework, a new framework for generating datasets in CARLA (0.9.13). We\\nprovide use cases for pedestrian detection, autoencoding, pose estimation, and\\npose lifting. We also showcase baseline results. For more information, visit\\nhttps://project-arcane.eu/.\\n</summary>\\n    <author>\\n      <name>Maciej Wielgosz</name>\\n    </author>\\n    <author>\\n      <name>Antonio M. L\\xc3\\xb3pez</name>\\n    </author>\\n    <author>\\n      <name>Muhammad Naveed Riaz</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2305.00204v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2305.00204v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2305.17786v1</id>\\n    <updated>2023-05-28T18:17:31Z</updated>\\n    <published>2023-05-28T18:17:31Z</published>\\n    <title>Real-time Object Detection: YOLOv1 Re-Implementation in PyTorch</title>\\n    <summary>  Real-time object detection is a crucial problem to solve when in comes to\\ncomputer vision systems that needs to make appropriate decision based on\\ndetection in a timely manner. I have chosen the YOLO v1 architecture to\\nimplement it using PyTorch framework, with goal to familiarize with entire\\nobject detection pipeline I attempted different techniques to modify the\\noriginal architecture to improve the results. Finally, I compare the metrics of\\nmy implementation to the original.\\n</summary>\\n    <author>\\n      <name>Michael Shenoda</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2305.17786v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2305.17786v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2306.00360v2</id>\\n    <updated>2023-07-22T23:32:49Z</updated>\\n    <published>2023-06-01T05:40:58Z</published>\\n    <title>How Do ConvNets Understand Image Intensity?</title>\\n    <summary>  Convolutional Neural Networks (ConvNets) usually rely on edge/shape\\ninformation to classify images. Visualization methods developed over the last\\ndecade confirm that ConvNets rely on edge information. We investigate\\nsituations where the ConvNet needs to rely on image intensity in addition to\\nshape. We show that the ConvNet relies on image intensity information using\\nvisualization.\\n</summary>\\n    <author>\\n      <name>Jackson Kaunismaa</name>\\n    </author>\\n    <author>\\n      <name>Michael Guerzhoy</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">ICLR Tiny Papers 2023</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/2306.00360v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2306.00360v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2306.15445v2</id>\\n    <updated>2023-07-16T13:49:21Z</updated>\\n    <published>2023-06-27T13:02:24Z</published>\\n    <title>UniUD Submission to the EPIC-Kitchens-100 Multi-Instance Retrieval\\n  Challenge 2023</title>\\n    <summary>  In this report, we present the technical details of our submission to the\\nEPIC-Kitchens-100 Multi-Instance Retrieval Challenge 2023. To participate in\\nthe challenge, we ensembled two models trained with two different loss\\nfunctions on 25% of the training data. Our submission, visible on the public\\nleaderboard, obtains an average score of 56.81% nDCG and 42.63% mAP.\\n</summary>\\n    <author>\\n      <name>Alex Falcon</name>\\n    </author>\\n    <author>\\n      <name>Giuseppe Serra</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2306.15445v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2306.15445v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2307.00083v1</id>\\n    <updated>2023-06-30T18:39:57Z</updated>\\n    <published>2023-06-30T18:39:57Z</published>\\n    <title>A Parts Based Registration Loss for Detecting Knee Joint Areas</title>\\n    <summary>  In this paper, a parts based loss is considered for finetune registering knee\\njoint areas. Here the parts are defined as abstract feature vectors with\\nlocation and they are automatically selected from a reference image. For a test\\nimage the detected parts are encouraged to have a similar spatial configuration\\nthan the corresponding parts in the reference image.\\n</summary>\\n    <author>\\n      <name>Juha Tiirola</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2307.00083v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2307.00083v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2307.00411v1</id>\\n    <updated>2023-07-01T19:04:11Z</updated>\\n    <published>2023-07-01T19:04:11Z</published>\\n    <title>Applications of Binary Similarity and Distance Measures</title>\\n    <summary>  In the recent past, binary similarity measures have been applied in solving\\nbiometric identification problems, including fingerprint, handwritten character\\ndetection, and in iris image recognition. The application of the relevant\\nmeasurements has also resulted in more accurate data analysis. This paper\\nsurveys the applicability of binary similarity and distance measures in various\\nfields.\\n</summary>\\n    <author>\\n      <name>Manoj Muniswamaiah</name>\\n    </author>\\n    <author>\\n      <name>Tilak Agerwala</name>\\n    </author>\\n    <author>\\n      <name>Charles C. Tappert</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2307.00411v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2307.00411v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2307.05601v1</id>\\n    <updated>2023-07-10T20:28:58Z</updated>\\n    <published>2023-07-10T20:28:58Z</published>\\n    <title>Unsupervised Domain Adaptation with Deep Neural-Network</title>\\n    <summary>  This report contributes to the field of unsupervised domain adaptation by\\nproviding an analysis of existing methods, introducing a new approach, and\\ndemonstrating the potential for improving visual recognition tasks across\\ndifferent domains. The results of this study open up opportunities for further\\nstudy and development of advanced methods in the field of domain adaptation.\\n</summary>\\n    <author>\\n      <name>Artem Bituitskii</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Master\\'s thesis, 34 pages, 13 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2307.05601v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2307.05601v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2307.13215v1</id>\\n    <updated>2023-07-25T02:56:20Z</updated>\\n    <published>2023-07-25T02:56:20Z</published>\\n    <title>Image Segmentation Keras : Implementation of Segnet, FCN, UNet, PSPNet\\n  and other models in Keras</title>\\n    <summary>  Semantic segmentation plays a vital role in computer vision tasks, enabling\\nprecise pixel-level understanding of images. In this paper, we present a\\ncomprehensive library for semantic segmentation, which contains implementations\\nof popular segmentation models like SegNet, FCN, UNet, and PSPNet. We also\\nevaluate and compare these models on several datasets, offering researchers and\\npractitioners a powerful toolset for tackling diverse segmentation challenges.\\n</summary>\\n    <author>\\n      <name>Divam Gupta</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2307.13215v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2307.13215v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2309.15097v1</id>\\n    <updated>2023-09-26T17:43:58Z</updated>\\n    <published>2023-09-26T17:43:58Z</published>\\n    <title>Case Study: Ensemble Decision-Based Annotation of Unconstrained Real\\n  Estate Images</title>\\n    <summary>  We describe a proof-of-concept for annotating real estate images using simple\\niterative rule-based semi-supervised learning. In this study, we have gained\\nimportant insights into the content characteristics and uniqueness of\\nindividual image classes as well as essential requirements for a practical\\nimplementation.\\n</summary>\\n    <author>\\n      <name>Miroslav Despotovic</name>\\n    </author>\\n    <author>\\n      <name>Zedong Zhang</name>\\n    </author>\\n    <author>\\n      <name>Eric Stumpe</name>\\n    </author>\\n    <author>\\n      <name>Matthias Zeppelzauer</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">2 pages, 3 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2309.15097v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2309.15097v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4.8\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2310.10517v1</id>\\n    <updated>2023-10-16T15:33:58Z</updated>\\n    <published>2023-10-16T15:33:58Z</published>\\n    <title>Distribution prediction for image compression: An experimental\\n  re-compressor for JPEG images</title>\\n    <summary>  We propose a new scheme to re-compress JPEG images in a lossless way. Using a\\nJPEG image as an input the algorithm partially decodes the signal to obtain\\nquantized DCT coefficients and then re-compress them in a more effective way.\\n</summary>\\n    <author>\\n      <name>Maxim Koroteev</name>\\n    </author>\\n    <author>\\n      <name>Yaroslav Borisov</name>\\n    </author>\\n    <author>\\n      <name>Pavel Frolov</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages, 5 figures, 2 tables</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2310.10517v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2310.10517v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2312.07357v1</id>\\n    <updated>2023-12-12T15:26:06Z</updated>\\n    <published>2023-12-12T15:26:06Z</published>\\n    <title>Automatic coral reef fish identification and 3D measurement in the wild</title>\\n    <summary>  In this paper we present a pipeline using stereo images in order to\\nautomatically identify, track in 3D fish, and measure fish population.\\n</summary>\\n    <author>\\n      <name>Cyril Barrelet</name>\\n    </author>\\n    <author>\\n      <name>Marc Chaumont</name>\\n    </author>\\n    <author>\\n      <name>G\\xc3\\xa9rard Subsol</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This paper is in its draft version and should be improved in order to\\n  be published. This paper is issued from one Year of Engineering work</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2312.07357v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2312.07357v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2312.09876v1</id>\\n    <updated>2023-12-15T15:24:49Z</updated>\\n    <published>2023-12-15T15:24:49Z</published>\\n    <title>Automatic Image Colourizer</title>\\n    <summary>  In this project we have designed and described a model which colourize a\\ngray-scale image, with no human intervention. We propose a fully automatic\\nprocess of colouring and re-colouring faded or gray-scale image with vibrant\\nand pragmatic colours. We have used Convolutional Neural Network to hallucinate\\ninput images and feed-forwarded by training thousands of images. This approach\\nresults in trailblazing results.\\n</summary>\\n    <author>\\n      <name>Aditya Parikh</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2312.09876v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2312.09876v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2312.16987v1</id>\\n    <updated>2023-12-28T12:15:58Z</updated>\\n    <published>2023-12-28T12:15:58Z</published>\\n    <title>Image Quality, Uniformity and Computation Improvement of Compressive\\n  Light Field Displays with U-Net</title>\\n    <summary>  We apply the U-Net model for compressive light field synthesis. Compared to\\nmethods based on stacked CNN and iterative algorithms, this method offers\\nbetter image quality, uniformity and less computation.\\n</summary>\\n    <author>\\n      <name>Chen Gao</name>\\n    </author>\\n    <author>\\n      <name>Haifeng Li</name>\\n    </author>\\n    <author>\\n      <name>Xu Liu</name>\\n    </author>\\n    <author>\\n      <name>Xiaodi Tan</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, 6 figures, conference</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2312.16987v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2312.16987v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.GR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"78-06\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.3.7\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2401.06167v1</id>\\n    <updated>2024-01-02T03:49:41Z</updated>\\n    <published>2024-01-02T03:49:41Z</published>\\n    <title>Enhancing Multimodal Understanding with CLIP-Based Image-to-Text\\n  Transformation</title>\\n    <summary>  The process of transforming input images into corresponding textual\\nexplanations stands as a crucial and complex endeavor within the domains of\\ncomputer vision and natural language processing. In this paper, we propose an\\ninnovative ensemble approach that harnesses the capabilities of Contrastive\\nLanguage-Image Pretraining models.\\n</summary>\\n    <author>\\n      <name>Chang Che</name>\\n    </author>\\n    <author>\\n      <name>Qunwei Lin</name>\\n    </author>\\n    <author>\\n      <name>Xinyu Zhao</name>\\n    </author>\\n    <author>\\n      <name>Jiaxin Huang</name>\\n    </author>\\n    <author>\\n      <name>Liqiang Yu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2401.06167v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2401.06167v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2401.14675v1</id>\\n    <updated>2024-01-26T06:40:00Z</updated>\\n    <published>2024-01-26T06:40:00Z</published>\\n    <title>Multi-model learning by sequential reading of untrimmed videos for\\n  action recognition</title>\\n    <summary>  We propose a new method for learning videos by aggregating multiple models by\\nsequentially extracting video clips from untrimmed video. The proposed method\\nreduces the correlation between clips by feeding clips to multiple models in\\nturn and synchronizes these models through federated learning. Experimental\\nresults show that the proposed method improves the performance compared to the\\nno synchronization.\\n</summary>\\n    <author>\\n      <name>Kodai Kamiya</name>\\n    </author>\\n    <author>\\n      <name>Toru Tamaki</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">The International Workshop on Frontiers of Computer Vision\\n  (IW-FCV2024)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2401.14675v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2401.14675v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2402.04953v1</id>\\n    <updated>2024-02-07T15:37:17Z</updated>\\n    <published>2024-02-07T15:37:17Z</published>\\n    <title>4-Dimensional deformation part model for pose estimation using Kalman\\n  filter constraints</title>\\n    <summary>  The main goal of this article is to analyze the effect on pose estimation\\naccuracy when using a Kalman filter added to 4-dimensional deformation part\\nmodel partial solutions. The experiments run with two data sets showing that\\nthis method improves pose estimation accuracy compared with state-of-the-art\\nmethods and that a Kalman filter helps to increase this accuracy.\\n</summary>\\n    <author>\\n      <name>Enrique Martinez-Berti</name>\\n    </author>\\n    <author>\\n      <name>Antonio-Jose Sanchez-Salmeron</name>\\n    </author>\\n    <author>\\n      <name>Carlos Ricolfe-Viala</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2402.04953v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.04953v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2402.06212v1</id>\\n    <updated>2024-02-09T06:44:21Z</updated>\\n    <published>2024-02-09T06:44:21Z</published>\\n    <title>Halo Reduction in Display Systems through Smoothed Local Histogram\\n  Equalization and Human Visual System Modeling</title>\\n    <summary>  Halo artifacts significantly impact display quality. We propose a method to\\nreduce halos in Local Histogram Equalization (LHE) algorithms by separately\\naddressing dark and light variants. This approach results in visually natural\\nimages by exploring the relationship between lateral inhibition and halo\\nartifacts in the human visual system.\\n</summary>\\n    <author>\\n      <name>Prasoon Ambalathankandy</name>\\n    </author>\\n    <author>\\n      <name>Yafei Ou</name>\\n    </author>\\n    <author>\\n      <name>Masayuki Ikebe</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.36463/idw.2023.1488</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.36463/idw.2023.1488\" rel=\"related\"/>\\n    <link href=\"http://arxiv.org/abs/2402.06212v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.06212v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4.3\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2403.15990v1</id>\\n    <updated>2024-03-24T02:55:45Z</updated>\\n    <published>2024-03-24T02:55:45Z</published>\\n    <title>Mars Spectrometry 2: Gas Chromatography -- Second place solution</title>\\n    <summary>  The Mars Spectrometry 2: Gas Chromatography challenge was sponsored by NASA\\nand run on the DrivenData competition platform in 2022. This report describes\\nthe solution which achieved the second-best score on the competition\\'s test\\ndataset. The solution utilized two-dimensional, image-like representations of\\nthe competition\\'s chromatography data samples. A number of different\\nConvolutional Neural Network models were trained and ensembled for the final\\nsubmission.\\n</summary>\\n    <author>\\n      <name>Dmitry A. Konovalov</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2403.15990v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2403.15990v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2404.00597v1</id>\\n    <updated>2024-03-31T08:08:05Z</updated>\\n    <published>2024-03-31T08:08:05Z</published>\\n    <title>Parameter and Data-Efficient Spectral StyleDCGAN</title>\\n    <summary>  We present a simple, highly parameter, and data-efficient adversarial network\\nfor unconditional face generation. Our method: Spectral Style-DCGAN or SSD\\nutilizes only 6.574 million parameters and 4739 dog faces from the Animal Faces\\nHQ (AFHQ) dataset as training samples while preserving fidelity at low\\nresolutions up to 64x64. Code available at\\nhttps://github.com/Aryan-Garg/StyleDCGAN.\\n</summary>\\n    <author>\\n      <name>Aryan Garg</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Notable ICLR Tiny Paper 2024</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2404.00597v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2404.00597v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2404.10319v1</id>\\n    <updated>2024-04-16T06:59:26Z</updated>\\n    <published>2024-04-16T06:59:26Z</published>\\n    <title>Application of Deep Learning Methods to Processing of Noisy Medical\\n  Video Data</title>\\n    <summary>  Cells count become a challenging problem when the cells move in a continuous\\nstream, and their boundaries are difficult for visual detection. To resolve\\nthis problem we modified the training and decision making processes using\\ncurriculum learning and multi-view predictions techniques, respectively.\\n</summary>\\n    <author>\\n      <name>Danil Afonchikov</name>\\n    </author>\\n    <author>\\n      <name>Elena Kornaeva</name>\\n    </author>\\n    <author>\\n      <name>Irina Makovik</name>\\n    </author>\\n    <author>\\n      <name>Alexey Kornaev</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2404.10319v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2404.10319v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2405.00196v1</id>\\n    <updated>2024-04-30T20:59:53Z</updated>\\n    <published>2024-04-30T20:59:53Z</published>\\n    <title>Synthetic Image Verification in the Era of Generative AI: What Works and\\n  What Isn\\'t There Yet</title>\\n    <summary>  In this work we present an overview of approaches for the detection and\\nattribution of synthetic images and highlight their strengths and weaknesses.\\nWe also point out and discuss hot topics in this field and outline promising\\ndirections for future research.\\n</summary>\\n    <author>\\n      <name>Diangarti Tariang</name>\\n    </author>\\n    <author>\\n      <name>Riccardo Corvi</name>\\n    </author>\\n    <author>\\n      <name>Davide Cozzolino</name>\\n    </author>\\n    <author>\\n      <name>Giovanni Poggi</name>\\n    </author>\\n    <author>\\n      <name>Koki Nagano</name>\\n    </author>\\n    <author>\\n      <name>Luisa Verdoliva</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2405.00196v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2405.00196v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2405.05260v1</id>\\n    <updated>2024-03-18T20:46:35Z</updated>\\n    <published>2024-03-18T20:46:35Z</published>\\n    <title>Financial Table Extraction in Image Documents</title>\\n    <summary>  Table extraction has long been a pervasive problem in financial services.\\nThis is more challenging in the image domain, where content is locked behind\\ncumbersome pixel format. Luckily, advances in deep learning for image\\nsegmentation, OCR, and sequence modeling provides the necessary heavy lifting\\nto achieve impressive results. This paper presents an end-to-end pipeline for\\nidentifying, extracting and transcribing tabular content in image documents,\\nwhile retaining the original spatial relations with high fidelity.\\n</summary>\\n    <author>\\n      <name>William Watson</name>\\n    </author>\\n    <author>\\n      <name>Bo Liu</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1145/3383455.3422520</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1145/3383455.3422520\" rel=\"related\"/>\\n    <link href=\"http://arxiv.org/abs/2405.05260v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2405.05260v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2405.11351v1</id>\\n    <updated>2024-05-18T17:43:50Z</updated>\\n    <published>2024-05-18T17:43:50Z</published>\\n    <title>PlantTracing: Tracing Arabidopsis Thaliana Apex with CenterTrack</title>\\n    <summary>  This work applies an encoder-decoder-based machine learning network to detect\\nand track the motion and growth of the flowering stem apex of Arabidopsis\\nThaliana. Based on the CenterTrack, a machine learning back-end network, we\\ntrained a model based on ten time-lapsed labeled videos and tested against\\nthree videos.\\n</summary>\\n    <author>\\n      <name>Yuanzhe Liu</name>\\n    </author>\\n    <author>\\n      <name>Yixiang Mao</name>\\n    </author>\\n    <author>\\n      <name>Yao Wang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, 13 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2405.11351v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2405.11351v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2406.18587v1</id>\\n    <updated>2024-06-06T21:02:51Z</updated>\\n    <published>2024-06-06T21:02:51Z</published>\\n    <title>Nomic Embed Vision: Expanding the Latent Space</title>\\n    <summary>  This technical report describes the training of nomic-embed-vision, a highly\\nperformant, open-code, open-weights image embedding model that shares the same\\nlatent space as nomic-embed-text. Together, nomic-embed-vision and\\nnomic-embed-text form the first unified latent space to achieve high\\nperformance across vision, language, and multimodal tasks.\\n</summary>\\n    <author>\\n      <name>Zach Nussbaum</name>\\n    </author>\\n    <author>\\n      <name>Brandon Duderstadt</name>\\n    </author>\\n    <author>\\n      <name>Andriy Mulyar</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2406.18587v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2406.18587v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2407.04265v1</id>\\n    <updated>2024-07-05T05:38:20Z</updated>\\n    <published>2024-07-05T05:38:20Z</published>\\n    <title>Parametric Curve Segment Extraction by Support Regions</title>\\n    <summary>  We introduce a method to extract curve segments in parametric form from the\\nimage directly using the Laplacian of Gaussian (LoG) filter response. Our\\nsegmentation gives convex and concave curves. To do so, we form curve support\\nregions by grouping pixels of the thresholded filter response. Then, we model\\neach support region boundary by Fourier series and extract the corresponding\\nparametric curve segment.\\n</summary>\\n    <author>\\n      <name>Cem \\xc3\\x9cnsalan</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages, 5 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2407.04265v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2407.04265v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2407.04592v1</id>\\n    <updated>2024-07-05T15:40:39Z</updated>\\n    <published>2024-07-05T15:40:39Z</published>\\n    <title>Smell and Emotion: Recognising emotions in smell-related artworks</title>\\n    <summary>  Emotions and smell are underrepresented in digital art history. In this\\nexploratory work, we show that recognising emotions from smell-related artworks\\nis technically feasible but has room for improvement. Using style transfer and\\nhyperparameter optimization we achieve a minor performance boost and open up\\nthe field for future extensions.\\n</summary>\\n    <author>\\n      <name>Vishal Patoliya</name>\\n    </author>\\n    <author>\\n      <name>Mathias Zinnen</name>\\n    </author>\\n    <author>\\n      <name>Andreas Maier</name>\\n    </author>\\n    <author>\\n      <name>Vincent Christlein</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages, 3 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2407.04592v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2407.04592v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2407.05312v1</id>\\n    <updated>2024-07-07T09:52:04Z</updated>\\n    <published>2024-07-07T09:52:04Z</published>\\n    <title>An Improved Method for Personalizing Diffusion Models</title>\\n    <summary>  Diffusion models have demonstrated impressive image generation capabilities.\\nPersonalized approaches, such as textual inversion and Dreambooth, enhance\\nmodel individualization using specific images. These methods enable generating\\nimages of specific objects based on diverse textual contexts. Our proposed\\napproach aims to retain the model\\'s original knowledge during new information\\nintegration, resulting in superior outcomes while necessitating less training\\ntime compared to Dreambooth and textual inversion.\\n</summary>\\n    <author>\\n      <name>Yan Zeng</name>\\n    </author>\\n    <author>\\n      <name>Masanori Suganuma</name>\\n    </author>\\n    <author>\\n      <name>Takayuki Okatani</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2407.05312v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2407.05312v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2407.18290v1</id>\\n    <updated>2024-07-11T02:49:13Z</updated>\\n    <published>2024-07-11T02:49:13Z</published>\\n    <title>Several questions of visual generation in 2024</title>\\n    <summary>  This paper does not propose any new algorithms but instead outlines various\\nproblems in the field of visual generation based on the author\\'s personal\\nunderstanding. The core of these problems lies in how to decompose visual\\nsignals, with all other issues being closely related to this central problem\\nand stemming from unsuitable approaches to signal decomposition. This paper\\naims to draw researchers\\' attention to the significance of Visual Signal\\nDecomposition.\\n</summary>\\n    <author>\\n      <name>Shuyang Gu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">12 pages, 3 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2407.18290v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2407.18290v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2408.07225v1</id>\\n    <updated>2024-08-01T19:40:55Z</updated>\\n    <published>2024-08-01T19:40:55Z</published>\\n    <title>Longitudinal Evaluation of Child Face Recognition and the Impact of\\n  Underlying Age</title>\\n    <summary>  The need for reliable identification of children in various emerging\\napplications has sparked interest in leveraging child face recognition\\ntechnology. This study introduces a longitudinal approach to enrollment and\\nverification accuracy for child face recognition, focusing on the YFA database\\ncollected by Clarkson University CITeR research group over an 8 year period, at\\n6 month intervals.\\n</summary>\\n    <author>\\n      <name>Surendra Singh</name>\\n    </author>\\n    <author>\\n      <name>Keivan Bahmani</name>\\n    </author>\\n    <author>\\n      <name>Stephanie Schuckers</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2408.07225v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2408.07225v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2408.08529v1</id>\\n    <updated>2024-08-16T04:57:21Z</updated>\\n    <published>2024-08-16T04:57:21Z</published>\\n    <title>Privacy-Preserving Vision Transformer Using Images Encrypted with\\n  Restricted Random Permutation Matrices</title>\\n    <summary>  We propose a novel method for privacy-preserving fine-tuning vision\\ntransformers (ViTs) with encrypted images. Conventional methods using encrypted\\nimages degrade model performance compared with that of using plain images due\\nto the influence of image encryption. In contrast, the proposed encryption\\nmethod using restricted random permutation matrices can provide a higher\\nperformance than the conventional ones.\\n</summary>\\n    <author>\\n      <name>Kouki Horio</name>\\n    </author>\\n    <author>\\n      <name>Kiyoshi Nishikawa</name>\\n    </author>\\n    <author>\\n      <name>Hitoshi Kiya</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, 9 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2408.08529v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2408.08529v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2408.15374v2</id>\\n    <updated>2024-11-21T23:51:29Z</updated>\\n    <published>2024-08-27T19:22:06Z</published>\\n    <title>CycleGAN with Better Cycles</title>\\n    <summary>  CycleGAN provides a framework to train image-to-image translation with\\nunpaired datasets using cycle consistency loss [4]. While results are great in\\nmany applications, the pixel level cycle consistency can potentially be\\nproblematic and causes unrealistic images in certain cases. In this project, we\\npropose three simple modifications to cycle consistency, and show that such an\\napproach achieves better results with fewer artifacts.\\n</summary>\\n    <author>\\n      <name>Tongzhou Wang</name>\\n    </author>\\n    <author>\\n      <name>Yihan Lin</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Technical Report 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2408.15374v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2408.15374v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2409.02448v1</id>\\n    <updated>2024-09-04T05:06:34Z</updated>\\n    <published>2024-09-04T05:06:34Z</published>\\n    <title>Detecting Korean Food Using Image using Hierarchical Model</title>\\n    <summary>  A solution was made available for Korean Food lovers who have dietary\\nrestrictions to identify the Korean food before consuming. Just by uploading a\\nclear photo of the dish, people can get to know what they are eating. Image\\nprocessing techniques together with machine learning helped to come up with\\nthis solution.\\n</summary>\\n    <author>\\n      <name>Hoang Khanh Lam</name>\\n    </author>\\n    <author>\\n      <name>Kahandakanaththage Maduni Pramuditha Perera</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2409.02448v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2409.02448v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2409.15028v1</id>\\n    <updated>2024-09-23T13:55:16Z</updated>\\n    <published>2024-09-23T13:55:16Z</published>\\n    <title>Region Mixup</title>\\n    <summary>  This paper introduces a simple extension of mixup (Zhang et al., 2018) data\\naugmentation to enhance generalization in visual recognition tasks. Unlike the\\nvanilla mixup method, which blends entire images, our approach focuses on\\ncombining regions from multiple images.\\n</summary>\\n    <author>\\n      <name>Saptarshi Saha</name>\\n    </author>\\n    <author>\\n      <name>Utpal Garain</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Published as a Tiny Paper at ICLR 2024</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">The Second Tiny Papers Track at ICLR 2024</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/2409.15028v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2409.15028v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.2.10\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2409.15997v2</id>\\n    <updated>2024-09-26T21:56:01Z</updated>\\n    <published>2024-09-24T11:57:12Z</published>\\n    <title>Improvements to SDXL in NovelAI Diffusion V3</title>\\n    <summary>  In this technical report, we document the changes we made to SDXL in the\\nprocess of training NovelAI Diffusion V3, our state of the art anime image\\ngeneration model.\\n</summary>\\n    <author>\\n      <name>Juan Ossa</name>\\n    </author>\\n    <author>\\n      <name>Eren Do\\xc4\\x9fan</name>\\n    </author>\\n    <author>\\n      <name>Alex Birch</name>\\n    </author>\\n    <author>\\n      <name>F. Johnson</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">14 pages, 8 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2409.15997v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2409.15997v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2410.05680v1</id>\\n    <updated>2024-10-08T04:40:16Z</updated>\\n    <published>2024-10-08T04:40:16Z</published>\\n    <title>Convolutional neural networks applied to modification of images</title>\\n    <summary>  The reader will learn how digital images are edited using linear algebra and\\ncalculus. Starting from the concept of filter towards machine learning\\ntechniques such as convolutional neural networks.\\n</summary>\\n    <author>\\n      <name>Carlos I. Aguirre-Velez</name>\\n    </author>\\n    <author>\\n      <name>Jose Antonio Arciniega-Nevarez</name>\\n    </author>\\n    <author>\\n      <name>Eric Dolores-Cuenca</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1007/978-3-030-93954-0_5-1</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1007/978-3-030-93954-0_5-1\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">23 pages</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">In: Sriraman, B. (eds) Handbook of Visual, Experimental and\\n  Computational Mathematics . Springer, Cham. (2023)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/2410.05680v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2410.05680v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"A.1; G.m\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2410.07125v1</id>\\n    <updated>2024-09-20T21:41:57Z</updated>\\n    <published>2024-09-20T21:41:57Z</published>\\n    <title>A Simplified Positional Cell Type Visualization using Spatially\\n  Aggregated Clusters</title>\\n    <summary>  We introduce a novel method for overlaying cell type proportion data onto\\ntissue images. This approach preserves spatial context while avoiding visual\\nclutter or excessively obscuring the underlying slide. Our proposed technique\\ninvolves clustering the data and aggregating neighboring points of the same\\ncluster into polygons.\\n</summary>\\n    <author>\\n      <name>Lee Mason</name>\\n    </author>\\n    <author>\\n      <name>Jonas Almeida</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">For the Bio+MedVis 2024 redesign challenge</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2410.07125v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2410.07125v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.GR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2410.13871v2</id>\\n    <updated>2024-11-05T09:07:14Z</updated>\\n    <published>2024-10-02T12:14:31Z</published>\\n    <title>Explaining an image classifier with a generative model conditioned by\\n  uncertainty</title>\\n    <summary>  We propose to condition a generative model by a given image classifier\\nuncertainty in order to analyze and explain its behavior. Preliminary\\nexperiments on synthetic data and a corrupted version of MNIST dataset\\nillustrate the idea.\\n</summary>\\n    <author>\\n      <name>Adrien LeCoz</name>\\n    </author>\\n    <author>\\n      <name>St\\xc3\\xa9phane Herbin</name>\\n    </author>\\n    <author>\\n      <name>Faouzi Adjed</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Uncertainty meets Explainability | Workshop and Tutorial @\\n  ECML-PKDD 2023, Sep 2023, Torino, Italy</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/2410.13871v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2410.13871v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2410.14958v1</id>\\n    <updated>2024-10-19T03:17:29Z</updated>\\n    <published>2024-10-19T03:17:29Z</published>\\n    <title>Neural Radiance Field Image Refinement through End-to-End Sampling Point\\n  Optimization</title>\\n    <summary>  Neural Radiance Field (NeRF), capable of synthesizing high-quality novel\\nviewpoint images, suffers from issues like artifact occurrence due to its fixed\\nsampling points during rendering. This study proposes a method that optimizes\\nsampling points to reduce artifacts and produce more detailed images.\\n</summary>\\n    <author>\\n      <name>Kazuhiro Ohta</name>\\n    </author>\\n    <author>\\n      <name>Satoshi Ono</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2410.14958v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2410.14958v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2410.18051v1</id>\\n    <updated>2024-10-23T17:25:26Z</updated>\\n    <published>2024-10-23T17:25:26Z</published>\\n    <title>Real time anomalies detection on video</title>\\n    <summary>  Nowadays, many places use security cameras. Unfortunately, when an incident\\noccurs, these technologies are used to show past events. So it can be\\nconsidered as a deterrence tool than a detection tool. In this article, we will\\npropose a deep learning approach trying to solve this problematic. This\\napproach uses convolutional models (CNN) to extract relevant characteristics\\nlinked to the video images, theses characteristics will form times series to be\\nanalyzed by LSTM / GRU models.\\n</summary>\\n    <author>\\n      <name>Fabien Poirier</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2410.18051v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2410.18051v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2410.22777v1</id>\\n    <updated>2024-10-30T07:46:06Z</updated>\\n    <published>2024-10-30T07:46:06Z</published>\\n    <title>Bregman implementation of Meyer\\'s $G-$norm for cartoon + textures\\n  decomposition</title>\\n    <summary>  In this paper, we design a very simple algorithm based on Split Bregman\\niterations to numerically solve the cartoon + textures decomposition model of\\nMeyer. This results in a significant gain in speed compared to Chambolle\\'s\\nnonlinear projectors.\\n</summary>\\n    <author>\\n      <name>Jerome Gilles</name>\\n    </author>\\n    <author>\\n      <name>Stanley Osher</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2410.22777v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2410.22777v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.FA\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2411.05603v1</id>\\n    <updated>2024-11-08T14:47:28Z</updated>\\n    <published>2024-11-08T14:47:28Z</published>\\n    <title>Efficient Audio-Visual Fusion for Video Classification</title>\\n    <summary>  We present Attend-Fusion, a novel and efficient approach for audio-visual\\nfusion in video classification tasks. Our method addresses the challenge of\\nexploiting both audio and visual modalities while maintaining a compact model\\narchitecture. Through extensive experiments on the YouTube-8M dataset, we\\ndemonstrate that our Attend-Fusion achieves competitive performance with\\nsignificantly reduced model complexity compared to larger baseline models.\\n</summary>\\n    <author>\\n      <name>Mahrukh Awan</name>\\n    </author>\\n    <author>\\n      <name>Asmar Nadeem</name>\\n    </author>\\n    <author>\\n      <name>Armin Mustafa</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">CVMP Short Paper</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2411.05603v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2411.05603v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2411.08878v1</id>\\n    <updated>2024-11-13T18:55:10Z</updated>\\n    <published>2024-11-13T18:55:10Z</published>\\n    <title>A Short Note on Evaluating RepNet for Temporal Repetition Counting in\\n  Videos</title>\\n    <summary>  We discuss some consistent issues on how RepNet has been evaluated in various\\npapers. As a way to mitigate these issues, we report RepNet performance results\\non different datasets, and release evaluation code and the RepNet checkpoint to\\nobtain these results. Code URL:\\nhttps://github.com/google-research/google-research/blob/master/repnet/\\n</summary>\\n    <author>\\n      <name>Debidatta Dwibedi</name>\\n    </author>\\n    <author>\\n      <name>Yusuf Aytar</name>\\n    </author>\\n    <author>\\n      <name>Jonathan Tompson</name>\\n    </author>\\n    <author>\\n      <name>Pierre Sermanet</name>\\n    </author>\\n    <author>\\n      <name>Andrew Zisserman</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2411.08878v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2411.08878v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2411.10705v1</id>\\n    <updated>2024-11-16T05:17:27Z</updated>\\n    <published>2024-11-16T05:17:27Z</published>\\n    <title>Poster: Reliable 3D Reconstruction for Ad-hoc Edge Implementations</title>\\n    <summary>  Ad-hoc edge deployments to support real-time complex video processing\\napplications such as, multi-view 3D reconstruction often suffer from\\nspatio-temporal system disruptions that greatly impact reconstruction quality.\\nIn this poster paper, we present a novel portfolio theory-inspired edge\\nresource management strategy to ensure reliable multi-view 3D reconstruction by\\naccounting for possible system disruptions.\\n</summary>\\n    <author>\\n      <name>Md Nurul Absur</name>\\n    </author>\\n    <author>\\n      <name>Swastik Brahma</name>\\n    </author>\\n    <author>\\n      <name>Saptarshi Debroy</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">3 Pages, 2 figures, IEEE SEC 2024</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2411.10705v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2411.10705v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2411.12331v1</id>\\n    <updated>2024-11-19T08:32:17Z</updated>\\n    <published>2024-11-19T08:32:17Z</published>\\n    <title>Accelerating UMAP for Large-Scale Datasets Through Spectral Coarsening</title>\\n    <summary>  This paper introduces an innovative approach to dramatically accelerate UMAP\\nusing spectral data compression.The proposed method significantly reduces the\\nsize of the dataset, preserving its essential manifold structure through an\\nadvanced spectral compression technique. This allows UMAP to perform much\\nfaster while maintaining the quality of its embeddings. Experiments on\\nreal-world datasets, such as USPS, demonstrate the method\\'s ability to achieve\\nsubstantial data reduction without compromising embedding fidelity.\\n</summary>\\n    <author>\\n      <name>Yongyu Wang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2411.12331v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2411.12331v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2411.18314v1</id>\\n    <updated>2024-11-27T13:10:18Z</updated>\\n    <published>2024-11-27T13:10:18Z</published>\\n    <title>Real-time Video Target Tracking Algorithm Utilizing Convolutional Neural\\n  Networks (CNN)</title>\\n    <summary>  Thispaperaimstoresearchandimplementa\\nreal-timevideotargettrackingalgorithmbasedon\\nConvolutionalNeuralNetworks(CNN),enhancingthe\\naccuracyandrobustnessoftargettrackingincomplex\\nscenarios.Addressingthelimitationsoftraditionaltracking\\nalgorithmsinhandlingissuessuchastargetocclusion,morphologicalchanges,andbackgroundinterference,our\\napproachintegratestargetdetectionandtrackingstrategies.It\\ncontinuouslyupdatesthetargetmodelthroughanonline\\nlearningmechanismtoadapttochangesinthetarget\\'s\\nappearance.Experimentalresultsdemonstratethat,when\\ndealingwithsituationsinvolvingrapidmotion,partial\\nocclusion,andcomplexbackgrounds,theproposedalgorithm\\nexhibitshighertrackingsuccessratesandlowerfailurerates\\ncomparedtoseveralmainstreamtrackingalgorithms.This\\nstudysuccessfullyappliesCNNtoreal-timevideotarget\\ntracking,improvingtheaccuracyandstabilityofthetracking\\nalgorithmwhilemaintaininghighprocessingspeeds,thus\\nmeetingthedemandsofreal-timeapplications.Thisalgorithm\\nisexpectedtoprovidenewsolutionsfortargettrackingtasksin\\nvideosurveillanceandintelligenttransportationdomains.\\n</summary>\\n    <author>\\n      <name>Chaoyi Tan</name>\\n    </author>\\n    <author>\\n      <name>Xiangtian Li</name>\\n    </author>\\n    <author>\\n      <name>Xiaobo Wang</name>\\n    </author>\\n    <author>\\n      <name>Zhen Qi</name>\\n    </author>\\n    <author>\\n      <name>Ao Xiang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2411.18314v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2411.18314v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2412.17517v1</id>\\n    <updated>2024-12-23T12:30:28Z</updated>\\n    <published>2024-12-23T12:30:28Z</published>\\n    <title>Dataset for Real-World Human Action Detection Using FMCW mmWave Radar</title>\\n    <summary>  Human action detection using privacy-preserving mmWave radar sensors is\\nstudied for its applications in healthcare and home automation. Unlike existing\\nresearch, limited to simulations in controlled environments, we present a\\nreal-world mmWave radar dataset with baseline results for human action\\ndetection.\\n</summary>\\n    <author>\\n      <name>Dylan jayabahu</name>\\n    </author>\\n    <author>\\n      <name>Parthipan Siva</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">To be published in JCVIS (proceedings of 10th Annual Conference on\\n  Vision and Intelligent Systems)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2412.17517v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2412.17517v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2503.00400v1</id>\\n    <updated>2025-03-01T08:29:31Z</updated>\\n    <published>2025-03-01T08:29:31Z</published>\\n    <title>Inteval Analysis for two spherical functions arising from robust\\n  Perspective-n-Lines problem</title>\\n    <summary>  This report presents a comprehensive interval analysis of two spherical\\nfunctions derived from the robust Perspective-n-Lines (PnL) problem. The study\\nis motivated by the application of a dimension-reduction technique to achieve\\nglobal solutions for the robust PnL problem. We establish rigorous theoretical\\nresults, supported by detailed proofs, and validate our findings through\\nextensive numerical simulations.\\n</summary>\\n    <author>\\n      <name>Xiang Zheng</name>\\n    </author>\\n    <author>\\n      <name>Haodong Jiang</name>\\n    </author>\\n    <author>\\n      <name>Junfeng Wu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2503.00400v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2503.00400v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2504.06099v1</id>\\n    <updated>2025-04-08T14:41:42Z</updated>\\n    <published>2025-04-08T14:41:42Z</published>\\n    <title>Towards Varroa destructor mite detection using a narrow spectra\\n  illumination</title>\\n    <summary>  This paper focuses on the development and modification of a beehive\\nmonitoring device and Varroa destructor detection on the bees with the help of\\nhyperspectral imagery while utilizing a U-net, semantic segmentation\\narchitecture, and conventional computer vision methods. The main objectives\\nwere to collect a dataset of bees and mites, and propose the computer vision\\nmodel which can achieve the detection between bees and mites.\\n</summary>\\n    <author>\\n      <name>Samuel Bielik</name>\\n    </author>\\n    <author>\\n      <name>Simon Bilik</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2504.06099v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2504.06099v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2504.17619v1</id>\\n    <updated>2025-04-24T14:43:55Z</updated>\\n    <published>2025-04-24T14:43:55Z</published>\\n    <title>Enhancing CNNs robustness to occlusions with bioinspired filters for\\n  border completion</title>\\n    <summary>  We exploit the mathematical modeling of the visual cortex mechanism for\\nborder completion to define custom filters for CNNs. We see a consistent\\nimprovement in performance, particularly in accuracy, when our modified LeNet 5\\nis tested with occluded MNIST images.\\n</summary>\\n    <author>\\n      <name>Catarina P. Coutinho</name>\\n    </author>\\n    <author>\\n      <name>Aneeqa Merhab</name>\\n    </author>\\n    <author>\\n      <name>Janko Petkovic</name>\\n    </author>\\n    <author>\\n      <name>Ferdinando Zanchetta</name>\\n    </author>\\n    <author>\\n      <name>Rita Fioresi</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Submitted to the 7th International Conference on Geometric Science of\\n  Information</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2504.17619v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2504.17619v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2505.03204v2</id>\\n    <updated>2025-05-07T04:09:12Z</updated>\\n    <published>2025-05-06T05:38:17Z</published>\\n    <title>DCS-ST for Classification of Breast Cancer Histopathology Images with\\n  Limited Annotations</title>\\n    <summary>  Deep learning methods have shown promise in classifying breast cancer\\nhistopathology images, but their performance often declines with limited\\nannotated data, a critical challenge in medical imaging due to the high cost\\nand expertise required for annotations.\\n</summary>\\n    <author>\\n      <name>Liu Suxing</name>\\n    </author>\\n    <author>\\n      <name>Byungwon Min</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2505.03204v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2505.03204v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2505.06389v1</id>\\n    <updated>2025-05-09T19:33:35Z</updated>\\n    <published>2025-05-09T19:33:35Z</published>\\n    <title>Deep Learning-Based Robust Optical Guidance for Hypersonic Platforms</title>\\n    <summary>  Sensor-based guidance is required for long-range platforms. To bypass the\\nstructural limitation of classical registration on reference image framework,\\nwe offer in this paper to encode a stack of images of the scene into a deep\\nnetwork. Relying on a stack is showed to be relevant on bimodal scene (e.g.\\nwhen the scene can or can not be snowy).\\n</summary>\\n    <author>\\n      <name>Adrien Chan-Hon-Tong</name>\\n    </author>\\n    <author>\\n      <name>Aur\\xc3\\xa9lien Plyer</name>\\n    </author>\\n    <author>\\n      <name>Baptiste Cadalen</name>\\n    </author>\\n    <author>\\n      <name>Laurent Serre</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2505.06389v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2505.06389v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2505.17808v1</id>\\n    <updated>2025-05-23T12:25:13Z</updated>\\n    <published>2025-05-23T12:25:13Z</published>\\n    <title>An Attention Infused Deep Learning System with Grad-CAM Visualization\\n  for Early Screening of Glaucoma</title>\\n    <summary>  This research work reveals the eye opening wisdom of the hybrid labyrinthine\\ndeep learning models synergy born out of combining a trailblazing convolutional\\nneural network with a disruptive Vision Transformer, both intertwined together\\nwith a radical Cross Attention module. Here, two high yielding datasets for\\nartificial intelligence models in detecting glaucoma, namely ACRIMA and\\nDrishti, are utilized.\\n</summary>\\n    <author>\\n      <name>Ramanathan Swaminathan</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages in general IEEE format, 8 figures, 4 tables, pdflatex</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2505.17808v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2505.17808v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2506.06748v1</id>\\n    <updated>2025-06-07T10:33:16Z</updated>\\n    <published>2025-06-07T10:33:16Z</published>\\n    <title>THU-Warwick Submission for EPIC-KITCHEN Challenge 2025: Semi-Supervised\\n  Video Object Segmentation</title>\\n    <summary>  In this report, we describe our approach to egocentric video object\\nsegmentation. Our method combines large-scale visual pretraining from SAM2 with\\ndepth-based geometric cues to handle complex scenes and long-term tracking. By\\nintegrating these signals in a unified framework, we achieve strong\\nsegmentation performance. On the VISOR test set, our method reaches a J&amp;F score\\nof 90.1%.\\n</summary>\\n    <author>\\n      <name>Mingqi Gao</name>\\n    </author>\\n    <author>\\n      <name>Haoran Duan</name>\\n    </author>\\n    <author>\\n      <name>Tianlu Zhang</name>\\n    </author>\\n    <author>\\n      <name>Jungong Han</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2506.06748v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2506.06748v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2506.13043v1</id>\\n    <updated>2025-06-16T02:16:17Z</updated>\\n    <published>2025-06-16T02:16:17Z</published>\\n    <title>ViewPCL: a point cloud based active learning method for multi-view\\n  segmentation</title>\\n    <summary>  We propose a novel active learning framework for multi-view semantic\\nsegmentation. This framework relies on a new score that measures the\\ndiscrepancy between point cloud distributions generated from the extra\\ngeometrical information derived from the model\\'s prediction across different\\nviews. Our approach results in a data efficient and explainable active learning\\nmethod. The source code is available at https://github.com/chilai235/viewpclAL.\\n</summary>\\n    <author>\\n      <name>Christian Hilaire</name>\\n    </author>\\n    <author>\\n      <name>Sima Didari</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2506.13043v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2506.13043v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2506.13458v1</id>\\n    <updated>2025-06-16T13:15:02Z</updated>\\n    <published>2025-06-16T13:15:02Z</published>\\n    <title>Leveraging Vision-Language Pre-training for Human Activity Recognition\\n  in Still Images</title>\\n    <summary>  Recognising human activity in a single photo enables indexing, safety and\\nassistive applications, yet lacks motion cues. Using 285 MSCOCO images labelled\\nas walking, running, sitting, and standing, scratch CNNs scored 41% accuracy.\\nFine-tuning multimodal CLIP raised this to 76%, demonstrating that contrastive\\nvision-language pre-training decisively improves still-image action recognition\\nin real-world deployments.\\n</summary>\\n    <author>\\n      <name>Cristina Mahanta</name>\\n    </author>\\n    <author>\\n      <name>Gagan Bhatia</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2506.13458v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2506.13458v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2508.06537v1</id>\\n    <updated>2025-08-04T10:03:40Z</updated>\\n    <published>2025-08-04T10:03:40Z</published>\\n    <title>Benchmarking Deep Learning-Based Object Detection Models on Feature\\n  Deficient Astrophotography Imagery Dataset</title>\\n    <summary>  Object detection models are typically trained on datasets like ImageNet,\\nCOCO, and PASCAL VOC, which focus on everyday objects. However, these lack\\nsignal sparsity found in non-commercial domains. MobilTelesco, a\\nsmartphone-based astrophotography dataset, addresses this by providing sparse\\nnight-sky images. We benchmark several detection models on it, highlighting\\nchallenges under feature-deficient conditions.\\n</summary>\\n    <author>\\n      <name>Shantanusinh Parmar</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2508.06537v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2508.06537v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"astro-ph.IM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2508.17472v1</id>\\n    <updated>2025-08-24T17:59:38Z</updated>\\n    <published>2025-08-24T17:59:38Z</published>\\n    <title>T2I-ReasonBench: Benchmarking Reasoning-Informed Text-to-Image\\n  Generation</title>\\n    <summary>  We propose T2I-ReasonBench, a benchmark evaluating reasoning capabilities of\\ntext-to-image (T2I) models. It consists of four dimensions: Idiom\\nInterpretation, Textual Image Design, Entity-Reasoning and\\nScientific-Reasoning. We propose a two-stage evaluation protocol to assess the\\nreasoning accuracy and image quality. We benchmark various T2I generation\\nmodels, and provide comprehensive analysis on their performances.\\n</summary>\\n    <author>\\n      <name>Kaiyue Sun</name>\\n    </author>\\n    <author>\\n      <name>Rongyao Fang</name>\\n    </author>\\n    <author>\\n      <name>Chengqi Duan</name>\\n    </author>\\n    <author>\\n      <name>Xian Liu</name>\\n    </author>\\n    <author>\\n      <name>Xihui Liu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Code: https://github.com/KaiyueSun98/T2I-ReasonBench</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2508.17472v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2508.17472v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0001025v1</id>\\n    <updated>2000-01-28T14:23:18Z</updated>\\n    <published>2000-01-28T14:23:18Z</published>\\n    <title>Computational Geometry Column 38</title>\\n    <summary>  Recent results on curve reconstruction are described.\\n</summary>\\n    <author>\\n      <name>Joseph O\\'Rourke</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">3 pages, 1 figure, 18 refs</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/cs/0001025v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0001025v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"F.2.2; I.5.3\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1301.3590v1</id>\\n    <updated>2013-01-16T05:20:01Z</updated>\\n    <published>2013-01-16T05:20:01Z</published>\\n    <title>Tree structured sparse coding on cubes</title>\\n    <summary>  A brief description of tree structured sparse coding on the binary cube.\\n</summary>\\n    <author>\\n      <name>Arthur Szlam</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1301.3590v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1301.3590v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1607.04311v1</id>\\n    <updated>2016-07-14T20:44:27Z</updated>\\n    <published>2016-07-14T20:44:27Z</published>\\n    <title>Defensive Distillation is Not Robust to Adversarial Examples</title>\\n    <summary>  We show that defensive distillation is not secure: it is no more resistant to\\ntargeted misclassification attacks than unprotected neural networks.\\n</summary>\\n    <author>\\n      <name>Nicholas Carlini</name>\\n    </author>\\n    <author>\\n      <name>David Wagner</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1607.04311v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1607.04311v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1701.05549v1</id>\\n    <updated>2017-01-19T18:43:56Z</updated>\\n    <published>2017-01-19T18:43:56Z</published>\\n    <title>Deep Neural Networks - A Brief History</title>\\n    <summary>  Introduction to deep neural networks and their history.\\n</summary>\\n    <author>\\n      <name>Krzysztof J. Cios</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">14 pages, 14 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1701.05549v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1701.05549v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2309.15477v1</id>\\n    <updated>2023-09-27T08:18:04Z</updated>\\n    <published>2023-09-27T08:18:04Z</published>\\n    <title>A Tutorial on Uniform B-Spline</title>\\n    <summary>  This document facilitates understanding of core concepts about uniform\\nB-spline and its matrix representation.\\n</summary>\\n    <author>\\n      <name>Yi Zhou</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2309.15477v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2309.15477v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.GR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.GR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0004012v1</id>\\n    <updated>2000-04-21T17:32:29Z</updated>\\n    <published>2000-04-21T17:32:29Z</published>\\n    <title>Assisted Video Sequences Indexing : Motion Analysis Based on Interest\\n  Points</title>\\n    <summary>  This work deals with content-based video indexing. Our viewpoint is\\nsemi-automatic analysis of compressed video. We consider the possible\\napplications of motion analysis and moving object detection : assisting moving\\nobject indexing, summarising videos, and allowing image and motion queries. We\\npropose an approach based on interest points. As first results, we test and\\ncompare the stability of different types of interest point detectors in\\ncompressed sequences.\\n</summary>\\n    <author>\\n      <name>Emmanuel Etievent</name>\\n    </author>\\n    <author>\\n      <name>Frank Lebourgeois</name>\\n    </author>\\n    <author>\\n      <name>Jean-Michel Jolion</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">HTML, 8 pages, 6 figures, http://rfv.insa-lyon.fr/~etievent/</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Iciap 99, Venezia, 27-29 sept., 1059-1062</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/cs/0004012v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0004012v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4.8; I.4.9\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0006047v1</id>\\n    <updated>2000-06-30T22:17:42Z</updated>\\n    <published>2000-06-30T22:17:42Z</published>\\n    <title>Geometric Morphology of Granular Materials</title>\\n    <summary>  We present a new method to transform the spectral pixel information of a\\nmicrograph into an affine geometric description, which allows us to analyze the\\nmorphology of granular materials. We use spectral and pulse-coupled neural\\nnetwork based segmentation techniques to generate blobs, and a newly developed\\nalgorithm to extract dilated contours. A constrained Delaunay tesselation of\\nthe contour points results in a triangular mesh. This mesh is the basic\\ningredient of the Chodal Axis Transform, which provides a morphological\\ndecomposition of shapes. Such decomposition allows for grain separation and the\\nefficient computation of the statistical features of granular materials.\\n</summary>\\n    <author>\\n      <name>B. R. Schlei</name>\\n    </author>\\n    <author>\\n      <name>L. Prasad</name>\\n    </author>\\n    <author>\\n      <name>A. N. Skourikhine</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1117/12.404821</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1117/12.404821\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages, 9 figures. For more information visit\\n  http://www.nis.lanl.gov/~bschlei/labvis/index.html</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/cs/0006047v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0006047v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.2.10;I.4.6;I.4.10\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0109116v1</id>\\n    <updated>2001-09-26T22:14:40Z</updated>\\n    <published>2001-09-26T22:14:40Z</published>\\n    <title>Digital Color Imaging</title>\\n    <summary>  This paper surveys current technology and research in the area of digital\\ncolor imaging. In order to establish the background and lay down terminology,\\nfundamental concepts of color perception and measurement are first presented\\nus-ing vector-space notation and terminology. Present-day color recording and\\nreproduction systems are reviewed along with the common mathematical models\\nused for representing these devices. Algorithms for processing color images for\\ndisplay and communication are surveyed, and a forecast of research trends is\\nattempted. An extensive bibliography is provided.\\n</summary>\\n    <author>\\n      <name>Gaurav Sharma</name>\\n    </author>\\n    <author>\\n      <name>H. Joel Trussell</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/83.597268</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/83.597268\" rel=\"related\"/>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IEEE Trans. Image Proc., vol. 6, no. 7, pp. 901-932, Jul. 1997</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/cs/0109116v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0109116v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.GR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"A.1;I.4,I.3.3,I.2.10;I.3.7;B.4.2\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0201019v1</id>\\n    <updated>2002-01-22T21:00:35Z</updated>\\n    <published>2002-01-22T21:00:35Z</published>\\n    <title>Structure from Motion: Theoretical Foundations of a Novel Approach Using\\n  Custom Built Invariants</title>\\n    <summary>  We rephrase the problem of 3D reconstruction from images in terms of\\nintersections of projections of orbits of custom built Lie groups actions. We\\nthen use an algorithmic method based on moving frames \"a la Fels-Olver\" to\\nobtain a fundamental set of invariants of these groups actions. The invariants\\nare used to define a set of equations to be solved by the points of the 3D\\nobject, providing a new technique for recovering 3D structure from motion.\\n</summary>\\n    <author>\\n      <name>Pierre-Louis Bazin</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Brown University</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Mireille Boutin</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Brown University</arxiv:affiliation>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">15 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/cs/0201019v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0201019v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.DG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4.8;I.2.10\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0303015v1</id>\\n    <updated>2003-03-18T21:30:36Z</updated>\\n    <published>2003-03-18T21:30:36Z</published>\\n    <title>Statistical efficiency of curve fitting algorithms</title>\\n    <summary>  We study the problem of fitting parametrized curves to noisy data. Under\\ncertain assumptions (known as Cartesian and radial functional models), we\\nderive asymptotic expressions for the bias and the covariance matrix of the\\nparameter estimates. We also extend Kanatani\\'s version of the Cramer-Rao lower\\nbound, which he proved for unbiased estimates only, to more general estimates\\nthat include many popular algorithms (most notably, the orthogonal least\\nsquares and algebraic fits). We then show that the gradient-weighted algebraic\\nfit is statistically efficient and describe all other statistically efficient\\nalgebraic fits.\\n</summary>\\n    <author>\\n      <name>N. Chernov</name>\\n    </author>\\n    <author>\\n      <name>C. Lesort</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">17 pages, 3 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/cs/0303015v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0303015v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4.8;I.5.1;I.2.10;G.3;G.1.2\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0303024v1</id>\\n    <updated>2003-03-24T02:36:21Z</updated>\\n    <published>2003-03-24T02:36:21Z</published>\\n    <title>Differential Methods in Catadioptric Sensor Design with Applications to\\n  Panoramic Imaging</title>\\n    <summary>  We discuss design techniques for catadioptric sensors that realize given\\nprojections. In general, these problems do not have solutions, but approximate\\nsolutions may often be found that are visually acceptable. There are several\\nmethods to approach this problem, but here we focus on what we call the\\n``vector field approach\\'\\'. An application is given where a true panoramic\\nmirror is derived, i.e. a mirror that yields a cylindrical projection to the\\nviewer without any digital unwarping.\\n</summary>\\n    <author>\\n      <name>R. Andrew Hicks</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, 7 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/cs/0303024v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0303024v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.2.9\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0307046v1</id>\\n    <updated>2003-07-20T05:18:59Z</updated>\\n    <published>2003-07-20T05:18:59Z</published>\\n    <title>A New Analytical Radial Distortion Model for Camera Calibration</title>\\n    <summary>  Common approach to radial distortion is by the means of polynomial\\napproximation, which introduces distortion-specific parameters into the camera\\nmodel and requires estimation of these distortion parameters. The task of\\nestimating radial distortion is to find a radial distortion model that allows\\neasy undistortion as well as satisfactory accuracy. This paper presents a new\\nradial distortion model with an easy analytical undistortion formula, which\\nalso belongs to the polynomial approximation category. Experimental results are\\npresented to show that with this radial distortion model, satisfactory accuracy\\nis achieved.\\n</summary>\\n    <author>\\n      <name>Lili Ma</name>\\n    </author>\\n    <author>\\n      <name>YangQuan Chen</name>\\n    </author>\\n    <author>\\n      <name>Kevin L. Moore</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">2 Postscript figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/cs/0307046v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0307046v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4.1\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0307047v1</id>\\n    <updated>2003-07-20T05:54:42Z</updated>\\n    <published>2003-07-20T05:54:42Z</published>\\n    <title>Rational Radial Distortion Models with Analytical Undistortion Formulae</title>\\n    <summary>  The common approach to radial distortion is by the means of polynomial\\napproximation, which introduces distortion-specific parameters into the camera\\nmodel and requires estimation of these distortion parameters. The task of\\nestimating radial distortion is to find a radial distortion model that allows\\neasy undistortion as well as satisfactory accuracy. This paper presents a new\\nclass of rational radial distortion models with easy analytical undistortion\\nformulae. Experimental results are presented to show that with this class of\\nrational radial distortion models, satisfactory and comparable accuracy is\\nachieved.\\n</summary>\\n    <author>\\n      <name>Lili Ma</name>\\n    </author>\\n    <author>\\n      <name>YangQuan Chen</name>\\n    </author>\\n    <author>\\n      <name>Kevin L. Moore</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/cs/0307047v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0307047v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4.1\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0307072v1</id>\\n    <updated>2003-07-31T19:33:48Z</updated>\\n    <published>2003-07-31T19:33:48Z</published>\\n    <title>Camera Calibration: a USU Implementation</title>\\n    <summary>  The task of camera calibration is to estimate the intrinsic and extrinsic\\nparameters of a camera model. Though there are some restricted techniques to\\ninfer the 3-D information about the scene from uncalibrated cameras, effective\\ncamera calibration procedures will open up the possibility of using a wide\\nrange of existing algorithms for 3-D reconstruction and recognition.\\n  The applications of camera calibration include vision-based metrology, robust\\nvisual platooning and visual docking of mobile robots where the depth\\ninformation is important.\\n</summary>\\n    <author>\\n      <name>Lili Ma</name>\\n    </author>\\n    <author>\\n      <name>YangQuan Chen</name>\\n    </author>\\n    <author>\\n      <name>Kevin L. Moore</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">39 pages, 19 eps figures, source codes are in the codes.m and\\n  corners.dat</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/cs/0307072v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0307072v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4.1\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0308037v1</id>\\n    <updated>2003-08-22T10:31:53Z</updated>\\n    <published>2003-08-22T10:31:53Z</published>\\n    <title>Distributed and Parallel Net Imaging</title>\\n    <summary>  A very complex vision system is developed to detect luminosity variations\\nconnected with the discovery of new planets in the Universe. The traditional\\nimaging system can not manage a so large load. A private net is implemented to\\nperform an automatic vision and decision architecture. It lets to carry out an\\non-line discrimination of interesting events by using two levels of triggers.\\nThis system can even manage many Tbytes of data per day. The architecture\\navails itself of a distributed parallel network system based on a maximum of\\n256 standard workstations with Microsoft Window as OS.\\n</summary>\\n    <author>\\n      <name>G. Iovane</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">3 pages, 8 figures, Procedding of NIDays 2003 (sponsored by National\\n  Instruments), Rome 2003. Winner (2nd classified) of the price \"Best\\n  Application of Measurement and Automation</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/cs/0308037v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0308037v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"astro-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4,I.5\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0308038v1</id>\\n    <updated>2003-08-22T18:47:33Z</updated>\\n    <published>2003-08-22T18:47:33Z</published>\\n    <title>Image Analysis in Astronomy for very large vision machine</title>\\n    <summary>  It is developed a very complex system (hardware/software) to detect\\nluminosity variations connected with the discovery of new planets outside the\\nSolar System. Traditional imaging approaches are very demanding in terms of\\ncomputing time; then, the implementation of an automatic vision and decision\\nsoftware architecture is presented. It allows to perform an on-line\\ndiscrimination of interesting events by using two levels of triggers. A\\nfundamental challenge was to work with very large CCD camera (even 16k*16k\\npixels) in line with very large telescopes. Then, the architecture can use a\\ndistributed parallel network system based on a maximum of 256 standard\\nworkstations.\\n</summary>\\n    <author>\\n      <name>G. Iovane</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">3 pages, 9 figures, Proceeding of NIWEEK 2002 (sponsored by National\\n  Instruments), Austin (Usa), 2002</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/cs/0308038v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0308038v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"astro-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4,I.5\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0311012v1</id>\\n    <updated>2003-11-12T19:15:41Z</updated>\\n    <published>2003-11-12T19:15:41Z</published>\\n    <title>A rigorous definition of axial lines: ridges on isovist fields</title>\\n    <summary>  We suggest that \\'axial lines\\' defined by (Hillier and Hanson, 1984) as lines\\nof uninterrupted movement within urban streetscapes or buildings, appear as\\nridges in isovist fields (Benedikt, 1979). These are formed from the maximum\\ndiametric lengths of the individual isovists, sometimes called viewsheds, that\\nmake up these fields (Batty and Rana, 2004). We present an image processing\\ntechnique for the identification of lines from ridges, discuss current\\nstrengths and weaknesses of the method, and show how it can be implemented\\neasily and effectively.\\n</summary>\\n    <author>\\n      <name>Rui Carvalho</name>\\n    </author>\\n    <author>\\n      <name>Michael Batty</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">18 pages, 5 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/cs/0311012v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0311012v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I2.10; I.4.10\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0402020v1</id>\\n    <updated>2004-02-11T16:34:16Z</updated>\\n    <published>2004-02-11T16:34:16Z</published>\\n    <title>Geometrical Complexity of Classification Problems</title>\\n    <summary>  Despite encouraging recent progresses in ensemble approaches, classification\\nmethods seem to have reached a plateau in development. Further advances depend\\non a better understanding of geometrical and topological characteristics of\\npoint sets in high-dimensional spaces, the preservation of such characteristics\\nunder feature transformations and sampling processes, and their interaction\\nwith geometrical models used in classifiers. We discuss an attempt to measure\\nsuch properties from data sets and relate them to classifier accuracies.\\n</summary>\\n    <author>\\n      <name>Tin Kam Ho</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proceedings of the 7th Course on Ensemble Methods for Learning\\n  Machines at the International School on Neural Nets ``E.R. Caianiello\\'\\'</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/cs/0402020v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0402020v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.5.0\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0404046v1</id>\\n    <updated>2004-04-22T13:42:48Z</updated>\\n    <published>2004-04-22T13:42:48Z</published>\\n    <title>Visualising the structure of architectural open spaces based on shape\\n  analysis</title>\\n    <summary>  This paper proposes the application of some well known two-dimensional\\ngeometrical shape descriptors for the visualisation of the structure of\\narchitectural open spaces. The paper demonstrates the use of visibility\\nmeasures such as distance to obstacles and amount of visible space to calculate\\nshape descriptors such as convexity and skeleton of the open space. The aim of\\nthe paper is to indicate a simple, objective and quantifiable approach to\\nunderstand the structure of open spaces otherwise impossible due to the complex\\nconstruction of built structures.\\n</summary>\\n    <author>\\n      <name>Sanjay Rana</name>\\n    </author>\\n    <author>\\n      <name>Mike Batty</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages, 9 figures</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">International Journal of Architectural Computing, 2(1), 2004</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/cs/0404046v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0404046v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DS\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.3.5;I.4.8;I.5.2\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0405093v2</id>\\n    <updated>2004-10-20T10:07:43Z</updated>\\n    <published>2004-05-25T11:36:34Z</published>\\n    <title>Computerized Face Detection and Recognition</title>\\n    <summary>  This publication presents methods for face detection, analysis and\\nrecognition: fast normalized cross-correlation (fast correlation coefficient)\\nbetween multiple templates based face pre-detection method, method for\\ndetection of exact face contour based on snakes and Generalized Gradient Vector\\nFlow field, method for combining recognition algorithms based on Cumulative\\nMatch Characteristics in order to increase recognition speed and accuracy, and\\nface recognition method based on Principal Component Analysis of the Wavelet\\nPacket Decomposition allowing to use PCA - based recognition method with large\\nnumber of training images. For all the methods are presented experimental\\nresults and comparisons of speed and accuracy with large face databases.\\n</summary>\\n    <author>\\n      <name>Vytautas Perlibakas</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">PhD dissertation summary. 35 pages, 12 figures, 7 tables</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/cs/0405093v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0405093v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4.8; I.5\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0405095v1</id>\\n    <updated>2004-05-25T22:40:42Z</updated>\\n    <published>2004-05-25T22:40:42Z</published>\\n    <title>Blind Detection and Compensation of Camera Lens Geometric Distortions</title>\\n    <summary>  This paper presents a blind detection and compensation technique for camera\\nlens geometric distortions. The lens distortion introduces higher-order\\ncorrelations in the frequency domain and in turn it can be detected using\\nhigher-order spectral analysis tools without assuming any specific calibration\\ntarget. The existing blind lens distortion removal method only considered a\\nsingle-coefficient radial distortion model. In this paper, two coefficients are\\nconsidered to model approximately the geometric distortion. All the models\\nconsidered have analytical closed-form inverse formulae.\\n</summary>\\n    <author>\\n      <name>Lili Ma</name>\\n    </author>\\n    <author>\\n      <name>YangQuan Chen</name>\\n    </author>\\n    <author>\\n      <name>Kevin L. Moore</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages, 4 figures, 2 tables</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">SIAM Imaging Science, 2004</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/cs/0405095v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0405095v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I 4.1\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0503001v1</id>\\n    <updated>2005-03-01T05:17:33Z</updated>\\n    <published>2005-03-01T05:17:33Z</published>\\n    <title>Top-Down Unsupervised Image Segmentation (it sounds like oxymoron, but\\n  actually it is not)</title>\\n    <summary>  Pattern recognition is generally assumed as an interaction of two inversely\\ndirected image-processing streams: the bottom-up information details gathering\\nand localization (segmentation) stream, and the top-down information features\\naggregation, association and interpretation (recognition) stream. Inspired by\\nrecent evidence from biological vision research and by the insights of\\nKolmogorov Complexity theory, we propose a new, just top-down evolving,\\nprocedure of initial image segmentation. We claim that traditional top-down\\ncognitive reasoning, which is supposed to guide the segmentation process to its\\nfinal result, is not at all a part of the image information content evaluation.\\nAnd that initial image segmentation is certainly an unsupervised process. We\\npresent some illustrative examples, which support our claims.\\n</summary>\\n    <author>\\n      <name>Emanuel Diamant</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/cs/0503001v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0503001v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0504037v2</id>\\n    <updated>2006-10-31T14:34:47Z</updated>\\n    <published>2005-04-11T12:41:19Z</published>\\n    <title>Bayesian Restoration of Digital Images Employing Markov Chain Monte\\n  Carlo a Review</title>\\n    <summary>  A review of Bayesian restoration of digital images based on Monte Carlo\\ntechniques is presented. The topics covered include Likelihood, Prior and\\nPosterior distributions, Poisson, Binay symmetric channel, and Gaussian channel\\nmodels of Likelihood distribution,Ising and Potts spin models of Prior\\ndistribution, restoration of an image through Posterior maximization,\\nstatistical estimation of a true image from Posterior ensembles, Markov Chain\\nMonte Carlo methods and cluster algorithms.\\n</summary>\\n    <author>\\n      <name>K. P. N. Murthy</name>\\n    </author>\\n    <author>\\n      <name>M. Janani</name>\\n    </author>\\n    <author>\\n      <name>B. Shenbga Priya</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">42 pages; 16 figures; revised version with several typos removed and\\n  mistakes in equations corrected</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/cs/0504037v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0504037v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cond-mat.stat-mech\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.comp-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0604062v1</id>\\n    <updated>2006-04-14T04:40:29Z</updated>\\n    <published>2006-04-14T04:40:29Z</published>\\n    <title>Biologically Inspired Hierarchical Model for Feature Extraction and\\n  Localization</title>\\n    <summary>  Feature extraction and matching are among central problems of computer\\nvision. It is inefficent to search features over all locations and scales.\\nNeurophysiological evidence shows that to locate objects in a digital image the\\nhuman visual system employs visual attention to a specific object while\\nignoring others. The brain also has a mechanism to search from coarse to fine.\\nIn this paper, we present a feature extractor and an associated hierarchical\\nsearching model to simulate such processes. With the hierarchical\\nrepresentation of the object, coarse scanning is done through the matching of\\nthe larger scale and precise localization is conducted through the matching of\\nthe smaller scale. Experimental results justify the proposed model in its\\neffectiveness and efficiency to localize features.\\n</summary>\\n    <author>\\n      <name>Liang Wu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, 4 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/cs/0604062v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0604062v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0606060v1</id>\\n    <updated>2006-06-13T12:53:45Z</updated>\\n    <published>2006-06-13T12:53:45Z</published>\\n    <title>Complex Networks: New Concepts and Tools for Real-Time Imaging and\\n  Vision</title>\\n    <summary>  This article discusses how concepts and methods of complex networks can be\\napplied to real-time imaging and computer vision. After a brief introduction of\\ncomplex networks basic concepts, their use as means to represent and\\ncharacterize images, as well as for modeling visual saliency, are briefly\\ndescribed. The possibility to apply complex networks in order to model and\\nsimulate the performance of parallel and distributed computing systems for\\nperformance of visual methods is also proposed.\\n</summary>\\n    <author>\\n      <name>Luciano da Fontoura Costa</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">3 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/cs/0606060v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0606060v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.soc-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0608115v1</id>\\n    <updated>2006-08-29T13:24:37Z</updated>\\n    <published>2006-08-29T13:24:37Z</published>\\n    <title>Neural Network Clustering Based on Distances Between Objects</title>\\n    <summary>  We present an algorithm of clustering of many-dimensional objects, where only\\nthe distances between objects are used. Centers of classes are found with the\\naid of neuron-like procedure with lateral inhibition. The result of clustering\\ndoes not depend on starting conditions. Our algorithm makes it possible to give\\nan idea about classes that really exist in the empirical data. The results of\\ncomputer simulations are presented.\\n</summary>\\n    <author>\\n      <name>Leonid B. Litinskii</name>\\n    </author>\\n    <author>\\n      <name>Dmitry E. Romanov</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages,4 figures, presentation on ICANN (Athens, Greece, 2006)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/cs/0608115v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0608115v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0610002v1</id>\\n    <updated>2006-09-30T08:05:02Z</updated>\\n    <published>2006-09-30T08:05:02Z</published>\\n    <title>Conditional Expressions for Blind Deconvolution: Derivative form</title>\\n    <summary>  We developed novel conditional expressions (CEs) for Lane and Bates\\' blind\\ndeconvolution. The CEs are given in term of the derivatives of the zero-values\\nof the z-transform of given images. The CEs make it possible to automatically\\ndetect multiple blur convolved in the given images all at once without\\nperforming any analysis of the zero-sheets of the given images. We illustrate\\nthe multiple blur-detection by the CEs for a model image\\n</summary>\\n    <author>\\n      <name>S. Aogaki</name>\\n    </author>\\n    <author>\\n      <name>I. Moritani</name>\\n    </author>\\n    <author>\\n      <name>T. Sugai</name>\\n    </author>\\n    <author>\\n      <name>F. Takeutchi</name>\\n    </author>\\n    <author>\\n      <name>F. M. Toyama</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 page, 3 figures, conference</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/cs/0610002v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0610002v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0701127v3</id>\\n    <updated>2007-12-28T15:30:19Z</updated>\\n    <published>2007-01-20T15:45:03Z</published>\\n    <title>A novel set of rotationally and translationally invariant features for\\n  images based on the non-commutative bispectrum</title>\\n    <summary>  We propose a new set of rotationally and translationally invariant features\\nfor image or pattern recognition and classification. The new features are cubic\\npolynomials in the pixel intensities and provide a richer representation of the\\noriginal image than most existing systems of invariants. Our construction is\\nbased on the generalization of the concept of bispectrum to the\\nthree-dimensional rotation group SO(3), and a projection of the image onto the\\nsphere.\\n</summary>\\n    <author>\\n      <name>Risi Kondor</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">The claim that the invariants uniquely determine the original image\\n  had to be dropped</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/cs/0701127v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0701127v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4.7; I.2.10; I.5.4\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0701150v1</id>\\n    <updated>2007-01-24T15:13:06Z</updated>\\n    <published>2007-01-24T15:13:06Z</published>\\n    <title>Contains and Inside relationships within combinatorial Pyramids</title>\\n    <summary>  Irregular pyramids are made of a stack of successively reduced graphs\\nembedded in the plane. Such pyramids are used within the segmentation framework\\nto encode a hierarchy of partitions. The different graph models used within the\\nirregular pyramid framework encode different types of relationships between\\nregions. This paper compares different graph models used within the irregular\\npyramid framework according to a set of relationships between regions. We also\\ndefine a new algorithm based on a pyramid of combinatorial maps which allows to\\ndetermine if one region contains the other using only local calculus.\\n</summary>\\n    <author>\\n      <name>Luc Brun</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">GREYC</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Walter G. Kropatsch</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">PRIP</arxiv:affiliation>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">35 pages</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Pattern Recognition 39 (01/04/2006) 515-526</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/cs/0701150v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0701150v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0703088v1</id>\\n    <updated>2007-03-16T00:18:11Z</updated>\\n    <published>2007-03-16T00:18:11Z</published>\\n    <title>Plot 94 in ambiance X-Window</title>\\n    <summary>  &lt;PLOT &gt; is a collection of routines to draw surfaces, contours and so on. In\\nthis work we are presenting a version, that functions over work stations with\\nthe operative system UNIX, that count with the graphic ambiance X-WINDOW with\\nthe tools XLIB and OSF/MOTIF. This implant was realized for the work stations\\nDEC 5000-200, DEC IPX, and DEC ALFA of the CINVESTAV (Center of Investigation\\nand Advanced Studies). Also implanted in SILICON GRAPHICS of the CENAC\\n(National Center of Calculation of the Polytechnic National Institute\\n</summary>\\n    <author>\\n      <name>Ignacio Vega-Paez</name>\\n    </author>\\n    <author>\\n      <name>Carlos Alberto Hernandez-Hernandez</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proceedings in Information Systems Analysis and Synthesis ISAS\\n  1995, 5th, International Symposium on Systems Research, Informatics and\\n  Cybernetics, pp. 135-139, August 16-20, 95, Baden-Baden, Germany</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/cs/0703088v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0703088v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.GR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0704.3635v1</id>\\n    <updated>2007-04-26T22:22:45Z</updated>\\n    <published>2007-04-26T22:22:45Z</published>\\n    <title>Rough Sets Computations to Impute Missing Data</title>\\n    <summary>  Many techniques for handling missing data have been proposed in the\\nliterature. Most of these techniques are overly complex. This paper explores an\\nimputation technique based on rough set computations. In this paper,\\ncharacteristic relations are introduced to describe incompletely specified\\ndecision tables.It is shown that the basic rough set idea of lower and upper\\napproximations for incompletely specified decision tables may be defined in a\\nvariety of different ways. Empirical results obtained using real data are given\\nand they provide a valuable and promising insight to the problem of missing\\ndata. Missing data were predicted with an accuracy of up to 99%.\\n</summary>\\n    <author>\\n      <name>Fulufhelo Vincent Nelwamondo</name>\\n    </author>\\n    <author>\\n      <name>Tshilidzi Marwala</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">19 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/0704.3635v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0704.3635v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0705.0449v1</id>\\n    <updated>2007-05-03T12:47:31Z</updated>\\n    <published>2007-05-03T12:47:31Z</published>\\n    <title>Multiresolution Approximation of Polygonal Curves in Linear Complexity</title>\\n    <summary>  We propose a new algorithm to the problem of polygonal curve approximation\\nbased on a multiresolution approach. This algorithm is suboptimal but still\\nmaintains some optimality between successive levels of resolution using dynamic\\nprogramming. We show theoretically and experimentally that this algorithm has a\\nlinear complexity in time and space. We experimentally compare the outcomes of\\nour algorithm to the optimal \"full search\" dynamic programming solution and\\nfinally to classical merge and split approaches. The experimental evaluations\\nconfirm the theoretical derivations and show that the proposed approach\\nevaluated on 2D coastal maps either show a lower time complexity or provide\\npolygonal approximations closer to the input discrete curves.\\n</summary>\\n    <author>\\n      <name>Pierre-Fran\\xc3\\xa7ois Marteau</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">VALORIA</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Gilbas M\\xc3\\xa9nier</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">VALORIA</arxiv:affiliation>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/0705.0449v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0705.0449v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0705.0781v1</id>\\n    <updated>2007-05-06T06:02:46Z</updated>\\n    <published>2007-05-06T06:02:46Z</published>\\n    <title>Medical Image Segmentation and Localization using Deformable Templates</title>\\n    <summary>  This paper presents deformable templates as a tool for segmentation and\\nlocalization of biological structures in medical images. Structures are\\nrepresented by a prototype template, combined with a parametric warp mapping\\nused to deform the original shape. The localization procedure is achieved using\\na multi-stage, multi-resolution algorithm de-signed to reduce computational\\ncomplexity and time. The algorithm initially identifies regions in the image\\nmost likely to contain the desired objects and then examines these regions at\\nprogressively increasing resolutions. The final stage of the algorithm involves\\nwarping the prototype template to match the localized objects. The algorithm is\\npresented along with the results of four example applications using MRI, x-ray\\nand ultrasound images.\\n</summary>\\n    <author>\\n      <name>Jonathan M. Spiller</name>\\n    </author>\\n    <author>\\n      <name>T. Marwala</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/0705.0781v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0705.0781v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0705.0952v1</id>\\n    <updated>2007-05-07T19:19:55Z</updated>\\n    <published>2007-05-07T19:19:55Z</published>\\n    <title>An Independent Evaluation of Subspace Face Recognition Algorithms</title>\\n    <summary>  This paper explores a comparative study of both the linear and kernel\\nimplementations of three of the most popular Appearance-based Face Recognition\\nprojection classes, these being the methodologies of Principal Component\\nAnalysis, Linear Discriminant Analysis and Independent Component Analysis. The\\nexperimental procedure provides a platform of equal working conditions and\\nexamines the ten algorithms in the categories of expression, illumination,\\nocclusion and temporal delay. The results are then evaluated based on a\\nsequential combination of assessment tools that facilitate both intuitive and\\nstatistical decisiveness among the intra and interclass comparisons. The best\\ncategorical algorithms are then incorporated into a hybrid methodology, where\\nthe advantageous effects of fusion strategies are considered.\\n</summary>\\n    <author>\\n      <name>Dhiresh R. Surajpal</name>\\n    </author>\\n    <author>\\n      <name>Tshilidzi Marwala</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/0705.0952v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0705.0952v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0705.3593v2</id>\\n    <updated>2007-06-19T12:14:51Z</updated>\\n    <published>2007-05-24T14:41:11Z</published>\\n    <title>MI image registration using prior knowledge</title>\\n    <summary>  Subtraction of aligned images is a means to assess changes in a wide variety\\nof clinical applications. In this paper we explore the information theoretical\\norigin of Mutual Information (MI), which is based on Shannon\\'s entropy.However,\\nthe interpretation of standard MI registration as a communication channel\\nsuggests that MI is too restrictive a criterion. In this paper the concept of\\nMutual Information (MI) is extended to (Normalized) Focussed Mutual Information\\n(FMI) to incorporate prior knowledge to overcome some shortcomings of MI. We\\nuse this to develop new methodologies to successfully address specific\\nregistration problems, the follow-up of dental restorations, cephalometry, and\\nthe monitoring of implants.\\n</summary>\\n    <author>\\n      <name>W. Jacquet</name>\\n    </author>\\n    <author>\\n      <name>P. de Groen</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">16 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/0705.3593v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0705.3593v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4.3\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0706.1926v1</id>\\n    <updated>2007-06-13T15:15:00Z</updated>\\n    <published>2007-06-13T15:15:00Z</published>\\n    <title>Towards understanding and modelling office daily life</title>\\n    <summary>  Measuring and modeling human behavior is a very complex task. In this paper\\nwe present our initial thoughts on modeling and automatic recognition of some\\nhuman activities in an office. We argue that to successfully model human\\nactivities, we need to consider both individual behavior and group dynamics. To\\ndemonstrate these theoretical approaches, we introduce an experimental system\\nfor analyzing everyday activity in our office.\\n</summary>\\n    <author>\\n      <name>Michele Bezzi</name>\\n    </author>\\n    <author>\\n      <name>Robin Groenevelt</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages, ECHISE 2006 - 2nd International Workshop on Exploiting\\n  Context Histories in Smart Environments - Infrastructures and Design, 8th\\n  International Conference of Ubiquitous Computing (Ubicomp 2006), Orange\\n  County, CA, 17-21 September 2006</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/0706.1926v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0706.1926v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4.8; I.5.3\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0708.2432v1</id>\\n    <updated>2007-08-18T14:36:28Z</updated>\\n    <published>2007-08-18T14:36:28Z</published>\\n    <title>A structure from motion inequality</title>\\n    <summary>  We state an elementary inequality for the structure from motion problem for m\\ncameras and n points. This structure from motion inequality relates space\\ndimension, camera parameter dimension, the number of cameras and number points\\nand global symmetry properties and provides a rigorous criterion for which\\nreconstruction is not possible with probability 1. Mathematically the\\ninequality is based on Frobenius theorem which is a geometric incarnation of\\nthe fundamental theorem of linear algebra. The paper also provides a general\\nmathematical formalism for the structure from motion problem. It includes the\\nsituation the points can move while the camera takes the pictures.\\n</summary>\\n    <author>\\n      <name>Oliver Knill</name>\\n    </author>\\n    <author>\\n      <name>Jose Ramirez-Herran</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">15 pages, 22 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/0708.2432v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0708.2432v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.2.10\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0708.2438v1</id>\\n    <updated>2007-08-17T21:36:08Z</updated>\\n    <published>2007-08-17T21:36:08Z</published>\\n    <title>On Ullman\\'s theorem in computer vision</title>\\n    <summary>  Both in the plane and in space, we invert the nonlinear Ullman transformation\\nfor 3 points and 3 orthographic cameras. While Ullman\\'s theorem assures a\\nunique reconstruction modulo a reflection for 3 cameras and 4 points, we find a\\nlocally unique reconstruction for 3 cameras and 3 points. Explicit\\nreconstruction formulas allow to decide whether picture data of three cameras\\nseeing three points can be realized as a point-camera configuration.\\n</summary>\\n    <author>\\n      <name>Oliver Knill</name>\\n    </author>\\n    <author>\\n      <name>Jose Ramirez-Herran</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">16 pages, 13 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/0708.2438v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0708.2438v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.2.10\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0708.2974v1</id>\\n    <updated>2007-08-22T08:28:02Z</updated>\\n    <published>2007-08-22T08:28:02Z</published>\\n    <title>The Fuzzy Vault for fingerprints is Vulnerable to Brute Force Attack</title>\\n    <summary>  The \\\\textit{fuzzy vault} approach is one of the best studied and well\\naccepted ideas for binding cryptographic security into biometric\\nauthentication. The vault has been implemented in connection with fingerprint\\ndata by Uludag and Jain. We show that this instance of the vault is vulnerable\\nto brute force attack. An interceptor of the vault data can recover both secret\\nand template data using only generally affordable computational resources. Some\\npossible alternatives are then discussed and it is suggested that cryptographic\\nsecurity may be preferable to the one - way function approach to biometric\\nsecurity.\\n</summary>\\n    <author>\\n      <name>Preda Mihailescu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/0708.2974v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0708.2974v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"D.4.6\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0709.1771v1</id>\\n    <updated>2007-09-12T08:41:36Z</updated>\\n    <published>2007-09-12T08:41:36Z</published>\\n    <title>Variational local structure estimation for image super-resolution</title>\\n    <summary>  Super-resolution is an important but difficult problem in image/video\\nprocessing. If a video sequence or some training set other than the given\\nlow-resolution image is available, this kind of extra information can greatly\\naid in the reconstruction of the high-resolution image. The problem is\\nsubstantially more difficult with only a single low-resolution image on hand.\\nThe image reconstruction methods designed primarily for denoising is\\ninsufficient for super-resolution problem in the sense that it tends to\\noversmooth images with essentially no noise. We propose a new adaptive linear\\ninterpolation method based on variational method and inspired by local linear\\nembedding (LLE). The experimental result shows that our method avoids the\\nproblem of oversmoothing and preserves image structures well.\\n</summary>\\n    <author>\\n      <name>Heng Lian</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/0709.1771v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0709.1771v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0710.0243v1</id>\\n    <updated>2007-10-01T09:18:36Z</updated>\\n    <published>2007-10-01T09:18:36Z</published>\\n    <title>High-Order Nonparametric Belief-Propagation for Fast Image Inpainting</title>\\n    <summary>  In this paper, we use belief-propagation techniques to develop fast\\nalgorithms for image inpainting. Unlike traditional gradient-based approaches,\\nwhich may require many iterations to converge, our techniques achieve\\ncompetitive results after only a few iterations. On the other hand, while\\nbelief-propagation techniques are often unable to deal with high-order models\\ndue to the explosion in the size of messages, we avoid this problem by\\napproximating our high-order prior model using a Gaussian mixture. By using\\nsuch an approximation, we are able to inpaint images quickly while at the same\\ntime retaining good visual results.\\n</summary>\\n    <author>\\n      <name>Julian John McAuley</name>\\n    </author>\\n    <author>\\n      <name>Tiberio S. Caetano</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, 6 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/0710.0243v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0710.0243v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0710.0736v1</id>\\n    <updated>2007-10-03T08:51:44Z</updated>\\n    <published>2007-10-03T08:51:44Z</published>\\n    <title>Colour image segmentation by the vector-valued Allen-Cahn phase-field\\n  model: a multigrid solution</title>\\n    <summary>  We propose a new method for the numerical solution of a PDE-driven model for\\ncolour image segmentation and give numerical examples of the results. The\\nmethod combines the vector-valued Allen-Cahn phase field equation with initial\\ndata fitting terms. This method is known to be closely related to the\\nMumford-Shah problem and the level set segmentation by Chan and Vese. Our\\nnumerical solution is performed using a multigrid splitting of a finite element\\nspace, thereby producing an efficient and robust method for the segmentation of\\nlarge images.\\n</summary>\\n    <author>\\n      <name>David A Kay</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Oxford University Computational Laboratory</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Alessandro Tomasi</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">University of Sussex</arxiv:affiliation>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/TIP.2009.2026678</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/TIP.2009.2026678\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">17 pages, 9 figures</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IEEE Trans. Im. Proc. 18.10 (2009)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/0710.0736v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0710.0736v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NA\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4.6; G.1.8\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0710.2037v2</id>\\n    <updated>2007-10-11T11:32:25Z</updated>\\n    <published>2007-10-10T15:12:20Z</published>\\n    <title>An Affinity Propagation Based method for Vector Quantization Codebook\\n  Design</title>\\n    <summary>  In this paper, we firstly modify a parameter in affinity propagation (AP) to\\nimprove its convergence ability, and then, we apply it to vector quantization\\n(VQ) codebook design problem. In order to improve the quality of the resulted\\ncodebook, we combine the improved AP (IAP) with the conventional LBG algorithm\\nto generate an effective algorithm call IAP-LBG. According to the experimental\\nresults, the proposed method not only enhances the convergence abilities but\\nalso is capable of providing higher-quality codebooks than conventional LBG\\nmethod.\\n</summary>\\n    <author>\\n      <name>Wu Jiang</name>\\n    </author>\\n    <author>\\n      <name>Fei Ding</name>\\n    </author>\\n    <author>\\n      <name>Qiao-liang Xiang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">In this version we make some explaination about the network-support\\n  similarity</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/0710.2037v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0710.2037v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0712.4015v1</id>\\n    <updated>2007-12-24T17:11:56Z</updated>\\n    <published>2007-12-24T17:11:56Z</published>\\n    <title>A Fast Hierarchical Multilevel Image Segmentation Method using Unbiased\\n  Estimators</title>\\n    <summary>  This paper proposes a novel method for segmentation of images by hierarchical\\nmultilevel thresholding. The method is global, agglomerative in nature and\\ndisregards pixel locations. It involves the optimization of the ratio of the\\nunbiased estimators of within class to between class variances. We obtain a\\nrecursive relation at each step for the variances which expedites the process.\\nThe efficacy of the method is shown in a comparison with some well-known\\nmethods.\\n</summary>\\n    <author>\\n      <name>Sreechakra Goparaju</name>\\n    </author>\\n    <author>\\n      <name>Jayadev Acharya</name>\\n    </author>\\n    <author>\\n      <name>Ajoy K. Ray</name>\\n    </author>\\n    <author>\\n      <name>Jaideva C. Goswami</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages, 5 figures, submitted to \"IEEE Transactions on Pattern\\n  Analysis and Machine Intelligence\"</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/0712.4015v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0712.4015v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0802.3285v1</id>\\n    <updated>2008-02-22T10:48:44Z</updated>\\n    <published>2008-02-22T10:48:44Z</published>\\n    <title>Some Aspects of Testing Process for Transport Streams in Digital Video\\n  Broadcasting</title>\\n    <summary>  This paper presents some aspects related to the DVB (Digital Video\\nBroadcasting) investigation. The basic aspects of DVB are presented, with an\\nemphasis on DVB-T version of standard. The main purpose of this research is to\\nanalyze the way that the transmission of the transport streams is realized in\\ncase of the Terrestrial Digital Video Broadcasting (DVB-T). To accomplish this,\\nfirst, Digital Video Broadcasting standard is presented, and then the main\\naspects of DVB testing and analysis of the transport streams are investigated.\\nThe paper presents also the results obtained using two programs designed for\\nDVB analysis: Mosalina and TSA.\\n</summary>\\n    <author>\\n      <name>Radu Arsinte</name>\\n    </author>\\n    <author>\\n      <name>Ciprian Ilioaei</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages, 3 figures, 3 tables</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Acta Technica Napocensis, Electronics and Telecommunications,\\n  nr.1/2004 pp.59-74</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/0802.3285v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0802.3285v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.MM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0803.1586v1</id>\\n    <updated>2008-03-11T13:40:42Z</updated>\\n    <published>2008-03-11T13:40:42Z</published>\\n    <title>Spatio-activity based object detection</title>\\n    <summary>  We present the SAMMI lightweight object detection method which has a high\\nlevel of accuracy and robustness, and which is able to operate in an\\nenvironment with a large number of cameras. Background modeling is based on DCT\\ncoefficients provided by cameras. Foreground detection uses similarity in\\ntemporal characteristics of adjacent blocks of pixels, which is a\\ncomputationally inexpensive way to make use of object coherence. Scene model\\nupdating uses the approximated median method for improved performance.\\nEvaluation at pixel level and application level shows that SAMMI object\\ndetection performs better and faster than the conventional Mixture of Gaussians\\nmethod.\\n</summary>\\n    <author>\\n      <name>Jarrad Springett</name>\\n    </author>\\n    <author>\\n      <name>Jeroen Vendrig</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">To be submitted to: AVSS 2008 conference</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/0803.1586v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0803.1586v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0804.1046v1</id>\\n    <updated>2008-04-07T14:47:03Z</updated>\\n    <published>2008-04-07T14:47:03Z</published>\\n    <title>Discrete schemes for Gaussian curvature and their convergence</title>\\n    <summary>  In this paper, several discrete schemes for Gaussian curvature are surveyed.\\nThe convergence property of a modified discrete scheme for the Gaussian\\ncurvature is considered. Furthermore, a new discrete scheme for Gaussian\\ncurvature is resented. We prove that the new scheme converges at the regular\\nvertex with valence not less than 5. By constructing a counterexample, we also\\nshow that it is impossible for building a discrete scheme for Gaussian\\ncurvature which converges over the regular vertex with valence 4. Finally,\\nasymptotic errors of several discrete scheme for Gaussian curvature are\\ncompared.\\n</summary>\\n    <author>\\n      <name>Zhiqiang Xu</name>\\n    </author>\\n    <author>\\n      <name>Guoliang Xu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/0804.1046v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0804.1046v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.GR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NA\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0804.1448v1</id>\\n    <updated>2008-04-09T10:06:15Z</updated>\\n    <published>2008-04-09T10:06:15Z</published>\\n    <title>Fast k Nearest Neighbor Search using GPU</title>\\n    <summary>  The recent improvements of graphics processing units (GPU) offer to the\\ncomputer vision community a powerful processing platform. Indeed, a lot of\\nhighly-parallelizable computer vision problems can be significantly accelerated\\nusing GPU architecture. Among these algorithms, the k nearest neighbor search\\n(KNN) is a well-known problem linked with many applications such as\\nclassification, estimation of statistical properties, etc. The main drawback of\\nthis task lies in its computation burden, as it grows polynomially with the\\ndata size. In this paper, we show that the use of the NVIDIA CUDA API\\naccelerates the search for the KNN up to a factor of 120.\\n</summary>\\n    <author>\\n      <name>Vincent Garcia</name>\\n    </author>\\n    <author>\\n      <name>Eric Debreuve</name>\\n    </author>\\n    <author>\\n      <name>Michel Barlaud</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">13 pages, 2figures, submitted to CVGPU 2008</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/0804.1448v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0804.1448v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0805.2324v1</id>\\n    <updated>2008-05-15T13:15:08Z</updated>\\n    <published>2008-05-15T13:15:08Z</published>\\n    <title>A multilateral filtering method applied to airplane runway image</title>\\n    <summary>  By considering the features of the airport runway image filtering, an\\nimproved bilateral filtering method was proposed which can remove noise with\\nedge preserving. Firstly the steerable filtering decomposition is used to\\ncalculate the sub-band parameters of 4 orients, and the texture feature matrix\\nis then obtained from the sub-band local median energy. The texture similar,\\nthe spatial closer and the color similar functions are used to filter the\\nimage.The effect of the weighting function parameters is qualitatively analyzed\\nalso. In contrast with the standard bilateral filter and the simulation results\\nfor the real airport runway image show that the multilateral filtering is more\\neffective than the standard bilateral filtering.\\n</summary>\\n    <author>\\n      <name>Zhang Yu</name>\\n    </author>\\n    <author>\\n      <name>Shi Zhong-ke</name>\\n    </author>\\n    <author>\\n      <name>Wang Run-quan</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, 5 figures, 4 tables</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/0805.2324v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0805.2324v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0806.0870v1</id>\\n    <updated>2008-06-04T22:58:41Z</updated>\\n    <published>2008-06-04T22:58:41Z</published>\\n    <title>The Euler-Poincare theory of Metamorphosis</title>\\n    <summary>  In the pattern matching approach to imaging science, the process of\\n``metamorphosis\\'\\' is template matching with dynamical templates. Here, we\\nrecast the metamorphosis equations of into the Euler-Poincare variational\\nframework of and show that the metamorphosis equations contain the equations\\nfor a perfect complex fluid \\\\cite{Ho2002}. This result connects the ideas\\nunderlying the process of metamorphosis in image matching to the physical\\nconcept of order parameter in the theory of complex fluids. After developing\\nthe general theory, we reinterpret various examples, including point set, image\\nand density metamorphosis. We finally discuss the issue of matching measures\\nwith metamorphosis, for which we provide existence theorems for the initial and\\nboundary value problems.\\n</summary>\\n    <author>\\n      <name>Darryl D. Holm</name>\\n    </author>\\n    <author>\\n      <name>Alain Trouve</name>\\n    </author>\\n    <author>\\n      <name>Laurent Younes</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/0806.0870v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0806.0870v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"nlin.CD\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0806.1446v1</id>\\n    <updated>2008-06-08T10:15:04Z</updated>\\n    <published>2008-06-08T10:15:04Z</published>\\n    <title>Fast Wavelet-Based Visual Classification</title>\\n    <summary>  We investigate a biologically motivated approach to fast visual\\nclassification, directly inspired by the recent work of Serre et al.\\nSpecifically, trading-off biological accuracy for computational efficiency, we\\nexplore using wavelet and grouplet-like transforms to parallel the tuning of\\nvisual cortex V1 and V2 cells, alternated with max operations to achieve scale\\nand translation invariance. A feature selection procedure is applied during\\nlearning to accelerate recognition. We introduce a simple attention-like\\nfeedback mechanism, significantly improving recognition and robustness in\\nmultiple-object scenes. In experiments, the proposed algorithm achieves or\\nexceeds state-of-the-art success rate on object recognition, texture and\\nsatellite image classification, language identification and sound\\nclassification.\\n</summary>\\n    <author>\\n      <name>Guoshen Yu</name>\\n    </author>\\n    <author>\\n      <name>Jean-Jacques Slotine</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/0806.1446v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0806.1446v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0806.2006v2</id>\\n    <updated>2012-01-06T20:39:14Z</updated>\\n    <published>2008-06-12T06:42:07Z</published>\\n    <title>Fusion de classifieurs pour la classification d\\'images sonar</title>\\n    <summary>  In this paper, we present some high level information fusion approaches for\\nnumeric and symbolic data. We study the interest of such method particularly\\nfor classifier fusion. A comparative study is made in a context of sea bed\\ncharacterization from sonar images. The classi- fication of kind of sediment is\\na difficult problem because of the data complexity. We compare high level\\ninformation fusion and give the obtained performance.\\n</summary>\\n    <author>\\n      <name>Arnaud Martin</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">E3I2</arxiv:affiliation>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Revue Nationale des Technologies de l\\'Information E, 5 (2005)\\n  259-268</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/0806.2006v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0806.2006v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0806.2007v1</id>\\n    <updated>2008-06-12T06:44:55Z</updated>\\n    <published>2008-06-12T06:44:55Z</published>\\n    <title>Experts Fusion and Multilayer Perceptron Based on Belief Learning for\\n  Sonar Image Classification</title>\\n    <summary>  The sonar images provide a rapid view of the seabed in order to characterize\\nit. However, in such as uncertain environment, real seabed is unknown and the\\nonly information we can obtain, is the interpretation of different human\\nexperts, sometimes in conflict. In this paper, we propose to manage this\\nconflict in order to provide a robust reality for the learning step of\\nclassification algorithms. The classification is conducted by a multilayer\\nperceptron, taking into account the uncertainty of the reality in the learning\\nstage. The results of this seabed characterization are presented on real sonar\\nimages.\\n</summary>\\n    <author>\\n      <name>Arnaud Martin</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">E3I2</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Christophe Osswald</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">E3I2</arxiv:affiliation>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">International Conference on Information &amp; Communication Technologies:\\n  from Theory to Applications (ICTTA), Damascus : Syrie (2008)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/0806.2007v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0806.2007v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4; I.5\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0806.3887v1</id>\\n    <updated>2008-06-24T13:34:15Z</updated>\\n    <published>2008-06-24T13:34:15Z</published>\\n    <title>Conceptualization of seeded region growing by pixels aggregation. Part\\n  2: how to localize a final partition invariant about the seeded region\\n  initialisation order</title>\\n    <summary>  In the previous paper, we have conceptualized the localization and the\\norganization of seeded region growing by pixels aggregation (SRGPA) but we do\\nnot give the issue when there is a collision between two distinct regions\\nduring the growing process. In this paper, we propose two implementations to\\nmanage two classical growing processes: one without a boundary region region to\\ndivide the other regions and another with. Unfortunately, as noticed by Mehnert\\nand Jakway (1997), this partition depends on the seeded region initialisation\\norder (SRIO). We propose a growing process, invariant about SRIO such as the\\nboundary region is the set of ambiguous pixels.\\n</summary>\\n    <author>\\n      <name>Vincent Tariel</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/0806.3887v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0806.3887v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0806.3928v1</id>\\n    <updated>2008-06-24T17:02:47Z</updated>\\n    <published>2008-06-24T17:02:47Z</published>\\n    <title>Conceptualization of seeded region growing by pixels aggregation. Part\\n  3: a wide range of algorithms</title>\\n    <summary>  In the two previous papers of this serie, we have created a library, called\\nPopulation, dedicated to seeded region growing by pixels aggregation and we\\nhave proposed different growing processes to get a partition with or without a\\nboundary region to divide the other regions or to get a partition invariant\\nabout the seeded region initialisation order. Using this work, we implement\\nsome algorithms belonging to the field of SRGPA using this library and these\\ngrowing processes.\\n</summary>\\n    <author>\\n      <name>Vincent Tariel</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/0806.3928v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0806.3928v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0807.2928v1</id>\\n    <updated>2008-07-18T11:23:27Z</updated>\\n    <published>2008-07-18T11:23:27Z</published>\\n    <title>Visual Grouping by Neural Oscillators</title>\\n    <summary>  Distributed synchronization is known to occur at several scales in the brain,\\nand has been suggested as playing a key functional role in perceptual grouping.\\nState-of-the-art visual grouping algorithms, however, seem to give\\ncomparatively little attention to neural synchronization analogies. Based on\\nthe framework of concurrent synchronization of dynamic systems, simple networks\\nof neural oscillators coupled with diffusive connections are proposed to solve\\nvisual grouping problems. Multi-layer algorithms and feedback mechanisms are\\nalso studied. The same algorithm is shown to achieve promising results on\\nseveral classical visual grouping problems, including point clustering, contour\\nintegration and image segmentation.\\n</summary>\\n    <author>\\n      <name>Guoshen Yu</name>\\n    </author>\\n    <author>\\n      <name>Jean-Jacques Slotine</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/0807.2928v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0807.2928v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0807.4701v1</id>\\n    <updated>2008-07-29T16:28:44Z</updated>\\n    <published>2008-07-29T16:28:44Z</published>\\n    <title>An image processing analysis of skin textures</title>\\n    <summary>  Colour and coarseness of skin are visually different. When image processing\\nis involved in the skin analysis, it is important to quantitatively evaluate\\nsuch differences using texture features. In this paper, we discuss a texture\\nanalysis and measurements based on a statistical approach to the pattern\\nrecognition. Grain size and anisotropy are evaluated with proper diagrams. The\\npossibility to determine the presence of pattern defects is also discussed.\\n</summary>\\n    <author>\\n      <name>A. Sparavigna</name>\\n    </author>\\n    <author>\\n      <name>R. Marazzato</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1111/j.1600-0846.2009.00413.x</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1111/j.1600-0846.2009.00413.x\" rel=\"related\"/>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Skin Research and Technology, Volume 16 Issue 2, Pages 161 - 167,\\n  2010</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/0807.4701v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0807.4701v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0810.3418v1</id>\\n    <updated>2008-10-19T18:04:51Z</updated>\\n    <published>2008-10-19T18:04:51Z</published>\\n    <title>Detecting the Most Unusual Part of a Digital Image</title>\\n    <summary>  The purpose of this paper is to introduce an algorithm that can detect the\\nmost unusual part of a digital image. The most unusual part of a given shape is\\ndefined as a part of the image that has the maximal distance to all non\\nintersecting shapes with the same form.\\n  The method can be used to scan image databases with no clear model of the\\ninteresting part or large image databases, as for example medical databases.\\n</summary>\\n    <author>\\n      <name>K. Koroutchev</name>\\n    </author>\\n    <author>\\n      <name>E. Korutcheva</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/0810.3418v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0810.3418v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.GR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0902.0221v2</id>\\n    <updated>2009-09-14T10:14:32Z</updated>\\n    <published>2009-02-02T10:41:53Z</published>\\n    <title>Over-enhancement Reduction in Local Histogram Equalization using its\\n  Degrees of Freedom</title>\\n    <summary>  A well-known issue of local (adaptive) histogram equalization (LHE) is\\nover-enhancement (i.e., generation of spurious details) in homogenous areas of\\nthe image. In this paper, we show that the LHE problem has many solutions due\\nto the ambiguity in ranking pixels with the same intensity. The LHE solution\\nspace can be searched for the images having the maximum PSNR or structural\\nsimilarity (SSIM) with the input image. As compared to the results of the prior\\nart, these solutions are more similar to the input image while offering the\\nsame local contrast.\\n  Index Terms: histogram modification or specification, contrast enhancement\\n</summary>\\n    <author>\\n      <name>Alireza Avanaki</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/0902.0221v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0902.0221v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.MM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0903.0538v1</id>\\n    <updated>2009-03-03T14:08:24Z</updated>\\n    <published>2009-03-03T14:08:24Z</published>\\n    <title>Real-time Texture Error Detection</title>\\n    <summary>  This paper advocates an improved solution for real-time error detection of\\ntexture errors that occurs in the production process in textile industry. The\\nresearch is focused on the mono-color products with 3D texture model (Jaquard\\nfabrics). This is a more difficult task than, for example, 2D multicolor\\ntextures.\\n</summary>\\n    <author>\\n      <name>Dan Laurentiu Lacrama</name>\\n    </author>\\n    <author>\\n      <name>Florin Alexa</name>\\n    </author>\\n    <author>\\n      <name>Adriana Balta</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, exposed on 2nd \"European conference on Computer Science &amp;\\n  Applications\" - XA2008, Timisoara, Romania</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Ann. Univ. Tibiscus, Comp. Sci. Series 6 (2008), 127-134</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/0903.0538v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0903.0538v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0903.1448v1</id>\\n    <updated>2009-03-09T08:06:09Z</updated>\\n    <published>2009-03-09T08:06:09Z</published>\\n    <title>The Digital Restoration of Da Vinci\\'s Sketches</title>\\n    <summary>  A sketch, found in one of Leonardo da Vinci\\'s notebooks and covered by the\\nwritten notes of this genius, has been recently restored. The restoration\\nreveals a possible self-portrait of the artist, drawn when he was young. Here,\\nwe discuss the discovery of this self-portrait and the procedure used for\\nrestoration. Actually, this is a restoration performed on the digital image of\\nthe sketch, a procedure that can easily extended and applied to ancient\\ndocuments for studies of art and palaeography.\\n</summary>\\n    <author>\\n      <name>Amelia Sparavigna</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/0903.1448v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0903.1448v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.GR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0903.3676v1</id>\\n    <updated>2009-03-23T17:18:08Z</updated>\\n    <published>2009-03-23T17:18:08Z</published>\\n    <title>Combinatorial Ricci Curvature and Laplacians for Image Processing</title>\\n    <summary>  A new Combinatorial Ricci curvature and Laplacian operators for grayscale\\nimages are introduced and tested on 2D synthetic, natural and medical images.\\nAnalogue formulae for voxels are also obtained. These notions are based upon\\nmore general concepts developed by R. Forman. Further applications, in\\nparticular a fitting Ricci flow, are discussed.\\n</summary>\\n    <author>\\n      <name>Emil Saucan</name>\\n    </author>\\n    <author>\\n      <name>Eli Appleboilm</name>\\n    </author>\\n    <author>\\n      <name>Gershon Wolansky</name>\\n    </author>\\n    <author>\\n      <name>Yehoshua Y. Zeevi</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">12 pages, 8 figures (some of the these may be of lesser quality than\\n  those in the Technical report version)</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proceedings of CISP\\'09, Vol. 2, 992-997, 2009</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/0903.3676v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0903.3676v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0903.5045v1</id>\\n    <updated>2009-03-30T06:00:15Z</updated>\\n    <published>2009-03-30T06:00:15Z</published>\\n    <title>Digital Restoration of Ancient Papyri</title>\\n    <summary>  Image processing can be used for digital restoration of ancient papyri, that\\nis, for a restoration performed on their digital images. The digital\\nmanipulation allows reducing the background signals and enhancing the\\nreadability of texts. In the case of very old and damaged documents, this is\\nfundamental for identification of the patterns of letters. Some examples of\\nrestoration, obtained with an image processing which uses edges detection and\\nFourier filtering, are shown. One of them concerns 7Q5 fragment of the Dead Sea\\nScrolls.\\n</summary>\\n    <author>\\n      <name>Amelia Sparavigna</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/0903.5045v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0903.5045v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0904.0962v1</id>\\n    <updated>2009-04-06T16:25:08Z</updated>\\n    <published>2009-04-06T16:25:08Z</published>\\n    <title>Color Dipole Moments for Edge Detection</title>\\n    <summary>  Dipole and higher moments are physical quantities used to describe a charge\\ndistribution. In analogy with electromagnetism, it is possible to define the\\ndipole moments for a gray-scale image, according to the single aspect of a\\ngray-tone map. In this paper we define the color dipole moments for color\\nimages. For color maps in fact, we have three aspects, the three primary\\ncolors, to consider. Associating three color charges to each pixel, color\\ndipole moments can be easily defined and used for edge detection.\\n</summary>\\n    <author>\\n      <name>Amelia Sparavigna</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/0904.0962v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0904.0962v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0904.1613v1</id>\\n    <updated>2009-04-09T22:15:25Z</updated>\\n    <published>2009-04-09T22:15:25Z</published>\\n    <title>On the closed-form solution of the rotation matrix arising in computer\\n  vision problems</title>\\n    <summary>  We show the closed-form solution to the maximization of trace(A\\'R), where A\\nis given and R is unknown rotation matrix. This problem occurs in many computer\\nvision tasks involving optimal rotation matrix estimation. The solution has\\nbeen continuously reinvented in different fields as part of specific problems.\\nWe summarize the historical evolution of the problem and present the general\\nproof of the solution. We contribute to the proof by considering the degenerate\\ncases of A and discuss the uniqueness of R.\\n</summary>\\n    <author>\\n      <name>Andriy Myronenko</name>\\n    </author>\\n    <author>\\n      <name>Xubo Song</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/0904.1613v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0904.1613v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0904.3944v1</id>\\n    <updated>2009-04-24T21:19:59Z</updated>\\n    <published>2009-04-24T21:19:59Z</published>\\n    <title>Better Global Polynomial Approximation for Image Rectification</title>\\n    <summary>  When using images to locate objects, there is the problem of correcting for\\ndistortion and misalignment in the images. An elegant way of solving this\\nproblem is to generate an error correcting function that maps points in an\\nimage to their corrected locations. We generate such a function by fitting a\\npolynomial to a set of sample points. The objective is to identify a polynomial\\nthat passes \"sufficiently close\" to these points with \"good\" approximation of\\nintermediate points. In the past, it has been difficult to achieve good global\\npolynomial approximation using only sample points. We report on the development\\nof a global polynomial approximation algorithm for solving this problem. Key\\nWords: Polynomial approximation, interpolation, image rectification.\\n</summary>\\n    <author>\\n      <name>Christopher O. Ward</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.2316/Journal.205.2008.3.205-4669</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.2316/Journal.205.2008.3.205-4669\" rel=\"related\"/>\\n    <link href=\"http://arxiv.org/abs/0904.3944v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0904.3944v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0905.3347v1</id>\\n    <updated>2009-05-20T16:37:16Z</updated>\\n    <published>2009-05-20T16:37:16Z</published>\\n    <title>Information Distance in Multiples</title>\\n    <summary>  Information distance is a parameter-free similarity measure based on\\ncompression, used in pattern recognition, data mining, phylogeny, clustering,\\nand classification. The notion of information distance is extended from pairs\\nto multiples (finite lists). We study maximal overlap, metricity, universality,\\nminimal overlap, additivity, and normalized information distance in multiples.\\nWe use the theoretical notion of Kolmogorov complexity which for practical\\npurposes is approximated by the length of the compressed version of the file\\ninvolved, using a real-world compression program.\\n  {\\\\em Index Terms}-- Information distance, multiples, pattern recognition,\\ndata mining, similarity, Kolmogorov complexity\\n</summary>\\n    <author>\\n      <name>Paul M. B. Vitanyi</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">LateX 14 pages, Submitted to a technical journal</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/0905.3347v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0905.3347v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"J.3; E.4\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0906.3323v1</id>\\n    <updated>2009-06-17T23:24:38Z</updated>\\n    <published>2009-06-17T23:24:38Z</published>\\n    <title>Adaptive Regularization of Ill-Posed Problems: Application to Non-rigid\\n  Image Registration</title>\\n    <summary>  We introduce an adaptive regularization approach. In contrast to conventional\\nTikhonov regularization, which specifies a fixed regularization operator, we\\nestimate it simultaneously with parameters. From a Bayesian perspective we\\nestimate the prior distribution on parameters assuming that it is close to some\\ngiven model distribution. We constrain the prior distribution to be a\\nGauss-Markov random field (GMRF), which allows us to solve for the prior\\ndistribution analytically and provides a fast optimization algorithm. We apply\\nour approach to non-rigid image registration to estimate the spatial\\ntransformation between two images. Our evaluation shows that the adaptive\\nregularization approach significantly outperforms standard variational methods.\\n</summary>\\n    <author>\\n      <name>Andriy Myronenko</name>\\n    </author>\\n    <author>\\n      <name>Xubo Song</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/0906.3323v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0906.3323v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0907.0288v1</id>\\n    <updated>2009-07-02T04:57:32Z</updated>\\n    <published>2009-07-02T04:57:32Z</published>\\n    <title>An Iterative Fingerprint Enhancement Algorithm Based on Accurate\\n  Determination of Orientation Flow</title>\\n    <summary>  We describe an algorithm to enhance and binarize a fingerprint image. The\\nalgorithm is based on accurate determination of orientation flow of the ridges\\nof the fingerprint image by computing variance of the neighborhood pixels\\naround a pixel in different directions. We show that an iterative algorithm\\nwhich captures the mutual interdependence of orientation flow computation,\\nenhancement and binarization gives very good results on poor quality images.\\n</summary>\\n    <author>\\n      <name>Simant Dube</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages, 4 figures. Ongoing work. To be submitted to appropriate\\n  conference/journal</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/0907.0288v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0907.0288v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0907.5321v2</id>\\n    <updated>2009-08-04T06:01:53Z</updated>\\n    <published>2009-07-30T12:23:25Z</published>\\n    <title>Multiple pattern classification by sparse subspace decomposition</title>\\n    <summary>  A robust classification method is developed on the basis of sparse subspace\\ndecomposition. This method tries to decompose a mixture of subspaces of\\nunlabeled data (queries) into class subspaces as few as possible. Each query is\\nclassified into the class whose subspace significantly contributes to the\\ndecomposed subspace. Multiple queries from different classes can be\\nsimultaneously classified into their respective classes. A practical greedy\\nalgorithm of the sparse subspace decomposition is designed for the\\nclassification. The present method achieves high recognition rate and robust\\nperformance exploiting joint sparsity.\\n</summary>\\n    <author>\\n      <name>Tomoya Sakai</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/ICCVW.2009.5457702</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/ICCVW.2009.5457702\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, 3 figures, 2nd IEEE International Workshop on Subspace\\n  Methods, Workshop Proceedings of ICCV 2009</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/0907.5321v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0907.5321v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0909.5656v1</id>\\n    <updated>2009-09-30T18:50:24Z</updated>\\n    <published>2009-09-30T18:50:24Z</published>\\n    <title>Improvements of the 3D images captured with Time-of-Flight cameras</title>\\n    <summary>  3D Time-of-Flight camera\\'s images are affected by errors due to the diffuse\\n(indirect) light and to the flare light. The presented method improves the 3D\\nimage reducing the distance\\'s errors to dark surface objects. This is achieved\\nby placing one or two contrast tags in the scene at different distances from\\nthe ToF camera. The white and black parts of the tags are situated at the same\\ndistance to the camera but the distances measured by the camera are different.\\nThis difference is used to compute a correction vector. The distance to black\\nsurfaces is corrected by subtracting this vector from the captured vector\\nimage.\\n</summary>\\n    <author>\\n      <name>D. Falie</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/0909.5656v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0909.5656v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0910.1293v1</id>\\n    <updated>2009-10-07T15:42:03Z</updated>\\n    <published>2009-10-07T15:42:03Z</published>\\n    <title>Introducing New AdaBoost Features for Real-Time Vehicle Detection</title>\\n    <summary>  This paper shows how to improve the real-time object detection in complex\\nrobotics applications, by exploring new visual features as AdaBoost weak\\nclassifiers. These new features are symmetric Haar filters (enforcing global\\nhorizontal and vertical symmetry) and N-connexity control points. Experimental\\nevaluation on a car database show that the latter appear to provide the best\\nresults for the vehicle-detection problem.\\n</summary>\\n    <author>\\n      <name>Bogdan Stanciulescu</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">CAOR</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Amaury Breheret</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">CAOR</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Fabien Moutarde</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">CAOR</arxiv:affiliation>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">COGIS\\'07 conference on COGnitive systems with Interactive Sensors,\\n  Stanford, Palo Alto : United States (2007)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/0910.1293v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0910.1293v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0911.0490v1</id>\\n    <updated>2009-11-03T04:33:55Z</updated>\\n    <published>2009-11-03T04:33:55Z</published>\\n    <title>Breast Cancer Detection Using Multilevel Thresholding</title>\\n    <summary>  This paper presents an algorithm which aims to assist the radiologist in\\nidentifying breast cancer at its earlier stages. It combines several image\\nprocessing techniques like image negative, thresholding and segmentation\\ntechniques for detection of tumor in mammograms. The algorithm is verified by\\nusing mammograms from Mammographic Image Analysis Society. The results obtained\\nby applying these techniques are described.\\n</summary>\\n    <author>\\n      <name>Y. Ireaneus Anna Rejani</name>\\n    </author>\\n    <author>\\n      <name>S. Thamarai Selvi</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages IEEE format, International Journal of Computer Science and\\n  Information Security, IJCSIS 2009, ISSN 1947 5500, Impact Factor 0.423,\\n  http://sites.google.com/site/ijcsis/</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">International Journal of Computer Science and Information\\n  Security, IJCSIS, Vol. 6, No. 1, pp. 111-115, October 2009, USA</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/0911.0490v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0911.0490v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0911.4874v2</id>\\n    <updated>2009-12-04T12:58:23Z</updated>\\n    <published>2009-11-25T15:16:53Z</published>\\n    <title>Non-photorealistic image processing: an Impressionist rendering</title>\\n    <summary>  The paper describes an image processing for a non-photorealistic rendering.\\nThe algorithm is based on a random choice of a set of pixels from those ot the\\noriginal image and substitution of them with colour spots. An iterative\\nprocedure is applied to cover, at a desired level, the canvas. The resulting\\neffect mimics the impressionist painting and Pointillism.\\n</summary>\\n    <author>\\n      <name>Amelia Carolina Sparavigna</name>\\n    </author>\\n    <author>\\n      <name>Roberto Marazzato</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Keywords: Image processing. Non-photorealistic processing.\\n  Image-based rendering</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/0911.4874v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0911.4874v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1001.0927v1</id>\\n    <updated>2010-01-06T16:05:25Z</updated>\\n    <published>2010-01-06T16:05:25Z</published>\\n    <title>Accelerating Competitive Learning Graph Quantization</title>\\n    <summary>  Vector quantization(VQ) is a lossy data compression technique from signal\\nprocessing for which simple competitive learning is one standard method to\\nquantize patterns from the input space. Extending competitive learning VQ to\\nthe domain of graphs results in competitive learning for quantizing input\\ngraphs. In this contribution, we propose an accelerated version of competitive\\nlearning graph quantization (GQ) without trading computational time against\\nsolution quality. For this, we lift graphs locally to vectors in order to avoid\\nunnecessary calculations of intractable graph distances. In doing so, the\\naccelerated version of competitive learning GQ gradually turns locally into a\\ncompetitive learning VQ with increasing number of iterations. Empirical results\\nshow a significant speedup by maintaining a comparable solution quality.\\n</summary>\\n    <author>\\n      <name>Brijnesh J. Jain</name>\\n    </author>\\n    <author>\\n      <name>Klaus Obermayer</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">17 pages; submitted to CVIU</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1001.0927v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1001.0927v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1001.5352v1</id>\\n    <updated>2010-01-29T08:29:57Z</updated>\\n    <published>2010-01-29T08:29:57Z</published>\\n    <title>Kannada Character Recognition System A Review</title>\\n    <summary>  Intensive research has been done on optical character recognition ocr and a\\nlarge number of articles have been published on this topic during the last few\\ndecades. Many commercial OCR systems are now available in the market, but most\\nof these systems work for Roman, Chinese, Japanese and Arabic characters. There\\nare no sufficient number of works on Indian language character recognition\\nespecially Kannada script among 12 major scripts in India. This paper presents\\na review of existing work on printed Kannada script and their results. The\\ncharacteristics of Kannada script and Kannada Character Recognition System kcr\\nare discussed in detail. Finally fusion at the classifier level is proposed to\\nincrease the recognition accuracy.\\n</summary>\\n    <author>\\n      <name>K. Indira</name>\\n    </author>\\n    <author>\\n      <name>S. Sethu Selvi</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">12 pages, 8 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1001.5352v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1001.5352v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1002.0416v1</id>\\n    <updated>2010-02-02T08:15:20Z</updated>\\n    <published>2010-02-02T08:15:20Z</published>\\n    <title>Fusion of Multiple Matchers using SVM for Offline Signature\\n  Identification</title>\\n    <summary>  This paper uses Support Vector Machines (SVM) to fuse multiple classifiers\\nfor an offline signature system. From the signature images, global and local\\nfeatures are extracted and the signatures are verified with the help of\\nGaussian empirical rule, Euclidean and Mahalanobis distance based classifiers.\\nSVM is used to fuse matching scores of these matchers. Finally, recognition of\\nquery signatures is done by comparing it with all signatures of the database.\\nThe proposed system is tested on a signature database contains 5400 offline\\nsignatures of 600 individuals and the results are found to be promising.\\n</summary>\\n    <author>\\n      <name>Dakshina Ranjan Kisku</name>\\n    </author>\\n    <author>\\n      <name>Phalguni Gupta</name>\\n    </author>\\n    <author>\\n      <name>Jamuna Kanta Sing</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, 2 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1002.0416v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1002.0416v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"D.2.2; I.2.10\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1002.3344v1</id>\\n    <updated>2010-02-17T18:29:09Z</updated>\\n    <published>2010-02-17T18:29:09Z</published>\\n    <title>Iterative exact global histogram specification and SSIM gradient ascent:\\n  a proof of convergence, step size and parameter selection</title>\\n    <summary>  The SSIM-optimized exact global histogram specification (EGHS) is shown to\\nconverge in the sense that the first order approximation of the result\\'s\\nquality (i.e., its structural similarity with input) does not decrease in an\\niteration, when the step size is small. Each iteration is composed of SSIM\\ngradient ascent and basic EGHS with the specified target histogram. Selection\\nof step size and other parameters is also discussed.\\n</summary>\\n    <author>\\n      <name>Alireza Avanaki</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Supplement to published work, on SSIM-optimized exact global\\n  histogram specification; please see arXiv:0901.0065</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1002.3344v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1002.3344v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.MM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1002.4317v1</id>\\n    <updated>2010-02-23T12:32:34Z</updated>\\n    <published>2010-02-23T12:32:34Z</published>\\n    <title>CLD-shaped Brushstrokes in Non-Photorealistic Rendering</title>\\n    <summary>  Rendering techniques based on a random grid can be improved by adapting\\nbrushstrokes to the shape of different areas of the original picture. In this\\npaper, the concept of Coherence Length Diagram is applied to determine the\\nadaptive brushstrokes, in order to simulate an impressionist painting. Some\\nexamples are provided to instance the proposed algorithm.\\n</summary>\\n    <author>\\n      <name>Amelia Carolina Sparavigna</name>\\n    </author>\\n    <author>\\n      <name>Roberto Marazzato</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Keywords: Image processing, Non-photorealistic processing,\\n  Image-based rendering Coherence Length Diagram</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">International Journal of Software Engineering and Computing, 2011,\\n  Volume 3, Issue 1, Pages 11-15</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1002.4317v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1002.4317v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.GR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1003.4021v1</id>\\n    <updated>2010-03-21T20:21:09Z</updated>\\n    <published>2010-03-21T20:21:09Z</published>\\n    <title>System-theoretic approach to image interest point detection</title>\\n    <summary>  Interest point detection is a common task in various computer vision\\napplications. Although a big variety of detector are developed so far\\ncomputational efficiency of interest point based image analysis remains to be\\nthe problem. Current paper proposes a system-theoretic approach to interest\\npoint detection. Starting from the analysis of interdependency between detector\\nand descriptor it is shown that given a descriptor it is possible to introduce\\nto notion of detector redundancy. Furthermore for each detector it is possible\\nto construct its irredundant and equivalent modification. Modified detector\\npossesses lower computational complexity and is preferable. It is also shown\\nthat several known approaches to reduce computational complexity of image\\nregistration can be generalized in terms of proposed theory.\\n</summary>\\n    <author>\\n      <name>Vitaly Pimenov</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">12 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1003.4021v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1003.4021v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1003.4053v1</id>\\n    <updated>2010-03-22T03:39:46Z</updated>\\n    <published>2010-03-22T03:39:46Z</published>\\n    <title>A Comprehensive Review of Image Enhancement Techniques</title>\\n    <summary>  Principle objective of Image enhancement is to process an image so that\\nresult is more suitable than original image for specific application. Digital\\nimage enhancement techniques provide a multitude of choices for improving the\\nvisual quality of images. Appropriate choice of such techniques is greatly\\ninfluenced by the imaging modality, task at hand and viewing conditions. This\\npaper will provide an overview of underlying concepts, along with algorithms\\ncommonly used for image enhancement. The paper focuses on spatial domain\\ntechniques for image enhancement, with particular reference to point processing\\nmethods and histogram processing.\\n</summary>\\n    <author>\\n      <name>Raman Maini</name>\\n    </author>\\n    <author>\\n      <name>Himanshu Aggarwal</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Journal of Computing, Volume 2, Issue 3, March 2010,\\n  https://sites.google.com/site/journalofcomputing/</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1003.4053v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1003.4053v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1003.5865v1</id>\\n    <updated>2010-03-30T16:36:36Z</updated>\\n    <published>2010-03-30T16:36:36Z</published>\\n    <title>Offline Signature Identification by Fusion of Multiple Classifiers using\\n  Statistical Learning Theory</title>\\n    <summary>  This paper uses Support Vector Machines (SVM) to fuse multiple classifiers\\nfor an offline signature system. From the signature images, global and local\\nfeatures are extracted and the signatures are verified with the help of\\nGaussian empirical rule, Euclidean and Mahalanobis distance based classifiers.\\nSVM is used to fuse matching scores of these matchers. Finally, recognition of\\nquery signatures is done by comparing it with all signatures of the database.\\nThe proposed system is tested on a signature database contains 5400 offline\\nsignatures of 600 individuals and the results are found to be promising.\\n</summary>\\n    <author>\\n      <name>Dakshina Ranjan Kisku</name>\\n    </author>\\n    <author>\\n      <name>Phalguni Gupta</name>\\n    </author>\\n    <author>\\n      <name>Jamuna Kanta Sing</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">11 pages, 3 figures, IJSIA 2010</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1003.5865v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1003.5865v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"D.2.2; I.2.10\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1003.5891v1</id>\\n    <updated>2010-03-30T18:35:37Z</updated>\\n    <published>2010-03-30T18:35:37Z</published>\\n    <title>Recognition of Handwritten Roman Script Using Tesseract Open source OCR\\n  Engine</title>\\n    <summary>  In the present work, we have used Tesseract 2.01 open source Optical\\nCharacter Recognition (OCR) Engine under Apache License 2.0 for recognition of\\nhandwriting samples of lower case Roman script. Handwritten isolated and\\nfree-flow text samples were collected from multiple users. Tesseract is trained\\nto recognize user-specific handwriting samples of both the categories of\\ndocument pages. On a single user model, the system is trained with 1844\\nisolated handwritten characters and the performance is tested on 1133\\ncharacters, taken form the test set. The overall character-level accuracy of\\nthe system is observed as 83.5%. The system fails to segment 5.56% characters\\nand erroneously classifies 10.94% characters.\\n</summary>\\n    <author>\\n      <name>Sandip Rakshit</name>\\n    </author>\\n    <author>\\n      <name>Subhadip Basu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proc. National Conference on NAQC (2008) 141-145</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1003.5891v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1003.5891v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1004.1227v1</id>\\n    <updated>2010-04-08T02:39:49Z</updated>\\n    <published>2010-04-08T02:39:49Z</published>\\n    <title>Signature Recognition using Multi Scale Fourier Descriptor And Wavelet\\n  Transform</title>\\n    <summary>  This paper present a novel off-line signature recognition method based on\\nmulti scale Fourier Descriptor and wavelet transform . The main steps of\\nconstructing a signature recognition system are discussed and experiments on\\nreal data sets show that the average error rate can reach 1%. Finally we\\ncompare 8 distance measures between feature vectors with respect to the\\nrecognition performance.\\n  Key words: signature recognition; Fourier Descriptor; Wavelet transform;\\npersonal verification\\n</summary>\\n    <author>\\n      <name>Ismail A. Ismail</name>\\n    </author>\\n    <author>\\n      <name>Mohammed A. Ramadan</name>\\n    </author>\\n    <author>\\n      <name>Talaat S. El danaf</name>\\n    </author>\\n    <author>\\n      <name>Ahmed H. Samak</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IEEE Publication format, ISSN 1947 5500,\\n  http://sites.google.com/site/ijcsis/</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IJCSIS, Vol. 7 No. 3, March 2010,</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1004.1227v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1004.1227v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1004.5351v2</id>\\n    <updated>2010-05-11T19:04:22Z</updated>\\n    <published>2010-04-29T17:56:47Z</published>\\n    <title>Isometric Embeddings in Imaging and Vision: Facts and Fiction</title>\\n    <summary>  We explore the practicability of Nash\\'s Embedding Theorem in vision and\\nimaging sciences. In particular, we investigate the relevance of a result of\\nBurago and Zalgaller regarding the existence of isometric embeddings of\\npolyhedral surfaces in $\\\\mathbb{R}^3$ and we show that their proof does not\\nextended directly to higher dimensions.\\n</summary>\\n    <author>\\n      <name>Emil Saucan</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">23 pages, 1 figure Second version: Corrections made, subsection added</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1004.5351v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1004.5351v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.DG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"52B70, 57R40, 53C42, 30C65\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1005.1471v1</id>\\n    <updated>2010-05-10T08:49:56Z</updated>\\n    <published>2010-05-10T08:49:56Z</published>\\n    <title>Classification via Incoherent Subspaces</title>\\n    <summary>  This article presents a new classification framework that can extract\\nindividual features per class. The scheme is based on a model of incoherent\\nsubspaces, each one associated to one class, and a model on how the elements in\\na class are represented in this subspace. After the theoretical analysis an\\nalternate projection algorithm to find such a collection is developed. The\\nclassification performance and speed of the proposed method is tested on the AR\\nand YaleB databases and compared to that of Fisher\\'s LDA and a recent approach\\nbased on on $\\\\ell_1$ minimisation. Finally connections of the presented scheme\\nto already existing work are discussed and possible ways of extensions are\\npointed out.\\n</summary>\\n    <author>\\n      <name>Karin Schnass</name>\\n    </author>\\n    <author>\\n      <name>Pierre Vandergheynst</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">22 pages, 2 figures, 4 tables</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1005.1471v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1005.1471v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1005.2715v1</id>\\n    <updated>2010-05-16T00:31:19Z</updated>\\n    <published>2010-05-16T00:31:19Z</published>\\n    <title>On the Subspace of Image Gradient Orientations</title>\\n    <summary>  We introduce the notion of Principal Component Analysis (PCA) of image\\ngradient orientations. As image data is typically noisy, but noise is\\nsubstantially different from Gaussian, traditional PCA of pixel intensities\\nvery often fails to estimate reliably the low-dimensional subspace of a given\\ndata population. We show that replacing intensities with gradient orientations\\nand the $\\\\ell_2$ norm with a cosine-based distance measure offers, to some\\nextend, a remedy to this problem. Our scheme requires the eigen-decomposition\\nof a covariance matrix and is as computationally efficient as standard $\\\\ell_2$\\nPCA. We demonstrate some of its favorable properties on robust subspace\\nestimation.\\n</summary>\\n    <author>\\n      <name>Georgios Tzimiropoulos</name>\\n    </author>\\n    <author>\\n      <name>Stefanos Zafeiriou</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1005.2715v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1005.2715v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1005.4020v1</id>\\n    <updated>2010-05-21T17:30:08Z</updated>\\n    <published>2010-05-21T17:30:08Z</published>\\n    <title>Image Segmentation by Using Threshold Techniques</title>\\n    <summary>  This paper attempts to undertake the study of segmentation image techniques\\nby using five threshold methods as Mean method, P-tile method, Histogram\\nDependent Technique (HDT), Edge Maximization Technique (EMT) and visual\\nTechnique and they are compared with one another so as to choose the best\\ntechnique for threshold segmentation techniques image. These techniques applied\\non three satellite images to choose base guesses for threshold segmentation\\nimage.\\n</summary>\\n    <author>\\n      <name>Salem Saleh Al-amri</name>\\n    </author>\\n    <author>\\n      <name>N. V. Kalyankar</name>\\n    </author>\\n    <author>\\n      <name>Khamitkar S. D.</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">http://www.journalofcomputing.org</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Journal of Computing, Volume 2, Issue 5, May 2010</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1005.4020v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1005.4020v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1005.4034v1</id>\\n    <updated>2010-05-21T18:03:44Z</updated>\\n    <published>2010-05-21T18:03:44Z</published>\\n    <title>Face Synthesis (FASY) System for Generation of a Face Image from Human\\n  Description</title>\\n    <summary>  This paper aims at generating a new face based on the human like description\\nusing a new concept. The FASY (FAce SYnthesis) System is a Face Database\\nRetrieval and new Face generation System that is under development. One of its\\nmain features is the generation of the requested face when it is not found in\\nthe existing database, which allows a continuous growing of the database also.\\n</summary>\\n    <author>\\n      <name>Santanu Halder</name>\\n    </author>\\n    <author>\\n      <name>Debotosh Bhattacharjee</name>\\n    </author>\\n    <author>\\n      <name>Mita Nasipuri</name>\\n    </author>\\n    <author>\\n      <name>Dipak Kumar Basu</name>\\n    </author>\\n    <author>\\n      <name>Mahantapas Kundu</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">ICIIS 2008</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1005.4034v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1005.4034v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1006.4175v1</id>\\n    <updated>2010-06-21T20:59:43Z</updated>\\n    <published>2010-06-21T20:59:43Z</published>\\n    <title>Optimization of Weighted Curvature for Image Segmentation</title>\\n    <summary>  Minimization of boundary curvature is a classic regularization technique for\\nimage segmentation in the presence of noisy image data. Techniques for\\nminimizing curvature have historically been derived from descent methods which\\ncould be trapped in a local minimum and therefore required a good\\ninitialization. Recently, combinatorial optimization techniques have been\\napplied to the optimization of curvature which provide a solution that achieves\\nnearly a global optimum. However, when applied to image segmentation these\\nmethods required a meaningful data term. Unfortunately, for many images,\\nparticularly medical images, it is difficult to find a meaningful data term.\\nTherefore, we propose to remove the data term completely and instead weight the\\ncurvature locally, while still achieving a global optimum.\\n</summary>\\n    <author>\\n      <name>Noha El-Zehiry</name>\\n    </author>\\n    <author>\\n      <name>Leo Grady</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">15 pages , 6 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1006.4175v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1006.4175v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1006.4910v5</id>\\n    <updated>2022-12-05T10:44:47Z</updated>\\n    <published>2010-06-25T04:51:32Z</published>\\n    <title>Kalman Filters and Homography: Utilizing the Matrix $A$</title>\\n    <summary>  Many problems in Computer Vision can be reduced to either working around a\\nknown transform, or given a model for the transform computing the inverse\\nproblem of the transform itself. We will look at two ways of working with the\\nmatrix $A$ and see how transforms are at the root of image processing and\\nvision problems.\\n</summary>\\n    <author>\\n      <name>Burak Bayramli</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Typos fixed</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1006.4910v5\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1006.4910v5\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1006.5920v1</id>\\n    <updated>2010-06-30T16:54:43Z</updated>\\n    <published>2010-06-30T16:54:43Z</published>\\n    <title>A Two Stage Classification Approach for Handwritten Devanagari\\n  Characters</title>\\n    <summary>  The paper presents a two stage classification approach for handwritten\\ndevanagari characters The first stage is using structural properties like\\nshirorekha, spine in character and second stage exploits some intersection\\nfeatures of characters which are fed to a feedforward neural network. Simple\\nhistogram based method does not work for finding shirorekha, vertical bar\\n(Spine) in handwritten devnagari characters. So we designed a differential\\ndistance based technique to find a near straight line for shirorekha and spine.\\nThis approach has been tested for 50000 samples and we got 89.12% success\\n</summary>\\n    <author>\\n      <name>Sandhya Arora</name>\\n    </author>\\n    <author>\\n      <name>Debotosh Bhattacharjee</name>\\n    </author>\\n    <author>\\n      <name>Mita Nasipuri</name>\\n    </author>\\n    <author>\\n      <name>Latesh Malik</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">ICCIMA 2007</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1006.5920v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1006.5920v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1006.5924v1</id>\\n    <updated>2010-06-30T17:09:39Z</updated>\\n    <published>2010-06-30T17:09:39Z</published>\\n    <title>A novel approach for handwritten Devnagari character recognition</title>\\n    <summary>  In this paper a method for recognition of handwritten devanagari characters\\nis described. Here, feature vector is constituted by accumulated directional\\ngradient changes in different segments, number of intersections points for the\\ncharacter, type of spine present and type of shirorekha present in the\\ncharacter. One Multi-layer Perceptron with conjugate-gradient training is used\\nto classify these feature vectors. This method is applied to a database with\\n1000 sample characters and the recognition rate obtained is 88.12%\\n</summary>\\n    <author>\\n      <name>Sandhya Arora</name>\\n    </author>\\n    <author>\\n      <name>Latesh Malik</name>\\n    </author>\\n    <author>\\n      <name>Debotosh Bhattacharjee</name>\\n    </author>\\n    <author>\\n      <name>Mita Nasipuri</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">ICSIP 2006</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1006.5924v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1006.5924v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1006.5942v1</id>\\n    <updated>2010-06-30T18:01:47Z</updated>\\n    <published>2010-06-30T18:01:47Z</published>\\n    <title>FPGA Based Assembling of Facial Components for Human Face Construction</title>\\n    <summary>  This paper aims at VLSI realization for generation of a new face from textual\\ndescription. The FASY (FAce SYnthesis) System is a Face Database Retrieval and\\nnew Face generation System that is under development. One of its main features\\nis the generation of the requested face when it is not found in the existing\\ndatabase. The new face generation system works in three steps - searching\\nphase, assembling phase and tuning phase. In this paper the tuning phase using\\nhardware description language and its implementation in a Field Programmable\\nGate Array (FPGA) device is presented.\\n</summary>\\n    <author>\\n      <name>Santanu Halder</name>\\n    </author>\\n    <author>\\n      <name>Debotosh Bhattacharjee</name>\\n    </author>\\n    <author>\\n      <name>Mita Nasipuri</name>\\n    </author>\\n    <author>\\n      <name>Dipak Kumar Basu</name>\\n    </author>\\n    <author>\\n      <name>Mahantapas Kundu</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IJRTE 1(1):541-545(2009)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1006.5942v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1006.5942v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1007.0547v1</id>\\n    <updated>2010-07-04T12:19:16Z</updated>\\n    <published>2010-07-04T12:19:16Z</published>\\n    <title>A Fast Decision Technique for Hierarchical Hough Transform for Line\\n  Detection</title>\\n    <summary>  Many techniques have been proposed to speedup the performance of classic\\nHough Transform. These techniques are primarily based on converting the voting\\nprocedure to a hierarchy based voting method. These methods use approximate\\ndecision-making process. In this paper, we propose a fast decision making\\nprocess that enhances the speed and reduces the space requirements.\\nExperimental results demonstrate that the proposed algorithm is much faster\\nthan a similar Fast Hough Transform.\\n</summary>\\n    <author>\\n      <name>Chandan Singh</name>\\n    </author>\\n    <author>\\n      <name>Nitin Bhatia</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages, published at IEEE conference on Signal and Image Processing\\n  - 2006</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1007.0547v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1007.0547v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1007.0626v1</id>\\n    <updated>2010-07-05T07:33:45Z</updated>\\n    <published>2010-07-05T07:33:45Z</published>\\n    <title>Fusion of Wavelet Coefficients from Visual and Thermal Face Images for\\n  Human Face Recognition - A Comparative Study</title>\\n    <summary>  In this paper we present a comparative study on fusion of visual and thermal\\nimages using different wavelet transformations. Here, coefficients of discrete\\nwavelet transforms from both visual and thermal images are computed separately\\nand combined. Next, inverse discrete wavelet transformation is taken in order\\nto obtain fused face image. Both Haar and Daubechies (db2) wavelet transforms\\nhave been used to compare recognition results. For experiments IRIS\\nThermal/Visual Face Database was used. Experimental results using Haar and\\nDaubechies wavelets show that the performance of the approach presented here\\nachieves maximum success rate of 100% in many cases.\\n</summary>\\n    <author>\\n      <name>M. K. Bhowmik</name>\\n    </author>\\n    <author>\\n      <name>Debotosh Bhattacharjee</name>\\n    </author>\\n    <author>\\n      <name>M. Nasipuri</name>\\n    </author>\\n    <author>\\n      <name>D. K. Basu</name>\\n    </author>\\n    <author>\\n      <name>M. Kundu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1007.0626v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1007.0626v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1007.3881v3</id>\\n    <updated>2021-08-18T21:58:15Z</updated>\\n    <published>2010-07-22T13:22:50Z</published>\\n    <title>Orthogonal multifilters image processing of astronomical images from\\n  scanned photographic plates</title>\\n    <summary>  In this paper orthogonal multifilters for astronomical image processing are\\npresented. We obtained new orthogonal multifilters based on the orthogonal\\nwavelet of Haar and Daubechies. Recently, multiwavelets have been introduced as\\na more powerful multiscale analysis tool. It adds several degrees of freedom in\\nmultifilter design and makes it possible to have several useful properties such\\nas symmetry, orthogonality, short support, and a higher number of vanishing\\nmoments simultaneously. Multifilter decomposition of scanned photographic\\nplates with astronomical images is made.\\n</summary>\\n    <author>\\n      <name>Vasil Kolev</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages, The ACM proceedings of CompSysTech 2010</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1007.3881v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1007.3881v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NA\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"42Cxx, 65Txx, 68Uxx, 94Axx, 85-XX, 97Mxx, 94Axx,\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"D.2; E.4; G.1; I.4; I.5\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1008.3346v1</id>\\n    <updated>2010-08-19T16:38:35Z</updated>\\n    <published>2010-08-19T16:38:35Z</published>\\n    <title>A Miniature-Based Image Retrieval System</title>\\n    <summary>  Due to the rapid development of World Wide Web (WWW) and imaging technology,\\nmore and more images are available in the Internet and stored in databases.\\nSearching the related images by the querying image is becoming tedious and\\ndifficult. Most of the images on the web are compressed by methods based on\\ndiscrete cosine transform (DCT) including Joint Photographic Experts\\nGroup(JPEG) and H.261. This paper presents an efficient content-based image\\nindexing technique for searching similar images using discrete cosine transform\\nfeatures. Experimental results demonstrate its superiority with the existing\\ntechniques.\\n</summary>\\n    <author>\\n      <name>Md. Saiful Islam</name>\\n    </author>\\n    <author>\\n      <name>Md. Haider Ali</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages, 4 figures, 4 tables</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Dhaka University Journal of Science,Vol. 57, No. 2, pp. 187-191,\\n  July 2009</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1008.3346v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1008.3346v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1009.0854v1</id>\\n    <updated>2010-09-04T17:44:06Z</updated>\\n    <published>2010-09-04T17:44:06Z</published>\\n    <title>Fast Color Space Transformations Using Minimax Approximations</title>\\n    <summary>  Color space transformations are frequently used in image processing,\\ngraphics, and visualization applications. In many cases, these transformations\\nare complex nonlinear functions, which prohibits their use in time-critical\\napplications. In this paper, we present a new approach called Minimax\\nApproximations for Color-space Transformations (MACT).We demonstrate MACT on\\nthree commonly used color space transformations. Extensive experiments on a\\nlarge and diverse image set and comparisons with well-known multidimensional\\nlookup table interpolation methods show that MACT achieves an excellent balance\\namong four criteria: ease of implementation, memory usage, accuracy, and\\ncomputational speed.\\n</summary>\\n    <author>\\n      <name>M. Emre Celebi</name>\\n    </author>\\n    <author>\\n      <name>Hassan Kingravi</name>\\n    </author>\\n    <author>\\n      <name>Fatih Celiker</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1049/iet-ipr.2008.0172</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1049/iet-ipr.2008.0172\" rel=\"related\"/>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IET Image Processing 4 (2010) 70-80</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1009.0854v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1009.0854v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"G.1.2; I.4.m\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1009.0957v1</id>\\n    <updated>2010-09-05T23:49:38Z</updated>\\n    <published>2010-09-05T23:49:38Z</published>\\n    <title>Distance Measures for Reduced Ordering Based Vector Filters</title>\\n    <summary>  Reduced ordering based vector filters have proved successful in removing\\nlong-tailed noise from color images while preserving edges and fine image\\ndetails. These filters commonly utilize variants of the Minkowski distance to\\norder the color vectors with the aim of distinguishing between noisy and\\nnoise-free vectors. In this paper, we review various alternative distance\\nmeasures and evaluate their performance on a large and diverse set of images\\nusing several effectiveness and efficiency criteria. The results demonstrate\\nthat there are in fact strong alternatives to the popular Minkowski metrics.\\n</summary>\\n    <author>\\n      <name>M. Emre Celebi</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1049/iet-ipr.2009.0056</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1049/iet-ipr.2009.0056\" rel=\"related\"/>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IET Image Processing 3 (2009) 249-260</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1009.0957v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1009.0957v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4.3\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1009.0958v1</id>\\n    <updated>2010-09-05T23:53:27Z</updated>\\n    <published>2010-09-05T23:53:27Z</published>\\n    <title>Real-Time Implementation of Order-Statistics Based Directional Filters</title>\\n    <summary>  Vector filters based on order-statistics have proved successful in removing\\nimpulsive noise from color images while preserving edges and fine image\\ndetails. Among these filters, the ones that involve the cosine distance\\nfunction (directional filters) have particularly high computational\\nrequirements, which limits their use in time critical applications. In this\\npaper, we introduce two methods to speed up these filters. Experiments on a\\ndiverse set of color images show that the proposed methods provide substantial\\ncomputational gains without significant loss of accuracy.\\n</summary>\\n    <author>\\n      <name>M. Emre Celebi</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1049/iet-ipr:20080080</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1049/iet-ipr:20080080\" rel=\"related\"/>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IET Image Processing 3 (2009) 1-9</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1009.0958v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1009.0958v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4.3\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1009.0959v1</id>\\n    <updated>2010-09-06T00:02:35Z</updated>\\n    <published>2010-09-06T00:02:35Z</published>\\n    <title>Cost-Effective Implementation of Order-Statistics Based Vector Filters\\n  Using Minimax Approximations</title>\\n    <summary>  Vector operators based on robust order statistics have proved successful in\\ndigital multichannel imaging applications, particularly color image filtering\\nand enhancement, in dealing with impulsive noise while preserving edges and\\nfine image details. These operators often have very high computational\\nrequirements which limits their use in time-critical applications. This paper\\nintroduces techniques to speed up vector filters using the minimax\\napproximation theory. Extensive experiments on a large and diverse set of color\\nimages show that proposed approximations achieve an excellent balance among\\nease of implementation, accuracy, and computational speed.\\n</summary>\\n    <author>\\n      <name>M. Emre Celebi</name>\\n    </author>\\n    <author>\\n      <name>Hassan A. Kingravi</name>\\n    </author>\\n    <author>\\n      <name>Rastislav Lukac</name>\\n    </author>\\n    <author>\\n      <name>Fatih Celiker</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1364/JOSAA.26.001518</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1364/JOSAA.26.001518\" rel=\"related\"/>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Journal of the Optical Society of America A 26 (2009) 1518-1524</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1009.0959v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1009.0959v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4.3\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1009.0961v1</id>\\n    <updated>2010-09-06T00:13:25Z</updated>\\n    <published>2010-09-06T00:13:25Z</published>\\n    <title>A Fast Switching Filter for Impulsive Noise Removal from Color Images</title>\\n    <summary>  In this paper, we present a fast switching filter for impulsive noise removal\\nfrom color images. The filter exploits the HSL color space, and is based on the\\npeer group concept, which allows for the fast detection of noise in a\\nneighborhood without resorting to pairwise distance computations between each\\npixel. Experiments on large set of diverse images demonstrate that the proposed\\napproach is not only extremely fast, but also gives excellent results in\\ncomparison to various state-of-the-art filters.\\n</summary>\\n    <author>\\n      <name>M. Emre Celebi</name>\\n    </author>\\n    <author>\\n      <name>Hassan A. Kingravi</name>\\n    </author>\\n    <author>\\n      <name>Bakhtiyar Uddin</name>\\n    </author>\\n    <author>\\n      <name>Y. Alp Aslandogan</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.2352/J.ImagingSci.Technol.(2007)51:2(155)</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.2352/J.ImagingSci.Technol.(2007)51:2(155)\" rel=\"related\"/>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Journal of Imaging Science and Technology 51 (2007) 155-165</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1009.0961v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1009.0961v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4.3\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1009.4004v2</id>\\n    <updated>2011-12-19T02:31:16Z</updated>\\n    <published>2010-09-21T06:32:52Z</published>\\n    <title>A family of statistical symmetric divergences based on Jensen\\'s\\n  inequality</title>\\n    <summary>  We introduce a novel parametric family of symmetric information-theoretic\\ndistances based on Jensen\\'s inequality for a convex functional generator. In\\nparticular, this family unifies the celebrated Jeffreys divergence with the\\nJensen-Shannon divergence when the Shannon entropy generator is chosen. We then\\ndesign a generic algorithm to compute the unique centroid defined as the\\nminimum average divergence. This yields a smooth family of centroids linking\\nthe Jeffreys to the Jensen-Shannon centroid. Finally, we report on our\\nexperimental results.\\n</summary>\\n    <author>\\n      <name>Frank Nielsen</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">15 pages, 2 figure</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1009.4004v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1009.4004v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1010.3467v1</id>\\n    <updated>2010-10-18T02:31:21Z</updated>\\n    <published>2010-10-18T02:31:21Z</published>\\n    <title>Fast Inference in Sparse Coding Algorithms with Applications to Object\\n  Recognition</title>\\n    <summary>  Adaptive sparse coding methods learn a possibly overcomplete set of basis\\nfunctions, such that natural image patches can be reconstructed by linearly\\ncombining a small subset of these bases. The applicability of these methods to\\nvisual object recognition tasks has been limited because of the prohibitive\\ncost of the optimization algorithms required to compute the sparse\\nrepresentation. In this work we propose a simple and efficient algorithm to\\nlearn basis functions. After training, this model also provides a fast and\\nsmooth approximator to the optimal representation, achieving even better\\naccuracy than exact sparse coding algorithms on visual object recognition\\ntasks.\\n</summary>\\n    <author>\\n      <name>Koray Kavukcuoglu</name>\\n    </author>\\n    <author>\\n      <name>Marc\\'Aurelio Ranzato</name>\\n    </author>\\n    <author>\\n      <name>Yann LeCun</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1010.3467v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1010.3467v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1011.3023v4</id>\\n    <updated>2013-11-20T16:42:37Z</updated>\\n    <published>2010-11-12T20:15:25Z</published>\\n    <title>Classification with Scattering Operators</title>\\n    <summary>  A scattering vector is a local descriptor including multiscale and\\nmulti-direction co-occurrence information. It is computed with a cascade of\\nwavelet decompositions and complex modulus. This scattering representation is\\nlocally translation invariant and linearizes deformations. A supervised\\nclassification algorithm is computed with a PCA model selection on scattering\\nvectors. State of the art results are obtained for handwritten digit\\nrecognition and texture classification.\\n</summary>\\n    <author>\\n      <name>Joan Bruna</name>\\n    </author>\\n    <author>\\n      <name>St\\xc3\\xa9phane Mallat</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages. CVPR 2011</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1011.3023v4\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1011.3023v4\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1011.4321v1</id>\\n    <updated>2010-11-18T22:20:45Z</updated>\\n    <published>2010-11-18T22:20:45Z</published>\\n    <title>A Fuzzy Clustering Model for Fuzzy Data with Outliers</title>\\n    <summary>  In this paper a fuzzy clustering model for fuzzy data with outliers is\\nproposed. The model is based on Wasserstein distance between interval valued\\ndata which is generalized to fuzzy data. In addition, Keller\\'s approach is used\\nto identify outliers and reduce their influences. We have also defined a\\ntransformation to change our distance to the Euclidean distance. With the help\\nof this approach, the problem of fuzzy clustering of fuzzy data is reduced to\\nfuzzy clustering of crisp data. In order to show the performance of the\\nproposed clustering algorithm, two simulation experiments are discussed.\\n</summary>\\n    <author>\\n      <name>M. H. Fazel Zarandi</name>\\n    </author>\\n    <author>\\n      <name>Zahra S. Razaee</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">18 pages, Journal paper</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1011.4321v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1011.4321v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1012.2491v1</id>\\n    <updated>2010-12-11T21:48:51Z</updated>\\n    <published>2010-12-11T21:48:51Z</published>\\n    <title>Affine Invariant, Model-Based Object Recognition Using Robust Metrics\\n  and Bayesian Statistics</title>\\n    <summary>  We revisit the problem of model-based object recognition for intensity images\\nand attempt to address some of the shortcomings of existing Bayesian methods,\\nsuch as unsuitable priors and the treatment of residuals with a non-robust\\nerror norm. We do so by using a refor- mulation of the Huber metric and\\ncarefully chosen prior distributions. Our proposed method is invariant to\\n2-dimensional affine transforma- tions and, because it is relatively easy to\\ntrain and use, it is suited for general object matching problems.\\n</summary>\\n    <author>\\n      <name>Vasileios Zografos</name>\\n    </author>\\n    <author>\\n      <name>Bernard Buxton</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1007/11559573_51</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1007/11559573_51\" rel=\"related\"/>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Image Analysis and Recognition Lecture Notes in Computer Science,\\n  2005, Volume 3656/2005, 407-414</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1012.2491v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1012.2491v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1012.3802v1</id>\\n    <updated>2010-12-17T03:27:54Z</updated>\\n    <published>2010-12-17T03:27:54Z</published>\\n    <title>Detecting Image Forgeries using Geometric Cues</title>\\n    <summary>  This chapter presents a framework for detecting fake regions by using various\\nmethods including watermarking technique and blind approaches. In particular,\\nwe describe current categories on blind approaches which can be divided into\\nfive: pixel-based techniques, format-based techniques, camera-based techniques,\\nphysically-based techniques and geometric-based techniques. Then we take a\\nsecond look on the geometric-based techniques and further categorize them in\\ndetail. In the following section, the state-of-the-art methods involved in the\\ngeometric technique are elaborated.\\n</summary>\\n    <author>\\n      <name>Lin Wu</name>\\n    </author>\\n    <author>\\n      <name>Yang Wang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">18 pages, 10 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1012.3802v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1012.3802v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1012.3951v1</id>\\n    <updated>2010-12-17T18:23:35Z</updated>\\n    <published>2010-12-17T18:23:35Z</published>\\n    <title>Diffusion-geometric maximally stable component detection in deformable\\n  shapes</title>\\n    <summary>  Maximally stable component detection is a very popular method for feature\\nanalysis in images, mainly due to its low computation cost and high\\nrepeatability. With the recent advance of feature-based methods in geometric\\nshape analysis, there is significant interest in finding analogous approaches\\nin the 3D world. In this paper, we formulate a diffusion-geometric framework\\nfor stable component detection in non-rigid 3D shapes, which can be used for\\ngeometric feature detection and description. A quantitative evaluation of our\\nmethod on the SHREC\\'10 feature detection benchmark shows its potential as a\\nsource of high-quality features.\\n</summary>\\n    <author>\\n      <name>Roee Litman</name>\\n    </author>\\n    <author>\\n      <name>Alex M. Bronstein</name>\\n    </author>\\n    <author>\\n      <name>Michael M. Bronstein</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1016/j.cag.2011.03.011</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1016/j.cag.2011.03.011\" rel=\"related\"/>\\n    <link href=\"http://arxiv.org/abs/1012.3951v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1012.3951v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4.7; I.4.8\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1012.5933v1</id>\\n    <updated>2010-12-29T13:11:41Z</updated>\\n    <published>2010-12-29T13:11:41Z</published>\\n    <title>Affine-invariant diffusion geometry for the analysis of deformable 3D\\n  shapes</title>\\n    <summary>  We introduce an (equi-)affine invariant diffusion geometry by which surfaces\\nthat go through squeeze and shear transformations can still be properly\\nanalyzed. The definition of an affine invariant metric enables us to construct\\nan invariant Laplacian from which local and global geometric structures are\\nextracted. Applications of the proposed framework demonstrate its power in\\ngeneralizing and enriching the existing set of tools for shape analysis.\\n</summary>\\n    <author>\\n      <name>Dan Raviv</name>\\n    </author>\\n    <author>\\n      <name>Alexander M. Bronstein</name>\\n    </author>\\n    <author>\\n      <name>Michael M. Bronstein</name>\\n    </author>\\n    <author>\\n      <name>Ron Kimmel</name>\\n    </author>\\n    <author>\\n      <name>Nir Sochen</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1012.5933v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1012.5933v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1101.2243v1</id>\\n    <updated>2010-12-12T20:49:00Z</updated>\\n    <published>2010-12-12T20:49:00Z</published>\\n    <title>Illustrating Color Evolution and Color Blindness by the Decoding Model\\n  of Color Vision</title>\\n    <summary>  A symmetrical model of color vision, the decoding model as a new version of\\nzone model, was introduced. The model adopts new continuous-valued logic and\\nworks in a way very similar to the way a 3-8 decoder in a numerical circuit\\nworks. By the decoding model, Young and Helmholtz\\'s tri-pigment theory and\\nHering\\'s opponent theory are unified more naturally; opponent process, color\\nevolution, and color blindness are illustrated more concisely. According to the\\ndecoding model, we can obtain a transform from RGB system to HSV system, which\\nis formally identical to the popular transform for computer graphics provided\\nby Smith (1978). Advantages, problems, and physiological tests of the decoding\\nmodel are also discussed.\\n</summary>\\n    <author>\\n      <name>Chenguang Lu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">11 pages, 9 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1101.2243v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1101.2243v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1101.2312v1</id>\\n    <updated>2011-01-12T10:16:11Z</updated>\\n    <published>2011-01-12T10:16:11Z</published>\\n    <title>Automatic segmentation of HeLa cell images</title>\\n    <summary>  In this work, the possibilities for segmentation of cells from their\\nbackground and each other in digital image were tested, combined and improoved.\\nLot of images with young, adult and mixture cells were able to prove the\\nquality of described algorithms. Proper segmentation is one of the main task of\\nimage analysis and steps order differ from work to work, depending on input\\nimages. Reply for biologicaly given question was looking for in this work,\\nincluding filtration, details emphasizing, segmentation and sphericity\\ncomputing. Order of algorithms and way to searching for them was also\\ndescribed. Some questions and ideas for further work were mentioned in the\\nconclusion part.\\n</summary>\\n    <author>\\n      <name>Jan Urban</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1101.2312v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1101.2312v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1101.4301v1</id>\\n    <updated>2011-01-22T16:41:20Z</updated>\\n    <published>2011-01-22T16:41:20Z</published>\\n    <title>Diffusion framework for geometric and photometric data fusion in\\n  non-rigid shape analysis</title>\\n    <summary>  In this paper, we explore the use of the diffusion geometry framework for the\\nfusion of geometric and photometric information in local and global shape\\ndescriptors. Our construction is based on the definition of a diffusion process\\non the shape manifold embedded into a high-dimensional space where the\\nembedding coordinates represent the photometric information. Experimental\\nresults show that such data fusion is useful in coping with different\\nchallenges of shape analysis where pure geometric and pure photometric methods\\nfail.\\n</summary>\\n    <author>\\n      <name>Artiom Kovnatsky</name>\\n    </author>\\n    <author>\\n      <name>Michael M. Bronstein</name>\\n    </author>\\n    <author>\\n      <name>Alexander M. Bronstein</name>\\n    </author>\\n    <author>\\n      <name>Ron Kimmel</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1101.4301v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1101.4301v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1103.1587v2</id>\\n    <updated>2011-03-10T18:46:23Z</updated>\\n    <published>2011-03-08T17:50:56Z</published>\\n    <title>All Roads Lead To Rome</title>\\n    <summary>  This short article presents a class of projection-based solution algorithms\\nto the problem considered in the pioneering work on compressed sensing -\\nperfect reconstruction of a phantom image from 22 radial lines in the frequency\\ndomain. Under the framework of projection-based image reconstruction, we will\\nshow experimentally that several old and new tools of nonlinear filtering\\n(including Perona-Malik diffusion, nonlinear diffusion, Translation-Invariant\\nthresholding and SA-DCT thresholding) all lead to perfect reconstruction of the\\nphantom image.\\n</summary>\\n    <author>\\n      <name>Xin Li</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages, 1 figure, submitted</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IEEE SPM\\'2011 as a Column Paper for DSP Tips&amp;Tricks</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1103.1587v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1103.1587v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1103.3440v1</id>\\n    <updated>2011-03-17T15:52:15Z</updated>\\n    <published>2011-03-17T15:52:15Z</published>\\n    <title>Off-Line Handwritten Signature Identification Using Rotated Complex\\n  Wavelet Filters</title>\\n    <summary>  In this paper, a new method for handwritten signature identification based on\\nrotated complex wavelet filters is proposed. We have proposed to use the\\nrotated complex wavelet filters (RCWF) and dual tree complex wavelet\\ntransform(DTCWT) together to derive signature feature extraction, which\\ncaptures information in twelve different directions. In identification phase,\\nCanberra distance measure is used. The proposed method is compared with\\ndiscrete wavelet transform (DWT). From experimental results it is found that\\nsignature identification rate of proposed method is superior over DWT\\n</summary>\\n    <author>\\n      <name>M. S. Shirdhonkar</name>\\n    </author>\\n    <author>\\n      <name>Manesh Kokare</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IJCSI International Journal of Computer Science Issues, Vol. 8,\\n  Issue 1, January 2011 ISSN (Online): 1694-0814</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1103.3440v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1103.3440v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1104.0579v1</id>\\n    <updated>2011-04-04T14:14:47Z</updated>\\n    <published>2011-04-04T14:14:47Z</published>\\n    <title>Image Retrieval Method Using Top-surf Descriptor</title>\\n    <summary>  This report presents the results and details of a content-based image\\nretrieval project using the Top-surf descriptor. The experimental results are\\npreliminary, however, it shows the capability of deducing objects from parts of\\nthe objects or from the objects that are similar. This paper uses a dataset\\nconsisting of 1200 images of which 800 images are equally divided into 8\\ncategories, namely airplane, beach, motorbike, forest, elephants, horses, bus\\nand building, while the other 400 images are randomly picked from the Internet.\\nThe best results achieved are from building category.\\n</summary>\\n    <author>\\n      <name>Ye Ji</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1104.0579v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1104.0579v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1104.1472v1</id>\\n    <updated>2011-04-08T03:15:43Z</updated>\\n    <published>2011-04-08T03:15:43Z</published>\\n    <title>Gaussian Affine Feature Detector</title>\\n    <summary>  A new method is proposed to get image features\\' geometric information. Using\\nGaussian as an input signal, a theoretical optimal solution to calculate\\nfeature\\'s affine shape is proposed. Based on analytic result of a feature\\nmodel, the method is different from conventional iterative approaches. From the\\nmodel, feature\\'s parameters such as position, orientation, background\\nluminance, contrast, area and aspect ratio can be extracted. Tested with\\nsynthesized and benchmark data, the method achieves or outperforms existing\\napproaches in term of accuracy, speed and stability. The method can detect\\nsmall, long or thin objects precisely, and works well under general conditions,\\nsuch as for low contrast, blurred or noisy images.\\n</summary>\\n    <author>\\n      <name>Xiaopeng Xu</name>\\n    </author>\\n    <author>\\n      <name>Xiaochun Zhang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">A paper about two dimension image signal detection, including\\n  position, length, width, height, orentation</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1104.1472v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1104.1472v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1104.1485v1</id>\\n    <updated>2011-04-08T05:18:15Z</updated>\\n    <published>2011-04-08T05:18:15Z</published>\\n    <title>Fuzzy Rules and Evidence Theory for Satellite Image Analysis</title>\\n    <summary>  Design of a fuzzy rule based classifier is proposed. The performance of the\\nclassifier for multispectral satellite image classification is improved using\\nDempster- Shafer theory of evidence that exploits information of the\\nneighboring pixels. The classifiers are tested rigorously with two known images\\nand their performance are found to be better than the results available in the\\nliterature. We also demonstrate the improvement of performance while using D-S\\ntheory along with fuzzy rule based classifiers over the basic fuzzy rule based\\nclassifiers for all the test cases.\\n</summary>\\n    <author>\\n      <name>Arijit Laha</name>\\n    </author>\\n    <author>\\n      <name>J. Das</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages, International Conference on Advances in Pattern Recognition\\n  2003 (ICAPR03)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1104.1485v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1104.1485v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1104.2171v1</id>\\n    <updated>2011-04-12T11:20:06Z</updated>\\n    <published>2011-04-12T11:20:06Z</published>\\n    <title>From a Modified Ambrosio-Tortorelli to a Randomized Part Hierarchy Tree</title>\\n    <summary>  We demonstrate the possibility of coding parts, features that are higher\\nlevel than boundaries, using a modified AT field after augmenting the\\ninteraction term of the AT energy with a non-local term and weakening the\\nseparation into boundary/not-boundary phases. The iteratively extracted parts\\nusing the level curves with double point singularities are organized as a\\nproper binary tree. Inconsistencies due to non-generic configurations for level\\ncurves as well as due to visual changes such as occlusion are successfully\\nhandled once the tree is endowed with a probabilistic structure. The work is a\\nstep in establishing the AT function as a bridge between low and high level\\nvisual processing.\\n</summary>\\n    <author>\\n      <name>Sibel Tari</name>\\n    </author>\\n    <author>\\n      <name>Murat Genctav</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Scale Space and Variational Methods 2011</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1104.2171v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1104.2171v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1104.3742v1</id>\\n    <updated>2011-04-19T13:36:15Z</updated>\\n    <published>2011-04-19T13:36:15Z</published>\\n    <title>Hue Histograms to Spatiotemporal Local Features for Action Recognition</title>\\n    <summary>  Despite the recent developments in spatiotemporal local features for action\\nrecognition in video sequences, local color information has so far been\\nignored. However, color has been proved an important element to the success of\\nautomated recognition of objects and scenes. In this paper we extend the\\nspace-time interest point descriptor STIP to take into account the color\\ninformation on the features\\' neighborhood. We compare the performance of our\\ncolor-aware version of STIP (which we have called HueSTIP) with the original\\none.\\n</summary>\\n    <author>\\n      <name>Fillipe Souza</name>\\n    </author>\\n    <author>\\n      <name>Eduardo Valle</name>\\n    </author>\\n    <author>\\n      <name>Guillermo Ch\\xc3\\xa1vez</name>\\n    </author>\\n    <author>\\n      <name>Arnaldo Ara\\xc3\\xbajo</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1104.3742v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1104.3742v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1104.4295v1</id>\\n    <updated>2011-04-21T15:45:52Z</updated>\\n    <published>2011-04-21T15:45:52Z</published>\\n    <title>Improving digital signal interpolation: L2-optimal kernels with\\n  kernel-invariant interpolation speed</title>\\n    <summary>  Interpolation is responsible for digital signal resampling and can\\nsignificantly degrade the original signal quality if not done properly. For\\nmany years, optimal interpolation algorithms were sought within constrained\\nclasses of interpolation kernel functions. We derive a new family of\\nunconstrained L2-optimal interpolation kernels, and compare their properties to\\nthe previously known. Although digital images are used to illustrate this work,\\nour L2-optimal kernels can be applied to interpolate any digital signals.\\n</summary>\\n    <author>\\n      <name>Oleg S. Pianykh</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1104.4295v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1104.4295v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1105.0821v1</id>\\n    <updated>2011-05-04T13:40:16Z</updated>\\n    <published>2011-05-04T13:40:16Z</published>\\n    <title>Considerations and Results in Multimedia and DVB Application Development\\n  on Philips Nexperia Platform</title>\\n    <summary>  This paper presents some experiments regarding applications development on\\nhigh performance media processors included in Philips Nexperia Family. The\\nPNX1302 dedicated DVB-T kit used has some limitations. Our work has succeeded\\nto overcome these limitations and to make possible a general-purpose use of\\nthis kit. For exemplification two typical applications, important both for\\nmultimedia and DVB, are analyzed: MPEG2 video stream decoding and MP3 audio\\ndecoding. These original implementations are compared (in speed, memory\\nrequirements and costs) with Philips Nexperia Library.\\n</summary>\\n    <author>\\n      <name>Radu Arsinte</name>\\n    </author>\\n    <author>\\n      <name>Ciprian Ilioaei</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">3 pages, 1 figure</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Scientific Bulletin of the \"Politehnica\" University Timi\\\\c{s}oara,\\n  Transaction on Electronics and Telecomunications, Tom 49(63), Fascicola 2,\\n  2004, pag. 138-141</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1105.0821v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1105.0821v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1105.3828v2</id>\\n    <updated>2013-02-02T07:28:58Z</updated>\\n    <published>2011-05-19T09:50:01Z</published>\\n    <title>An Algorithmic Solution to the Five-Point Pose Problem Based on the\\n  Cayley Representation of Rotations</title>\\n    <summary>  We give a new algorithmic solution to the well-known five-point relative pose\\nproblem. Our approach does not deal with the famous cubic constraint on an\\nessential matrix. Instead, we use the Cayley representation of rotations in\\norder to obtain a polynomial system from epipolar constraints. Solving that\\nsystem, we directly get relative rotation and translation parameters of the\\ncameras in terms of roots of a 10th degree polynomial.\\n</summary>\\n    <author>\\n      <name>Evgeniy Martyushev</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages, 5 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1105.3828v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1105.3828v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1105.5307v1</id>\\n    <updated>2011-05-26T14:31:58Z</updated>\\n    <published>2011-05-26T14:31:58Z</published>\\n    <title>Efficient Learning of Sparse Invariant Representations</title>\\n    <summary>  We propose a simple and efficient algorithm for learning sparse invariant\\nrepresentations from unlabeled data with fast inference. When trained on short\\nmovies sequences, the learned features are selective to a range of orientations\\nand spatial frequencies, but robust to a wide range of positions, similar to\\ncomplex cells in the primary visual cortex. We give a hierarchical version of\\nthe algorithm, and give guarantees of fast convergence under certain\\nconditions.\\n</summary>\\n    <author>\\n      <name>Karol Gregor</name>\\n    </author>\\n    <author>\\n      <name>Yann LeCun</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages + 6 supplement pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1105.5307v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1105.5307v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1105.6060v1</id>\\n    <updated>2011-05-30T18:30:51Z</updated>\\n    <published>2011-05-30T18:30:51Z</published>\\n    <title>Alignment of Microtubule Imagery</title>\\n    <summary>  This work discusses preliminary work aimed at simulating and visualizing the\\ngrowth process of a tiny structure inside the cell---the microtubule.\\nDifficulty of recording the process lies in the fact that the tissue\\npreparation method for electronic microscopes is highly destructive to live\\ncells. Here in this paper, our approach is to take pictures of microtubules at\\ndifferent time slots and then appropriately combine these images into a\\ncoherent video. Experimental results are given on real data.\\n</summary>\\n    <author>\\n      <name>Feiyang Yu</name>\\n    </author>\\n    <author>\\n      <name>Ard Oerlemans</name>\\n    </author>\\n    <author>\\n      <name>Erwin M. Bakker</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1105.6060v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1105.6060v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1106.0107v1</id>\\n    <updated>2011-06-01T07:22:04Z</updated>\\n    <published>2011-06-01T07:22:04Z</published>\\n    <title>Handwritten Character Recognition of South Indian Scripts: A Review</title>\\n    <summary>  Handwritten character recognition is always a frontier area of research in\\nthe field of pattern recognition and image processing and there is a large\\ndemand for OCR on hand written documents. Even though, sufficient studies have\\nperformed in foreign scripts like Chinese, Japanese and Arabic characters, only\\na very few work can be traced for handwritten character recognition of Indian\\nscripts especially for the South Indian scripts. This paper provides an\\noverview of offline handwritten character recognition in South Indian Scripts,\\nnamely Malayalam, Tamil, Kannada and Telungu.\\n</summary>\\n    <author>\\n      <name>John Jomy</name>\\n    </author>\\n    <author>\\n      <name>K. V. Pramod</name>\\n    </author>\\n    <author>\\n      <name>Balakrishnan Kannan</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Paper presented on the \"National Conference on Indian Language\\n  Computing\", Kochi, February 19-20, 2011. 6 pages, 5 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1106.0107v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1106.0107v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1106.5341v1</id>\\n    <updated>2011-06-27T09:47:28Z</updated>\\n    <published>2011-06-27T09:47:28Z</published>\\n    <title>Pose Estimation from a Single Depth Image for Arbitrary Kinematic\\n  Skeletons</title>\\n    <summary>  We present a method for estimating pose information from a single depth image\\ngiven an arbitrary kinematic structure without prior training. For an arbitrary\\nskeleton and depth image, an evolutionary algorithm is used to find the optimal\\nkinematic configuration to explain the observed image. Results show that our\\napproach can correctly estimate poses of 39 and 78 degree-of-freedom models\\nfrom a single depth image, even in cases of significant self-occlusion.\\n</summary>\\n    <author>\\n      <name>Daniel L. Ly</name>\\n    </author>\\n    <author>\\n      <name>Ashutosh Saxena</name>\\n    </author>\\n    <author>\\n      <name>Hod Lipson</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">2 pages, 2 figures, RGB-D workshop in Robotics: Science and Systems\\n  (RSS 2011)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1106.5341v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1106.5341v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1106.5571v1</id>\\n    <updated>2011-06-28T06:08:38Z</updated>\\n    <published>2011-06-28T06:08:38Z</published>\\n    <title>Mobile Augmented Reality Applications</title>\\n    <summary>  Augmented reality have undergone considerable improvement in past years. Many\\nspecial techniques and hardware devices were developed, but the crucial\\nbreakthrough came with the spread of intelligent mobile phones. This enabled\\nmass spread of augmented reality applications. However mobile devices have\\nlimited hardware capabilities, which narrows down the methods usable for scene\\nanalysis. In this article we propose an augmented reality application which is\\nusing cloud computing to enable using of more complex computational methods\\nsuch as neural networks. Our goal is to create an affordable augmented reality\\napplication suitable which will help car designers in by \\'virtualizing\\' car\\nmodifications.\\n</summary>\\n    <author>\\n      <name>David Prochazka</name>\\n    </author>\\n    <author>\\n      <name>Michael Stencl</name>\\n    </author>\\n    <author>\\n      <name>Ondrej Popelka</name>\\n    </author>\\n    <author>\\n      <name>Jiri Stastny</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proceedings of Mendel 2011: 17th International Conference on Soft\\n  Computing, pp. 469-476, ISBN 978-80-214-4302-0</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1106.5571v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1106.5571v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"H.5.1\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1107.1058v1</id>\\n    <updated>2011-07-06T08:43:38Z</updated>\\n    <published>2011-07-06T08:43:38Z</published>\\n    <title>Online Vehicle Detection For Estimating Traffic Status</title>\\n    <summary>  We propose a traffic congestion estimation system based on unsupervised\\non-line learning algorithm. The system does not rely on background extraction\\nor motion detection. It extracts local features inside detection regions of\\nvariable size which are drawn on lanes in advance. The extracted features are\\nthen clustered into two classes using K-means and Gaussian Mixture Models(GMM).\\nA Bayes classifier is used to detect vehicles according to the previous cluster\\ninformation which keeps updated whenever system is running by on-line EM\\nalgorithm. Experimental result shows that our system can be adapted to various\\ntraffic scenes for estimating traffic status.\\n</summary>\\n    <author>\\n      <name>Ranch Y. Q. Lai</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1107.1058v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1107.1058v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1107.1081v1</id>\\n    <updated>2011-07-06T10:02:42Z</updated>\\n    <published>2011-07-06T10:02:42Z</published>\\n    <title>Spatial Features for Multi-Font/Multi-Size Kannada Numerals and Vowels\\n  Recognition</title>\\n    <summary>  This paper presents multi-font/multi-size Kannada numerals and vowels\\nrecognition based on spatial features. Directional spatial features viz stroke\\ndensity, stroke length and the number of stokes in an image are employed as\\npotential features to characterize the printed Kannada numerals and vowels.\\nBased on these features 1100 numerals and 1400 vowels are classified with\\nMulti-class Support Vector Machines (SVM). The proposed system achieves the\\nrecognition accuracy as 98.45% and 90.64% for numerals and vowels respectively.\\n</summary>\\n    <author>\\n      <name>B. V. Dhandra</name>\\n    </author>\\n    <author>\\n      <name>Mallikarjun Hangarge</name>\\n    </author>\\n    <author>\\n      <name>Gururaj Mukarambi</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, 4 Figures, 4 Tables, \"International Conference on\\n  Communication, Computation, Control and Nanotechnology (2010)\"</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1107.1081v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1107.1081v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1107.2336v1</id>\\n    <updated>2011-07-12T16:21:06Z</updated>\\n    <published>2011-07-12T16:21:06Z</published>\\n    <title>A Variation of the Box-Counting Algorithm Applied to Colour Images</title>\\n    <summary>  The box counting method for fractal dimension estimation had not been applied\\nto large or colour images thus far due to the processing time required. In this\\nletter we present a fast, easy to implement and very easily expandable to any\\nnumber of dimensions variation, the box merging method. It is applied here in\\nRGB images which are considered as sets in 5-D space.\\n</summary>\\n    <author>\\n      <name>N. S. Nikolaidis</name>\\n    </author>\\n    <author>\\n      <name>I. N. Nikolaidis</name>\\n    </author>\\n    <author>\\n      <name>C. C. Tsouros</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages, 3 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1107.2336v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1107.2336v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"28A78, 28A80\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.3.7\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1107.2693v1</id>\\n    <updated>2011-07-13T22:46:58Z</updated>\\n    <published>2011-07-13T22:46:58Z</published>\\n    <title>A Fuzzy View on k-Means Based Signal Quantization with Application in\\n  Iris Segmentation</title>\\n    <summary>  This paper shows that the k-means quantization of a signal can be interpreted\\nboth as a crisp indicator function and as a fuzzy membership assignment\\ndescribing fuzzy clusters and fuzzy boundaries. Combined crisp and fuzzy\\nindicator functions are defined here as natural generalizations of the ordinary\\ncrisp and fuzzy indicator functions, respectively. An application to iris\\nsegmentation is presented together with a demo program.\\n</summary>\\n    <author>\\n      <name>Nicolaie Popescu-Bodorin</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4, pages, 3 figures, 17th Telecommunications Forum TELFOR 2009,\\n  Belgrade, Serbia</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1107.2693v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1107.2693v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68U10, 68T10\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4.6; I.5.1; I.5.3; I.5.4\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1107.4958v1</id>\\n    <updated>2011-07-25T14:20:34Z</updated>\\n    <published>2011-07-25T14:20:34Z</published>\\n    <title>Efficient and Accurate Gaussian Image Filtering Using Running Sums</title>\\n    <summary>  This paper presents a simple and efficient method to convolve an image with a\\nGaussian kernel. The computation is performed in a constant number of\\noperations per pixel using running sums along the image rows and columns. We\\ninvestigate the error function used for kernel approximation and its relation\\nto the properties of the input signal. Based on natural image statistics we\\npropose a quadratic form kernel error function so that the output image l2\\nerror is minimized. We apply the proposed approach to approximate the Gaussian\\nkernel by linear combination of constant functions. This results in very\\nefficient Gaussian filtering method. Our experiments show that the proposed\\ntechnique is faster than state of the art methods while preserving a similar\\naccuracy.\\n</summary>\\n    <author>\\n      <name>Elhanan Elboher</name>\\n    </author>\\n    <author>\\n      <name>Michael Werman</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1107.4958v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1107.4958v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1107.5349v1</id>\\n    <updated>2011-07-26T22:29:35Z</updated>\\n    <published>2011-07-26T22:29:35Z</published>\\n    <title>Multi Layer Analysis</title>\\n    <summary>  This thesis presents a new methodology to analyze one-dimensional signals\\ntrough a new approach called Multi Layer Analysis, for short MLA. It also\\nprovides some new insights on the relationship between one-dimensional signals\\nprocessed by MLA and tree kernels, test of randomness and signal processing\\ntechniques. The MLA approach has a wide range of application to the fields of\\npattern discovery and matching, computational biology and many other areas of\\ncomputer science and signal processing. This thesis includes also some\\napplications of this approach to real problems in biology and seismology.\\n</summary>\\n    <author>\\n      <name>Luca Pinello</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1107.5349v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1107.5349v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DS\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-bio.QM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1108.0007v1</id>\\n    <updated>2011-07-29T20:06:59Z</updated>\\n    <published>2011-07-29T20:06:59Z</published>\\n    <title>A Invertible Dimension Reduction of Curves on a Manifold</title>\\n    <summary>  In this paper, we propose a novel lower dimensional representation of a shape\\nsequence. The proposed dimension reduction is invertible and computationally\\nmore efficient in comparison to other related works. Theoretically, the\\ndifferential geometry tools such as moving frame and parallel transportation\\nare successfully adapted into the dimension reduction problem of high\\ndimensional curves. Intuitively, instead of searching for a global flat\\nsubspace for curve embedding, we deployed a sequence of local flat subspaces\\nadaptive to the geometry of both of the curve and the manifold it lies on. In\\npractice, the experimental results of the dimension reduction and\\nreconstruction algorithms well illustrate the advantages of the proposed\\ntheoretical innovation.\\n</summary>\\n    <author>\\n      <name>Sheng Yi</name>\\n    </author>\\n    <author>\\n      <name>Hamid Krim</name>\\n    </author>\\n    <author>\\n      <name>Larry K. Norris</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1108.0007v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1108.0007v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.DG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1108.1169v1</id>\\n    <updated>2011-08-04T19:00:14Z</updated>\\n    <published>2011-08-04T19:00:14Z</published>\\n    <title>Learning Representations by Maximizing Compression</title>\\n    <summary>  We give an algorithm that learns a representation of data through\\ncompression. The algorithm 1) predicts bits sequentially from those previously\\nseen and 2) has a structure and a number of computations similar to an\\nautoencoder. The likelihood under the model can be calculated exactly, and\\narithmetic coding can be used directly for compression. When training on digits\\nthe algorithm learns filters similar to those of restricted boltzman machines\\nand denoising autoencoders. Independent samples can be drawn from the model by\\na single sweep through the pixels. The algorithm has a good compression\\nperformance when compared to other methods that work under random ordering of\\npixels.\\n</summary>\\n    <author>\\n      <name>Karol Gregor</name>\\n    </author>\\n    <author>\\n      <name>Yann LeCun</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, 3 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1108.1169v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1108.1169v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1108.3525v1</id>\\n    <updated>2011-08-17T17:06:41Z</updated>\\n    <published>2011-08-17T17:06:41Z</published>\\n    <title>Hamiltonian Streamline Guided Feature Extraction with Applications to\\n  Face Detection</title>\\n    <summary>  We propose a new feature extraction method based on two dynamical systems\\ninduced by intensity landscape: the negative gradient system and the\\nHamiltonian system. We build features based on the Hamiltonian streamlines.\\nThese features contain nice global topological information about the intensity\\nlandscape, and can be used for object detection. We show that for training\\nimages of same size, our feature space is much smaller than that generated by\\nHaar-like features. The training time is extremely short, and detection speed\\nand accuracy is similar to Haar-like feature based classifiers.\\n</summary>\\n    <author>\\n      <name>Yingjie Miao</name>\\n    </author>\\n    <author>\\n      <name>Jason J. Corso</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1108.3525v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1108.3525v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.DS\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1108.4315v1</id>\\n    <updated>2011-08-22T13:49:57Z</updated>\\n    <published>2011-08-22T13:49:57Z</published>\\n    <title>Edge detection based on morphological amoebas</title>\\n    <summary>  Detecting the edges of objects within images is critical for quality image\\nprocessing. We present an edge-detecting technique that uses morphological\\namoebas that adjust their shape based on variation in image contours. We\\nevaluate the method both quantitatively and qualitatively for edge detection of\\nimages, and compare it to classic morphological methods. Our amoeba-based\\nedge-detection system performed better than the classic edge detectors.\\n</summary>\\n    <author>\\n      <name>Won Yeol Lee</name>\\n    </author>\\n    <author>\\n      <name>Young Woo Kim</name>\\n    </author>\\n    <author>\\n      <name>Se Yun Kim</name>\\n    </author>\\n    <author>\\n      <name>Jae Young Lim</name>\\n    </author>\\n    <author>\\n      <name>Dong Hoon Lim</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1179/1743131X11Y.0000000013</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1179/1743131X11Y.0000000013\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">To appear in The Imaging Science Journal</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1108.4315v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1108.4315v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1108.5710v1</id>\\n    <updated>2011-08-29T19:06:30Z</updated>\\n    <published>2011-08-29T19:06:30Z</published>\\n    <title>Generalized Fast Approximate Energy Minimization via Graph Cuts:\\n  Alpha-Expansion Beta-Shrink Moves</title>\\n    <summary>  We present alpha-expansion beta-shrink moves, a simple generalization of the\\nwidely-used alpha-beta swap and alpha-expansion algorithms for approximate\\nenergy minimization. We show that in a certain sense, these moves dominate both\\nalpha-beta-swap and alpha-expansion moves, but unlike previous generalizations\\nthe new moves require no additional assumptions and are still solvable in\\npolynomial-time. We show promising experimental results with the new moves,\\nwhich we believe could be used in any context where alpha-expansions are\\ncurrently employed.\\n</summary>\\n    <author>\\n      <name>Mark Schmidt</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">INRIA Paris - Rocquencourt</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Karteek Alahari</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">INRIA Paris - Rocquencourt</arxiv:affiliation>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Conference on Uncertainty in Artificial Intelligence (2011)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1108.5710v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1108.5710v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1108.5720v1</id>\\n    <updated>2011-08-29T19:28:13Z</updated>\\n    <published>2011-08-29T19:28:13Z</published>\\n    <title>Conjugate Variables as a Resource in Signal and Image Processing</title>\\n    <summary>  In this paper we develop a new technique to model joint distributions of\\nsignals. Our technique is based on quantum mechanical conjugate variables. We\\nshow that the transition probability of quantum states leads to a distance\\nfunction on the signals. This distance function obeys the triangle inequality\\non all quantum states and becomes a metric on pure quantum states. Treating\\nsignals as conjugate variables allows us to create a new approach to segment\\nthem.\\n  Keywords: Quantum information, transition probability, Euclidean distance,\\nFubini-study metric, Bhattacharyya coefficients, conjugate variable,\\nsignal/sensor fusion, signal and image segmentation.\\n</summary>\\n    <author>\\n      <name>Michael N\\xc3\\xb6lle</name>\\n    </author>\\n    <author>\\n      <name>Martin Suda</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">22 pages, 2 tables, 6 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1108.5720v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1108.5720v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.data-an\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"quant-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1109.0090v1</id>\\n    <updated>2011-09-01T04:47:08Z</updated>\\n    <published>2011-09-01T04:47:08Z</published>\\n    <title>An Efficient Codebook Initialization Approach for LBG Algorithm</title>\\n    <summary>  In VQ based image compression technique has three major steps namely (i)\\nCodebook Design, (ii) VQ Encoding Process and (iii) VQ Decoding Process. The\\nperformance of VQ based image compression technique depends upon the\\nconstructed codebook. A widely used technique for VQ codebook design is the\\nLinde-Buzo-Gray (LBG) algorithm. However the performance of the standard LBG\\nalgorithm is highly dependent on the choice of the initial codebook. In this\\npaper, we have proposed a simple and very effective approach for codebook\\ninitialization for LBG algorithm. The simulation results show that the proposed\\nscheme is computationally efficient and gives expected performance as compared\\nto the standard LBG algorithm.\\n</summary>\\n    <author>\\n      <name>Arup Kumar Pal</name>\\n    </author>\\n    <author>\\n      <name>Anup Sar</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.5121/ijcsea.2011.1407</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.5121/ijcsea.2011.1407\" rel=\"related\"/>\\n    <link href=\"http://arxiv.org/abs/1109.0090v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1109.0090v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1109.1068v1</id>\\n    <updated>2011-09-06T05:34:28Z</updated>\\n    <published>2011-09-06T05:34:28Z</published>\\n    <title>An Automatic Clustering Technique for Optimal Clusters</title>\\n    <summary>  This paper proposes a simple, automatic and efficient clustering algorithm,\\nnamely, Automatic Merging for Optimal Clusters (AMOC) which aims to generate\\nnearly optimal clusters for the given datasets automatically. The AMOC is an\\nextension to standard k-means with a two phase iterative procedure combining\\ncertain validation techniques in order to find optimal clusters with automation\\nof merging of clusters. Experiments on both synthetic and real data have proved\\nthat the proposed algorithm finds nearly optimal clustering structures in terms\\nof number of clusters, compactness and separation.\\n</summary>\\n    <author>\\n      <name>K. Karteeka Pavan</name>\\n    </author>\\n    <author>\\n      <name>Allam Appa Rao</name>\\n    </author>\\n    <author>\\n      <name>A. V. Dattatreya Rao</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.5121/ijcsea.2011.1412</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.5121/ijcsea.2011.1412\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">12 pages, 5 figures, 2 tables</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">International journal of Computer Sciene Engineering and\\n  Applications, Vol., No.4, 2011, pp 133-144</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1109.1068v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1109.1068v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"62H30\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.5.3\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1109.3126v1</id>\\n    <updated>2011-09-14T16:24:26Z</updated>\\n    <published>2011-09-14T16:24:26Z</published>\\n    <title>A Non-Iterative Solution to the Four-Point Three-Views Pose Problem in\\n  Case of Collinear Cameras</title>\\n    <summary>  We give a non-iterative solution to a particular case of the four-point\\nthree-views pose problem when three camera centers are collinear. Using the\\nwell-known Cayley representation of orthogonal matrices, we derive from the\\nepipolar constraints a system of three polynomial equations in three variables.\\nThe eliminant of that system is a multiple of a 36th degree univariate\\npolynomial. The true (unique) solution to the problem can be expressed in terms\\nof one of real roots of that polynomial. Experiments on synthetic data confirm\\nthat our method is robust enough even in case of planar configurations.\\n</summary>\\n    <author>\\n      <name>Evgeniy Martyushev</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">12 pages, 5 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1109.3126v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1109.3126v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1109.3850v2</id>\\n    <updated>2020-01-16T03:19:40Z</updated>\\n    <published>2011-09-18T07:48:36Z</published>\\n    <title>Digital (co)homology modules and digital Pontryagin algebras</title>\\n    <summary>  In the current study, we explore digital homology and cohomology modules, and\\ninvestigate their fundamental properties on pointed digital images. We also\\nexamine pointed digital Hopf spaces and base point preserving digital Hopf\\nfunctions between the pointed digital Hopf spaces with suitable digital\\nmultiplications, and explore the digital primitive homology and cohomology\\nclasses, the digital Pontryagin algebras and coalgebras on the digital Hopf\\nspaces as digital images.\\n</summary>\\n    <author>\\n      <name>Dae-Woong Lee</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">22 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1109.3850v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1109.3850v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1109.4683v1</id>\\n    <updated>2011-09-22T00:55:32Z</updated>\\n    <published>2011-09-22T00:55:32Z</published>\\n    <title>Detachable Object Detection: Segmentation and Depth Ordering From\\n  Short-Baseline Video</title>\\n    <summary>  We describe an approach for segmenting an image into regions that correspond\\nto surfaces in the scene that are partially surrounded by the medium. It\\nintegrates both appearance and motion statistics into a cost functional, that\\nis seeded with occluded regions and minimized efficiently by solving a linear\\nprogramming problem. Where a short observation time is insufficient to\\ndetermine whether the object is detachable, the results of the minimization can\\nbe used to seed a more costly optimization based on a longer sequence of video\\ndata. The result is an entirely unsupervised scheme to detect and segment an\\narbitrary and unknown number of objects. We test our scheme to highlight the\\npotential, as well as limitations, of our approach.\\n</summary>\\n    <author>\\n      <name>Alper Ayvaci</name>\\n    </author>\\n    <author>\\n      <name>Stefano Soatto</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1109.4683v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1109.4683v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1109.4744v1</id>\\n    <updated>2011-09-22T09:26:23Z</updated>\\n    <published>2011-09-22T09:26:23Z</published>\\n    <title>Probabilistic prototype models for attributed graphs</title>\\n    <summary>  This contribution proposes a new approach towards developing a class of\\nprobabilistic methods for classifying attributed graphs. The key concept is\\nrandom attributed graph, which is defined as an attributed graph whose nodes\\nand edges are annotated by random variables. Every node/edge has two random\\nprocesses associated with it- occurence probability and the probability\\ndistribution over the attribute values. These are estimated within the maximum\\nlikelihood framework. The likelihood of a random attributed graph to generate\\nan outcome graph is used as a feature for classification. The proposed approach\\nis fast and robust to noise.\\n</summary>\\n    <author>\\n      <name>S. Deepak Srinivasan</name>\\n    </author>\\n    <author>\\n      <name>Klaus Obermayer</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1109.4744v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1109.4744v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1110.0872v1</id>\\n    <updated>2011-10-04T23:58:55Z</updated>\\n    <published>2011-10-04T23:58:55Z</published>\\n    <title>Non-Gaussian Scale Space Filtering with 2 by 2 Matrix of Linear Filters</title>\\n    <summary>  Construction of a scale space with a convolution filter has been studied\\nextensively in the past. It has been proven that the only convolution kernel\\nthat satisfies the scale space requirements is a Gaussian type. In this paper,\\nwe consider a matrix of convolution filters introduced in [1] as a building\\nkernel for a scale space, and shows that we can construct a non-Gaussian scale\\nspace with a $2\\\\times 2$ matrix of filters. The paper derives sufficient\\nconditions for the matrix of filters for being a scale space kernel, and\\npresent some numerical demonstrations.\\n</summary>\\n    <author>\\n      <name>Toshiro Kubota</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1110.0872v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1110.0872v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1110.3194v1</id>\\n    <updated>2011-10-14T13:02:36Z</updated>\\n    <published>2011-10-14T13:02:36Z</published>\\n    <title>Controlled Total Variation regularization for inverse problems</title>\\n    <summary>  This paper provides a new algorithm for solving inverse problems, based on\\nthe minimization of the $L^2$ norm and on the control of the Total Variation.\\nIt consists in relaxing the role of the Total Variation in the classical Total\\nVariation minimization approach, which permits us to get better approximation\\nto the inverse problems. The numerical results on the deconvolution problem\\nshow that our method outperforms some previous ones.\\n</summary>\\n    <author>\\n      <name>Qiyu Jin</name>\\n    </author>\\n    <author>\\n      <name>Ion Grama</name>\\n    </author>\\n    <author>\\n      <name>Quansheng Liu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages, 10 figures and 2 tables</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1110.3194v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1110.3194v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1110.5404v1</id>\\n    <updated>2011-10-25T03:54:51Z</updated>\\n    <published>2011-10-25T03:54:51Z</published>\\n    <title>Face Recognition Based on SVM and 2DPCA</title>\\n    <summary>  The paper will present a novel approach for solving face recognition problem.\\nOur method combines 2D Principal Component Analysis (2DPCA), one of the\\nprominent methods for extracting feature vectors, and Support Vector Machine\\n(SVM), the most powerful discriminative method for classification. Experiments\\nbased on proposed method have been conducted on two public data sets FERET and\\nAT&amp;T; the results show that the proposed method could improve the\\nclassification rates.\\n</summary>\\n    <author>\\n      <name>Thai Hoang Le</name>\\n    </author>\\n    <author>\\n      <name>Len Bui</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages, 7 figures, 2 tables, International Journal of Signal\\n  Processing, Image Processing and Pattern Recognition Vol. 4, No. 3,\\n  September, 2011</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1110.5404v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1110.5404v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1111.0885v1</id>\\n    <updated>2011-11-03T15:46:47Z</updated>\\n    <published>2011-11-03T15:46:47Z</published>\\n    <title>Graph Regularized Nonnegative Matrix Factorization for Hyperspectral\\n  Data Unmixing</title>\\n    <summary>  Spectral unmixing is an important tool in hyperspectral data analysis for\\nestimating endmembers and abundance fractions in a mixed pixel. This paper\\nexamines the applicability of a recently developed algorithm called graph\\nregularized nonnegative matrix factorization (GNMF) for this aim. The proposed\\napproach exploits the intrinsic geometrical structure of the data besides\\nconsidering positivity and full additivity constraints. Simulated data based on\\nthe measured spectral signatures, is used for evaluating the proposed\\nalgorithm. Results in terms of abundance angle distance (AAD) and spectral\\nangle distance (SAD) show that this method can effectively unmix hyperspectral\\ndata.\\n</summary>\\n    <author>\\n      <name>Roozbeh Rajabi</name>\\n    </author>\\n    <author>\\n      <name>Mahdi Khodadadzadeh</name>\\n    </author>\\n    <author>\\n      <name>Hassan Ghassemian</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, conference</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1111.0885v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1111.0885v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1111.1014v1</id>\\n    <updated>2011-11-03T23:50:36Z</updated>\\n    <published>2011-11-03T23:50:36Z</published>\\n    <title>Sparsity and Robustness in Face Recognition</title>\\n    <summary>  This report concerns the use of techniques for sparse signal representation\\nand sparse error correction for automatic face recognition. Much of the recent\\ninterest in these techniques comes from the paper \"Robust Face Recognition via\\nSparse Representation\" by Wright et al. (2009), which showed how, under certain\\ntechnical conditions, one could cast the face recognition problem as one of\\nseeking a sparse representation of a given input face image in terms of a\\n\"dictionary\" of training images and images of individual pixels. In this\\nreport, we have attempted to clarify some frequently encountered questions\\nabout this work and particularly, on the validity of using sparse\\nrepresentation techniques for face recognition.\\n</summary>\\n    <author>\\n      <name>John Wright</name>\\n    </author>\\n    <author>\\n      <name>Arvind Ganesh</name>\\n    </author>\\n    <author>\\n      <name>Allen Yang</name>\\n    </author>\\n    <author>\\n      <name>Zihan Zhou</name>\\n    </author>\\n    <author>\\n      <name>Yi Ma</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1111.1014v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1111.1014v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1111.1311v1</id>\\n    <updated>2011-11-05T14:09:05Z</updated>\\n    <published>2011-11-05T14:09:05Z</published>\\n    <title>Covariant fractional extension of the modified Laplace-operator used in\\n  3D-shape recovery</title>\\n    <summary>  Extending the Liouville-Caputo definition of a fractional derivative to a\\nnonlocal covariant generalization of arbitrary bound operators acting on\\nmultidimensional Riemannian spaces an appropriate approach for the 3D shape\\nrecovery of aperture afflicted 2D slide sequences is proposed. We demonstrate,\\nthat the step from a local to a nonlocal algorithm yields an order of magnitude\\nin accuracy and by using the specific fractional approach an additional factor\\n2 in accuracy of the derived results.\\n</summary>\\n    <author>\\n      <name>Richard Herrmann</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.2478/s13540-012-0024-1</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.2478/s13540-012-0024-1\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages, 3 figures, draft for proceedings IFAC FDA12 in Nanjing,\\n  China</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Fract. Calc. Appl. Anal. (2012) Vol. 15 Num. 2, 332--343</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1111.1311v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1111.1311v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1111.2391v1</id>\\n    <updated>2011-11-10T04:28:08Z</updated>\\n    <published>2011-11-10T04:28:08Z</published>\\n    <title>A Novel Approach to Texture classification using statistical feature</title>\\n    <summary>  Texture is an important spatial feature which plays a vital role in content\\nbased image retrieval. The enormous growth of the internet and the wide use of\\ndigital data have increased the need for both efficient image database creation\\nand retrieval procedure. This paper describes a new approach for texture\\nclassification by combining statistical texture features of Local Binary\\nPattern and Texture spectrum. Since most significant information of a texture\\noften appears in the high frequency channels, the features are extracted by the\\ncomputation of LBP and Texture Spectrum and Legendre Moments. Euclidean\\ndistance is used for similarity measurement. The experimental result shows that\\n97.77% classification accuracy is obtained by the proposed method.\\n</summary>\\n    <author>\\n      <name>B. Vijayalakshmi</name>\\n    </author>\\n    <author>\\n      <name>V. Subbiah Bharathi</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1111.2391v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1111.2391v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1111.3818v1</id>\\n    <updated>2011-11-16T14:37:07Z</updated>\\n    <published>2011-11-16T14:37:07Z</published>\\n    <title>Good Pairs of Adjacency Relations in Arbitrary Dimensions</title>\\n    <summary>  In this text we show, that the notion of a \"good pair\" that was introduced in\\nthe paper \"Digital Manifolds and the Theorem of Jordan-Brouwer\" has actually\\nknown models. We will show, how to choose cubical adjacencies, the\\ngeneralizations of the well known 4- and 8-neighborhood to arbitrary\\ndimensions, in order to find good pairs. Furthermore, we give another proof for\\nthe well known fact that the Khalimsky-topology implies good pairs. The outcome\\nis consistent with the known theory as presented by T.Y. Kong, A. Rosenfeld,\\nG.T. Herman and M. Khachan et.al and gives new insights in higher dimensions.\\n</summary>\\n    <author>\\n      <name>Martin H\\xc3\\xbcnniger</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">29 pages, 5 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1111.3818v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1111.3818v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1111.4290v1</id>\\n    <updated>2011-11-18T06:34:07Z</updated>\\n    <published>2011-11-18T06:34:07Z</published>\\n    <title>A Single Euler Number Feature for Multi-font Multi-size Kannada Numeral\\n  Recognition</title>\\n    <summary>  In this paper a novel approach is proposed based on single Euler number\\nfeature which is free from thinning and size normalization for multi-font and\\nmulti-size Kannada numeral recognition system. A nearest neighbor\\nclassification is used for classification of Kannada numerals by considering\\nthe Euclidian distance. A total 1500 numeral images with different font sizes\\nbetween (10..84) are tested for algorithm efficiency and the overall the\\nclassification accuracy is found to be 99.00% .The said method is thinning\\nfree, fast, and showed encouraging results on varying font styles and sizes of\\nKannada numerals.\\n</summary>\\n    <author>\\n      <name>B. V. Dhandra</name>\\n    </author>\\n    <author>\\n      <name>R. G. Benne</name>\\n    </author>\\n    <author>\\n      <name>Mallikarjun Hangarge</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, 1 figure, 5 tables, \"Recent Trends in Information\\n  Technology(RTIT-2009)\"</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1111.4290v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1111.4290v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1111.4654v1</id>\\n    <updated>2011-11-20T17:41:01Z</updated>\\n    <published>2011-11-20T17:41:01Z</published>\\n    <title>A self-portrait of young Leonardo</title>\\n    <summary>  One of the most famous drawings by Leonardo da Vinci is a self-portrait in\\nred chalk, where he looks quite old. In fact, there is a sketch in one of his\\nnotebooks, partially covered by written notes, that can be a self-portrait of\\nthe artist when he was young. The use of image processing, to remove the\\nhandwritten text and improve the image, allows a comparison of the two\\nportraits.\\n</summary>\\n    <author>\\n      <name>Amelia Carolina Sparavigna</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Image processing, digital restoration, Leonardo da Vinci</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1111.4654v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1111.4654v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1112.1120v1</id>\\n    <updated>2011-12-05T23:25:07Z</updated>\\n    <published>2011-12-05T23:25:07Z</published>\\n    <title>Classification with Invariant Scattering Representations</title>\\n    <summary>  A scattering transform defines a signal representation which is invariant to\\ntranslations and Lipschitz continuous relatively to deformations. It is\\nimplemented with a non-linear convolution network that iterates over wavelet\\nand modulus operators. Lipschitz continuity locally linearizes deformations.\\nComplex classes of signals and textures can be modeled with low-dimensional\\naffine spaces, computed with a PCA in the scattering domain. Classification is\\nperformed with a penalized model selection. State of the art results are\\nobtained for handwritten digit recognition over small training sets, and for\\ntexture classification.\\n</summary>\\n    <author>\\n      <name>Joan Bruna</name>\\n    </author>\\n    <author>\\n      <name>St\\xc3\\xa9phane Mallat</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages, 2 figures; IVMSP Workshop, 2011 IEEE 10th</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1112.1120v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1112.1120v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.FA\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1112.4060v1</id>\\n    <updated>2011-12-17T14:50:50Z</updated>\\n    <published>2011-12-17T14:50:50Z</published>\\n    <title>A real time vehicles detection algorithm for vision based sensors</title>\\n    <summary>  A vehicle detection plays an important role in the traffic control at\\nsignalised intersections. This paper introduces a vision-based algorithm for\\nvehicles presence recognition in detection zones. The algorithm uses linguistic\\nvariables to evaluate local attributes of an input image. The image attributes\\nare categorised as vehicle, background or unknown features. Experimental\\nresults on complex traffic scenes show that the proposed algorithm is effective\\nfor a real-time vehicles detection.\\n</summary>\\n    <author>\\n      <name>Bart\\xc5\\x82omiej P\\xc5\\x82aczek</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1007/978-3-642-15907-7_26</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1007/978-3-642-15907-7_26\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">The final publication is available at http://www.springerlink.com</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">P{\\\\l}aczek B., A real time vehicles detection algorithm for vision\\n  based sensors, Lecture Notes in Computer Science 6375, Springer-Verlag,\\n  Berlin Heidelberg, 2010, pp. 211-218</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1112.4060v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1112.4060v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1112.6291v1</id>\\n    <updated>2011-12-29T12:34:43Z</updated>\\n    <published>2011-12-29T12:34:43Z</published>\\n    <title>Descriptor learning for omnidirectional image matching</title>\\n    <summary>  Feature matching in omnidirectional vision systems is a challenging problem,\\nmainly because complicated optical systems make the theoretical modelling of\\ninvariance and construction of invariant feature descriptors hard or even\\nimpossible. In this paper, we propose learning invariant descriptors using a\\ntraining set of similar and dissimilar descriptor pairs. We use the\\nsimilarity-preserving hashing framework, in which we are trying to map the\\ndescriptor data to the Hamming space preserving the descriptor similarity on\\nthe training set. A neural network is used to solve the underlying optimization\\nproblem. Our approach outperforms not only straightforward descriptor matching,\\nbut also state-of-the-art similarity-preserving hashing methods.\\n</summary>\\n    <author>\\n      <name>Jonathan Masci</name>\\n    </author>\\n    <author>\\n      <name>Davide Migliore</name>\\n    </author>\\n    <author>\\n      <name>Michael M. Bronstein</name>\\n    </author>\\n    <author>\\n      <name>J\\xc3\\xbcrgen Schmidhuber</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1112.6291v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1112.6291v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1201.1417v1</id>\\n    <updated>2011-11-29T06:24:33Z</updated>\\n    <published>2011-11-29T06:24:33Z</published>\\n    <title>Picture Collage with Genetic Algorithm and Stereo vision</title>\\n    <summary>  In this paper, a salient region extraction method for creating picture\\ncollage based on stereo vision is proposed. Picture collage is a kind of visual\\nimage summary to arrange all input images on a given canvas, allowing overlay,\\nto maximize visible visual information. The salient regions of each image are\\nfirstly extracted and represented as a depth map. The output picture collage\\nshows as many visible salient regions (without being overlaid by others) from\\nall images as possible. A very efficient Genetic algorithm is used here for the\\noptimization. The experimental results showed the superior performance of the\\nproposed method.\\n</summary>\\n    <author>\\n      <name>Hesam Ekhtiyar</name>\\n    </author>\\n    <author>\\n      <name>Mahdi Sheida</name>\\n    </author>\\n    <author>\\n      <name>Mahmood Amintoosi</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1201.1417v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1201.1417v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1201.2050v1</id>\\n    <updated>2012-01-10T13:41:56Z</updated>\\n    <published>2012-01-10T13:41:56Z</published>\\n    <title>Adaptive Noise Reduction Scheme for Salt and Pepper</title>\\n    <summary>  In this paper, a new adaptive noise reduction scheme for images corrupted by\\nimpulse noise is presented. The proposed scheme efficiently identifies and\\nreduces salt and pepper noise. MAG (Mean Absolute Gradient) is used to identify\\npixels which are most likely corrupted by salt and pepper noise that are\\ncandidates for further median based noise reduction processing. Directional\\nfiltering is then applied after noise reduction to achieve a good tradeoff\\nbetween detail preservation and noise removal. The proposed scheme can remove\\nsalt and pepper noise with noise density as high as 90% and produce better\\nresult in terms of qualitative and quantitative measures of images.\\n</summary>\\n    <author>\\n      <name>Tina Gebreyohannes</name>\\n    </author>\\n    <author>\\n      <name>Dong-Yoon Kim</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages, 5 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1201.2050v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1201.2050v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1201.3109v1</id>\\n    <updated>2012-01-15T17:42:07Z</updated>\\n    <published>2012-01-15T17:42:07Z</published>\\n    <title>Automatic system for counting cells with elliptical shape</title>\\n    <summary>  This paper presents a new method for automatic quantification of ellipse-like\\ncells in images, an important and challenging problem that has been studied by\\nthe computer vision community. The proposed method can be described by two main\\nsteps. Initially, image segmentation based on the k-means algorithm is\\nperformed to separate different types of cells from the background. Then, a\\nrobust and efficient strategy is performed on the blob contour for touching\\ncells splitting. Due to the contour processing, the method achieves excellent\\nresults of detection compared to manual detection performed by specialists.\\n</summary>\\n    <author>\\n      <name>Wesley Nunes Gon\\xc3\\xa7alves</name>\\n    </author>\\n    <author>\\n      <name>Odemir Martinez Bruno</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Learning and NonLinear Models, Volume 9, Issue 1, 2011</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1201.3109v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1201.3109v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1201.3803v1</id>\\n    <updated>2012-01-16T07:33:56Z</updated>\\n    <published>2012-01-16T07:33:56Z</published>\\n    <title>Image Labeling and Segmentation using Hierarchical Conditional Random\\n  Field Model</title>\\n    <summary>  The use of hierarchical Conditional Random Field model deal with the problem\\nof labeling images . At the time of labeling a new image, selection of the\\nnearest cluster and using the related CRF model to label this image. When one\\ngive input image, one first use the CRF model to get initial pixel labels then\\nfinding the cluster with most similar images. Then at last relabeling the input\\nimage by the CRF model associated with this cluster. This paper presents a\\napproach to label and segment specific image having correct information.\\n</summary>\\n    <author>\\n      <name>Manoj K. Vairalkar</name>\\n    </author>\\n    <author>\\n      <name>Sonali. Nimbhorkar</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">08 pages, 6 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1201.3803v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1201.3803v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1202.0549v1</id>\\n    <updated>2012-01-29T19:19:33Z</updated>\\n    <published>2012-01-29T19:19:33Z</published>\\n    <title>Comparing Background Subtraction Algorithms and Method of Car Counting</title>\\n    <summary>  In this paper, we compare various image background subtraction algorithms\\nwith the ground truth of cars counted. We have given a sample of thousand\\nimages, which are the snap shots of current traffic as records at various\\nintersections and highways. We have also counted an approximate number of cars\\nthat are visible in these images. In order to ascertain the accuracy of\\nalgorithms to be used for the processing of million images, we compare them on\\nmany metrics that includes (i) Scalability (ii) Accuracy (iii) Processing time.\\n</summary>\\n    <author>\\n      <name>Gautam S. Thakur</name>\\n    </author>\\n    <author>\\n      <name>Mohsen Ali</name>\\n    </author>\\n    <author>\\n      <name>Pan Hui</name>\\n    </author>\\n    <author>\\n      <name>Ahmed Helmy</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1202.0549v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1202.0549v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1202.1585v1</id>\\n    <updated>2012-02-08T03:07:39Z</updated>\\n    <published>2012-02-08T03:07:39Z</published>\\n    <title>Robust seed selection algorithm for k-means type algorithms</title>\\n    <summary>  Selection of initial seeds greatly affects the quality of the clusters and in\\nk-means type algorithms. Most of the seed selection methods result different\\nresults in different independent runs. We propose a single, optimal, outlier\\ninsensitive seed selection algorithm for k-means type algorithms as extension\\nto k-means++. The experimental results on synthetic, real and on microarray\\ndata sets demonstrated that effectiveness of the new algorithm in producing the\\nclustering results\\n</summary>\\n    <author>\\n      <name>K. Karteeka Pavan</name>\\n    </author>\\n    <author>\\n      <name>Allam Appa Rao</name>\\n    </author>\\n    <author>\\n      <name>A. V. Dattatreya Rao</name>\\n    </author>\\n    <author>\\n      <name>G. R. Sridhar</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.5121/ijcsit.2011.3513</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.5121/ijcsit.2011.3513\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">17 pages, 5 tables, 9figures</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">International Journal of Computer Science and Technology (IJCSIT),\\n  Vol 3, No 5, Oct 2011 pp 147-163</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1202.1585v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1202.1585v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"62H30\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.5.3\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1202.2528v1</id>\\n    <updated>2012-02-12T13:40:11Z</updated>\\n    <published>2012-02-12T13:40:11Z</published>\\n    <title>Using Covariance Matrices as Feature Descriptors for Vehicle Detection\\n  from a Fixed Camera</title>\\n    <summary>  A method is developed to distinguish between cars and trucks present in a\\nvideo feed of a highway. The method builds upon previously done work using\\ncovariance matrices as an accurate descriptor for regions. Background\\nsubtraction and other similar proven image processing techniques are used to\\nidentify the regions where the vehicles are most likely to be, and a distance\\nmetric comparing the vehicle inside the region to a fixed library of vehicles\\nis used to determine the class of vehicle.\\n</summary>\\n    <author>\\n      <name>Kevin Mader</name>\\n    </author>\\n    <author>\\n      <name>Gil Reese</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Written as part of the requirements for the SC/EC520 course in\\n  Digital Image Processing at Boston University</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1202.2528v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1202.2528v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1202.3884v1</id>\\n    <updated>2012-02-17T11:41:28Z</updated>\\n    <published>2012-02-17T11:41:28Z</published>\\n    <title>A feature extraction technique based on character geometry for character\\n  recognition</title>\\n    <summary>  This paper describes a geometry based technique for feature extraction\\napplicable to segmentation-based word recognition systems. The proposed system\\nextracts the geometric features of the character contour. This features are\\nbased on the basic line types that forms the character skeleton. The system\\ngives a feature vector as its output. The feature vectors so generated from a\\ntraining set, were then used to train a pattern recognition engine based on\\nNeural Networks so that the system can be benchmarked.\\n</summary>\\n    <author>\\n      <name>Dinesh Dileep Gaurav</name>\\n    </author>\\n    <author>\\n      <name>Renu Ramesh</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1202.3884v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1202.3884v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1202.4495v1</id>\\n    <updated>2012-02-20T23:48:38Z</updated>\\n    <published>2012-02-20T23:48:38Z</published>\\n    <title>Stochastic-Based Pattern Recognition Analysis</title>\\n    <summary>  In this work we review the basic principles of stochastic logic and propose\\nits application to probabilistic-based pattern-recognition analysis. The\\nproposed technique is intrinsically a parallel comparison of input data to\\nvarious pre-stored categories using Bayesian techniques. We design smart\\npulse-based stochastic-logic blocks to provide an efficient pattern recognition\\nanalysis. The proposed rchitecture is applied to a specific navigation problem.\\nThe resulting system is orders of magnitude faster than processor-based\\nsolutions.\\n</summary>\\n    <author>\\n      <name>V. Canals</name>\\n    </author>\\n    <author>\\n      <name>A. Morro</name>\\n    </author>\\n    <author>\\n      <name>J. L. Rossell\\xc3\\xb3</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1016/j.patrec.2010.07.008</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1016/j.patrec.2010.07.008\" rel=\"related\"/>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Published in Pattern Recognition Letters in 2010</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1202.4495v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1202.4495v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1203.0265v1</id>\\n    <updated>2012-02-29T17:57:12Z</updated>\\n    <published>2012-02-29T17:57:12Z</published>\\n    <title>Image Fusion and Re-Modified SPIHT for Fused Image</title>\\n    <summary>  This paper presents the Discrete Wavelet based fusion techniques for\\ncombining perceptually important image features. SPIHT (Set Partitioning in\\nHierarchical Trees) algorithm is an efficient method for lossy and lossless\\ncoding of fused image. This paper presents some modifications on the SPIHT\\nalgorithm. It is based on the idea of insignificant correlation of wavelet\\ncoefficient among the medium and high frequency sub bands. In RE-MSPIHT\\nalgorithm, wavelet coefficients are scaled prior to SPIHT coding based on the\\nsub band importance, with the goal of minimizing the MSE.\\n</summary>\\n    <author>\\n      <name>S. Chitra</name>\\n    </author>\\n    <author>\\n      <name>J. B. Bhattacharjee</name>\\n    </author>\\n    <author>\\n      <name>B. Thilakavathi</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">16 pages</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Ann. Univ. Tibiscus Comp. Sci. Series VII/2 (2009), 143-158</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1203.0265v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1203.0265v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1203.1513v2</id>\\n    <updated>2012-03-08T10:29:32Z</updated>\\n    <published>2012-03-05T17:12:42Z</published>\\n    <title>Invariant Scattering Convolution Networks</title>\\n    <summary>  A wavelet scattering network computes a translation invariant image\\nrepresentation, which is stable to deformations and preserves high frequency\\ninformation for classification. It cascades wavelet transform convolutions with\\nnon-linear modulus and averaging operators. The first network layer outputs\\nSIFT-type descriptors whereas the next layers provide complementary invariant\\ninformation which improves classification. The mathematical analysis of wavelet\\nscattering networks explains important properties of deep convolution networks\\nfor classification.\\n  A scattering representation of stationary processes incorporates higher order\\nmoments and can thus discriminate textures having the same Fourier power\\nspectrum. State of the art classification results are obtained for handwritten\\ndigits and texture discrimination, using a Gaussian kernel SVM and a generative\\nPCA classifier.\\n</summary>\\n    <author>\\n      <name>Joan Bruna</name>\\n    </author>\\n    <author>\\n      <name>St\\xc3\\xa9phane Mallat</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">15 pages double column, 9 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1203.1513v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1203.1513v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1203.1765v1</id>\\n    <updated>2012-03-08T12:07:27Z</updated>\\n    <published>2012-03-08T12:07:27Z</published>\\n    <title>A comparative evaluation of two algorithms of detection of masses on\\n  mammograms</title>\\n    <summary>  In this paper, we implement and carry out the comparison of two methods of\\ncomputer-aided-detection of masses on mammograms. The two algorithms basically\\nconsist of 3 steps each: segmentation, binarization and noise suppression using\\ndifferent techniques for each step. A database of 60 images was used to compare\\nthe performance of the two algorithms in terms of general detection efficiency,\\nconservation of size and shape of detected masses.\\n</summary>\\n    <author>\\n      <name>Guillaume Kom</name>\\n    </author>\\n    <author>\\n      <name>Alain Tiedeu</name>\\n    </author>\\n    <author>\\n      <name>Martin Kom</name>\\n    </author>\\n    <author>\\n      <name>John Ngundam</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.5121/sipij.2012.3102</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.5121/sipij.2012.3102\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages, 5 figures, 1 table, Vol.3, No.1, February 2012,pp19-27;\\n  Signal &amp; Image Processing : An International Journal (SIPIJ),2012</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1203.1765v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1203.1765v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1203.3114v1</id>\\n    <updated>2012-03-14T15:31:16Z</updated>\\n    <published>2012-03-14T15:31:16Z</published>\\n    <title>Integrated three-dimensional reconstruction using reflectance fields</title>\\n    <summary>  A method to obtain three-dimensional data of real-world objects by\\nintegrating their material properties is presented. The material properties are\\ndefined by capturing the Reflectance Fields of the real-world objects. It is\\nshown, unlike conventional reconstruction methods, the method is able to use\\nthe reflectance information to recover surface depth for objects having a\\nnon-Lambertian surface reflectance. It is, for recovering 3D data of objects\\nexhibiting an anisotropic BRDF with an error less than 0.3%.\\n</summary>\\n    <author>\\n      <name>Maria-Luisa Sosas</name>\\n    </author>\\n    <author>\\n      <name>Miguel-Octavio Arias</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages, 3 figures; Published in IJCSI Journal, Volume 9, Issue 1,\\n  No. 3, January 2012</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1203.3114v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1203.3114v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1203.3230v1</id>\\n    <updated>2012-03-14T22:46:29Z</updated>\\n    <published>2012-03-14T22:46:29Z</published>\\n    <title>Reconstruction error in a motion capture system</title>\\n    <summary>  Marker-based motion capture (MoCap) systems can be composed by several dozens\\nof cameras with the purpose of reconstructing the trajectories of hundreds of\\ntargets. With a large amount of cameras it becomes interesting to determine the\\noptimal reconstruction strategy. For such aim it is of fundamental importance\\nto understand the information provided by different camera measurements and how\\nthey are combined, i.e. how the reconstruction error changes by considering\\ndifferent cameras. In this work, first, an approximation of the reconstruction\\nerror variance is derived. The results obtained in some simulations suggest\\nthat the proposed strategy allows to obtain a good approximation of the real\\nerror variance with significant reduction of the computational time.\\n</summary>\\n    <author>\\n      <name>Andrea Masiero</name>\\n    </author>\\n    <author>\\n      <name>Angelo Cenedese</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">3 pages, 5 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1203.3230v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1203.3230v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1204.1198v1</id>\\n    <updated>2012-04-05T12:28:11Z</updated>\\n    <published>2012-04-05T12:28:11Z</published>\\n    <title>A Complete Workflow for Development of Bangla OCR</title>\\n    <summary>  Developing a Bangla OCR requires bunch of algorithm and methods. There were\\nmany effort went on for developing a Bangla OCR. But all of them failed to\\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\\nabout the problem scope of currently existing Bangla OCR\\'s. In this paper, we\\npresent the basic steps required for developing a Bangla OCR and a complete\\nworkflow for development of a Bangla OCR with mentioning all the possible\\nalgorithms required.\\n</summary>\\n    <author>\\n      <name>Farjana Yeasmin Omee</name>\\n    </author>\\n    <author>\\n      <name>Shiam Shabbir Himel</name>\\n    </author>\\n    <author>\\n      <name>Md. Abu Naser Bikas</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">International Journal of Computer Applications, Volume 21, No.9,\\n  May 2011</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1204.1198v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1204.1198v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1204.1611v1</id>\\n    <updated>2012-04-07T08:17:40Z</updated>\\n    <published>2012-04-07T08:17:40Z</published>\\n    <title>Vision-based Human Gender Recognition: A Survey</title>\\n    <summary>  Gender is an important demographic attribute of people. This paper provides a\\nsurvey of human gender recognition in computer vision. A review of approaches\\nexploiting information from face and whole body (either from a still image or\\ngait sequence) is presented. We highlight the challenges faced and survey the\\nrepresentative methods of these approaches. Based on the results, good\\nperformance have been achieved for datasets captured under controlled\\nenvironments, but there is still much work that can be done to improve the\\nrobustness of gender recognition under real-life environments.\\n</summary>\\n    <author>\\n      <name>Choon Boon Ng</name>\\n    </author>\\n    <author>\\n      <name>Yong Haur Tay</name>\\n    </author>\\n    <author>\\n      <name>Bok Min Goi</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">30 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1204.1611v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1204.1611v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1204.1811v1</id>\\n    <updated>2012-04-09T06:44:42Z</updated>\\n    <published>2012-04-09T06:44:42Z</published>\\n    <title>Skin-color based videos categorization</title>\\n    <summary>  On dedicated websites, people can upload videos and share it with the rest of\\nthe world. Currently these videos are cat- egorized manually by the help of the\\nuser community. In this paper, we propose a combination of color spaces with\\nthe Bayesian network approach for robust detection of skin color followed by an\\nautomated video categorization. Exper- imental results show that our method can\\nachieve satisfactory performance for categorizing videos based on skin color.\\n</summary>\\n    <author>\\n      <name>Rehanullah Khan</name>\\n    </author>\\n    <author>\\n      <name>Asad Maqsood</name>\\n    </author>\\n    <author>\\n      <name>Zeeshan Khan</name>\\n    </author>\\n    <author>\\n      <name>Muhammad Ishaq</name>\\n    </author>\\n    <author>\\n      <name>Arsalan Arif</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">International Journal of Computer Science Issues (IJCSI), Volume 9,\\n  Issue 1, No 3, January 2012</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1204.1811v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1204.1811v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1204.2062v1</id>\\n    <updated>2012-04-10T07:10:06Z</updated>\\n    <published>2012-04-10T07:10:06Z</published>\\n    <title>SVD-EBP Algorithm for Iris Pattern Recognition</title>\\n    <summary>  This paper proposes a neural network approach based on Error Back Propagation\\n(EBP) for classification of different eye images. To reduce the complexity of\\nlayered neural network the dimensions of input vectors are optimized using\\nSingular Value Decomposition (SVD). The main of this work is to provide for\\nbest method for feature extraction and classification. The details of this\\ncombined system named as SVD-EBP system, and results thereof are presented in\\nthis paper.\\n  Keywords- Singular value decomposition(SVD), Error back Propagation(EBP).\\n</summary>\\n    <author>\\n      <name>Babasaheb G. Patil</name>\\n    </author>\\n    <author>\\n      <name>Shaila Subbaraman</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Dec2011-volume2.Issue 12 (IJACSA)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1204.2062v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1204.2062v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1204.2741v1</id>\\n    <updated>2012-04-12T14:47:41Z</updated>\\n    <published>2012-04-12T14:47:41Z</published>\\n    <title>Simultaneous Object Detection, Tracking, and Event Recognition</title>\\n    <summary>  The common internal structure and algorithmic organization of object\\ndetection, detection-based tracking, and event recognition facilitates a\\ngeneral approach to integrating these three components. This supports\\nmultidirectional information flow between these components allowing object\\ndetection to influence tracking and event recognition and event recognition to\\ninfluence tracking and object detection. The performance of the combination can\\nexceed the performance of the components in isolation. This can be done with\\nlinear asymptotic complexity.\\n</summary>\\n    <author>\\n      <name>Andrei Barbu</name>\\n    </author>\\n    <author>\\n      <name>Aaron Michaux</name>\\n    </author>\\n    <author>\\n      <name>Siddharth Narayanaswamy</name>\\n    </author>\\n    <author>\\n      <name>Jeffrey Mark Siskind</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Advances in Cognitive Systems, Vol. 2, pp. 203-220, 2012</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1204.2741v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1204.2741v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1204.3618v1</id>\\n    <updated>2012-04-13T20:16:11Z</updated>\\n    <published>2012-04-13T20:16:11Z</published>\\n    <title>Compensating Interpolation Distortion by Using New Optimized Modular\\n  Method</title>\\n    <summary>  A modular method was suggested before to recover a band limited signal from\\nthe sample and hold and linearly interpolated (or, in general, an\\nnth-order-hold) version of the regular samples. In this paper a novel approach\\nfor compensating the distortion of any interpolation based on modular method\\nhas been proposed. In this method the performance of the modular method is\\noptimized by adding only some simply calculated coefficients. This approach\\ncauses drastic improvement in terms of signal-to-noise ratios with fewer\\nmodules compared to the classical modular method. Simulation results clearly\\nconfirm the improvement of the proposed method and also its superior robustness\\nagainst additive noise.\\n</summary>\\n    <author>\\n      <name>Mohammad Tofighi</name>\\n    </author>\\n    <author>\\n      <name>Ali Ayremlou</name>\\n    </author>\\n    <author>\\n      <name>Farokh Marvasti</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages. Journal paper</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1204.3618v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1204.3618v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.MM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1205.3999v1</id>\\n    <updated>2012-05-17T18:15:45Z</updated>\\n    <published>2012-05-17T18:15:45Z</published>\\n    <title>Optimal Weights Mixed Filter for Removing Mixture of Gaussian and\\n  Impulse Noises</title>\\n    <summary>  According to the character of Gaussian, we modify the Rank-Ordered Absolute\\nDifferences (ROAD) to Rank-Ordered Absolute Differences of mixture of Gaussian\\nand impulse noises (ROADG). It will be more effective to detect impulse noise\\nwhen the impulse is mixed with Gaussian noise. Combining rightly the ROADG with\\nOptimal Weights Filter (OWF), we obtain a new method to deal with the mixed\\nnoise, called Optimal Weights Mixed Filter (OWMF). The simulation results show\\nthat the method is effective to remove the mixed noise.\\n</summary>\\n    <author>\\n      <name>Qiyu Jin</name>\\n    </author>\\n    <author>\\n      <name>Ion Grama</name>\\n    </author>\\n    <author>\\n      <name>Quansheng Liu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages, 3 figures and 3 tables</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1205.3999v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1205.3999v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1205.4463v2</id>\\n    <updated>2012-12-30T00:58:09Z</updated>\\n    <published>2012-05-20T22:07:27Z</published>\\n    <title>Pilgrims Face Recognition Dataset -- HUFRD</title>\\n    <summary>  In this work, we define a new pilgrims face recognition dataset, called HUFRD\\ndataset. The new developed dataset presents various pilgrims\\' images taken from\\noutside the Holy Masjid El-Harram in Makkah during the 2011-2012 Hajj and Umrah\\nseasons. Such dataset will be used to test our developed facial recognition and\\ndetection algorithms, as well as assess in the missing and found recognition\\nsystem \\\\cite{crowdsensing}.\\n</summary>\\n    <author>\\n      <name>Salah A. Aly</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages, 13 images, 1 table of a new HUFRD work</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1205.4463v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1205.4463v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1205.4831v1</id>\\n    <updated>2012-05-22T08:00:45Z</updated>\\n    <published>2012-05-22T08:00:45Z</published>\\n    <title>Gray Level Co-Occurrence Matrices: Generalisation and Some New Features</title>\\n    <summary>  Gray Level Co-occurrence Matrices (GLCM) are one of the earliest techniques\\nused for image texture analysis. In this paper we defined a new feature called\\ntrace extracted from the GLCM and its implications in texture analysis are\\ndiscussed in the context of Content Based Image Retrieval (CBIR). The\\ntheoretical extension of GLCM to n-dimensional gray scale images are also\\ndiscussed. The results indicate that trace features outperform Haralick\\nfeatures when applied to CBIR.\\n</summary>\\n    <author>\\n      <name>Bino Sebastian V</name>\\n    </author>\\n    <author>\\n      <name>A. Unnikrishnan</name>\\n    </author>\\n    <author>\\n      <name>Kannan Balakrishnan</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages, 3 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1205.4831v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1205.4831v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1206.0238v1</id>\\n    <updated>2012-06-01T16:20:41Z</updated>\\n    <published>2012-06-01T16:20:41Z</published>\\n    <title>Rapid Feature Extraction for Optical Character Recognition</title>\\n    <summary>  Feature extraction is one of the fundamental problems of character\\nrecognition. The performance of character recognition system is depends on\\nproper feature extraction and correct classifier selection. In this article, a\\nrapid feature extraction method is proposed and named as Celled Projection (CP)\\nthat compute the projection of each section formed through partitioning an\\nimage. The recognition performance of the proposed method is compared with\\nother widely used feature extraction methods that are intensively studied for\\nmany different scripts in literature. The experiments have been conducted using\\nBangla handwritten numerals along with three different well known classifiers\\nwhich demonstrate comparable results including 94.12% recognition accuracy\\nusing celled projection.\\n</summary>\\n    <author>\\n      <name>M. Zahid Hossain</name>\\n    </author>\\n    <author>\\n      <name>M. Ashraful Amin</name>\\n    </author>\\n    <author>\\n      <name>Hong Yan</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages, 1 figure</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1206.0238v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1206.0238v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.5.2; I.7.5\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1206.2437v1</id>\\n    <updated>2012-06-12T04:23:38Z</updated>\\n    <published>2012-06-12T04:23:38Z</published>\\n    <title>A Novel Windowing Technique for Efficient Computation of MFCC for\\n  Speaker Recognition</title>\\n    <summary>  In this paper, we propose a novel family of windowing technique to compute\\nMel Frequency Cepstral Coefficient (MFCC) for automatic speaker recognition\\nfrom speech. The proposed method is based on fundamental property of discrete\\ntime Fourier transform (DTFT) related to differentiation in frequency domain.\\nClassical windowing scheme such as Hamming window is modified to obtain\\nderivatives of discrete time Fourier transform coefficients. It has been\\nmathematically shown that the slope and phase of power spectrum are inherently\\nincorporated in newly computed cepstrum. Speaker recognition systems based on\\nour proposed family of window functions are shown to attain substantial and\\nconsistent performance improvement over baseline single tapered Hamming window\\nas well as recently proposed multitaper windowing technique.\\n</summary>\\n    <author>\\n      <name>Md. Sahidullah</name>\\n    </author>\\n    <author>\\n      <name>Goutam Saha</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/LSP.2012.2235067</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/LSP.2012.2235067\" rel=\"related\"/>\\n    <link href=\"http://arxiv.org/abs/1206.2437v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1206.2437v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1206.2807v1</id>\\n    <updated>2012-06-13T13:49:23Z</updated>\\n    <published>2012-06-13T13:49:23Z</published>\\n    <title>An efficient hierarchical graph based image segmentation</title>\\n    <summary>  Hierarchical image segmentation provides region-oriented scalespace, i.e., a\\nset of image segmentations at different detail levels in which the\\nsegmentations at finer levels are nested with respect to those at coarser\\nlevels. Most image segmentation algorithms, such as region merging algorithms,\\nrely on a criterion for merging that does not lead to a hierarchy, and for\\nwhich the tuning of the parameters can be difficult. In this work, we propose a\\nhierarchical graph based image segmentation relying on a criterion popularized\\nby Felzenzwalb and Huttenlocher. We illustrate with both real and synthetic\\nimages, showing efficiency, ease of use, and robustness of our method.\\n</summary>\\n    <author>\\n      <name>Silvio Jamil F. Guimar\\xc3\\xa3es</name>\\n    </author>\\n    <author>\\n      <name>Jean Cousty</name>\\n    </author>\\n    <author>\\n      <name>Yukiko Kenmochi</name>\\n    </author>\\n    <author>\\n      <name>Laurent Najman</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1206.2807v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1206.2807v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1206.4609v1</id>\\n    <updated>2012-06-18T14:45:17Z</updated>\\n    <published>2012-06-18T14:45:17Z</published>\\n    <title>On multi-view feature learning</title>\\n    <summary>  Sparse coding is a common approach to learning local features for object\\nrecognition. Recently, there has been an increasing interest in learning\\nfeatures from spatio-temporal, binocular, or other multi-observation data,\\nwhere the goal is to encode the relationship between images rather than the\\ncontent of a single image. We provide an analysis of multi-view feature\\nlearning, which shows that hidden variables encode transformations by detecting\\nrotation angles in the eigenspaces shared among multiple image warps. Our\\nanalysis helps explain recent experimental results showing that\\ntransformation-specific features emerge when training complex cell models on\\nvideos. Our analysis also shows that transformation-invariant features can\\nemerge as a by-product of learning representations of transformations.\\n</summary>\\n    <author>\\n      <name>Roland Memisevic</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">University of Frankfurt</arxiv:affiliation>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">ICML2012</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1206.4609v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1206.4609v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1206.4866v1</id>\\n    <updated>2012-06-21T13:14:59Z</updated>\\n    <published>2012-06-21T13:14:59Z</published>\\n    <title>Portraits of Julius Caesar: a proposal for 3D analysis</title>\\n    <summary>  Here I suggest the use of a 3D scanning and rendering to create some virtual\\ncopies of ancient artifacts to study and compare them. In particular, this\\napproach could be interesting for some roman marble busts, two of which are\\nportraits of Julius Caesar, and the third is a realistic portrait of a man\\nrecently found at Arles, France. The comparison of some images indicates that a\\nthree-dimensional visualization is necessary.\\n</summary>\\n    <author>\\n      <name>Amelia Carolina Sparavigna</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Key-words: Image processing, 3D Scanner, 3D visualization, Ancient\\n  Rome, Julius Caesar</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1206.4866v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1206.4866v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1206.6437v1</id>\\n    <updated>2012-06-27T19:59:59Z</updated>\\n    <published>2012-06-27T19:59:59Z</published>\\n    <title>Large Scale Variational Bayesian Inference for Structured Scale Mixture\\n  Models</title>\\n    <summary>  Natural image statistics exhibit hierarchical dependencies across multiple\\nscales. Representing such prior knowledge in non-factorial latent tree models\\ncan boost performance of image denoising, inpainting, deconvolution or\\nreconstruction substantially, beyond standard factorial \"sparse\" methodology.\\nWe derive a large scale approximate Bayesian inference algorithm for linear\\nmodels with non-factorial (latent tree-structured) scale mixture priors.\\nExperimental results on a range of denoising and inpainting problems\\ndemonstrate substantially improved performance compared to MAP estimation or to\\ninference with factorial priors.\\n</summary>\\n    <author>\\n      <name>Young Jun Ko</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Ecole Polytechnique Federale de Lausanne</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Matthias Seeger</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Ecole Polytechnique Federale de Lausanne</arxiv:affiliation>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Appears in Proceedings of the 29th International Conference on\\n  Machine Learning (ICML 2012)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1206.6437v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1206.6437v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1207.0805v3</id>\\n    <updated>2013-03-30T05:25:58Z</updated>\\n    <published>2012-07-03T14:32:20Z</published>\\n    <title>Anatomical Structure Segmentation in Liver MRI Images</title>\\n    <summary>  Segmentation of medical images is a challenging task owing to their\\ncomplexity. A standard segmentation problem within Magnetic Resonance Imaging\\n(MRI) is the task of labeling voxels according to their tissue type. Image\\nsegmentation provides volumetric quantification of liver area and thus helps in\\nthe diagnosis of disorders, such as Hepatitis, Cirrhosis, Jaundice,\\nHemochromatosis etc.This work deals with comparison of segmentation by applying\\nLevel Set Method,Fuzzy Level Information C-Means Clustering Algorithm and\\nGradient Vector Flow Snake Algorithm.The results are compared using the\\nparameters such as Number of pixels correctly classified, and percentage of\\narea segmented.\\n</summary>\\n    <author>\\n      <name>G. Geethu Lakshmi</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Withdrawn by author for final modification</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1207.0805v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1207.0805v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1207.1522v1</id>\\n    <updated>2012-07-06T04:58:52Z</updated>\\n    <published>2012-07-06T04:58:52Z</published>\\n    <title>Multimodal similarity-preserving hashing</title>\\n    <summary>  We introduce an efficient computational framework for hashing data belonging\\nto multiple modalities into a single representation space where they become\\nmutually comparable. The proposed approach is based on a novel coupled siamese\\nneural network architecture and allows unified treatment of intra- and\\ninter-modality similarity learning. Unlike existing cross-modality similarity\\nlearning approaches, our hashing functions are not limited to binarized linear\\nprojections and can assume arbitrarily complex forms. We show experimentally\\nthat our method significantly outperforms state-of-the-art hashing approaches\\non multimedia retrieval tasks.\\n</summary>\\n    <author>\\n      <name>Jonathan Masci</name>\\n    </author>\\n    <author>\\n      <name>Michael M. Bronstein</name>\\n    </author>\\n    <author>\\n      <name>Alexander A. Bronstein</name>\\n    </author>\\n    <author>\\n      <name>J\\xc3\\xbcrgen Schmidhuber</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1207.1522v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1207.1522v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1207.2922v1</id>\\n    <updated>2012-07-12T11:11:35Z</updated>\\n    <published>2012-07-12T11:11:35Z</published>\\n    <title>ROI Segmentation for Feature Extraction from Human Facial Images</title>\\n    <summary>  Human Computer Interaction (HCI) is the biggest goal of computer vision\\nresearchers. Features form the different facial images are able to provide a\\nvery deep knowledge about the activities performed by the different facial\\nmovements. In this paper we presented a technique for feature extraction from\\nvarious regions of interest with the help of Skin color segmentation technique,\\nThresholding, knowledge based technique for face recognition.\\n</summary>\\n    <author>\\n      <name> Surbhi</name>\\n    </author>\\n    <author>\\n      <name>Vishal Arora</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, 2 figures; International Journal of Research in Computer\\n  Science, pp. 61-64 (2012)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1207.2922v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1207.2922v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1207.3510v2</id>\\n    <updated>2012-12-18T22:15:36Z</updated>\\n    <published>2012-07-15T14:50:17Z</published>\\n    <title>HMRF-EM-image: Implementation of the Hidden Markov Random Field Model\\n  and its Expectation-Maximization Algorithm</title>\\n    <summary>  In this project, we study the hidden Markov random field (HMRF) model and its\\nexpectation-maximization (EM) algorithm. We implement a MATLAB toolbox named\\nHMRF-EM-image for 2D image segmentation using the HMRF-EM framework. This\\ntoolbox also implements edge-prior-preserving image segmentation, and can be\\neasily reconfigured for other problems, such as 3D image segmentation.\\n</summary>\\n    <author>\\n      <name>Quan Wang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This work originally appears as the final project of Prof. Birsen\\n  Yazici\\'s course Detection and Estimation Theory at RPI</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1207.3510v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1207.3510v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1207.3607v1</id>\\n    <updated>2012-07-16T09:23:06Z</updated>\\n    <published>2012-07-16T09:23:06Z</published>\\n    <title>Fusing image representations for classification using support vector\\n  machines</title>\\n    <summary>  In order to improve classification accuracy different image representations\\nare usually combined. This can be done by using two different fusing schemes.\\nIn feature level fusion schemes, image representations are combined before the\\nclassification process. In classifier fusion, the decisions taken separately\\nbased on individual representations are fused to make a decision. In this paper\\nthe main methods derived for both strategies are evaluated. Our experimental\\nresults show that classifier fusion performs better. Specifically Bayes belief\\nintegration is the best performing strategy for image classification task.\\n</summary>\\n    <author>\\n      <name>Can Demirkesen</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">BIT Lab, LJK</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Hocine Cherifi</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">BIT Lab, Le2i</arxiv:affiliation>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/IVCNZ.2009.5378367</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/IVCNZ.2009.5378367\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Image and Vision Computing New Zealand, 2009. IVCNZ \\'09. 24th\\n  International Conference, Wellington : Nouvelle-Z\\\\\\'elande (2009)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1207.3607v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1207.3607v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1207.5007v1</id>\\n    <updated>2012-07-20T17:37:27Z</updated>\\n    <published>2012-07-20T17:37:27Z</published>\\n    <title>Multisegmentation through wavelets: Comparing the efficacy of Daubechies\\n  vs Coiflets</title>\\n    <summary>  In this paper, we carry out a comparative study of the efficacy of wavelets\\nbelonging to Daubechies and Coiflet family in achieving image segmentation\\nthrough a fast statistical algorithm.The fact that wavelets belonging to\\nDaubechies family optimally capture the polynomial trends and those of Coiflet\\nfamily satisfy mini-max condition, makes this comparison interesting. In the\\ncontext of the present algorithm, it is found that the performance of Coiflet\\nwavelets is better, as compared to Daubechies wavelet.\\n</summary>\\n    <author>\\n      <name>Madhur Srivastava</name>\\n    </author>\\n    <author>\\n      <name>Yashwant Yashu</name>\\n    </author>\\n    <author>\\n      <name>Satish K. Singh</name>\\n    </author>\\n    <author>\\n      <name>Prasanta K. Panigrahi</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proceedings in Signal Processing and Real Time Operating System (\\n  SPRTOS), March 26 - 27 , 2011</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1207.5007v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1207.5007v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1207.6774v1</id>\\n    <updated>2012-07-29T13:07:09Z</updated>\\n    <published>2012-07-29T13:07:09Z</published>\\n    <title>A Survey Of Activity Recognition And Understanding The Behavior In Video\\n  Survelliance</title>\\n    <summary>  This paper presents a review of human activity recognition and behaviour\\nunderstanding in video sequence. The key objective of this paper is to provide\\na general review on the overall process of a surveillance system used in the\\ncurrent trend. Visual surveillance system is directed on automatic\\nidentification of events of interest, especially on tracking and classification\\nof moving objects. The processing step of the video surveillance system\\nincludes the following stages: Surrounding model, object representation, object\\ntracking, activity recognition and behaviour understanding. It describes\\ntechniques that use to define a general set of activities that are applicable\\nto a wide range of scenes and environments in video sequence.\\n</summary>\\n    <author>\\n      <name>A. R. Revathi</name>\\n    </author>\\n    <author>\\n      <name>Dhananjay Kumar</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">14 pages, 5 figures, 5 tables</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1207.6774v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1207.6774v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1208.0378v1</id>\\n    <updated>2012-08-02T00:54:02Z</updated>\\n    <published>2012-08-02T00:54:02Z</published>\\n    <title>Fast Planar Correlation Clustering for Image Segmentation</title>\\n    <summary>  We describe a new optimization scheme for finding high-quality correlation\\nclusterings in planar graphs that uses weighted perfect matching as a\\nsubroutine. Our method provides lower-bounds on the energy of the optimal\\ncorrelation clustering that are typically fast to compute and tight in\\npractice. We demonstrate our algorithm on the problem of image segmentation\\nwhere this approach outperforms existing global optimization techniques in\\nminimizing the objective and is competitive with the state of the art in\\nproducing high-quality segmentations.\\n</summary>\\n    <author>\\n      <name>Julian Yarkony</name>\\n    </author>\\n    <author>\\n      <name>Alexander T. Ihler</name>\\n    </author>\\n    <author>\\n      <name>Charless C. Fowlkes</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This is the extended version of a paper to appear at the 12th\\n  European Conference on Computer Vision (ECCV 2012)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1208.0378v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1208.0378v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DS\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1208.3687v1</id>\\n    <updated>2012-08-17T20:38:56Z</updated>\\n    <published>2012-08-17T20:38:56Z</published>\\n    <title>Information-theoretic Dictionary Learning for Image Classification</title>\\n    <summary>  We present a two-stage approach for learning dictionaries for object\\nclassification tasks based on the principle of information maximization. The\\nproposed method seeks a dictionary that is compact, discriminative, and\\ngenerative. In the first stage, dictionary atoms are selected from an initial\\ndictionary by maximizing the mutual information measure on dictionary\\ncompactness, discrimination and reconstruction. In the second stage, the\\nselected dictionary atoms are updated for improved reconstructive and\\ndiscriminative power using a simple gradient ascent algorithm on mutual\\ninformation. Experiments using real datasets demonstrate the effectiveness of\\nour approach for image classification tasks.\\n</summary>\\n    <author>\\n      <name>Qiang Qiu</name>\\n    </author>\\n    <author>\\n      <name>Vishal M. Patel</name>\\n    </author>\\n    <author>\\n      <name>Rama Chellappa</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1208.3687v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1208.3687v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1208.5365v1</id>\\n    <updated>2012-08-27T11:20:49Z</updated>\\n    <published>2012-08-27T11:20:49Z</published>\\n    <title>A Missing and Found Recognition System for Hajj and Umrah</title>\\n    <summary>  This note describes an integrated recognition system for identifying missing\\nand found objects as well as missing, dead, and found people during Hajj and\\nUmrah seasons in the two Holy cities of Makkah and Madina in the Kingdom of\\nSaudi Arabia. It is assumed that the total estimated number of pilgrims will\\nreach 20 millions during the next decade. The ultimate goal of this system is\\nto integrate facial recognition and object identification solutions into the\\nHajj and Umrah rituals. The missing and found computerized system is part of\\nthe CrowdSensing system for Hajj and Umrah crowd estimation, management and\\nsafety.\\n</summary>\\n    <author>\\n      <name>Salah A. Aly</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">website available via http://www.mfhajj.com</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1208.5365v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1208.5365v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1209.1558v1</id>\\n    <updated>2012-09-07T14:52:02Z</updated>\\n    <published>2012-09-07T14:52:02Z</published>\\n    <title>A Comparative Study between Moravec and Harris Corner Detection of Noisy\\n  Images Using Adaptive Wavelet Thresholding Technique</title>\\n    <summary>  In this paper a comparative study between Moravec and Harris Corner Detection\\nhas been done for obtaining features required to track and recognize objects\\nwithin a noisy image. Corner detection of noisy images is a challenging task in\\nimage processing. Natural images often get corrupted by noise during\\nacquisition and transmission. As Corner detection of these noisy images does\\nnot provide desired results, hence de-noising is required. Adaptive wavelet\\nthresholding approach is applied for the same.\\n</summary>\\n    <author>\\n      <name>Nilanjan Dey</name>\\n    </author>\\n    <author>\\n      <name>Pradipti Nandi</name>\\n    </author>\\n    <author>\\n      <name>Nilanjana Barman</name>\\n    </author>\\n    <author>\\n      <name>Debolina Das</name>\\n    </author>\\n    <author>\\n      <name>Subhabrata Chakraborty</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, 13 figures</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">International Journal of Engineering Research and Applications\\n  (IJERA) Vol. 2, Issue 1, Jan-Feb 2012, pp.599-606</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1209.1558v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1209.1558v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1209.2295v2</id>\\n    <updated>2012-09-12T07:31:14Z</updated>\\n    <published>2012-09-11T12:01:08Z</published>\\n    <title>Multimodal diffusion geometry by joint diagonalization of Laplacians</title>\\n    <summary>  We construct an extension of diffusion geometry to multiple modalities\\nthrough joint approximate diagonalization of Laplacian matrices. This naturally\\nextends classical data analysis tools based on spectral geometry, such as\\ndiffusion maps and spectral clustering. We provide several synthetic and real\\nexamples of manifold learning, retrieval, and clustering demonstrating that the\\njoint diffusion geometry frequently better captures the inherent structure of\\nmulti-modal data. We also show that many previous attempts to construct\\nmultimodal spectral clustering can be seen as particular cases of joint\\napproximate diagonalization of the Laplacians.\\n</summary>\\n    <author>\\n      <name>Davide Eynard</name>\\n    </author>\\n    <author>\\n      <name>Klaus Glashoff</name>\\n    </author>\\n    <author>\\n      <name>Michael M. Bronstein</name>\\n    </author>\\n    <author>\\n      <name>Alexander M. Bronstein</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1209.2295v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1209.2295v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1209.2903v1</id>\\n    <updated>2012-09-13T14:15:16Z</updated>\\n    <published>2012-09-13T14:15:16Z</published>\\n    <title>A Novel Approach of Harris Corner Detection of Noisy Images using\\n  Adaptive Wavelet Thresholding Technique</title>\\n    <summary>  In this paper we propose a method of corner detection for obtaining features\\nwhich is required to track and recognize objects within a noisy image. Corner\\ndetection of noisy images is a challenging task in image processing. Natural\\nimages often get corrupted by noise during acquisition and transmission. Though\\nCorner detection of these noisy images does not provide desired results, hence\\nde-noising is required. Adaptive wavelet thresholding approach is applied for\\nthe same.\\n</summary>\\n    <author>\\n      <name>Nilanjan Dey</name>\\n    </author>\\n    <author>\\n      <name>Pradipti Nandi</name>\\n    </author>\\n    <author>\\n      <name>Nilanjana Barman</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages, 10 figures. arXiv admin note: substantial text overlap with\\n  arXiv:1209.1558</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">International Journal of Computer Science &amp; Technology(IJCST) Vol.\\n  2, ISSUE 4, OCT. - DEC. 2011</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1209.2903v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1209.2903v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1209.4420v1</id>\\n    <updated>2012-09-20T04:20:40Z</updated>\\n    <published>2012-09-20T04:20:40Z</published>\\n    <title>An Efficient Color Face Verification Based on 2-Directional\\n  2-Dimensional Feature Extraction</title>\\n    <summary>  A novel and uniform framework for face verification is presented in this\\npaper. First of all, a 2-directional 2-dimensional feature extraction method is\\nadopted to extract client-specific template - 2D discrimant projection matrix.\\nThen the face skin color information is utilized as an additive feature to\\nenhance decision making strategy that makes use of not only 2D grey feature but\\nalso 2D skin color feature. A fusion decision of both is applied to experiment\\nthe performance on the XM2VTS database according to Lausanne protocol.\\nExperimental results show that the framework achieves high verification\\naccuracy and verification speed.\\n</summary>\\n    <author>\\n      <name>Lan-Ting LI</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1209.4420v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1209.4420v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1209.5039v1</id>\\n    <updated>2012-09-23T07:52:01Z</updated>\\n    <published>2012-09-23T07:52:01Z</published>\\n    <title>Creation of Digital Test Form for Prepress Department</title>\\n    <summary>  The main problem in colour management in prepress department is lack of\\navailability of literature on colour management and knowledge gap between\\nprepress department and press department. So a digital test from has been\\ncreated by Adobe Photoshop to analyse the ICC profile and to create a new\\nprofile and this analysed data is used to study about various grey scale of RGB\\nand CMYK images. That helps in conversion of image from RGB to CMYK in prepress\\ndepartment.\\n</summary>\\n    <author>\\n      <name>Jaswinder Singh Dilawari</name>\\n    </author>\\n    <author>\\n      <name>Ravinder Khanna</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 Pages,4 Figures</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">(IJCSIS) International Journal of Computer Science and Information\\n  Security, Vol. 10, No. 9, September 2012 (IJCSIS) International Journal of\\n  Computer Science and Information Security, Vol. 10, No. 9, September 2012</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1209.5039v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1209.5039v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1209.5041v1</id>\\n    <updated>2012-09-23T09:15:20Z</updated>\\n    <published>2012-09-23T09:15:20Z</published>\\n    <title>An Implementation of Computer Graphics as Prepress Image Enhancement\\n  Process</title>\\n    <summary>  The production of a printed product involves three stages: prepress, the\\nprinting process (press) itself, and finishing (post press). There are various\\ntypes of equipments (printers, scanners) and various qualities image are\\npresent in the market. These give different color rendering each time during\\nreproduction. So, a color key tool has been developed keeping Color Management\\nScheme (CMS) in mind so that during reproduction no color rendering takes place\\nirrespective of use of any device and resolution level has also been improved.\\n</summary>\\n    <author>\\n      <name>Jaswinder Singh Dilawari</name>\\n    </author>\\n    <author>\\n      <name>Ravinder Khanna</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 Pages,8 Figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1209.5041v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1209.5041v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1209.5417v1</id>\\n    <updated>2012-09-24T20:47:27Z</updated>\\n    <published>2012-09-24T20:47:27Z</published>\\n    <title>Model based neuro-fuzzy ASR on Texas processor</title>\\n    <summary>  In this paper an algorithm for recognizing speech has been proposed. The\\nrecognized speech is used to execute related commands which use the MFCC and\\ntwo kind of classifiers, first one uses MLP and second one uses fuzzy inference\\nsystem as a classifier. The experimental results demonstrate the high gain and\\nefficiency of the proposed algorithm. We have implemented this system based on\\ngraphical design and tested on a fix point digital signal processor (DSP) of\\n600 MHz, with reference DM6437-EVM of Texas instrument.\\n</summary>\\n    <author>\\n      <name>Hesam Ekhtiyar</name>\\n    </author>\\n    <author>\\n      <name>Mehdi Sheida</name>\\n    </author>\\n    <author>\\n      <name>Somaye Sobati Moghadam</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1209.5417v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1209.5417v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1209.5756v1</id>\\n    <updated>2012-09-25T20:11:23Z</updated>\\n    <published>2012-09-25T20:11:23Z</published>\\n    <title>Environmental Sounds Spectrogram Classification using Log-Gabor Filters\\n  and Multiclass Support Vector Machines</title>\\n    <summary>  This paper presents novel approaches for efficient feature extraction using\\nenvironmental sound magnitude spectrogram. We propose approach based on the\\nvisual domain. This approach included three methods. The first method is based\\non extraction for each spectrogram a single log-Gabor filter followed by mutual\\ninformation procedure. In the second method, the spectrogram is passed by the\\nsame steps of the first method but with an averaged bank of 12 log-Gabor\\nfilter. The third method consists of spectrogram segmentation into three\\npatches, and after that for each spectrogram patch we applied the second\\nmethod. The classification results prove that the second method is the most\\nefficient in our environmental sound classification system.\\n</summary>\\n    <author>\\n      <name>Sameh Souli</name>\\n    </author>\\n    <author>\\n      <name>Zied Lachiri</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1209.5756v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1209.5756v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1209.6037v1</id>\\n    <updated>2012-09-26T19:25:56Z</updated>\\n    <published>2012-09-26T19:25:56Z</published>\\n    <title>Reproduction of Images by Gamut Mapping and Creation of New Test Charts\\n  in Prepress Process</title>\\n    <summary>  With the advent of digital images the problem of keeping picture\\nvisualization uniformity arises because each printing or scanning device has\\nits own color chart. So, universal color profiles are made by ICC to bring\\nuniformity in various types of devices. Keeping that color profile in mind\\nvarious new color charts are created and calibrated with the help of standard\\nIT8 test charts available in the market. The main objective to color\\nreproduction is to produce the identical picture at device output. For that\\nprinciples for gamut mapping has been designed\\n</summary>\\n    <author>\\n      <name>Jaswinder Singh Dilawari</name>\\n    </author>\\n    <author>\\n      <name>Ravinder Khanna</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 Pages,10 Figures; International Journal of Scientific and\\n  Engineering Research,Volume 3, Issue 10, October 2012 Edition</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1209.6037v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1209.6037v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1210.0026v1</id>\\n    <updated>2012-09-28T20:29:37Z</updated>\\n    <published>2012-09-28T20:29:37Z</published>\\n    <title>Coupled quasi-harmonic bases</title>\\n    <summary>  The use of Laplacian eigenbases has been shown to be fruitful in many\\ncomputer graphics applications. Today, state-of-the-art approaches to shape\\nanalysis, synthesis, and correspondence rely on these natural harmonic bases\\nthat allow using classical tools from harmonic analysis on manifolds. However,\\nmany applications involving multiple shapes are obstacled by the fact that\\nLaplacian eigenbases computed independently on different shapes are often\\nincompatible with each other. In this paper, we propose the construction of\\ncommon approximate eigenbases for multiple shapes using approximate joint\\ndiagonalization algorithms. We illustrate the benefits of the proposed approach\\non tasks from shape editing, pose transfer, correspondence, and similarity.\\n</summary>\\n    <author>\\n      <name>A. Kovnatsky</name>\\n    </author>\\n    <author>\\n      <name>M. M. Bronstein</name>\\n    </author>\\n    <author>\\n      <name>A. M. Bronstein</name>\\n    </author>\\n    <author>\\n      <name>K. Glashoff</name>\\n    </author>\\n    <author>\\n      <name>R. Kimmel</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages, 9 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1210.0026v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1210.0026v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.GR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1210.0347v1</id>\\n    <updated>2012-10-01T10:38:08Z</updated>\\n    <published>2012-10-01T10:38:08Z</published>\\n    <title>Enhanced Techniques for PDF Image Segmentation and Text Extraction</title>\\n    <summary>  Extracting text objects from the PDF images is a challenging problem. The\\ntext data present in the PDF images contain certain useful information for\\nautomatic annotation, indexing etc. However variations of the text due to\\ndifferences in text style, font, size, orientation, alignment as well as\\ncomplex structure make the problem of automatic text extraction extremely\\ndifficult and challenging job. This paper presents two techniques under\\nblock-based classification. After a brief introduction of the classification\\nmethods, two methods were enhanced and results were evaluated. The performance\\nmetrics for segmentation and time consumption are tested for both the models.\\n</summary>\\n    <author>\\n      <name>D. Sasirekha</name>\\n    </author>\\n    <author>\\n      <name>E. Chandra</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages, 5 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1210.0347v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1210.0347v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1210.0818v1</id>\\n    <updated>2012-10-02T16:03:58Z</updated>\\n    <published>2012-10-02T16:03:58Z</published>\\n    <title>Multibiometric: Feature Level Fusion Using FKP Multi-Instance biometric</title>\\n    <summary>  This paper proposed the use of multi-instance feature level fusion as a means\\nto improve the performance of Finger Knuckle Print (FKP) verification. A\\nlog-Gabor filter has been used to extract the image local orientation\\ninformation, and represent the FKP features. Experiments are performed using\\nthe FKP database, which consists of 7,920 images. Results indicate that the\\nmulti-instance verification approach outperforms higher performance than using\\nany single instance. The influence on biometric performance using feature level\\nfusion under different fusion rules have been demonstrated in this paper.\\n</summary>\\n    <author>\\n      <name>Harbi AlMahafzah</name>\\n    </author>\\n    <author>\\n      <name>Mohammad Imran</name>\\n    </author>\\n    <author>\\n      <name>H. S. Sheshadri</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages paper</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IJCSI International Journal of Computer Science Issues, Vol. 9,\\n  Issue 4, No 3, July 2012</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1210.0818v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1210.0818v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1210.0829v1</id>\\n    <updated>2012-10-02T16:26:39Z</updated>\\n    <published>2012-10-02T16:26:39Z</published>\\n    <title>A Survey of Multibiometric Systems</title>\\n    <summary>  Most biometric systems deployed in real-world applications are unimodal.\\nUsing unimodal biometric systems have to contend with a variety of problems\\nsuch as: Noise in sensed data; Intra-class variations; Inter-class\\nsimilarities; Non-universality; Spoof attacks. These problems have addressed by\\nusing multibiometric systems, which expected to be more reliable due to the\\npresence of multiple, independent pieces of evidence.\\n</summary>\\n    <author>\\n      <name>Harbi AlMahafzah</name>\\n    </author>\\n    <author>\\n      <name>Maen Zaid AlRwashdeh</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.5120/6182-8612</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.5120/6182-8612\" rel=\"related\"/>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">International Journal of Computer Application volume 43 No 15\\n  April 2012</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1210.0829v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1210.0829v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1210.0866v1</id>\\n    <updated>2012-10-02T18:08:54Z</updated>\\n    <published>2012-10-02T18:08:54Z</published>\\n    <title>Classification of Hepatic Lesions using the Matching Metric</title>\\n    <summary>  In this paper we present a methodology of classifying hepatic (liver) lesions\\nusing multidimensional persistent homology, the matching metric (also called\\nthe bottleneck distance), and a support vector machine. We present our\\nclassification results on a dataset of 132 lesions that have been outlined and\\nannotated by radiologists. We find that topological features are useful in the\\nclassification of hepatic lesions. We also find that two-dimensional persistent\\nhomology outperforms one-dimensional persistent homology in this application.\\n</summary>\\n    <author>\\n      <name>Aaron Adcock</name>\\n    </author>\\n    <author>\\n      <name>Daniel Rubin</name>\\n    </author>\\n    <author>\\n      <name>Gunnar Carlsson</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1210.0866v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1210.0866v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.AT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1210.2474v1</id>\\n    <updated>2012-10-09T02:57:12Z</updated>\\n    <published>2012-10-09T02:57:12Z</published>\\n    <title>Level Set Estimation from Compressive Measurements using Box Constrained\\n  Total Variation Regularization</title>\\n    <summary>  Estimating the level set of a signal from measurements is a task that arises\\nin a variety of fields, including medical imaging, astronomy, and digital\\nelevation mapping. Motivated by scenarios where accurate and complete\\nmeasurements of the signal may not available, we examine here a simple\\nprocedure for estimating the level set of a signal from highly incomplete\\nmeasurements, which may additionally be corrupted by additive noise. The\\nproposed procedure is based on box-constrained Total Variation (TV)\\nregularization. We demonstrate the performance of our approach, relative to\\nexisting state-of-the-art techniques for level set estimation from compressive\\nmeasurements, via several simulation examples.\\n</summary>\\n    <author>\\n      <name>Akshay Soni</name>\\n    </author>\\n    <author>\\n      <name>Jarvis Haupt</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1210.2474v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1210.2474v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.AP\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1210.3404v2</id>\\n    <updated>2012-10-15T20:47:33Z</updated>\\n    <published>2012-10-12T00:31:46Z</published>\\n    <title>A polygon-based interpolation operator for super-resolution imaging</title>\\n    <summary>  We outline the super-resolution reconstruction problem posed as a\\nmaximization of probability. We then introduce an interpolation method based on\\npolygonal pixel overlap, express it as a linear operator, and use it to improve\\nreconstruction. Polygon interpolation outperforms the simpler bilinear\\ninterpolation operator and, unlike Gaussian modeling of pixels, requires no\\nparameter estimation. A free software implementation that reproduces the\\nresults shown is provided.\\n</summary>\\n    <author>\\n      <name>St\\xc3\\xa9fan J. van der Walt</name>\\n    </author>\\n    <author>\\n      <name>B. M. Herbst</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages; update typo in abstract</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1210.3404v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1210.3404v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1210.6157v1</id>\\n    <updated>2012-10-23T07:57:24Z</updated>\\n    <published>2012-10-23T07:57:24Z</published>\\n    <title>Novel Architecture for 3D model in virtual communities from detected\\n  face</title>\\n    <summary>  In this research paper we suggest how to extract a face from an image, modify\\nit, characterize it in terms of high-level properties, and apply it to the\\ncreation of a personalized avatar. In this research work we tested, we\\nimplemented the algorithm on several hundred facial images, including many\\ntaken under uncontrolled acquisition conditions, and found to exhibit\\nsatisfactory performance for immediate practical use.\\n</summary>\\n    <author>\\n      <name>Vibekananda Dutta</name>\\n    </author>\\n    <author>\\n      <name>Dr Nishtha Kesswani</name>\\n    </author>\\n    <author>\\n      <name>Deepti Gahalot</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">http://www.ijascse.in/publications-2012--2</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1210.6157v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1210.6157v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1210.7403v1</id>\\n    <updated>2012-10-28T05:27:55Z</updated>\\n    <published>2012-10-28T05:27:55Z</published>\\n    <title>Resolution Enhancement of Range Images via Color-Image Segmentation</title>\\n    <summary>  We report a method for super-resolution of range images. Our approach\\nleverages the interpretation of LR image as sparse samples on the HR grid.\\nBased on this interpretation, we demonstrate that our recently reported\\napproach, which reconstructs dense range images from sparse range data by\\nexploiting a registered colour image, can be applied for the task of resolution\\nenhancement of range images. Our method only uses a single colour image in\\naddition to the range observation in the super-resolution process. Using the\\nproposed approach, we demonstrate super-resolution results for large factors\\n(e.g. 4) with good localization accuracy.\\n</summary>\\n    <author>\\n      <name>Arnav Bhavsar</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1210.7403v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1210.7403v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1210.7631v1</id>\\n    <updated>2012-10-29T11:53:35Z</updated>\\n    <published>2012-10-29T11:53:35Z</published>\\n    <title>The fortresses of Ejin: an example of outlining a site from satellite\\n  images</title>\\n    <summary>  From 1960\\'s to 1970\\'s, the Chinese Army built some fortified artificial\\nhills. Some of them are located in the Inner Mongolia, Western China. These\\nlarge fortresses are surrounded by moats. For some of them it is still possible\\nto see earthworks, trenches and ditches, the planning of which could have a\\nsymbolic meaning. We can argue this result form their digital outlining,\\nobtained after an image processing of satellite images, based on edge\\ndetection.\\n</summary>\\n    <author>\\n      <name>Amelia Carolina Sparavigna</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Keywords: Satellite Imagery, Image processing, GIS, fortresses, China</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1210.7631v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1210.7631v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1211.0191v1</id>\\n    <updated>2012-10-25T23:21:46Z</updated>\\n    <published>2012-10-25T23:21:46Z</published>\\n    <title>Performance Evaluation of Random Set Based Pedestrian Tracking\\n  Algorithms</title>\\n    <summary>  The paper evaluates the error performance of three random finite set based\\nmulti-object trackers in the context of pedestrian video tracking. The\\nevaluation is carried out using a publicly available video dataset of 4500\\nframes (town centre street) for which the ground truth is available. The input\\nto all pedestrian tracking algorithms is an identical set of head and body\\ndetections, obtained using the Histogram of Oriented Gradients (HOG) detector.\\nThe tracking error is measured using the recently proposed OSPA metric for\\ntracks, adopted as the only known mathematically rigorous metric for measuring\\nthe distance between two sets of tracks. A comparative analysis is presented\\nunder various conditions.\\n</summary>\\n    <author>\\n      <name>Branko Ristic</name>\\n    </author>\\n    <author>\\n      <name>Jamie Sherrah</name>\\n    </author>\\n    <author>\\n      <name>\\xc3\\x81ngel F. Garc\\xc3\\xada-Fern\\xc3\\xa1ndez</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages, 3 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1211.0191v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1211.0191v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1211.1482v4</id>\\n    <updated>2013-01-16T03:05:42Z</updated>\\n    <published>2012-11-07T08:19:04Z</published>\\n    <title>Gender Recognition in Walk Gait through 3D Motion by Quadratic Bezier\\n  Curve and Statistical Techniques</title>\\n    <summary>  Motion capture is the process of recording the movement of objects or people.\\nIt is used in military, entertainment, sports, and medical applications, and\\nfor validation of computer vision[2] and robotics. In filmmaking and video game\\ndevelopment, it refers to recording actions of human actors, and using that\\ninformation to animate digital character models in 2D or 3D computer animation.\\nWhen it includes face and fingers or captures subtle\\n</summary>\\n    <author>\\n      <name>Sajid Ali</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">wrongly uploaded</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1211.1482v4\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1211.1482v4\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1211.1800v1</id>\\n    <updated>2012-11-08T09:24:21Z</updated>\\n    <published>2012-11-08T09:24:21Z</published>\\n    <title>A Comparative study of Arabic handwritten characters invariant feature</title>\\n    <summary>  This paper is practically interested in the unchangeable feature of Arabic\\nhandwritten character. It presents results of comparative study achieved on\\ncertain features extraction techniques of handwritten character, based on Hough\\ntransform, Fourier transform, Wavelet transform and Gabor Filter. Obtained\\nresults show that Hough Transform and Gabor filter are insensible to the\\nrotation and translation, Fourier Transform is sensible to the rotation but\\ninsensible to the translation, in contrast to Hough Transform and Gabor filter,\\nWavelets Transform is sensitive to the rotation as well as to the translation.\\n</summary>\\n    <author>\\n      <name>Hamdi Hassen</name>\\n    </author>\\n    <author>\\n      <name>Maher khemakhem</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">(IJACSA) International Journal of Advanced Computer Science and\\n  Applications, Vol. 2, No. 12, 2011</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1211.1800v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1211.1800v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1211.2116v1</id>\\n    <updated>2012-11-09T12:59:11Z</updated>\\n    <published>2012-11-09T12:59:11Z</published>\\n    <title>Localisation of Numerical Date Field in an Indian Handwritten Document</title>\\n    <summary>  This paper describes a method to localise all those areas which may\\nconstitute the date field in an Indian handwritten document. Spatial patterns\\nof the date field are studied from various handwritten documents and an\\nalgorithm is developed through statistical analysis to identify those sets of\\nconnected components which may constitute the date. Common date patterns\\nfollowed in India are considered to classify the date formats in different\\nclasses. Reported results demonstrate promising performance of the proposed\\napproach\\n</summary>\\n    <author>\\n      <name>S Arunkumar</name>\\n    </author>\\n    <author>\\n      <name>Pallab Kumar Sahu</name>\\n    </author>\\n    <author>\\n      <name>Sudeep Gorai</name>\\n    </author>\\n    <author>\\n      <name>Kalyan Ghosh</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1211.2116v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1211.2116v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1211.4307v1</id>\\n    <updated>2012-11-19T05:44:24Z</updated>\\n    <published>2012-11-19T05:44:24Z</published>\\n    <title>Efficient Superimposition Recovering Algorithm</title>\\n    <summary>  In this article, we address the issue of recovering latent transparent layers\\nfrom superimposition images. Here, we assume we have the estimated\\ntransformations and extracted gradients of latent layers. To rapidly recover\\nhigh-quality image layers, we propose an Efficient Superimposition Recovering\\nAlgorithm (ESRA) by extending the framework of accelerated gradient method. In\\naddition, a key building block (in each iteration) in our proposed method is\\nthe proximal operator calculating. Here we propose to employ a dual approach\\nand present our Parallel Algorithm with Constrained Total Variation (PACTV)\\nmethod. Our recovering method not only reconstructs high-quality layers without\\ncolor-bias problem, but also theoretically guarantees good convergence\\nperformance.\\n</summary>\\n    <author>\\n      <name>Han Li</name>\\n    </author>\\n    <author>\\n      <name>Kun Gai</name>\\n    </author>\\n    <author>\\n      <name>Pinghua Gong</name>\\n    </author>\\n    <author>\\n      <name>Changshui Zhang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1211.4307v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1211.4307v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1211.4385v1</id>\\n    <updated>2012-11-19T12:21:49Z</updated>\\n    <published>2012-11-19T12:21:49Z</published>\\n    <title>Artificial Neural Network Based Optical Character Recognition</title>\\n    <summary>  Optical Character Recognition deals in recognition and classification of\\ncharacters from an image. For the recognition to be accurate, certain\\ntopological and geometrical properties are calculated, based on which a\\ncharacter is classified and recognized. Also, the Human psychology perceives\\ncharacters by its overall shape and features such as strokes, curves,\\nprotrusions, enclosures etc. These properties, also called Features are\\nextracted from the image by means of spatial pixel-based calculation. A\\ncollection of such features, called Vectors, help in defining a character\\nuniquely, by means of an Artificial Neural Network that uses these Feature\\nVectors.\\n</summary>\\n    <author>\\n      <name>Vivek Shrivastava</name>\\n    </author>\\n    <author>\\n      <name>Navdeep Sharma</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.5121/sipij.2012.3506</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.5121/sipij.2012.3506\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Signal &amp; Image Processing : An International Journal (SIPIJ) Vol.3,\\n  No.5, October 2012</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1211.4385v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1211.4385v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1211.4503v1</id>\\n    <updated>2012-11-19T17:13:26Z</updated>\\n    <published>2012-11-19T17:13:26Z</published>\\n    <title>An Effective Fingerprint Classification and Search Method</title>\\n    <summary>  This paper presents an effective fingerprint classification method designed\\nbased on a hierarchical agglomerative clustering technique. The performance of\\nthe technique was evaluated in terms of several real-life datasets and a\\nsignificant improvement in reducing the misclassification error has been\\nnoticed. This paper also presents a query based faster fingerprint search\\nmethod over the clustered fingerprint databases. The retrieval accuracy of the\\nsearch method has been found effective in light of several real-life databases.\\n</summary>\\n    <author>\\n      <name>Monowar H. Bhuyan</name>\\n    </author>\\n    <author>\\n      <name>D. K. Bhattacharyya</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages, 8 figures, 6 tables, referred journal publication</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">International Journal of Computer Science and Network Security,\\n  Vol. 9, No.11, pp. 39-48, 2009</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1211.4503v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1211.4503v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68U35\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.5.3\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1211.4860v1</id>\\n    <updated>2012-11-20T20:54:30Z</updated>\\n    <published>2012-11-20T20:54:30Z</published>\\n    <title>Domain Adaptations for Computer Vision Applications</title>\\n    <summary>  A basic assumption of statistical learning theory is that train and test data\\nare drawn from the same underlying distribution. Unfortunately, this assumption\\ndoesn\\'t hold in many applications. Instead, ample labeled data might exist in a\\nparticular `source\\' domain while inference is needed in another, `target\\'\\ndomain. Domain adaptation methods leverage labeled data from both domains to\\nimprove classification on unseen data in the target domain. In this work we\\nsurvey domain transfer learning methods for various application domains with\\nfocus on recent work in Computer Vision.\\n</summary>\\n    <author>\\n      <name>Oscar Beijbom</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1211.4860v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1211.4860v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1211.5556v1</id>\\n    <updated>2012-11-23T17:13:07Z</updated>\\n    <published>2012-11-23T17:13:07Z</published>\\n    <title>Improving Perceptual Color Difference using Basic Color Terms</title>\\n    <summary>  We suggest a new color distance based on two observations. First, perceptual\\ncolor differences were designed to be used to compare very similar colors. They\\ndo not capture human perception for medium and large color differences well.\\nThresholding was proposed to solve the problem for large color differences,\\ni.e. two totally different colors are always the same distance apart. We show\\nthat thresholding alone cannot improve medium color differences. We suggest to\\nalleviate this problem using basic color terms. Second, when a color distance\\nis used for edge detection, many small distances around the just noticeable\\ndifference may account for false edges. We suggest to reduce the effect of\\nsmall distances.\\n</summary>\\n    <author>\\n      <name>Ofir Pele</name>\\n    </author>\\n    <author>\\n      <name>Michael Werman</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1211.5556v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1211.5556v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.GR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1212.0030v1</id>\\n    <updated>2012-11-30T22:35:19Z</updated>\\n    <published>2012-11-30T22:35:19Z</published>\\n    <title>Viewpoint Invariant Object Detector</title>\\n    <summary>  Object Detection is the task of identifying the existence of an object class\\ninstance and locating it within an image. Difficulties in handling high\\nintra-class variations constitute major obstacles to achieving high performance\\non standard benchmark datasets (scale, viewpoint, lighting conditions and\\norientation variations provide good examples). Suggested model aims at\\nproviding more robustness to detecting objects suffering severe distortion due\\nto &lt; 60{\\\\deg} viewpoint changes. In addition, several model computational\\nbottlenecks have been resolved leading to a significant increase in the model\\nperformance (speed and space) without compromising the resulting accuracy.\\nFinally, we produced two illustrative applications showing the potential of the\\nobject detection technology being deployed in real life applications; namely\\ncontent-based image search and content-based video search.\\n</summary>\\n    <author>\\n      <name>Osama Khalil</name>\\n    </author>\\n    <author>\\n      <name>Andrew Habib</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1212.0030v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1212.0030v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1212.0134v1</id>\\n    <updated>2012-12-01T16:59:07Z</updated>\\n    <published>2012-12-01T16:59:07Z</published>\\n    <title>Fingertip Detection: A Fast Method with Natural Hand</title>\\n    <summary>  Many vision based applications have used fingertips to track or manipulate\\ngestures in their applications. Gesture identification is a natural way to pass\\nthe signals to the machine, as the human express its feelings most of the time\\nwith hand expressions. Here a novel time efficient algorithm has been described\\nfor fingertip detection. This method is invariant to hand direction and in\\npreprocessing it cuts only hand part from the full image, hence further\\ncomputation would be much faster than processing full image. Binary silhouette\\nof the input image is generated using HSV color space based skin filter and\\nhand cropping done based on intensity histogram of the hand image\\n</summary>\\n    <author>\\n      <name>J. L. Raheja</name>\\n    </author>\\n    <author>\\n      <name>Karen Das</name>\\n    </author>\\n    <author>\\n      <name>Ankit Chaudhary</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">International Journal of Embedded Systems and Computer\\n  Engineering, 2011</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1212.0134v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1212.0134v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1212.0142v2</id>\\n    <updated>2013-04-02T18:05:46Z</updated>\\n    <published>2012-12-01T18:13:03Z</published>\\n    <title>Pedestrian Detection with Unsupervised Multi-Stage Feature Learning</title>\\n    <summary>  Pedestrian detection is a problem of considerable practical interest. Adding\\nto the list of successful applications of deep learning methods to vision, we\\nreport state-of-the-art and competitive results on all major pedestrian\\ndatasets with a convolutional network model. The model uses a few new twists,\\nsuch as multi-stage features, connections that skip layers to integrate global\\nshape information with local distinctive motif information, and an unsupervised\\nmethod based on convolutional sparse coding to pre-train the filters at each\\nstage.\\n</summary>\\n    <author>\\n      <name>Pierre Sermanet</name>\\n    </author>\\n    <author>\\n      <name>Koray Kavukcuoglu</name>\\n    </author>\\n    <author>\\n      <name>Soumith Chintala</name>\\n    </author>\\n    <author>\\n      <name>Yann LeCun</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">12 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1212.0142v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1212.0142v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1212.0402v1</id>\\n    <updated>2012-12-03T14:45:31Z</updated>\\n    <published>2012-12-03T14:45:31Z</published>\\n    <title>UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild</title>\\n    <summary>  We introduce UCF101 which is currently the largest dataset of human actions.\\nIt consists of 101 action classes, over 13k clips and 27 hours of video data.\\nThe database consists of realistic user uploaded videos containing camera\\nmotion and cluttered background. Additionally, we provide baseline action\\nrecognition results on this new dataset using standard bag of words approach\\nwith overall performance of 44.5%. To the best of our knowledge, UCF101 is\\ncurrently the most challenging dataset of actions due to its large number of\\nclasses, large number of clips and also unconstrained nature of such clips.\\n</summary>\\n    <author>\\n      <name>Khurram Soomro</name>\\n    </author>\\n    <author>\\n      <name>Amir Roshan Zamir</name>\\n    </author>\\n    <author>\\n      <name>Mubarak Shah</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1212.0402v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1212.0402v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1212.0888v1</id>\\n    <updated>2012-12-04T21:59:35Z</updated>\\n    <published>2012-12-04T21:59:35Z</published>\\n    <title>Unmixing of Hyperspectral Data Using Robust Statistics-based NMF</title>\\n    <summary>  Mixed pixels are presented in hyperspectral images due to low spatial\\nresolution of hyperspectral sensors. Spectral unmixing decomposes mixed pixels\\nspectra into endmembers spectra and abundance fractions. In this paper using of\\nrobust statistics-based nonnegative matrix factorization (RNMF) for spectral\\nunmixing of hyperspectral data is investigated. RNMF uses a robust cost\\nfunction and iterative updating procedure, so is not sensitive to outliers.\\nThis method has been applied to simulated data using USGS spectral library,\\nAVIRIS and ROSIS datasets. Unmixing results are compared to traditional NMF\\nmethod based on SAD and AAD measures. Results demonstrate that this method can\\nbe used efficiently for hyperspectral unmixing purposes.\\n</summary>\\n    <author>\\n      <name>Roozbeh Rajabi</name>\\n    </author>\\n    <author>\\n      <name>Hassan Ghassemian</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, conference</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1212.0888v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1212.0888v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1212.2546v1</id>\\n    <updated>2012-12-11T17:29:04Z</updated>\\n    <published>2012-12-11T17:29:04Z</published>\\n    <title>A Learning Framework for Morphological Operators using Counter-Harmonic\\n  Mean</title>\\n    <summary>  We present a novel framework for learning morphological operators using\\ncounter-harmonic mean. It combines concepts from morphology and convolutional\\nneural networks. A thorough experimental validation analyzes basic\\nmorphological operators dilation and erosion, opening and closing, as well as\\nthe much more complex top-hat transform, for which we report a real-world\\napplication from the steel industry. Using online learning and stochastic\\ngradient descent, our system learns both the structuring element and the\\ncomposition of operators. It scales well to large datasets and online settings.\\n</summary>\\n    <author>\\n      <name>Jonathan Masci</name>\\n    </author>\\n    <author>\\n      <name>Jes\\xc3\\xbas Angulo</name>\\n    </author>\\n    <author>\\n      <name>J\\xc3\\xbcrgen Schmidhuber</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Submitted to ISMM\\'13</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1212.2546v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1212.2546v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1212.3034v1</id>\\n    <updated>2012-12-13T01:55:15Z</updated>\\n    <published>2012-12-13T01:55:15Z</published>\\n    <title>Multi-target tracking algorithms in 3D</title>\\n    <summary>  Ladars provide a unique capability for identification of objects and motions\\nin scenes with fixed 3D field of view (FOV). This paper describes algorithms\\nfor multi-target tracking in 3D scenes including the preprocessing\\n(mathematical morphology and Parzen windows), labeling of connected components,\\nsorting of targets by selectable attributes (size, length of track, velocity),\\nand handling of target states (acquired, coasting, re-acquired and tracked) in\\norder to assemble the target trajectories. This paper is derived from working\\nalgorithms coded in Matlab, which were tested and reviewed by others, and does\\nnot speculate about usage of general formulas or frameworks.\\n</summary>\\n    <author>\\n      <name>Rastislav Telgarsky</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages, 2 figures, conference proceedings</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Scientific Issues, MATHEMATICA IV, Ruzomberok 2012</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1212.3034v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1212.3034v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"65D18, 68W05\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"G.4; I.2.10; I.4.7; I.4.8; I.4.9; I.5.3\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1212.4608v1</id>\\n    <updated>2012-12-19T09:40:09Z</updated>\\n    <published>2012-12-19T09:40:09Z</published>\\n    <title>Perceptually Motivated Shape Context Which Uses Shape Interiors</title>\\n    <summary>  In this paper, we identify some of the limitations of current-day shape\\nmatching techniques. We provide examples of how contour-based shape matching\\ntechniques cannot provide a good match for certain visually similar shapes. To\\novercome this limitation, we propose a perceptually motivated variant of the\\nwell-known shape context descriptor. We identify that the interior properties\\nof the shape play an important role in object recognition and develop a\\ndescriptor that captures these interior properties. We show that our method can\\neasily be augmented with any other shape matching algorithm. We also show from\\nour experiments that the use of our descriptor can significantly improve the\\nretrieval rates.\\n</summary>\\n    <author>\\n      <name>Vittal Premachandran</name>\\n    </author>\\n    <author>\\n      <name>Ramakrishna Kakarala</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1212.4608v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1212.4608v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1212.5877v2</id>\\n    <updated>2013-03-29T02:49:46Z</updated>\\n    <published>2012-12-24T08:58:02Z</published>\\n    <title>Blinking Molecule Tracking</title>\\n    <summary>  We discuss a method for tracking individual molecules which globally\\noptimizes the likelihood of the connections between molecule positions fast and\\nwith high reliability even for high spot densities and blinking molecules. Our\\nmethod works with cost functions which can be freely chosen to combine costs\\nfor distances between spots in space and time and which can account for the\\nreliability of positioning a molecule. To this end, we describe a top-down\\npolyhedral approach to the problem of tracking many individual molecules. This\\nimmediately yields an effective implementation using standard linear\\nprogramming solvers. Our method can be applied to 2D and 3D tracking.\\n</summary>\\n    <author>\\n      <name>Andreas Karrenbauer</name>\\n    </author>\\n    <author>\\n      <name>Dominik W\\xc3\\xb6ll</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">12th International Symposium on Experimental Algorithms 2013</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1212.5877v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1212.5877v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1301.0435v1</id>\\n    <updated>2013-01-03T12:16:25Z</updated>\\n    <published>2013-01-03T12:16:25Z</published>\\n    <title>Investigating the performance of Correspondence Algorithms in Vision\\n  based Driver-assistance in Indoor Environment</title>\\n    <summary>  This paper presents the experimental comparison of fourteen stereo matching\\nalgorithms in variant illumination conditions. Different adaptations of global\\nand local stereo matching techniques are chosen for evaluation The variant\\nstrength and weakness of the chosen correspondence algorithms are explored by\\nemploying the methodology of the prediction error strategy. The algorithms are\\ngauged on the basis of their performance on real world data set taken in\\nvarious indoor lighting conditions and at different times of the day\\n</summary>\\n    <author>\\n      <name>F. Mahmood</name>\\n    </author>\\n    <author>\\n      <name>Syed. M. B. Haider</name>\\n    </author>\\n    <author>\\n      <name>F. Kunwar</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.5120/9718-3663</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.5120/9718-3663\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages, 9 figures,Published with International Journal of Computer\\n  Applications (IJCA)</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IJCA 60(9):6-12, 2012</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1301.0435v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1301.0435v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1301.0612v1</id>\\n    <updated>2012-12-12T15:59:10Z</updated>\\n    <published>2012-12-12T15:59:10Z</published>\\n    <title>Adaptive Foreground and Shadow Detection inImage Sequences</title>\\n    <summary>  This paper presents a novel method of foreground segmentation that\\ndistinguishes moving objects from their moving cast shadows in monocular image\\nsequences. The models of background, edge information, and shadow are set up\\nand adaptively updated. A Bayesian belief network is proposed to describe the\\nrelationships among the segmentation label, background, intensity, and edge\\ninformation. The notion of Markov random field is used to encourage the spatial\\nconnectivity of the segmented regions. The solution is obtained by maximizing\\nthe posterior possibility density of the segmentation field.\\n</summary>\\n    <author>\\n      <name>Yang Wang</name>\\n    </author>\\n    <author>\\n      <name>Tele Tan</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Appears in Proceedings of the Eighteenth Conference on Uncertainty in\\n  Artificial Intelligence (UAI2002)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1301.0612v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1301.0612v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1301.1671v1</id>\\n    <updated>2013-01-08T20:56:17Z</updated>\\n    <published>2013-01-08T20:56:17Z</published>\\n    <title>Causal graph-based video segmentation</title>\\n    <summary>  Numerous approaches in image processing and computer vision are making use of\\nsuper-pixels as a pre-processing step. Among the different methods producing\\nsuch over-segmentation of an image, the graph-based approach of Felzenszwalb\\nand Huttenlocher is broadly employed. One of its interesting properties is that\\nthe regions are computed in a greedy manner in quasi-linear time. The algorithm\\nmay be trivially extended to video segmentation by considering a video as a 3D\\nvolume, however, this can not be the case for causal segmentation, when\\nsubsequent frames are unknown. We propose an efficient video segmentation\\napproach that computes temporally consistent pixels in a causal manner, filling\\nthe need for causal and real time applications.\\n</summary>\\n    <author>\\n      <name>Camille Couprie</name>\\n    </author>\\n    <author>\\n      <name>Cl\\xc3\\xa9ment Farabet</name>\\n    </author>\\n    <author>\\n      <name>Yann LeCun</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages, 5 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1301.1671v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1301.1671v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1301.2351v1</id>\\n    <updated>2013-01-10T22:57:01Z</updated>\\n    <published>2013-01-10T22:57:01Z</published>\\n    <title>Application of Hopfield Network to Saccades</title>\\n    <summary>  Human eye movement mechanisms (saccades) are very useful for scene analysis,\\nincluding object representation and pattern recognition. In this letter, a\\nHopfield neural network to emulate saccades is proposed. The network uses an\\nenergy function that includes location and identification tasks. Computer\\nsimulation shows that the network performs those tasks cooperatively. The\\nresult suggests that the network is applicable to shift-invariant pattern\\nrecognition.\\n</summary>\\n    <author>\\n      <name>Teruyoshi Washizawa</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/72.286896</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/72.286896\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages, 6 figures</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IEEE Transactions on NEURAL NETWORKS, vol.4, no.6, pp-995-997,\\n  NOVEMBER 1993</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1301.2351v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1301.2351v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-bio.NC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1301.2820v3</id>\\n    <updated>2013-03-13T22:48:38Z</updated>\\n    <published>2013-01-13T20:49:30Z</published>\\n    <title>Clustering Learning for Robotic Vision</title>\\n    <summary>  We present the clustering learning technique applied to multi-layer\\nfeedforward deep neural networks. We show that this unsupervised learning\\ntechnique can compute network filters with only a few minutes and a much\\nreduced set of parameters. The goal of this paper is to promote the technique\\nfor general-purpose robotic vision systems. We report its use in static image\\ndatasets and object tracking datasets. We show that networks trained with\\nclustering learning can outperform large networks trained for many hours on\\ncomplex datasets.\\n</summary>\\n    <author>\\n      <name>Eugenio Culurciello</name>\\n    </author>\\n    <author>\\n      <name>Jordan Bates</name>\\n    </author>\\n    <author>\\n      <name>Aysegul Dundar</name>\\n    </author>\\n    <author>\\n      <name>Jose Carrasco</name>\\n    </author>\\n    <author>\\n      <name>Clement Farabet</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Code for this paper is available here:\\n  https://github.com/culurciello/CL_paper1_code</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1301.2820v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1301.2820v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1301.3385v2</id>\\n    <updated>2013-01-16T14:56:44Z</updated>\\n    <published>2013-01-15T15:34:07Z</published>\\n    <title>Recurrent Online Clustering as a Spatio-Temporal Feature Extractor in\\n  DeSTIN</title>\\n    <summary>  This paper presents a basic enhancement to the DeSTIN deep learning\\narchitecture by replacing the explicitly calculated transition tables that are\\nused to capture temporal features with a simpler, more scalable mechanism. This\\nmechanism uses feedback of state information to cluster over a space comprised\\nof both the spatial input and the current state. The resulting architecture\\nachieves state-of-the-art results on the MNIST classification benchmark.\\n</summary>\\n    <author>\\n      <name>Steven R. Young</name>\\n    </author>\\n    <author>\\n      <name>Itamar Arel</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">3 pages, 2 figures, Submitted to ICLR 2013</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1301.3385v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1301.3385v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1301.3457v2</id>\\n    <updated>2013-04-10T18:32:07Z</updated>\\n    <published>2013-01-15T19:18:52Z</published>\\n    <title>A Geometric Descriptor for Cell-Division Detection</title>\\n    <summary>  We describe a method for cell-division detection based on a geometric-driven\\ndescriptor that can be represented as a 5-layers processing network, based\\nmainly on wavelet filtering and a test for mirror symmetry between pairs of\\npixels. After the centroids of the descriptors are computed for a sequence of\\nframes, the two-steps piecewise constant function that best fits the sequence\\nof centroids determines the frame where the division occurs.\\n</summary>\\n    <author>\\n      <name>Marcelo Cicconet</name>\\n    </author>\\n    <author>\\n      <name>Italo Lima</name>\\n    </author>\\n    <author>\\n      <name>Davi Geiger</name>\\n    </author>\\n    <author>\\n      <name>Kris Gunsalus</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This paper has been withdrawn by the author since the review process\\n  for the conference to which it was applied ended</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1301.3457v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1301.3457v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1301.3572v2</id>\\n    <updated>2013-03-14T18:18:17Z</updated>\\n    <published>2013-01-16T03:31:30Z</published>\\n    <title>Indoor Semantic Segmentation using depth information</title>\\n    <summary>  This work addresses multi-class segmentation of indoor scenes with RGB-D\\ninputs. While this area of research has gained much attention recently, most\\nworks still rely on hand-crafted features. In contrast, we apply a multiscale\\nconvolutional network to learn features directly from the images and the depth\\ninformation. We obtain state-of-the-art on the NYU-v2 depth dataset with an\\naccuracy of 64.5%. We illustrate the labeling of indoor scenes in videos\\nsequences that could be processed in real-time using appropriate hardware such\\nas an FPGA.\\n</summary>\\n    <author>\\n      <name>Camille Couprie</name>\\n    </author>\\n    <author>\\n      <name>Cl\\xc3\\xa9ment Farabet</name>\\n    </author>\\n    <author>\\n      <name>Laurent Najman</name>\\n    </author>\\n    <author>\\n      <name>Yann LeCun</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, 3 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1301.3572v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1301.3572v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1301.3755v1</id>\\n    <updated>2013-01-16T17:05:57Z</updated>\\n    <published>2013-01-16T17:05:57Z</published>\\n    <title>Gradient Driven Learning for Pooling in Visual Pipeline Feature\\n  Extraction Models</title>\\n    <summary>  Hyper-parameter selection remains a daunting task when building a pattern\\nrecognition architecture which performs well, particularly in recently\\nconstructed visual pipeline models for feature extraction. We re-formulate\\npooling in an existing pipeline as a function of adjustable pooling map weight\\nparameters and propose the use of supervised error signals from gradient\\ndescent to tune the established maps within the model. This technique allows us\\nto learn what would otherwise be a design choice within the model and\\nspecialize the maps to aggregate areas of invariance for the task presented.\\nPreliminary results show moderate potential gains in classification accuracy\\nand highlight areas of importance within the intermediate feature\\nrepresentation space.\\n</summary>\\n    <author>\\n      <name>Derek Rose</name>\\n    </author>\\n    <author>\\n      <name>Itamar Arel</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">3 pages, 2 figures, submitted to ICLR2013 workshop</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1301.3755v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1301.3755v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1302.0077v1</id>\\n    <updated>2013-02-01T05:08:58Z</updated>\\n    <published>2013-02-01T05:08:58Z</published>\\n    <title>Sparse MRI for motion correction</title>\\n    <summary>  MR image sparsity/compressibility has been widely exploited for imaging\\nacceleration with the development of compressed sensing. A sparsity-based\\napproach to rigid-body motion correction is presented for the first time in\\nthis paper. A motion is sought after such that the compensated MR image is\\nmaximally sparse/compressible among the infinite candidates. Iterative\\nalgorithms are proposed that jointly estimate the motion and the image content.\\nThe proposed method has a lot of merits, such as no need of additional data and\\nloose requirement for the sampling sequence. Promising results are presented to\\ndemonstrate its performance.\\n</summary>\\n    <author>\\n      <name>Zai Yang</name>\\n    </author>\\n    <author>\\n      <name>Cishen Zhang</name>\\n    </author>\\n    <author>\\n      <name>Lihua Xie</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">To appear in Proceedings of ISBI 2013. 4 pages, 1 figure</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1302.0077v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1302.0077v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.bio-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.med-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1302.1690v1</id>\\n    <updated>2013-02-07T10:17:07Z</updated>\\n    <published>2013-02-07T10:17:07Z</published>\\n    <title>A Fast Learning Algorithm for Image Segmentation with Max-Pooling\\n  Convolutional Networks</title>\\n    <summary>  We present a fast algorithm for training MaxPooling Convolutional Networks to\\nsegment images. This type of network yields record-breaking performance in a\\nvariety of tasks, but is normally trained on a computationally expensive\\npatch-by-patch basis. Our new method processes each training image in a single\\npass, which is vastly more efficient.\\n  We validate the approach in different scenarios and report a 1500-fold\\nspeed-up. In an application to automated steel defect detection and\\nsegmentation, we obtain excellent performance with short training times.\\n</summary>\\n    <author>\\n      <name>Jonathan Masci</name>\\n    </author>\\n    <author>\\n      <name>Alessandro Giusti</name>\\n    </author>\\n    <author>\\n      <name>Dan Cire\\xc5\\x9fan</name>\\n    </author>\\n    <author>\\n      <name>Gabriel Fricout</name>\\n    </author>\\n    <author>\\n      <name>J\\xc3\\xbcrgen Schmidhuber</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1302.1690v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1302.1690v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1302.1700v1</id>\\n    <updated>2013-02-07T10:33:47Z</updated>\\n    <published>2013-02-07T10:33:47Z</published>\\n    <title>Fast Image Scanning with Deep Max-Pooling Convolutional Neural Networks</title>\\n    <summary>  Deep Neural Networks now excel at image classification, detection and\\nsegmentation. When used to scan images by means of a sliding window, however,\\ntheir high computational complexity can bring even the most powerful hardware\\nto its knees. We show how dynamic programming can speedup the process by orders\\nof magnitude, even when max-pooling layers are present.\\n</summary>\\n    <author>\\n      <name>Alessandro Giusti</name>\\n    </author>\\n    <author>\\n      <name>Dan C. Cire\\xc5\\x9fan</name>\\n    </author>\\n    <author>\\n      <name>Jonathan Masci</name>\\n    </author>\\n    <author>\\n      <name>Luca M. Gambardella</name>\\n    </author>\\n    <author>\\n      <name>J\\xc3\\xbcrgen Schmidhuber</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">11 pages, 2 figures, 3 tables, 21 references, submitted to ICIP 2013</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">International Conference on Image Processing (ICIP) 2013,\\n  Melbourne</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1302.1700v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1302.1700v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1302.5056v1</id>\\n    <updated>2013-01-15T18:47:11Z</updated>\\n    <published>2013-01-15T18:47:11Z</published>\\n    <title>Pooling-Invariant Image Feature Learning</title>\\n    <summary>  Unsupervised dictionary learning has been a key component in state-of-the-art\\ncomputer vision recognition architectures. While highly effective methods exist\\nfor patch-based dictionary learning, these methods may learn redundant features\\nafter the pooling stage in a given early vision architecture. In this paper, we\\noffer a novel dictionary learning scheme to efficiently take into account the\\ninvariance of learned features after the spatial pooling stage. The algorithm\\nis built on simple clustering, and thus enjoys efficiency and scalability. We\\ndiscuss the underlying mechanism that justifies the use of clustering\\nalgorithms, and empirically show that the algorithm finds better dictionaries\\nthan patch-based methods with the same dictionary size.\\n</summary>\\n    <author>\\n      <name>Yangqing Jia</name>\\n    </author>\\n    <author>\\n      <name>Oriol Vinyals</name>\\n    </author>\\n    <author>\\n      <name>Trevor Darrell</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1302.5056v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1302.5056v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1302.5894v1</id>\\n    <updated>2013-02-24T10:49:39Z</updated>\\n    <published>2013-02-24T10:49:39Z</published>\\n    <title>Four Side Distance: A New Fourier Shape Signature</title>\\n    <summary>  Shape is one of the main features in content based image retrieval (CBIR).\\nThis paper proposes a new shape signature. In this technique, features of each\\nshape are extracted based on four sides of the rectangle that covers the shape.\\nThe proposed technique is Fourier based and it is invariant to translation,\\nscaling and rotation. The retrieval performance between some commonly used\\nFourier based signatures and the proposed four sides distance (FSD) signature\\nhas been tested using MPEG-7 database. Experimental results are shown that the\\nFSD signature has better performance compared with those signatures.\\n</summary>\\n    <author>\\n      <name>Sonya Eini</name>\\n    </author>\\n    <author>\\n      <name>Abdolah Chalechale</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages, 7 figures, International Journal of Advanced Studies in\\n  Computers, Science and Engineering</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1302.5894v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1302.5894v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1302.5957v1</id>\\n    <updated>2013-02-24T21:38:20Z</updated>\\n    <published>2013-02-24T21:38:20Z</published>\\n    <title>Shape Characterization via Boundary Distortion</title>\\n    <summary>  In this paper, we derive new shape descriptors based on a directional\\ncharacterization. The main idea is to study the behavior of the shape\\nneighborhood under family of transformations. We obtain a description invariant\\nwith respect to rotation, reflection, translation and scaling. A well-defined\\nmetric is then proposed on the associated feature space. We show the continuity\\nof this metric. Some results on shape retrieval are provided on two databases\\nto show the accuracy of the proposed shape metric.\\n</summary>\\n    <author>\\n      <name>Xavier Descombes</name>\\n    </author>\\n    <author>\\n      <name>Serguei Komech</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">14 pages, 5 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1302.5957v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1302.5957v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1302.7082v1</id>\\n    <updated>2013-02-28T04:50:31Z</updated>\\n    <published>2013-02-28T04:50:31Z</published>\\n    <title>K Means Segmentation of Alzheimers Disease in PET scan datasets: An\\n  implementation</title>\\n    <summary>  The Positron Emission Tomography (PET) scan image requires expertise in the\\nsegmentation where clustering algorithm plays an important role in the\\nautomation process. The algorithm optimization is concluded based on the\\nperformance, quality and number of clusters extracted. This paper is proposed\\nto study the commonly used K Means clustering algorithm and to discuss a brief\\nlist of toolboxes for reproducing and extending works presented in medical\\nimage analysis. This work is compiled using AForge .NET framework in windows\\nenvironment and MATrix LABoratory (MATLAB 7.0.1)\\n</summary>\\n    <author>\\n      <name>A. Meena</name>\\n    </author>\\n    <author>\\n      <name>K. Raja</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">International Joint Conference on Advances in Signal Processing and\\n  Information Technology, SPIT2012</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">LNICST, ISSN:1867 To 8211 pp. 158 To 162, 2012</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1302.7082v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1302.7082v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1303.0634v1</id>\\n    <updated>2013-03-04T08:06:07Z</updated>\\n    <published>2013-03-04T08:06:07Z</published>\\n    <title>Indian Sign Language Recognition Using Eigen Value Weighted Euclidean\\n  Distance Based Classification Technique</title>\\n    <summary>  Sign Language Recognition is one of the most growing fields of research\\ntoday. Many new techniques have been developed recently in these fields. Here\\nin this paper, we have proposed a system using Eigen value weighted Euclidean\\ndistance as a classification technique for recognition of various Sign\\nLanguages of India. The system comprises of four parts: Skin Filtering, Hand\\nCropping, Feature Extraction and Classification. Twenty four signs were\\nconsidered in this paper, each having ten samples, thus a total of two hundred\\nforty images was considered for which recognition rate obtained was 97 percent.\\n</summary>\\n    <author>\\n      <name>Joyeeta Singha</name>\\n    </author>\\n    <author>\\n      <name>Karen Das</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1303.0634v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1303.0634v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1303.0635v1</id>\\n    <updated>2013-03-04T08:09:22Z</updated>\\n    <published>2013-03-04T08:09:22Z</published>\\n    <title>Recognition of Facial Expression Using Eigenvector Based Distributed\\n  Features and Euclidean Distance Based Decision Making Technique</title>\\n    <summary>  In this paper, an Eigenvector based system has been presented to recognize\\nfacial expressions from digital facial images. In the approach, firstly the\\nimages were acquired and cropping of five significant portions from the image\\nwas performed to extract and store the Eigenvectors specific to the\\nexpressions. The Eigenvectors for the test images were also computed, and\\nfinally the input facial image was recognized when similarity was obtained by\\ncalculating the minimum Euclidean distance between the test image and the\\ndifferent expressions.\\n</summary>\\n    <author>\\n      <name>Jeemoni Kalita</name>\\n    </author>\\n    <author>\\n      <name>Karen Das</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1303.0635v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1303.0635v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1303.4840v1</id>\\n    <updated>2013-03-20T04:59:08Z</updated>\\n    <published>2013-03-20T04:59:08Z</published>\\n    <title>Asynchronous Cellular Operations on Gray Images Extracting Topographic\\n  Shape Features and Their Relations</title>\\n    <summary>  A variety of operations of cellular automata on gray images is presented. All\\noperations are of a wave-front nature finishing in a stable state. They are\\nused to extract shape descripting gray objects robust to a variety of pattern\\ndistortions. Topographic terms are used: \"lakes\", \"dales\", \"dales of dales\". It\\nis shown how mutual object relations like \"above\" can be presented in terms of\\ngray image analysis and how it can be used for character classification and for\\ngray pattern decomposition. Algorithms can be realized with a parallel\\nasynchronous architecture. Keywords: Pattern Recognition, Mathematical\\nMorphology, Cellular Automata, Wave-front Algorithms, Gray Image Analysis,\\nTopographical Shape Descriptors, Asynchronous Parallel Processors, Holes,\\nCavities, Concavities, Graphs.\\n</summary>\\n    <author>\\n      <name>Igor Polkovnikov</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">19 pages, 37 figures, 10 function classes</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1303.4840v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1303.4840v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1303.4866v1</id>\\n    <updated>2013-03-20T08:15:07Z</updated>\\n    <published>2013-03-20T08:15:07Z</published>\\n    <title>A Robust Rapid Approach to Image Segmentation with Optimal Thresholding\\n  and Watershed Transform</title>\\n    <summary>  This paper describes a novel method for partitioning image into meaningful\\nsegments. The proposed method employs watershed transform, a well-known image\\nsegmentation technique. Along with that, it uses various auxiliary schemes such\\nas Binary Gradient Masking, dilation which segment the image in proper way. The\\nalgorithm proposed in this paper considers all these methods in effective way\\nand takes little time. It is organized in such a manner so that it operates on\\ninput image adaptively. Its robustness and efficiency makes it more convenient\\nand suitable for all types of images.\\n</summary>\\n    <author>\\n      <name>Ankit R. Chadha</name>\\n    </author>\\n    <author>\\n      <name>Neha S. Satam</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.5120/10949-5908</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.5120/10949-5908\" rel=\"related\"/>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">International Journal of Computer Applications (2013)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1303.4866v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1303.4866v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1303.6926v1</id>\\n    <updated>2013-03-27T18:57:12Z</updated>\\n    <published>2013-03-27T18:57:12Z</published>\\n    <title>A Comparative Analysis on the Applicability of Entropy in remote sensing</title>\\n    <summary>  Entropy is the measure of uncertainty in any data and is adopted for\\nmaximisation of mutual information in many remote sensing operations. The\\navailability of wide entropy variations motivated us for an investigation over\\nthe suitability preference of these versions to specific operations.\\nMethodologies were implemented in Matlab and were enhanced with entropy\\nvariations. Evaluation of various implementations was based on different\\nstatistical parameters with reference to the study area The popular available\\nversions like Tsalli\\'s, Shanon\\'s, and Renyi\\'s entropies were analysed in\\ncontext of various remote sensing operations namely thresholding, clustering\\nand registration.\\n</summary>\\n    <author>\\n      <name>Dr. S. K. Katiyar</name>\\n    </author>\\n    <author>\\n      <name>Arun P. V.</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1303.6926v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1303.6926v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1304.0019v1</id>\\n    <updated>2013-03-29T20:32:04Z</updated>\\n    <published>2013-03-29T20:32:04Z</published>\\n    <title>Age group and gender recognition from human facial images</title>\\n    <summary>  This work presents an automatic human gender and age group recognition system\\nbased on human facial images. It makes an extensive experiment with row pixel\\nintensity valued features and Discrete Cosine Transform (DCT) coefficient\\nfeatures with Principal Component Analysis and k-Nearest Neighbor\\nclassification to identify the best recognition approach. The final results\\nshow approaches using DCT coefficient outperform their counter parts resulting\\nin a 99% correct gender recognition rate and 68% correct age group recognition\\nrate (considering four distinct age groups) in unseen test images. Detailed\\nexperimental settings and obtained results are clearly presented and explained\\nin this report.\\n</summary>\\n    <author>\\n      <name>Tizita Nesibu Shewaye</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, October, 2012</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Ethiopian Society of Electrical Engineers 6th Scientific\\n  Conference on Electrical Engineering (CEE-2012)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1304.0019v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1304.0019v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1304.1022v1</id>\\n    <updated>2013-04-03T17:34:20Z</updated>\\n    <published>2013-04-03T17:34:20Z</published>\\n    <title>A software for aging faces applied to ancient marble busts</title>\\n    <summary>  The study and development of software able to show the effect of aging of\\nfaces is one of the tasks of face recognition technologies. Some software\\nsolutions are used for investigations, some others to show the effects of drugs\\non healthy appearance, however some other applications can be proposed for the\\nanalysis of visual arts. Here we use a freely available software, which is\\nproviding interesting results, for the comparison of ancient marble busts. An\\nanalysis of Augustus busts is proposed.\\n</summary>\\n    <author>\\n      <name>Amelia Carolina Sparavigna</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Image processing. Aging faces. Freely available software. Ancient\\n  marble busts. Augustus</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1304.1022v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1304.1022v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1304.1568v1</id>\\n    <updated>2013-04-04T22:07:27Z</updated>\\n    <published>2013-04-04T22:07:27Z</published>\\n    <title>Multiscale Fractal Descriptors Applied to Texture Classification</title>\\n    <summary>  This work proposes the combination of multiscale transform with fractal\\ndescriptors employed in the classification of gray-level texture images. We\\napply the space-scale transform (derivative + Gaussian filter) over the\\nBouligand-Minkowski fractal descriptors, followed by a threshold over the\\nfilter response, aiming at attenuating noise effects caused by the final part\\nof this response. The method is tested in the classification of a well-known\\ndata set (Brodatz) and compared with other classical texture descriptor\\ntechniques. The results demonstrate the advantage of the proposed approach,\\nachieving a higher success rate with a reduced amount of descriptors.\\n</summary>\\n    <author>\\n      <name>Jo\\xc3\\xa3o Batista Florindo</name>\\n    </author>\\n    <author>\\n      <name>Odemir Martinez Bruno</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1088/1742-6596/410/1/012022</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1088/1742-6596/410/1/012022\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages, 4 figures</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Journal of Physics: Conference Series, 410, 012022, 2013</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1304.1568v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1304.1568v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1304.1930v1</id>\\n    <updated>2013-04-06T19:24:40Z</updated>\\n    <published>2013-04-06T19:24:40Z</published>\\n    <title>Client-Driven Content Extraction Associated with Table</title>\\n    <summary>  The goal of the project is to extract content within table in document images\\nbased on learnt patterns. Real-world users i.e., clients first provide a set of\\nkey fields within the table which they think are important. These are first\\nused to represent the graph where nodes are labelled with semantics including\\nother features and edges are attributed with relations. Attributed relational\\ngraph (ARG) is then employed to mine similar graphs from a document image. Each\\nmined graph will represent an item within the table, and hence a set of such\\ngraphs will compose a table. We have validated the concept by using a\\nreal-world industrial problem.\\n</summary>\\n    <author>\\n      <name>K. C. Santosh</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">LORIA</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Abdel Bela\\xc3\\xafd</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">LORIA</arxiv:affiliation>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Machine Vision Applications (2013)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1304.1930v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1304.1930v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1304.5212v1</id>\\n    <updated>2013-04-18T18:41:47Z</updated>\\n    <published>2013-04-18T18:41:47Z</published>\\n    <title>Object Tracking in Videos: Approaches and Issues</title>\\n    <summary>  Mobile object tracking has an important role in the computer vision\\napplications. In this paper, we use a tracked target-based taxonomy to present\\nthe object tracking algorithms. The tracked targets are divided into three\\ncategories: points of interest, appearance and silhouette of mobile objects.\\nAdvantages and limitations of the tracking approaches are also analyzed to find\\nthe future directions in the object tracking domain.\\n</summary>\\n    <author>\\n      <name>Duc Phu Chau</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">INRIA Sophia Antipolis</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Fran\\xc3\\xa7ois Bremond</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">INRIA Sophia Antipolis</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Monique Thonnat</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">INRIA Sophia Antipolis</arxiv:affiliation>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">The International Workshop \"Rencontres UNS-UD\" (RUNSUD) (2013)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1304.5212v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1304.5212v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1304.6933v2</id>\\n    <updated>2013-04-26T08:35:18Z</updated>\\n    <published>2013-04-25T15:14:42Z</published>\\n    <title>Digit Recognition in Handwritten Weather Records</title>\\n    <summary>  This paper addresses the automatic recognition of handwritten temperature\\nvalues in weather records. The localization of table cells is based on line\\ndetection using projection profiles. Further, a stroke-preserving line removal\\nmethod which is based on gradient images is proposed. The presented digit\\nrecognition utilizes features which are extracted using a set of filters and a\\nSupport Vector Machine classifier. It was evaluated on the MNIST and the USPS\\ndataset and our own database with about 17,000 RGB digit images. An accuracy of\\n99.36% per digit is achieved for the entire system using a set of 84 weather\\nrecords.\\n</summary>\\n    <author>\\n      <name>Manuel Keglevic</name>\\n    </author>\\n    <author>\\n      <name>Robert Sablatnig</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Part of the OAGM/AAPR 2013 proceedings (arXiv:1304.1876), 8 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1304.6933v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1304.6933v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1304.6990v1</id>\\n    <updated>2013-04-25T19:44:26Z</updated>\\n    <published>2013-04-25T19:44:26Z</published>\\n    <title>Euclidean Upgrade from a Minimal Number of Segments</title>\\n    <summary>  In this paper, we propose an algebraic approach to upgrade a projective\\nreconstruction to a Euclidean one, and aim at computing the rectifying\\nhomography from a minimal number of 9 segments of known length. Constraints are\\nderived from these segments which yield a set of polynomial equations that we\\nsolve by means of Gr\\\\\"obner bases. We explain how a solver for such a system of\\nequations can be constructed from simplified template data. Moreover, we\\npresent experiments that demonstrate that the given problem can be solved in\\nthis way.\\n</summary>\\n    <author>\\n      <name>Tanja Schilling</name>\\n    </author>\\n    <author>\\n      <name>Tomas Pajdla</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Part of the OAGM/AAPR 2013 proceedings (arXiv:1304.1876)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1304.6990v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1304.6990v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1304.7153v1</id>\\n    <updated>2013-04-26T13:10:22Z</updated>\\n    <published>2013-04-26T13:10:22Z</published>\\n    <title>A Convex Approach for Image Hallucination</title>\\n    <summary>  In this paper we propose a global convex approach for image hallucination.\\nAltering the idea of classical multi image super resolution (SU) systems to\\nsingle image SU, we incorporate aligned images to hallucinate the output. Our\\nwork is based on the paper of Tappen et al. where they use a non-convex model\\nfor image hallucination. In comparison we formulate a convex primal\\noptimization problem and derive a fast converging primal-dual algorithm with a\\nglobal optimal solution. We use a database with face images to incorporate\\nhigh-frequency details to the high-resolution output. We show that we can\\nachieve state-of-the-art results by using a convex approach.\\n</summary>\\n    <author>\\n      <name>Peter Innerhofer</name>\\n    </author>\\n    <author>\\n      <name>Thomas Pock</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">submitted to \\\\\"OAGM-AAPR 2013, 8 pages, 3 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1304.7153v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1304.7153v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1304.7184v1</id>\\n    <updated>2013-04-26T14:33:52Z</updated>\\n    <published>2013-04-26T14:33:52Z</published>\\n    <title>Reading Ancient Coin Legends: Object Recognition vs. OCR</title>\\n    <summary>  Standard OCR is a well-researched topic of computer vision and can be\\nconsidered solved for machine-printed text. However, when applied to\\nunconstrained images, the recognition rates drop drastically. Therefore, the\\nemployment of object recognition-based techniques has become state of the art\\nin scene text recognition applications. This paper presents a scene text\\nrecognition method tailored to ancient coin legends and compares the results\\nachieved in character and word recognition experiments to a standard OCR\\nengine. The conducted experiments show that the proposed method outperforms the\\nstandard OCR engine on a set of 180 cropped coin legend words.\\n</summary>\\n    <author>\\n      <name>Albert Kavelar</name>\\n    </author>\\n    <author>\\n      <name>Sebastian Zambanini</name>\\n    </author>\\n    <author>\\n      <name>Martin Kampel</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Part of the OAGM/AAPR 2013 proceedings (arXiv:1304.1876)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1304.7184v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1304.7184v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1304.7399v1</id>\\n    <updated>2013-04-27T19:24:30Z</updated>\\n    <published>2013-04-27T19:24:30Z</published>\\n    <title>Bingham Procrustean Alignment for Object Detection in Clutter</title>\\n    <summary>  A new system for object detection in cluttered RGB-D images is presented. Our\\nmain contribution is a new method called Bingham Procrustean Alignment (BPA) to\\nalign models with the scene. BPA uses point correspondences between oriented\\nfeatures to derive a probability distribution over possible model poses. The\\norientation component of this distribution, conditioned on the position, is\\nshown to be a Bingham distribution. This result also applies to the classic\\nproblem of least-squares alignment of point sets, when point features are\\norientation-less, and gives a principled, probabilistic way to measure pose\\nuncertainty in the rigid alignment problem. Our detection system leverages BPA\\nto achieve more reliable object detections in clutter.\\n</summary>\\n    <author>\\n      <name>Jared Glover</name>\\n    </author>\\n    <author>\\n      <name>Sanja Popovic</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Submitted to IROS 2013</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1304.7399v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1304.7399v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.AP\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1305.0020v1</id>\\n    <updated>2013-04-30T20:28:37Z</updated>\\n    <published>2013-04-30T20:28:37Z</published>\\n    <title>Image Compression By Embedding Five Modulus Method Into JPEG</title>\\n    <summary>  The standard JPEG format is almost the optimum format in image compression.\\nThe compression ratio in JPEG sometimes reaches 30:1. The compression ratio of\\nJPEG could be increased by embedding the Five Modulus Method (FMM) into the\\nJPEG algorithm. The novel algorithm gives twice the time as the standard JPEG\\nalgorithm or more. The novel algorithm was called FJPEG (Five-JPEG). The\\nquality of the reconstructed image after compression is approximately\\napproaches the JPEG. Standard test images have been used to support and\\nimplement the suggested idea in this paper and the error metrics have been\\ncomputed and compared with JPEG.\\n</summary>\\n    <author>\\n      <name>Firas A. Jassim</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.5121/sipij.2013.4203</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.5121/sipij.2013.4203\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages, 6 tables, 6 figures</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Signal &amp; Image Processing : An International Journal (SIPIJ)\\n  Vol.4, No.2, April 2013</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1305.0020v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1305.0020v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.MM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1305.3939v1</id>\\n    <updated>2013-05-16T21:25:54Z</updated>\\n    <published>2013-05-16T21:25:54Z</published>\\n    <title>Analysis Of Interest Points Of Curvelet Coefficients Contributions Of\\n  Microscopic Images And Improvement Of Edges</title>\\n    <summary>  This paper focuses on improved edge model based on Curvelet coefficients\\nanalysis. Curvelet transform is a powerful tool for multiresolution\\nrepresentation of object with anisotropic edge. Curvelet coefficients\\ncontributions have been analyzed using Scale Invariant Feature Transform\\n(SIFT), commonly used to study local structure in images. The permutation of\\nCurvelet coefficients from original image and edges image obtained from\\ngradient operator is used to improve original edges. Experimental results show\\nthat this method brings out details on edges when the decomposition scale\\nincreases.\\n</summary>\\n    <author>\\n      <name>A. Djimeli</name>\\n    </author>\\n    <author>\\n      <name>D. Tchiotsop</name>\\n    </author>\\n    <author>\\n      <name>R. Tchinda</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.5121/sipij.2013.4201</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.5121/sipij.2013.4201\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages, 7 figures</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Signal &amp; Image Processing : An International Journal (SIPIJ)\\n  Vol.4, No.2, April 2013</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1305.3939v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1305.3939v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1305.4298v1</id>\\n    <updated>2013-05-18T20:45:40Z</updated>\\n    <published>2013-05-18T20:45:40Z</published>\\n    <title>Blockwise SURE Shrinkage for Non-Local Means</title>\\n    <summary>  In this letter, we investigate the shrinkage problem for the non-local means\\n(NLM) image denoising. In particular, we derive the closed-form of the optimal\\nblockwise shrinkage for NLM that minimizes the Stein\\'s unbiased risk estimator\\n(SURE). We also propose a constant complexity algorithm allowing fast blockwise\\nshrinkage. Simulation results show that the proposed blockwise shrinkage method\\nimproves NLM performance in attaining higher peak signal noise ratio (PSNR) and\\nstructural similarity index (SSIM), and makes NLM more robust against parameter\\nchanges. Similar ideas can be applicable to other patchwise image denoising\\ntechniques.\\n</summary>\\n    <author>\\n      <name>Yue Wu</name>\\n    </author>\\n    <author>\\n      <name>Brian Tracey</name>\\n    </author>\\n    <author>\\n      <name>Premkumar Natarajan</name>\\n    </author>\\n    <author>\\n      <name>Joseph P. Noonan</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1016/j.sigpro.2014.01.007</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1016/j.sigpro.2014.01.007\" rel=\"related\"/>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Signal Processing 103 (2014): 45-59</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1305.4298v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1305.4298v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1305.5160v1</id>\\n    <updated>2013-05-22T15:00:43Z</updated>\\n    <published>2013-05-22T15:00:43Z</published>\\n    <title>A novel automatic thresholding segmentation method with local adaptive\\n  thresholds</title>\\n    <summary>  A novel method for segmenting bright objects from dark background for\\ngrayscale image is proposed. The concept of this method can be stated simply\\nas: to pick out the local-thinnest bands on the grayscale grade-map. It turns\\nout to be a threshold-based method with local adaptive thresholds, where each\\nlocal threshold is determined by requiring the average normal-direction\\ngradient on the object boundary to be local minimal. The method is highly\\nautomatic and the segmentation mimics a man\\'s natural expectation even the\\nobject boundaries are fuzzy.\\n</summary>\\n    <author>\\n      <name>Bo Xiao</name>\\n    </author>\\n    <author>\\n      <name>Yuefeng Jing</name>\\n    </author>\\n    <author>\\n      <name>Yonghong Guan</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">3 pages, 6 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1305.5160v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1305.5160v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1305.6883v1</id>\\n    <updated>2013-05-29T17:58:04Z</updated>\\n    <published>2013-05-29T17:58:04Z</published>\\n    <title>Rotation invariants of two dimensional curves based on iterated\\n  integrals</title>\\n    <summary>  We introduce a novel class of rotation invariants of two dimensional curves\\nbased on iterated integrals. The invariants we present are in some sense\\ncomplete and we describe an algorithm to calculate them, giving explicit\\ncomputations up to order six. We present an application to online\\n(stroke-trajectory based) character recognition. This seems to be the first\\ntime in the literature that the use of iterated integrals of a curve is\\nproposed for (invariant) feature extraction in machine learning applications.\\n</summary>\\n    <author>\\n      <name>Joscha Diehl</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1305.6883v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1305.6883v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1306.1462v1</id>\\n    <updated>2013-06-06T16:30:57Z</updated>\\n    <published>2013-06-06T16:30:57Z</published>\\n    <title>K-Algorithm A Modified Technique for Noise Removal in Handwritten\\n  Documents</title>\\n    <summary>  OCR has been an active research area since last few decades. OCR performs the\\nrecognition of the text in the scanned document image and converts it into\\neditable form. The OCR process can have several stages like pre-processing,\\nsegmentation, recognition and post processing. The pre-processing stage is a\\ncrucial stage for the success of OCR, which mainly deals with noise removal. In\\nthe present paper, a modified technique for noise removal named as K-Algorithm\\nhas been proposed, which has two stages as filtering and binarization. The\\nproposed technique shows improvised results in comparison to median filtering\\ntechnique.\\n</summary>\\n    <author>\\n      <name>Kanika Bansal</name>\\n    </author>\\n    <author>\\n      <name>Rajiv Kumar</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.5121/ijist.2013.3301</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.5121/ijist.2013.3301\" rel=\"related\"/>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">International Journal of Information Sciences and Techniques, May\\n  2013, Volume 3, Number 3</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1306.1462v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1306.1462v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1306.1676v1</id>\\n    <updated>2013-06-07T09:55:29Z</updated>\\n    <published>2013-06-07T09:55:29Z</published>\\n    <title>Algebraic foundations of split hypercomplex nonlinear adaptive filtering</title>\\n    <summary>  A split hypercomplex learning algorithm for the training of nonlinear finite\\nimpulse response adaptive filters for the processing of hypercomplex signals of\\nany dimension is proposed. The derivation strictly takes into account the laws\\nof hypercomplex algebra and hypercomplex calculus, some of which have been\\nneglected in existing learning approaches (e.g. for quaternions). Already in\\nthe case of quaternions we can predict improvements in performance of\\nhypercomplex processes. The convergence of the proposed algorithms is\\nrigorously analyzed.\\n  Keywords: Quaternionic adaptive filtering, Hypercomplex adaptive filtering,\\nNonlinear adaptive filtering, Hypercomplex Multilayer Perceptron, Clifford\\ngeometric algebra\\n</summary>\\n    <author>\\n      <name>Eckhard Hitzer</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1002/mma.2660</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1002/mma.2660\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">14 pages, 1 figure</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Mathematical Methods in the Applied Sciences, Volume 36, Issue 9,\\n  pages 1042-1055, June 2013</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1306.1676v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1306.1676v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.RA\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"60G35, 15A66\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1306.2159v1</id>\\n    <updated>2013-06-10T10:35:26Z</updated>\\n    <published>2013-06-10T10:35:26Z</published>\\n    <title>Image segmentation by optimal and hierarchical piecewise constant\\n  approximations</title>\\n    <summary>  Piecewise constant image approximations of sequential number of segments or\\nclusters of disconnected pixels are treated. The method of majorizing of\\noptimal approximation sequence by hierarchical sequence of image approximations\\nis proposed. A generalization for multidimensional case of color and\\nmultispectral images is foreseen.\\n</summary>\\n    <author>\\n      <name>M. Kharinov</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, 5 formulas, 3 figures, 1 table, submitted to the Eleventh\\n  International Conference on Pattern Recognition and Image Analysis September\\n  23-28, 2013, Samara, Russia</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proc. of the 11-th. Int. Conf. (PRIA-11-2013), Russia, Samara,\\n  September 23-28, 2013. Vol. 1, pp. 213-216</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1306.2159v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1306.2159v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1306.2624v1</id>\\n    <updated>2013-06-11T19:41:45Z</updated>\\n    <published>2013-06-11T19:41:45Z</published>\\n    <title>Stopping Criterion for the Mean Shift Iterative Algorithm</title>\\n    <summary>  Image segmentation is a critical step in computer vision tasks constituting\\nan essential issue for pattern recognition and visual interpretation. In this\\npaper, we propose a new stopping criterion for the mean shift iterative\\nalgorithm by using images defined in Zn ring, with the goal of reaching a\\nbetter segmentation. We carried out also a study on the weak and strong of\\nequivalence classes between two images. An analysis on the convergence with\\nthis new stopping criterion is carried out too.\\n</summary>\\n    <author>\\n      <name>Yasel Garc\\xc3\\xa9s Su\\xc3\\xa1rez</name>\\n    </author>\\n    <author>\\n      <name>Esley Torres</name>\\n    </author>\\n    <author>\\n      <name>Osvaldo Pereira</name>\\n    </author>\\n    <author>\\n      <name>Claudia P\\xc3\\xa9rez</name>\\n    </author>\\n    <author>\\n      <name>Roberto Rogr\\xc3\\xadguez</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Have 8 pages. Is the first version of the more general paper</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1306.2624v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1306.2624v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.RA\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1306.3032v1</id>\\n    <updated>2013-06-13T06:28:07Z</updated>\\n    <published>2013-06-13T06:28:07Z</published>\\n    <title>A Face-like Structure Detection on Planet and Satellite Surfaces using\\n  Image Processing</title>\\n    <summary>  This paper demonstrates that face-like structures are everywhere, and can be\\nde-tected automatically even with computers. Huge amount of satellite images of\\nthe Earth, the Moon, the Mars are explored and many interesting face-like\\nstructure are detected. Throughout this fact, we believe that science and\\ntechnologies can alert people not to easily become an occultist.\\n</summary>\\n    <author>\\n      <name>Kazutaka Kurihara</name>\\n    </author>\\n    <author>\\n      <name>Masakazu Takasu</name>\\n    </author>\\n    <author>\\n      <name>Kazuhiro Sasao</name>\\n    </author>\\n    <author>\\n      <name>Hal Seki</name>\\n    </author>\\n    <author>\\n      <name>Takayuki Narabu</name>\\n    </author>\\n    <author>\\n      <name>Mitsuo Yamamoto</name>\\n    </author>\\n    <author>\\n      <name>Satoshi Iida</name>\\n    </author>\\n    <author>\\n      <name>Hiroyuki Yamamoto</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">ACE 2013, LNCS 8253, Springer, pp. 564-567, 2013</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1306.3032v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1306.3032v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1306.4345v1</id>\\n    <updated>2013-06-18T20:38:07Z</updated>\\n    <published>2013-06-18T20:38:07Z</published>\\n    <title>An Overview of the Research on Texture Based Plant Leaf Classification</title>\\n    <summary>  Plant classification has a broad application prospective in agriculture and\\nmedicine, and is especially significant to the biology diversity research. As\\nplants are vitally important for environmental protection, it is more important\\nto identify and classify them accurately. Plant leaf classification is a\\ntechnique where leaf is classified based on its different morphological\\nfeatures. The goal of this paper is to provide an overview of different aspects\\nof texture based plant leaf classification and related things. At last we will\\nbe concluding about the efficient method i.e. the method that gives better\\nperformance compared to the other methods.\\n</summary>\\n    <author>\\n      <name>Vishakha Metre</name>\\n    </author>\\n    <author>\\n      <name>Jayshree Ghorpade</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">12 pages,5 figures and 3 tables</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1306.4345v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1306.4345v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1306.6263v2</id>\\n    <updated>2016-04-21T00:04:34Z</updated>\\n    <published>2013-06-26T14:56:00Z</published>\\n    <title>Persian Heritage Image Binarization Competition (PHIBC 2012)</title>\\n    <summary>  The first competition on the binarization of historical Persian documents and\\nmanuscripts (PHIBC 2012) has been organized in conjunction with the first\\nIranian conference on pattern recognition and image analysis (PRIA 2013). The\\nmain objective of PHIBC 2012 is to evaluate performance of the binarization\\nmethodologies, when applied on the Persian heritage images. This paper provides\\na report on the methodology and performance of the three submitted algorithms\\nbased on evaluation measures has been used.\\n</summary>\\n    <author>\\n      <name>Seyed Morteza Ayatollahi</name>\\n    </author>\\n    <author>\\n      <name>Hossein Ziaei Nafchi</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/PRIA.2013.6528442</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/PRIA.2013.6528442\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, 2 figures, conference</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1306.6263v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1306.6263v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1307.0998v3</id>\\n    <updated>2014-07-01T00:24:20Z</updated>\\n    <published>2013-07-03T12:59:53Z</published>\\n    <title>A Unified Framework of Elementary Geometric Transformation\\n  Representation</title>\\n    <summary>  As an extension of projective homology, stereohomology is proposed via an\\nextension of Desargues theorem and the extended Desargues configuration.\\nGeometric transformations such as reflection, translation, central symmetry,\\ncentral projection, parallel projection, shearing, central dilation, scaling,\\nand so on are all included in stereohomology and represented as\\nHouseholder-Chen elementary matrices. Hence all these geometric transformations\\nare called elementary. This makes it possible to represent these elementary\\ngeometric transformations in homogeneous square matrices independent of a\\nparticular choice of coordinate system.\\n</summary>\\n    <author>\\n      <name>F. Lu</name>\\n    </author>\\n    <author>\\n      <name>Z. Chen</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">26 pages, 11 figures, 1 table, 21 referneces</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1307.0998v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1307.0998v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1307.2440v1</id>\\n    <updated>2013-07-09T13:14:11Z</updated>\\n    <published>2013-07-09T13:14:11Z</published>\\n    <title>Image Fusion Technologies In Commercial Remote Sensing Packages</title>\\n    <summary>  Several remote sensing software packages are used to the explicit purpose of\\nanalyzing and visualizing remotely sensed data, with the developing of remote\\nsensing sensor technologies from last ten years. Accord-ing to literature, the\\nremote sensing is still the lack of software tools for effective information\\nextraction from remote sensing data. So, this paper provides a state-of-art of\\nmulti-sensor image fusion technologies as well as review on the quality\\nevaluation of the single image or fused images in the commercial remote sensing\\npack-ages. It also introduces program (ALwassaiProcess) developed for image\\nfusion and classification.\\n</summary>\\n    <author>\\n      <name>Firouz Abdullah Al-Wassai</name>\\n    </author>\\n    <author>\\n      <name>N. V. Kalyankar</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Keywords: Commercial Processing Systems, Image Fusion, quality\\n  evaluation</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Journal of Global Research in Computer Science, 4 (5), May 2013,\\n  44-50</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1307.2440v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1307.2440v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1307.3043v2</id>\\n    <updated>2013-09-13T15:48:23Z</updated>\\n    <published>2013-07-11T10:07:19Z</published>\\n    <title>A two-layer Conditional Random Field for the classification of partially\\n  occluded objects</title>\\n    <summary>  Conditional Random Fields (CRF) are among the most popular techniques for\\nimage labelling because of their flexibility in modelling dependencies between\\nthe labels and the image features. This paper proposes a novel CRF-framework\\nfor image labeling problems which is capable to classify partially occluded\\nobjects. Our approach is evaluated on aerial near-vertical images as well as on\\nurban street-view images and compared with another methods.\\n</summary>\\n    <author>\\n      <name>Sergey Kosov</name>\\n    </author>\\n    <author>\\n      <name>Pushmeet Kohli</name>\\n    </author>\\n    <author>\\n      <name>Franz Rottensteiner</name>\\n    </author>\\n    <author>\\n      <name>Christian Heipke</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Conference Submission</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1307.3043v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1307.3043v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1307.3271v1</id>\\n    <updated>2013-07-11T21:01:23Z</updated>\\n    <published>2013-07-11T21:01:23Z</published>\\n    <title>Fuzzy Fibers: Uncertainty in dMRI Tractography</title>\\n    <summary>  Fiber tracking based on diffusion weighted Magnetic Resonance Imaging (dMRI)\\nallows for noninvasive reconstruction of fiber bundles in the human brain. In\\nthis chapter, we discuss sources of error and uncertainty in this technique,\\nand review strategies that afford a more reliable interpretation of the\\nresults. This includes methods for computing and rendering probabilistic\\ntractograms, which estimate precision in the face of measurement noise and\\nartifacts. However, we also address aspects that have received less attention\\nso far, such as model selection, partial voluming, and the impact of\\nparameters, both in preprocessing and in fiber tracking itself. We conclude by\\ngiving impulses for future research.\\n</summary>\\n    <author>\\n      <name>Thomas Schultz</name>\\n    </author>\\n    <author>\\n      <name>Anna Vilanova</name>\\n    </author>\\n    <author>\\n      <name>Ralph Brecheisen</name>\\n    </author>\\n    <author>\\n      <name>Gordon Kindlmann</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1307.3271v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1307.3271v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1307.6549v1</id>\\n    <updated>2013-07-19T10:57:31Z</updated>\\n    <published>2013-07-19T10:57:31Z</published>\\n    <title>Making Laplacians commute</title>\\n    <summary>  In this paper, we construct multimodal spectral geometry by finding a pair of\\nclosest commuting operators (CCO) to a given pair of Laplacians. The CCOs are\\njointly diagonalizable and hence have the same eigenbasis. Our construction\\nnaturally extends classical data analysis tools based on spectral geometry,\\nsuch as diffusion maps and spectral clustering. We provide several synthetic\\nand real examples of applications in dimensionality reduction, shape analysis,\\nand clustering, demonstrating that our method better captures the inherent\\nstructure of multi-modal data.\\n</summary>\\n    <author>\\n      <name>Michael M. Bronstein</name>\\n    </author>\\n    <author>\\n      <name>Klaus Glashoff</name>\\n    </author>\\n    <author>\\n      <name>Terry A. Loring</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1307.6549v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1307.6549v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.GR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1307.6962v2</id>\\n    <updated>2013-12-01T20:46:41Z</updated>\\n    <published>2013-07-26T09:06:44Z</published>\\n    <title>Reduced egomotion estimation drift using omnidirectional views</title>\\n    <summary>  Estimation of camera motion from a given image sequence becomes degraded as\\nthe length of the sequence increases. In this letter, this phenomenon is\\ndemonstrated and an approach to increase the estimation accuracy is proposed.\\nThe proposed method uses an omnidirectional camera in addition to the\\nperspective one and takes advantage of its enlarged view by exploiting the\\ncorrespondences between the omnidirectional and perspective images. Simulated\\nand real image experiments show that the proposed approach improves the\\nestimation accuracy.\\n</summary>\\n    <author>\\n      <name>Yalin Bastanlar</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Another publisher does not want this article to be shared at\\n  arxiv.org in order to publish it</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Electronic Letters on Computer Vision and Image Analysis,\\n  vol.13(3), p.1-12, 2014</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1307.6962v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1307.6962v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1307.7848v1</id>\\n    <updated>2013-07-30T07:18:20Z</updated>\\n    <published>2013-07-30T07:18:20Z</published>\\n    <title>An Integrated System for 3D Gaze Recovery and Semantic Analysis of Human\\n  Attention</title>\\n    <summary>  This work describes a computer vision system that enables pervasive mapping\\nand monitoring of human attention. The key contribution is that our methodology\\nenables full 3D recovery of the gaze pointer, human view frustum and associated\\nhuman centered measurements directly into an automatically computed 3D model in\\nreal-time. We apply RGB-D SLAM and descriptor matching methodologies for the 3D\\nmodeling, localization and fully automated annotation of ROIs (regions of\\ninterest) within the acquired 3D model. This innovative methodology will open\\nnew avenues for attention studies in real world environments, bringing new\\npotential into automated processing for human factors technologies.\\n</summary>\\n    <author>\\n      <name>Lucas Paletta</name>\\n    </author>\\n    <author>\\n      <name>Katrin Santner</name>\\n    </author>\\n    <author>\\n      <name>Gerald Fritz</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1307.7848v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1307.7848v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1307.8233v1</id>\\n    <updated>2013-07-31T06:24:28Z</updated>\\n    <published>2013-07-31T06:24:28Z</published>\\n    <title>A Prototyping Environment for Integrated Artificial Attention Systems</title>\\n    <summary>  Artificial visual attention systems aim to support technical systems in\\nvisual tasks by applying the concepts of selective attention observed in humans\\nand other animals. Such systems are typically evaluated against ground truth\\nobtained from human gaze-data or manually annotated test images. When applied\\nto robotics, the systems are required to be adaptable to the target system.\\nHere, we describe a flexible environment based on a robotic middleware layer\\nallowing the development and testing of attention-guided vision systems. In\\nsuch a framework, the systems can be tested with input from various sources,\\ndifferent attention algorithms at the core, and diverse subsequent tasks.\\n</summary>\\n    <author>\\n      <name>Jan T\\x7f\\xc3\\xbcnnermann</name>\\n    </author>\\n    <author>\\n      <name>Markus Hennig</name>\\n    </author>\\n    <author>\\n      <name>Michael Silbernagel</name>\\n    </author>\\n    <author>\\n      <name>B\\x7f\\xc3\\xa4rbel Mertsching</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1307.8233v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1307.8233v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1307.8405v1</id>\\n    <updated>2013-07-31T17:53:10Z</updated>\\n    <published>2013-07-31T17:53:10Z</published>\\n    <title>Who and Where: People and Location Co-Clustering</title>\\n    <summary>  In this paper, we consider the clustering problem on images where each image\\ncontains patches in people and location domains. We exploit the correlation\\nbetween people and location domains, and proposed a semi-supervised\\nco-clustering algorithm to cluster images. Our algorithm updates the\\ncorrelation links at the runtime, and produces clustering in both domains\\nsimultaneously. We conduct experiments in a manually collected dataset and a\\nFlickr dataset. The result shows that the such correlation improves the\\nclustering performance.\\n</summary>\\n    <author>\\n      <name>Zixuan Wang</name>\\n    </author>\\n    <author>\\n      <name>Jinyun Yan</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">2013 IEEE International Conference on Image Processing</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1307.8405v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1307.8405v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1308.2292v1</id>\\n    <updated>2013-08-10T08:19:02Z</updated>\\n    <published>2013-08-10T08:19:02Z</published>\\n    <title>Fast image segmentation and restoration using parametric curve evolution\\n  with junctions and topology changes</title>\\n    <summary>  Curve evolution schemes for image segmentation based on a region based\\ncontour model allowing for junctions, vector-valued images and topology changes\\nare introduced. Together with an a posteriori denoising in the segmented\\nhomogeneous regions this leads to a fast and efficient method for image\\nsegmentation and restoration. An uneven spread of mesh points is avoided by\\nusing the tangential degrees of freedom. Several numerical simulations on\\nartificial test problems and on real images illustrate the performance of the\\nmethod.\\n</summary>\\n    <author>\\n      <name>Heike Benninghoff</name>\\n    </author>\\n    <author>\\n      <name>Harald Garcke</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">26 pages, 16 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1308.2292v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1308.2292v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.AP\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.NA\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"94A08, 68U10, 65K10, 35K55, 49Q10\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1308.4902v1</id>\\n    <updated>2013-08-22T15:38:15Z</updated>\\n    <published>2013-08-22T15:38:15Z</published>\\n    <title>A review on handwritten character and numeral recognition for Roman,\\n  Arabic, Chinese and Indian scripts</title>\\n    <summary>  There are a lot of intensive researches on handwritten character recognition\\n(HCR) for almost past four decades. The research has been done on some of\\npopular scripts such as Roman, Arabic, Chinese and Indian. In this paper we\\npresent a review on HCR work on the four popular scripts. We have summarized\\nmost of the published paper from 2005 to recent and also analyzed the various\\nmethods in creating a robust HCR system. We also added some future direction of\\nresearch on HCR.\\n</summary>\\n    <author>\\n      <name>Aini Najwa Azmi</name>\\n    </author>\\n    <author>\\n      <name>Dewi Nasien</name>\\n    </author>\\n    <author>\\n      <name>Siti Mariyam Shamsuddin</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">International Journal of advanced studies in Computers, Science &amp;\\n  Engineering (IJASCSE), Volume 2, Issue 4, 2013</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1308.4902v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1308.4902v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1308.5315v1</id>\\n    <updated>2013-08-24T11:07:05Z</updated>\\n    <published>2013-08-24T11:07:05Z</published>\\n    <title>Edge-detection applied to moving sand dunes on Mars</title>\\n    <summary>  Here we discuss the application of an edge detection filter, the Sobel filter\\nof GIMP, to the recently discovered motion of some sand dunes on Mars. The\\nfilter allows a good comparison of an image HiRISE of 2007 and an image of 1999\\nrecorded by the Mars Global Surveyor of the dunes in the Nili Patera caldera,\\nmeasuring therefore the motion of the dunes on a longer period of time than\\nthat previously investigated.\\n</summary>\\n    <author>\\n      <name>Amelia Carolina Sparavigna</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.18483/ijSci.251</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.18483/ijSci.251\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Keywords: Edge detection, Sobel filter, GIMP, Image processing,\\n  Google Mars, Dune motion, Mars Reconnaissance Orbiter, Mars Global Surveyor;\\n  Ref.14 available at\\n  http://www.scribd.com/doc/162390676/Moving-Sand-Dunes-on-Mars</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">International Journal of Sciences, 2013, 2(8):102-104</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1308.5315v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1308.5315v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1308.5876v1</id>\\n    <updated>2013-08-27T13:57:16Z</updated>\\n    <published>2013-08-27T13:57:16Z</published>\\n    <title>Hierarchized block wise image approximation by greedy pursuit strategies</title>\\n    <summary>  An approach for effective implementation of greedy selection methodologies,\\nto approximate an image partitioned into blocks, is proposed. The method is\\nspecially designed for approximating partitions on a transformed image. It\\nevolves by selecting, at each iteration step, i) the elements for approximating\\neach of the blocks partitioning the image and ii) the hierarchized sequence in\\nwhich the blocks are approximated to reach the required global condition on\\nsparsity.\\n</summary>\\n    <author>\\n      <name>Laura Rebollo-Neira</name>\\n    </author>\\n    <author>\\n      <name>Ryszard Maciol</name>\\n    </author>\\n    <author>\\n      <name>Shabnam Bibi</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/LSP.2013.2283510</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/LSP.2013.2283510\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages. An example and the computing routines for implementing the\\n  approach are available on\\n  http://www.nonlinear-approx.info/examples/node0.html</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1308.5876v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1308.5876v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68U10, 94A08\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"G.1.2\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1309.0123v2</id>\\n    <updated>2013-10-02T01:53:49Z</updated>\\n    <published>2013-08-31T14:29:52Z</published>\\n    <title>A Robust Alternating Direction Method for Constrained Hybrid Variational\\n  Deblurring Model</title>\\n    <summary>  In this work, a new constrained hybrid variational deblurring model is\\ndeveloped by combining the non-convex first- and second-order total variation\\nregularizers. Moreover, a box constraint is imposed on the proposed model to\\nguarantee high deblurring performance. The developed constrained hybrid\\nvariational model could achieve a good balance between preserving image details\\nand alleviating ringing artifacts. In what follows, we present the\\ncorresponding numerical solution by employing an iteratively reweighted\\nalgorithm based on alternating direction method of multipliers. The\\nexperimental results demonstrate the superior performance of the proposed\\nmethod in terms of quantitative and qualitative image quality assessments.\\n</summary>\\n    <author>\\n      <name>Ryan Wen Liu</name>\\n    </author>\\n    <author>\\n      <name>Tian Xu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, 5 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1309.0123v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1309.0123v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"65K10, 68U10\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4.4; G.1.6\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1309.3418v1</id>\\n    <updated>2013-09-13T10:12:22Z</updated>\\n    <published>2013-09-13T10:12:22Z</published>\\n    <title>A Novel Approach in detecting pose orientation of a 3D face required for\\n  face</title>\\n    <summary>  In this paper we present a novel approach that takes as input a 3D image and\\ngives as output its pose i.e. it tells whether the face is oriented with\\nrespect the X, Y or Z axes with angles of rotation up to 40 degree. All the\\nexperiments have been performed on the FRAV3D Database. After applying the\\nproposed algorithm to the 3D facial surface we have obtained i.e. on 848 3D\\nface images our method detected the pose correctly for 566 face images,thus\\ngiving an approximately 67 % of correct pose detection.\\n</summary>\\n    <author>\\n      <name>Parama Bagchi</name>\\n    </author>\\n    <author>\\n      <name>Debotosh Bhattacharjee</name>\\n    </author>\\n    <author>\\n      <name>Mita Nasipuri</name>\\n    </author>\\n    <author>\\n      <name>Dipak Kumar Basu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1309.3418v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1309.3418v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1309.3425v1</id>\\n    <updated>2013-09-13T11:28:29Z</updated>\\n    <published>2013-09-13T11:28:29Z</published>\\n    <title>A method for nose-tip based 3D face registration using maximum intensity\\n  algorithm</title>\\n    <summary>  In this paper we present a novel technique of registering 3D images across\\npose. In this context, we have taken into account the images which are aligned\\nacross X, Y and Z axes. We have first determined the angle across which the\\nimage is rotated with respect to X, Y and Z axes and then translation is\\nperformed on the images. After testing the proposed method on 472 images from\\nthe FRAV3D database, the method correctly registers 358 images thus giving a\\nperformance rate of 75.84%.\\n</summary>\\n    <author>\\n      <name>Parama Bagchi</name>\\n    </author>\\n    <author>\\n      <name>Debotosh Bhattacharjee</name>\\n    </author>\\n    <author>\\n      <name>Mita Nasipuri</name>\\n    </author>\\n    <author>\\n      <name>Dipak kr. Basu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1309.3425v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1309.3425v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1310.0310v1</id>\\n    <updated>2013-10-01T14:15:48Z</updated>\\n    <published>2013-10-01T14:15:48Z</published>\\n    <title>A Novel Georeferenced Dataset for Stereo Visual Odometry</title>\\n    <summary>  In this work, we present a novel dataset for assessing the accuracy of stereo\\nvisual odometry. The dataset has been acquired by a small-baseline stereo rig\\nmounted on the top of a moving car. The groundtruth is supplied by a consumer\\ngrade GPS device without IMU. Synchronization and alignment between GPS\\nreadings and stereo frames are recovered after the acquisition. We show that\\nthe attained groundtruth accuracy allows to draw useful conclusions in\\npractice. The presented experiments address influence of camera calibration,\\nbaseline distance and zero-disparity features to the achieved reconstruction\\nperformance.\\n</summary>\\n    <author>\\n      <name>Ivan Kre\\xc5\\xa1o</name>\\n    </author>\\n    <author>\\n      <name>Marko \\xc5\\xa0evrovi\\xc4\\x87</name>\\n    </author>\\n    <author>\\n      <name>Sini\\xc5\\xa1a \\xc5\\xa0egvi\\xc4\\x87</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Part of the Proceedings of the Croatian Computer Vision Workshop,\\n  CCVW 2013, Year 1</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1310.0310v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1310.0310v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1310.2050v1</id>\\n    <updated>2013-10-08T09:04:34Z</updated>\\n    <published>2013-10-08T09:04:34Z</published>\\n    <title>A State Of the Art Report on Research in Multiple RGB-D sensor Setups</title>\\n    <summary>  That the Microsoft Kinect, an RGB-D sensor, transformed the gaming and end\\nconsumer sector has been anticipated by the developers. That it also impacted\\nin rigorous computer vision research has probably been a surprise to the whole\\ncommunity. Shortly before the commercial deployment of its successor, Kinect\\nOne, the research literature fills with resumees and state-of-the art papers to\\nsummarize the development over the past 3 years. This particular report\\ndescribes significant research projects which have built on sensoring setups\\nthat include two or more RGB-D sensors in one scene.\\n</summary>\\n    <author>\\n      <name>Kai Berger</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1310.2050v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1310.2050v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1310.2053v1</id>\\n    <updated>2013-10-08T09:16:56Z</updated>\\n    <published>2013-10-08T09:16:56Z</published>\\n    <title>The role of RGB-D benchmark datasets: an overview</title>\\n    <summary>  The advent of the Microsoft Kinect three years ago stimulated not only the\\ncomputer vision community for new algorithms and setups to tackle well-known\\nproblems in the community but also sparked the launch of several new benchmark\\ndatasets to which future algorithms can be compared 019 to. This review of the\\nliterature and industry developments concludes that the current RGB-D benchmark\\ndatasets can be useful to determine the accuracy of a variety of applications\\nof a single or multiple RGB-D sensors.\\n</summary>\\n    <author>\\n      <name>Kai Berger</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1310.2053v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1310.2053v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1310.3452v1</id>\\n    <updated>2013-10-13T06:58:57Z</updated>\\n    <published>2013-10-13T06:58:57Z</published>\\n    <title>Dense Scattering Layer Removal</title>\\n    <summary>  We propose a new model, together with advanced optimization, to separate a\\nthick scattering media layer from a single natural image. It is able to handle\\nchallenging underwater scenes and images taken in fog and sandstorm, both of\\nwhich are with significantly reduced visibility. Our method addresses the\\ncritical issue -- this is, originally unnoticeable impurities will be greatly\\nmagnified after removing the scattering media layer -- with transmission-aware\\noptimization. We introduce non-local structure-aware regularization to properly\\nconstrain transmission estimation without introducing the halo artifacts. A\\nselective-neighbor criterion is presented to convert the unconventional\\nconstrained optimization problem to an unconstrained one where the latter can\\nbe efficiently solved.\\n</summary>\\n    <author>\\n      <name>Qiong Yan</name>\\n    </author>\\n    <author>\\n      <name>Li Xu</name>\\n    </author>\\n    <author>\\n      <name>Jiaya Jia</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages, 10 figures, Siggraph Asia 2013 Technial Briefs</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1310.3452v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1310.3452v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4.1\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1310.3717v1</id>\\n    <updated>2013-10-14T15:19:45Z</updated>\\n    <published>2013-10-14T15:19:45Z</published>\\n    <title>Misfire Detection in IC Engine using Kstar Algorithm</title>\\n    <summary>  Misfire in an IC Engine continues to be a problem leading to reduced fuel\\nefficiency, increased power loss and emissions containing heavy concentration\\nof hydrocarbons. Misfiring creates a unique vibration pattern attributed to a\\nparticular cylinder. Useful features can be extracted from these patterns and\\ncan be analyzed to detect misfire. Statistical features from these vibration\\nsignals were extracted. Out of these, useful features were identified using the\\nJ48 decision tree algorithm and selected features were used for classification\\nusing the Kstar algorithm. In this paper performance analysis of Kstar\\nalgorithm is presented.\\n</summary>\\n    <author>\\n      <name>Anish Bahri</name>\\n    </author>\\n    <author>\\n      <name>V Sugumaran</name>\\n    </author>\\n    <author>\\n      <name>S Babu Devasenapati</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">12 Pages, 8 Figures, 4 Tables. International Journal of Research in\\n  Mechanical Engineering, 2013</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1310.3717v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1310.3717v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1310.6376v1</id>\\n    <updated>2013-10-23T20:10:36Z</updated>\\n    <published>2013-10-23T20:10:36Z</published>\\n    <title>Can Facial Uniqueness be Inferred from Impostor Scores?</title>\\n    <summary>  In Biometrics, facial uniqueness is commonly inferred from impostor\\nsimilarity scores. In this paper, we show that such uniqueness measures are\\nhighly unstable in the presence of image quality variations like pose, noise\\nand blur. We also experimentally demonstrate the instability of a recently\\nintroduced impostor-based uniqueness measure of [Klare and Jain 2013] when\\nsubject to poor quality facial images.\\n</summary>\\n    <author>\\n      <name>Abhishek Dutta</name>\\n    </author>\\n    <author>\\n      <name>Raymond Veldhuis</name>\\n    </author>\\n    <author>\\n      <name>Luuk Spreeuwers</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">A 6 page paper presented in the Biometric Technologies in Forensic\\n  Science (BTFS) 2013 Conference, Oct 14-15 2013, Nijmegen, Netherlands. Full\\n  proceeding is available at http://www.ru.nl/clst/btfs/btfs-2013/</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1310.6376v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1310.6376v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1310.6808v1</id>\\n    <updated>2013-10-25T02:37:44Z</updated>\\n    <published>2013-10-25T02:37:44Z</published>\\n    <title>Gender Classification Using Gradient Direction Pattern</title>\\n    <summary>  A novel methodology for gender classification is presented in this paper. It\\nextracts feature from local region of a face using gray color intensity\\ndifference. The facial area is divided into sub-regions and GDP histogram\\nextracted from those regions are concatenated into a single vector to represent\\nthe face. The classification accuracy obtained by using support vector machine\\nhas outperformed all traditional feature descriptors for gender classification.\\nIt is evaluated on the images collected from FERET database and obtained very\\nhigh accuracy.\\n</summary>\\n    <author>\\n      <name>Mohammad shahidul Islam</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">3 pages, 5 figures, 3 tables, SCI journal</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Sci.Int(Lahore),25(4),797-799,2013 ISSN 1013-5316; CODEN: SINTE 8</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1310.6808v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1310.6808v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.5.4\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1310.7114v1</id>\\n    <updated>2013-10-26T14:21:06Z</updated>\\n    <published>2013-10-26T14:21:06Z</published>\\n    <title>Efficient Information Theoretic Clustering on Discrete Lattices</title>\\n    <summary>  We consider the problem of clustering data that reside on discrete, low\\ndimensional lattices. Canonical examples for this setting are found in image\\nsegmentation and key point extraction. Our solution is based on a recent\\napproach to information theoretic clustering where clusters result from an\\niterative procedure that minimizes a divergence measure. We replace costly\\nprocessing steps in the original algorithm by means of convolutions. These\\nallow for highly efficient implementations and thus significantly reduce\\nruntime. This paper therefore bridges a gap between machine learning and signal\\nprocessing.\\n</summary>\\n    <author>\\n      <name>Christian Bauckhage</name>\\n    </author>\\n    <author>\\n      <name>Kristian Kersting</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This paper has been presented at the workshop LWA 2012</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1310.7114v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1310.7114v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1311.2102v1</id>\\n    <updated>2013-11-08T22:49:07Z</updated>\\n    <published>2013-11-08T22:49:07Z</published>\\n    <title>An Experimental Comparison of Trust Region and Level Sets</title>\\n    <summary>  High-order (non-linear) functionals have become very popular in segmentation,\\nstereo and other computer vision problems. Level sets is a well established\\ngeneral gradient descent framework, which is directly applicable to\\noptimization of such functionals and widely used in practice. Recently, another\\ngeneral optimization approach based on trust region methodology was proposed\\nfor regional non-linear functionals. Our goal is a comprehensive experimental\\ncomparison of these two frameworks in regard to practical efficiency,\\nrobustness to parameters, and optimality. We experiment on a wide range of\\nproblems with non-linear constraints on segment volume, appearance and shape.\\n</summary>\\n    <author>\\n      <name>Lena Gorelick</name>\\n    </author>\\n    <author>\\n      <name>Ismail BenAyed</name>\\n    </author>\\n    <author>\\n      <name>Frank R. Schmidt</name>\\n    </author>\\n    <author>\\n      <name>Yuri Boykov</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, 6 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1311.2102v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1311.2102v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1311.6049v1</id>\\n    <updated>2013-11-23T20:52:05Z</updated>\\n    <published>2013-11-23T20:52:05Z</published>\\n    <title>Skin Texture Recognition Using Neural Networks</title>\\n    <summary>  Skin recognition is used in many applications ranging from algorithms for\\nface detection, hand gesture analysis, and to objectionable image filtering. In\\nthis work a skin recognition system was developed and tested. While many skin\\nsegmentation algorithms relay on skin color, our work relies on both skin color\\nand texture features (features derives from the GLCM) to give a better and more\\nefficient recognition accuracy of skin textures. We used feed forward neural\\nnetworks to classify input textures images to be skin or non skin textures. The\\nsystem gave very encouraging results during the neural network generalization\\nface.\\n</summary>\\n    <author>\\n      <name>Nidhal K. El Abbadi</name>\\n    </author>\\n    <author>\\n      <name>Nazar Dahir</name>\\n    </author>\\n    <author>\\n      <name>Zaid Abd Alkareem</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, 6 figures, conference ACIT 2008, Tunisia</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1311.6049v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1311.6049v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1311.6758v1</id>\\n    <updated>2013-11-24T16:59:19Z</updated>\\n    <published>2013-11-24T16:59:19Z</published>\\n    <title>Detection of Partially Visible Objects</title>\\n    <summary>  An \"elephant in the room\" for most current object detection and localization\\nmethods is the lack of explicit modelling of partial visibility due to\\nocclusion by other objects or truncation by the image boundary. Based on a\\nsliding window approach, we propose a detection method which explicitly models\\npartial visibility by treating it as a latent variable. A novel non-maximum\\nsuppression scheme is proposed which takes into account the inferred partial\\nvisibility of objects while providing a globally optimal solution. The method\\ngives more detailed scene interpretations than conventional detectors in that\\nwe are able to identify the visible parts of an object. We report improved\\naverage precision on the PASCAL VOC 2010 dataset compared to a baseline\\ndetector.\\n</summary>\\n    <author>\\n      <name>Patrick Ott</name>\\n    </author>\\n    <author>\\n      <name>Mark Everingham</name>\\n    </author>\\n    <author>\\n      <name>Jiri Matas</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1311.6758v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1311.6758v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1311.6799v2</id>\\n    <updated>2013-12-10T16:40:09Z</updated>\\n    <published>2013-10-30T10:37:28Z</published>\\n    <title>Wavelet and Fast Fourier Transform based analysis of Solar Image</title>\\n    <summary>  Both of Wavelet and Fast Fourier Transform are strong signal processing tools\\nin the field of Data Analysis. In this paper fast fourier transform (FFT) and\\nWavelet Transform are employed to observe some important features of Solar\\nimage (December, 2004). We have tried to find out the periodicity and coherence\\nof different sections of the solar image. We plotted the distribution of energy\\nin solar surface by analyzing the solar image with scalograms and\\n3D-coefficient plots.\\n</summary>\\n    <author>\\n      <name>Sabyasachi Mukhopadhyay</name>\\n    </author>\\n    <author>\\n      <name>Debadatta Dash</name>\\n    </author>\\n    <author>\\n      <name>Swapnil Barmase</name>\\n    </author>\\n    <author>\\n      <name>Prasanta K Panigrahi</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This paper has been withdrawn by the author due to some modifications\\n  are required for this current paper</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1311.6799v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1311.6799v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1311.6932v1</id>\\n    <updated>2013-11-27T11:06:05Z</updated>\\n    <published>2013-11-27T11:06:05Z</published>\\n    <title>A novel framework for image forgery localization</title>\\n    <summary>  Image forgery localization is a very active and open research field for the\\ndifficulty to handle the large variety of manipulations a malicious user can\\nperform by means of more and more sophisticated image editing tools. Here, we\\npropose a localization framework based on the fusion of three very different\\ntools, based, respectively, on sensor noise, patch-matching, and machine\\nlearning. The binary masks provided by these tools are finally fused based on\\nsome suitable reliability indexes. According to preliminary experiments on the\\ntraining set, the proposed framework provides often a very good localization\\naccuracy and sometimes valuable clues for visual scrutiny.\\n</summary>\\n    <author>\\n      <name>Davide Cozzolino</name>\\n    </author>\\n    <author>\\n      <name>Diego Gragnaniello</name>\\n    </author>\\n    <author>\\n      <name>Luisa Verdoliva</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1311.6932v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1311.6932v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1311.6934v1</id>\\n    <updated>2013-11-27T11:17:55Z</updated>\\n    <published>2013-11-27T11:17:55Z</published>\\n    <title>Image forgery detection based on the fusion of machine learning and\\n  block-matching methods</title>\\n    <summary>  Dense local descriptors and machine learning have been used with success in\\nseveral applications, like classification of textures, steganalysis, and\\nforgery detection. We develop a new image forgery detector building upon some\\ndescriptors recently proposed in the steganalysis field suitably merging some\\nof such descriptors, and optimizing a SVM classifier on the available training\\nset. Despite the very good performance, very small forgeries are hardly ever\\ndetected because they contribute very little to the descriptors. Therefore we\\nalso develop a simple, but extremely specific, copy-move detector based on\\nregion matching and fuse decisions so as to reduce the missing detection rate.\\nOverall results appear to be extremely encouraging.\\n</summary>\\n    <author>\\n      <name>Davide Cozzolino</name>\\n    </author>\\n    <author>\\n      <name>Diego Gragnaniello</name>\\n    </author>\\n    <author>\\n      <name>Luisa Verdoliva</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1311.6934v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1311.6934v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1311.7251v1</id>\\n    <updated>2013-11-28T09:44:45Z</updated>\\n    <published>2013-11-28T09:44:45Z</published>\\n    <title>Spatially-Adaptive Reconstruction in Computed Tomography using Neural\\n  Networks</title>\\n    <summary>  We propose a supervised machine learning approach for boosting existing\\nsignal and image recovery methods and demonstrate its efficacy on example of\\nimage reconstruction in computed tomography. Our technique is based on a local\\nnonlinear fusion of several image estimates, all obtained by applying a chosen\\nreconstruction algorithm with different values of its control parameters.\\nUsually such output images have different bias/variance trade-off. The fusion\\nof the images is performed by feed-forward neural network trained on a set of\\nknown examples. Numerical experiments show an improvement in reconstruction\\nquality relatively to existing direct and iterative reconstruction methods.\\n</summary>\\n    <author>\\n      <name>Joseph Shtok</name>\\n    </author>\\n    <author>\\n      <name>Michael Zibulevsky</name>\\n    </author>\\n    <author>\\n      <name>Michael Elad</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1311.7251v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1311.7251v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1312.3787v2</id>\\n    <updated>2015-02-14T17:32:32Z</updated>\\n    <published>2013-12-13T12:25:09Z</published>\\n    <title>Analysis and Understanding of Various Models for Efficient\\n  Representation and Accurate Recognition of Human Faces</title>\\n    <summary>  In this paper we have tried to compare the various face recognition models\\nagainst their classical problems. We look at the methods followed by these\\napproaches and evaluate to what extent they are able to solve the problems. All\\nmethods proposed have some drawbacks under certain conditions. To overcome\\nthese drawbacks we propose a multi-model approach\\n</summary>\\n    <author>\\n      <name>Dharini S.</name>\\n    </author>\\n    <author>\\n      <name>Guru Prasad M.</name>\\n    </author>\\n    <author>\\n      <name>Hari haran. V.</name>\\n    </author>\\n    <author>\\n      <name>Kiran Tej J. L.</name>\\n    </author>\\n    <author>\\n      <name>Kunal Ghosh</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proceedings of National Conference on \"Emerging Trends in IT\" -\\n  eit10, March 2010</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1312.3787v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1312.3787v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1312.4659v3</id>\\n    <updated>2014-08-20T17:42:45Z</updated>\\n    <published>2013-12-17T06:36:10Z</published>\\n    <title>DeepPose: Human Pose Estimation via Deep Neural Networks</title>\\n    <summary>  We propose a method for human pose estimation based on Deep Neural Networks\\n(DNNs). The pose estimation is formulated as a DNN-based regression problem\\ntowards body joints. We present a cascade of such DNN regressors which results\\nin high precision pose estimates. The approach has the advantage of reasoning\\nabout pose in a holistic fashion and has a simple but yet powerful formulation\\nwhich capitalizes on recent advances in Deep Learning. We present a detailed\\nempirical analysis with state-of-art or better performance on four academic\\nbenchmarks of diverse real-world images.\\n</summary>\\n    <author>\\n      <name>Alexander Toshev</name>\\n    </author>\\n    <author>\\n      <name>Christian Szegedy</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/CVPR.2014.214</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/CVPR.2014.214\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IEEE Conference on Computer Vision and Pattern Recognition, 2014</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1312.4659v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1312.4659v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1312.4746v1</id>\\n    <updated>2013-12-17T12:35:19Z</updated>\\n    <published>2013-12-17T12:35:19Z</published>\\n    <title>Co-Sparse Textural Similarity for Image Segmentation</title>\\n    <summary>  We propose an algorithm for segmenting natural images based on texture and\\ncolor information, which leverages the co-sparse analysis model for image\\nsegmentation within a convex multilabel optimization framework. As a key\\ningredient of this method, we introduce a novel textural similarity measure,\\nwhich builds upon the co-sparse representation of image patches. We propose a\\nBayesian approach to merge textural similarity with information about color and\\nlocation. Combined with recently developed convex multilabel optimization\\nmethods this leads to an efficient algorithm for both supervised and\\nunsupervised segmentation, which is easily parallelized on graphics hardware.\\nThe approach provides competitive results in unsupervised segmentation and\\noutperforms state-of-the-art interactive segmentation methods on the Graz\\nBenchmark.\\n</summary>\\n    <author>\\n      <name>Claudia Nieuwenhuis</name>\\n    </author>\\n    <author>\\n      <name>Daniel Cremers</name>\\n    </author>\\n    <author>\\n      <name>Simon Hawe</name>\\n    </author>\\n    <author>\\n      <name>Martin Kleinsteuber</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1312.4746v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1312.4746v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1312.5402v1</id>\\n    <updated>2013-12-19T04:23:23Z</updated>\\n    <published>2013-12-19T04:23:23Z</published>\\n    <title>Some Improvements on Deep Convolutional Neural Network Based Image\\n  Classification</title>\\n    <summary>  We investigate multiple techniques to improve upon the current state of the\\nart deep convolutional neural network based image classification pipeline. The\\ntechiques include adding more image transformations to training data, adding\\nmore transformations to generate additional predictions at test time and using\\ncomplementary models applied to higher resolution images. This paper summarizes\\nour entry in the Imagenet Large Scale Visual Recognition Challenge 2013. Our\\nsystem achieved a top 5 classification error rate of 13.55% using no external\\ndata which is over a 20% relative improvement on the previous year\\'s winner.\\n</summary>\\n    <author>\\n      <name>Andrew G. Howard</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1312.5402v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1312.5402v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1312.5568v1</id>\\n    <updated>2013-12-19T14:41:29Z</updated>\\n    <published>2013-12-19T14:41:29Z</published>\\n    <title>An Adaptive Dictionary Learning Approach for Modeling Dynamical Textures</title>\\n    <summary>  Video representation is an important and challenging task in the computer\\nvision community. In this paper, we assume that image frames of a moving scene\\ncan be modeled as a Linear Dynamical System. We propose a sparse coding\\nframework, named adaptive video dictionary learning (AVDL), to model a video\\nadaptively. The developed framework is able to capture the dynamics of a moving\\nscene by exploring both sparse properties and the temporal correlations of\\nconsecutive video frames. The proposed method is compared with state of the art\\nvideo processing methods on several benchmark data sequences, which exhibit\\nappearance changes and heavy occlusions.\\n</summary>\\n    <author>\\n      <name>Xian Wei</name>\\n    </author>\\n    <author>\\n      <name>Hao Shen</name>\\n    </author>\\n    <author>\\n      <name>Martin Kleinsteuber</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1312.5568v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1312.5568v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1312.5604v2</id>\\n    <updated>2014-02-06T12:24:54Z</updated>\\n    <published>2013-12-19T16:01:41Z</published>\\n    <title>Learning Transformations for Classification Forests</title>\\n    <summary>  This work introduces a transformation-based learner model for classification\\nforests. The weak learner at each split node plays a crucial role in a\\nclassification tree. We propose to optimize the splitting objective by learning\\na linear transformation on subspaces using nuclear norm as the optimization\\ncriteria. The learned linear transformation restores a low-rank structure for\\ndata from the same class, and, at the same time, maximizes the separation\\nbetween different classes, thereby improving the performance of the split\\nfunction. Theoretical and experimental results support the proposed framework.\\n</summary>\\n    <author>\\n      <name>Qiang Qiu</name>\\n    </author>\\n    <author>\\n      <name>Guillermo Sapiro</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">arXiv admin note: text overlap with arXiv:1309.2074</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1312.5604v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1312.5604v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1312.5940v3</id>\\n    <updated>2014-03-10T18:44:50Z</updated>\\n    <published>2013-12-20T13:48:20Z</published>\\n    <title>Generic Deep Networks with Wavelet Scattering</title>\\n    <summary>  We introduce a two-layer wavelet scattering network, for object\\nclassification. This scattering transform computes a spatial wavelet transform\\non the first layer and a new joint wavelet transform along spatial, angular and\\nscale variables in the second layer. Numerical experiments demonstrate that\\nthis two layer convolution network, which involves no learning and no max\\npooling, performs efficiently on complex image data sets such as CalTech, with\\nstructural objects variability and clutter. It opens the possibility to\\nsimplify deep neural network learning by initializing the first layers with\\nwavelet filters.\\n</summary>\\n    <author>\\n      <name>Edouard Oyallon</name>\\n    </author>\\n    <author>\\n      <name>St\\xc3\\xa9phane Mallat</name>\\n    </author>\\n    <author>\\n      <name>Laurent Sifre</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Workshop, 3 pages, prepared for ICLR 2014</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1312.5940v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1312.5940v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1312.6370v1</id>\\n    <updated>2013-12-22T12:08:20Z</updated>\\n    <published>2013-12-22T12:08:20Z</published>\\n    <title>An Efficient Edge Detection Technique by Two Dimensional Rectangular\\n  Cellular Automata</title>\\n    <summary>  This paper proposes a new pattern of two dimensional cellular automata linear\\nrules that are used for efficient edge detection of an image. Since cellular\\nautomata is inherently parallel in nature, it has produced desired output\\nwithin a unit time interval. We have observed four linear rules among 512 total\\nlinear rules of a rectangular cellular automata in adiabatic or reflexive\\nboundary condition that produces an optimal result. These four rules are\\ndirectly applied once to the images and produced edge detected output. We\\ncompare our results with the existing edge detection algorithms and found that\\nour results shows better edge detection with an enhancement of edges.\\n</summary>\\n    <author>\\n      <name>Jahangir Mohammed</name>\\n    </author>\\n    <author>\\n      <name>Deepak Ranjan Nayak</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, 6 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1312.6370v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1312.6370v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1312.6410v1</id>\\n    <updated>2013-12-22T18:04:54Z</updated>\\n    <published>2013-12-22T18:04:54Z</published>\\n    <title>A Survey on Eye-Gaze Tracking Techniques</title>\\n    <summary>  Study of eye-movement is being employed in Human Computer Interaction (HCI)\\nresearch. Eye - gaze tracking is one of the most challenging problems in the\\narea of computer vision. The goal of this paper is to present a review of\\nlatest research in this continued growth of remote eye-gaze tracking. This\\noverview includes the basic definitions and terminologies, recent advances in\\nthe field and finally the need of future development in the field.\\n</summary>\\n    <author>\\n      <name>H. R. Chennamma</name>\\n    </author>\\n    <author>\\n      <name>Xiaohui Yuan</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages, Journal</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Indian Journal of Computer Science and Engineering, ISSN :\\n  0976-5166, Vol. 4, No. 5, Oct-Nov 2013, pp. 388-393</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1312.6410v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1312.6410v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1312.6885v1</id>\\n    <updated>2013-12-24T20:38:18Z</updated>\\n    <published>2013-12-24T20:38:18Z</published>\\n    <title>Deep learning for class-generic object detection</title>\\n    <summary>  We investigate the use of deep neural networks for the novel task of class\\ngeneric object detection. We show that neural networks originally designed for\\nimage recognition can be trained to detect objects within images, regardless of\\ntheir class, including objects for which no bounding box labels have been\\nprovided. In addition, we show that bounding box labels yield a 1% performance\\nincrease on the ImageNet recognition challenge.\\n</summary>\\n    <author>\\n      <name>Brody Huval</name>\\n    </author>\\n    <author>\\n      <name>Adam Coates</name>\\n    </author>\\n    <author>\\n      <name>Andrew Ng</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1312.6885v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1312.6885v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1401.1190v1</id>\\n    <updated>2014-01-06T20:25:26Z</updated>\\n    <published>2014-01-06T20:25:26Z</published>\\n    <title>Bangla Text Recognition from Video Sequence: A New Focus</title>\\n    <summary>  Extraction and recognition of Bangla text from video frame images is\\nchallenging due to complex color background, low-resolution etc. In this paper,\\nwe propose an algorithm for extraction and recognition of Bangla text form such\\nvideo frames with complex background. Here, a two-step approach has been\\nproposed. First, the text line is segmented into words using information based\\non line contours. First order gradient value of the text blocks are used to\\nfind the word gap. Next, a local binarization technique is applied on each word\\nand text line is reconstructed using those words. Secondly, this binarized text\\nblock is sent to OCR for recognition purpose.\\n</summary>\\n    <author>\\n      <name>Souvik Bhowmick</name>\\n    </author>\\n    <author>\\n      <name>Purnendu Banerjee</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">NATIONAL CONFERENCE ON COMPUTING AND SYSTEMS (NaCCS), pp.\\n  62-67,2012</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1401.1190v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1401.1190v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1401.1742v1</id>\\n    <updated>2014-01-08T16:22:09Z</updated>\\n    <published>2014-01-08T16:22:09Z</published>\\n    <title>Content Based Image Indexing and Retrieval</title>\\n    <summary>  In this paper, we present the efficient content based image retrieval systems\\nwhich employ the color, texture and shape information of images to facilitate\\nthe retrieval process. For efficient feature extraction, we extract the color,\\ntexture and shape feature of images automatically using edge detection which is\\nwidely used in signal processing and image compression. For facilitated the\\nspeedy retrieval we are implements the antipole-tree algorithm for indexing the\\nimages.\\n</summary>\\n    <author>\\n      <name>Avinash N Bhute</name>\\n    </author>\\n    <author>\\n      <name>B. B. Meshram</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">12 pages</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IJGIP 2013 Vol 3 issue 4</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1401.1742v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1401.1742v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.GR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.MM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1401.2058v1</id>\\n    <updated>2014-01-09T16:26:54Z</updated>\\n    <published>2014-01-09T16:26:54Z</published>\\n    <title>Gesture recognition based mouse events</title>\\n    <summary>  This paper presents the maneuver of mouse pointer and performs various mouse\\noperations such as left click, right click, double click, drag etc using\\ngestures recognition technique. Recognizing gestures is a complex task which\\ninvolves many aspects such as motion modeling, motion analysis, pattern\\nrecognition and machine learning. Keeping all the essential factors in mind a\\nsystem has been created which recognizes the movement of fingers and various\\npatterns formed by them. Color caps have been used for fingers to distinguish\\nit from the background color such as skin color. Thus recognizing the gestures\\nvarious mouse events have been performed. The application has been created on\\nMATLAB environment with operating system as windows 7.\\n</summary>\\n    <author>\\n      <name>Rachit Puri</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages, IJCSIT</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1401.2058v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1401.2058v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1401.2686v1</id>\\n    <updated>2014-01-13T00:19:34Z</updated>\\n    <published>2014-01-13T00:19:34Z</published>\\n    <title>A parameterless scale-space approach to find meaningful modes in\\n  histograms - Application to image and spectrum segmentation</title>\\n    <summary>  In this paper, we present an algorithm to automatically detect meaningful\\nmodes in a histogram. The proposed method is based on the behavior of local\\nminima in a scale-space representation. We show that the detection of such\\nmeaningful modes is equivalent in a two classes clustering problem on the\\nlength of minima scale-space curves. The algorithm is easy to implement, fast,\\nand does not require any parameters. We present several results on histogram\\nand spectrum segmentation, grayscale image segmentation and color image\\nreduction.\\n</summary>\\n    <author>\\n      <name>J\\xc3\\xa9r\\xc3\\xb4me Gilles</name>\\n    </author>\\n    <author>\\n      <name>Kathryn Heal</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1401.2686v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1401.2686v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1401.4648v1</id>\\n    <updated>2014-01-19T09:52:19Z</updated>\\n    <published>2014-01-19T09:52:19Z</published>\\n    <title>Visual Tracking using Particle Swarm Optimization</title>\\n    <summary>  The problem of robust extraction of visual odometry from a sequence of images\\nobtained by an eye in hand camera configuration is addressed. A novel approach\\ntoward solving planar template based tracking is proposed which performs a\\nnon-linear image alignment for successful retrieval of camera transformations.\\nIn order to obtain global optimum a bio-metaheuristic is used for optimization\\nof similarity among the planar regions. The proposed method is validated on\\nimage sequences with real as well as synthetic transformations and found to be\\nresilient to intensity variations. A comparative analysis of the various\\nsimilarity measures as well as various state-of-art methods reveal that the\\nalgorithm succeeds in tracking the planar regions robustly and has good\\npotential to be used in real applications.\\n</summary>\\n    <author>\\n      <name>Rafid Siddiqui</name>\\n    </author>\\n    <author>\\n      <name>Siamak Khatibi</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1401.4648v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1401.4648v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1401.5245v1</id>\\n    <updated>2014-01-21T09:57:23Z</updated>\\n    <published>2014-01-21T09:57:23Z</published>\\n    <title>Edge detection of binary images using the method of masks</title>\\n    <summary>  In this work the method of masks, creating and using of inverted image masks,\\ntogether with binary operation of image data are used in edge detection of\\nbinary images, monochrome images, which yields about 300 times faster than\\nordinary methods. The method is divided into three stages: Mask construction,\\nFundamental edge detection, and Edge Construction Comparison with an ordinary\\nmethod and a fuzzy based method is carried out.\\n</summary>\\n    <author>\\n      <name>Ayman M Bahaa-Eldeen</name>\\n    </author>\\n    <author>\\n      <name>Abdel-Moneim A. Wahdan</name>\\n    </author>\\n    <author>\\n      <name>Hani M. K. Mahdi</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Ain Shams University, Faculty of Engineering Scientific Bulletin,\\n  Volume 35, Issue 3, pp 349-355, (2000)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1401.5245v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1401.5245v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1401.5891v1</id>\\n    <updated>2014-01-23T08:33:44Z</updated>\\n    <published>2014-01-23T08:33:44Z</published>\\n    <title>Hierarchical pixel clustering for image segmentation</title>\\n    <summary>  In the paper a piecewise constant image approximations of sequential number\\nof pixel clusters or segments are treated. A majorizing of optimal\\napproximation sequence by hierarchical sequence of image approximations is\\nstudied. Transition from pixel clustering to image segmentation by reducing of\\nsegment numbers in clusters is provided. Algorithms are proved by elementary\\nformulas.\\n</summary>\\n    <author>\\n      <name>M. Kharinov</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages, 3 figures, 4 formulas, submitted to the 12 International\\n  Conference on Pattern Recognition and Information Processing May 28-30, 2014,\\n  Minsk, Belarus</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proc. of the 12th International Conference on Pattern Recognition\\n  and Information Processing (PRIP\\'2014), May 28-30, 2014, Minsk, Belarus,\\n  pp.103-107</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1401.5891v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1401.5891v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1401.6126v1</id>\\n    <updated>2013-12-19T08:17:24Z</updated>\\n    <published>2013-12-19T08:17:24Z</published>\\n    <title>Delegating Custom Object Detection Tasks to a Universal Classification\\n  System</title>\\n    <summary>  In this paper, a concept of multipurpose object detection system, recently\\nintroduced in our previous work, is clarified. The business aspect of this\\nmethod is transformation of a classifier into an object detector/locator via an\\nimage grid. This is a universal framework for locating objects of interest\\nthrough classification. The framework standardizes and simplifies\\nimplementation of custom systems by doing only a custom analysis of the\\nclassification results on the image grid.\\n</summary>\\n    <author>\\n      <name>Andrew Gleibman</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">3 pages, 2 figures, 6 refs. arXiv admin note: substantial text\\n  overlap with arXiv:1310.7170</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1401.6126v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1401.6126v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68T10\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.2.10; I.4.7; I.4.8; I.4.9; I.5; I.5.2; I.5.4; I.5.5\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1401.7486v1</id>\\n    <updated>2014-01-29T12:27:39Z</updated>\\n    <published>2014-01-29T12:27:39Z</published>\\n    <title>Use HMM and KNN for classifying corneal data</title>\\n    <summary>  These days to gain classification system with high accuracy that can classify\\ncomplicated pattern are so useful in medicine and industry. In this article a\\nprocess for getting the best classifier for Lasik data is suggested. However at\\nfirst it\\'s been tried to find the best line and curve by this classifier in\\norder to gain classifier fitting, and in the end by using the Markov method a\\nclassifier for topographies is gained.\\n</summary>\\n    <author>\\n      <name>Payam Porkar Rezaeiye</name>\\n    </author>\\n    <author>\\n      <name>mehrnoosh bazrafkan</name>\\n    </author>\\n    <author>\\n      <name>ali akbar movassagh</name>\\n    </author>\\n    <author>\\n      <name>Mojtaba Sedigh Fazli</name>\\n    </author>\\n    <author>\\n      <name>Gholam hossein bazyari</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1401.7486v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1401.7486v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1402.0936v2</id>\\n    <updated>2014-03-27T07:12:28Z</updated>\\n    <published>2014-02-05T05:31:59Z</published>\\n    <title>An Optimization Method For Slice Interpolation Of Medical Images</title>\\n    <summary>  Slice interpolation is a fast growing field in medical image processing.\\nIntensity-based interpolation and object-based interpolation are two major\\ngroups of methods in the literature. In this paper, we describe an\\nobject-oriented, optimization method based on a modified version of\\ncurvature-based image registration, in which a displacement field is computed\\nfor the missing slice between two known slices and used to interpolate the\\nintensities of the missing slice. The proposed approach is evaluated\\nquantitatively by using the Mean Squared Difference (MSD) as a metric. The\\nproduced results also show visual improvement in preserving sharp edges in\\nimages.\\n</summary>\\n    <author>\\n      <name>Ahmadreza Baghaie</name>\\n    </author>\\n    <author>\\n      <name>Zeyun Yu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, 5 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1402.0936v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1402.0936v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1402.1151v1</id>\\n    <updated>2014-02-05T20:18:26Z</updated>\\n    <published>2014-02-05T20:18:26Z</published>\\n    <title>Image Acquisition in an Underwater Vision System with NIR and VIS\\n  Illumination</title>\\n    <summary>  The paper describes the image acquisition system able to capture images in\\ntwo separated bands of light, used to underwater autonomous navigation. The\\nchannels are: the visible light spectrum and near infrared spectrum. The\\ncharacteristics of natural, underwater environment were also described together\\nwith the process of the underwater image creation. The results of an experiment\\nwith comparison of selected images acquired in these channels are discussed.\\n</summary>\\n    <author>\\n      <name>Wojciech Biega\\xc5\\x84ski</name>\\n    </author>\\n    <author>\\n      <name>Andrzej Kasi\\xc5\\x84ski</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Computer Science &amp; Information Technology, Volume 4, Number 1,\\n  2014, pp. 215-224</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1402.1151v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1402.1151v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1402.1331v1</id>\\n    <updated>2014-02-06T11:58:42Z</updated>\\n    <published>2014-02-06T11:58:42Z</published>\\n    <title>An Estimation Method of Measuring Image Quality for Compressed Images of\\n  Human Face</title>\\n    <summary>  Nowadays digital image compression and decompression techniques are very much\\nimportant. So our aim is to calculate the quality of face and other regions of\\nthe compressed image with respect to the original image. Image segmentation is\\ntypically used to locate objects and boundaries (lines, curves etc.)in images.\\nAfter segmentation the image is changed into something which is more meaningful\\nto analyze. Using Universal Image Quality Index(Q),Structural Similarity\\nIndex(SSIM) and Gradient-based Structural Similarity Index(G-SSIM) it can be\\nshown that face region is less compressed than any other region of the image.\\n</summary>\\n    <author>\\n      <name>Abhishek Bhattacharya</name>\\n    </author>\\n    <author>\\n      <name>Tanusree Chatterjee</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.14445/22312803/IJCTT-V7P144</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.14445/22312803/IJCTT-V7P144\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1402.1331v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1402.1331v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1402.1359v1</id>\\n    <updated>2014-02-06T14:09:25Z</updated>\\n    <published>2014-02-06T14:09:25Z</published>\\n    <title>Real-time Pedestrian Surveillance with Top View Cumulative Grids</title>\\n    <summary>  This manuscript presents an efficient approach to map pedestrian surveillance\\nfootage to an aerial view for global assessment of features. The analysis of\\nthe footages relies on low level computer vision and enable real-time\\nsurveillance. While we neglect object tracking, we introduce cumulative grids\\non top view scene flow visualization to highlight situations of interest in the\\nfootage. Our approach is tested on multiview footage both from RGB cameras and,\\nfor the first time in the field, on RGB-D-sensors.\\n</summary>\\n    <author>\\n      <name>Kai Berger</name>\\n    </author>\\n    <author>\\n      <name>Jeyarajan Thiyagalingam</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1402.1359v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1402.1359v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1402.2013v1</id>\\n    <updated>2014-02-10T01:22:35Z</updated>\\n    <published>2014-02-10T01:22:35Z</published>\\n    <title>Foreground segmentation based on multi-resolution and matting</title>\\n    <summary>  We propose a foreground segmentation algorithm that does foreground\\nextraction under different scales and refines the result by matting. First, the\\ninput image is filtered and resampled to 5 different resolutions. Then each of\\nthem is segmented by adaptive figure-ground classification and the best\\nsegmentation is automatically selected by an evaluation score that maximizes\\nthe difference between foreground and background. This segmentation is\\nupsampled to the original size, and a corresponding trimap is built.\\nClosed-form matting is employed to label the boundary region, and the result is\\nrefined by a final figure-ground classification. Experiments show the success\\nof our method in treating challenging images with cluttered background and\\nadapting to loose initial bounding-box.\\n</summary>\\n    <author>\\n      <name>Xintong Yu</name>\\n    </author>\\n    <author>\\n      <name>Xiaohan Liu</name>\\n    </author>\\n    <author>\\n      <name>Yisong Chen</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages. 7 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1402.2013v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1402.2013v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1402.2188v1</id>\\n    <updated>2014-02-10T15:41:48Z</updated>\\n    <published>2014-02-10T15:41:48Z</published>\\n    <title>Handwritten Character Recognition In Malayalam Scripts- A Review</title>\\n    <summary>  Handwritten character recognition is one of the most challenging and ongoing\\nareas of research in the field of pattern recognition. HCR research is matured\\nfor foreign languages like Chinese and Japanese but the problem is much more\\ncomplex for Indian languages. The problem becomes even more complicated for\\nSouth Indian languages due to its large character set and the presence of\\nvowels modifiers and compound characters. This paper provides an overview of\\nimportant contributions and advances in offline as well as online handwritten\\ncharacter recognition of Malayalam scripts.\\n</summary>\\n    <author>\\n      <name>Anitha Mary M. O. Chacko</name>\\n    </author>\\n    <author>\\n      <name>P. M Dhanya</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">11 pages,4 figures,2 tables</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">International Journal of Artificial Intelligence &amp; Applications\\n  (IJAIA), Vol. 5, No. 1, January 2014</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1402.2188v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1402.2188v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1402.2426v1</id>\\n    <updated>2014-02-11T10:26:31Z</updated>\\n    <published>2014-02-11T10:26:31Z</published>\\n    <title>Imaging with Rays: Microscopy, Medical Imaging, and Computer Vision</title>\\n    <summary>  In this paper we broadly consider techniques which utilize projections on\\nrays for data collection, with particular emphasis on optical techniques. We\\nformulate a variety of imaging techniques as either special cases or extensions\\nof tomographic reconstruction. We then consider how the techniques must be\\nextended to describe objects containing occlusion, as with a self-occluding\\nopaque object. We formulate the reconstruction problem as a regularized\\nnonlinear optimization problem to simultaneously solve for object brightness\\nand attenuation, where the attenuation can become infinite. We demonstrate\\nvarious simulated examples for imaging opaque objects, including sparse point\\nsources, a conventional multiview reconstruction technique, and a\\nsuper-resolving technique which exploits occlusion to resolve an image.\\n</summary>\\n    <author>\\n      <name>Keith Dillon</name>\\n    </author>\\n    <author>\\n      <name>Yeshaiahu Fainman</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1402.2426v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1402.2426v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1402.3869v2</id>\\n    <updated>2014-05-16T03:24:09Z</updated>\\n    <published>2014-02-17T02:13:30Z</published>\\n    <title>FTVd is beyond Fast Total Variation regularized Deconvolution</title>\\n    <summary>  In this paper, we revisit the \"FTVd\" algorithm for Fast Total Variation\\nRegularized Deconvolution, which has been widely used in the past few years.\\nBoth its original version implemented in the MATLAB software FTVd 3.0 and its\\nrelated variant implemented in the latter version FTVd 4.0 are considered\\n\\\\cite{Wang08FTVdsoftware}. We propose that the intermediate results during the\\niterations are the solutions of a series of combined Tikhonov and total\\nvariation regularized image deconvolution models and therefore some of them\\noften have even better image quality than the final solution, which is\\ncorresponding to the pure total variation regularized model.\\n</summary>\\n    <author>\\n      <name>Yilun Wang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1402.3869v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1402.3869v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"G.1.6; G.4; I.4.4\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1402.6416v1</id>\\n    <updated>2014-02-26T05:37:41Z</updated>\\n    <published>2014-02-26T05:37:41Z</published>\\n    <title>Deconstruction of compound objects from image sets</title>\\n    <summary>  We propose a method to recover the structure of a compound object from\\nmultiple silhouettes. Structure is expressed as a collection of 3D primitives\\nchosen from a pre-defined library, each with an associated pose. This has\\nseveral advantages over a volume or mesh representation both for estimation and\\nthe utility of the recovered model. The main challenge in recovering such a\\nmodel is the combinatorial number of possible arrangements of parts. We address\\nthis issue by exploiting the sparse nature of the problem, and show that our\\nmethod scales to objects constructed from large libraries of parts.\\n</summary>\\n    <author>\\n      <name>Anton van den Hengel</name>\\n    </author>\\n    <author>\\n      <name>John Bastian</name>\\n    </author>\\n    <author>\\n      <name>Anthony Dick</name>\\n    </author>\\n    <author>\\n      <name>Lachlan Fleming</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1402.6416v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1402.6416v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1403.0087v1</id>\\n    <updated>2014-03-01T14:08:22Z</updated>\\n    <published>2014-03-01T14:08:22Z</published>\\n    <title>Temporal Image Fusion</title>\\n    <summary>  This paper introduces temporal image fusion. The proposed technique builds\\nupon previous research in exposure fusion and expands it to deal with the\\nlimited Temporal Dynamic Range of existing sensors and camera technologies. In\\nparticular, temporal image fusion enables the rendering of long-exposure\\neffects on full frame-rate video, as well as the generation of arbitrarily long\\nexposures from a sequence of images of the same scene taken over time. We\\nexplore the problem of temporal under-exposure, and show how it can be\\naddressed by selectively enhancing dynamic structure. Finally, we show that the\\nuse of temporal image fusion together with content-selective image filters can\\nproduce a range of striking visual effects on a given input sequence.\\n</summary>\\n    <author>\\n      <name>Francisco J. Estrada</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1403.0087v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1403.0087v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.GR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1403.0728v1</id>\\n    <updated>2014-03-04T09:52:13Z</updated>\\n    <published>2014-03-04T09:52:13Z</published>\\n    <title>A Novel Method for Vectorization</title>\\n    <summary>  Vectorization of images is a key concern uniting computer graphics and\\ncomputer vision communities. In this paper we are presenting a novel idea for\\nefficient, customizable vectorization of raster images, based on Catmull Rom\\nspline fitting. The algorithm maintains a good balance between photo-realism\\nand photo abstraction, and hence is applicable to applications with artistic\\nconcerns or applications where less information loss is crucial. The resulting\\nalgorithm is fast, parallelizable and can satisfy general soft realtime\\nrequirements. Moreover, the smoothness of the vectorized images aesthetically\\noutperforms outputs of many polygon-based methods\\n</summary>\\n    <author>\\n      <name>Tolga Birdal</name>\\n    </author>\\n    <author>\\n      <name>Emrah Bala</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Prepared in Siggraph format, not published in a conference, 7 pages,\\n  9 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1403.0728v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1403.0728v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.GR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1403.1687v1</id>\\n    <updated>2014-03-07T08:57:12Z</updated>\\n    <published>2014-03-07T08:57:12Z</published>\\n    <title>Rigid-Motion Scattering for Texture Classification</title>\\n    <summary>  A rigid-motion scattering computes adaptive invariants along translations and\\nrotations, with a deep convolutional network. Convolutions are calculated on\\nthe rigid-motion group, with wavelets defined on the translation and rotation\\nvariables. It preserves joint rotation and translation information, while\\nproviding global invariants at any desired scale. Texture classification is\\nstudied, through the characterization of stationary processes from a single\\nrealization. State-of-the-art results are obtained on multiple texture data\\nbases, with important rotation and scaling variabilities.\\n</summary>\\n    <author>\\n      <name>Laurent SIfre</name>\\n    </author>\\n    <author>\\n      <name>St\\xc3\\xa9phane Mallat</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">19 pages, submitted to International Journal of Computer Vision</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1403.1687v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1403.1687v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1403.2482v1</id>\\n    <updated>2014-03-11T06:48:58Z</updated>\\n    <published>2014-03-11T06:48:58Z</published>\\n    <title>Removing Mixture of Gaussian and Impulse Noise by Patch-Based Weighted\\n  Means</title>\\n    <summary>  We first establish a law of large numbers and a convergence theorem in\\ndistribution to show the rate of convergence of the non-local means filter for\\nremoving Gaussian noise. We then introduce the notion of degree of similarity\\nto measure the role of similarity for the non-local means filter. Based on the\\nconvergence theorems, we propose a patch-based weighted means filter for\\nremoving impulse noise and its mixture with Gaussian noise by combining the\\nessential idea of the trilateral filter and that of the non-local means filter.\\nOur experiments show that our filter is competitive compared to recently\\nproposed methods.\\n</summary>\\n    <author>\\n      <name>Haijuan Hu</name>\\n    </author>\\n    <author>\\n      <name>Bing Li</name>\\n    </author>\\n    <author>\\n      <name>Quansheng Liu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1403.2482v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1403.2482v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1403.3021v1</id>\\n    <updated>2014-03-12T16:44:20Z</updated>\\n    <published>2014-03-12T16:44:20Z</published>\\n    <title>Image reconstruction from limited range projections using orthogonal\\n  moments</title>\\n    <summary>  A set of orthonormal polynomials is proposed for image reconstruction from\\nprojection data. The relationship between the projection moments and image\\nmoments is discussed in detail, and some interesting properties are\\ndemonstrated. Simulation results are provided to validate the method and to\\ncompare its performance with previous works.\\n</summary>\\n    <author>\\n      <name>Huazhong Shu</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">CRIBS, LIST</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Jian Zhou</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">CRIBS, LTSI</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Guo-Niu Han</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IRMA</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Limin M. Luo</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">CRIBS, LIST</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Jean-Louis Coatrieux</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">CRIBS, LTSI</arxiv:affiliation>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1016/j.patcog.2006.05.035</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1016/j.patcog.2006.05.035\" rel=\"related\"/>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Pattern Recognition 40, 2 (2007) 670-680</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1403.3021v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1403.3021v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.NA\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1403.3683v1</id>\\n    <updated>2014-03-14T19:45:56Z</updated>\\n    <published>2014-03-14T19:45:56Z</published>\\n    <title>Removal and Contraction Operations in $n$D Generalized Maps for\\n  Efficient Homology Computation</title>\\n    <summary>  In this paper, we show that contraction operations preserve the homology of\\n$n$D generalized maps, under some conditions. Removal and contraction\\noperations are used to propose an efficient algorithm that compute homology\\ngenerators of $n$D generalized maps. Its principle consists in simplifying a\\ngeneralized map as much as possible by using removal and contraction\\noperations. We obtain a generalized map having the same homology than the\\ninitial one, while the number of cells decreased significantly.\\n  Keywords: $n$D Generalized Maps; Cellular Homology; Homology Generators;\\nContraction and Removal Operations.\\n</summary>\\n    <author>\\n      <name>Guillaume Damiand</name>\\n    </author>\\n    <author>\\n      <name>Rocio Gonzalez-Diaz</name>\\n    </author>\\n    <author>\\n      <name>Samuel Peltier</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Research report</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1403.3683v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1403.3683v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1403.8067v2</id>\\n    <updated>2014-04-20T16:31:55Z</updated>\\n    <published>2014-03-31T16:09:27Z</published>\\n    <title>Robust Subspace Recovery via Bi-Sparsity Pursuit</title>\\n    <summary>  Successful applications of sparse models in computer vision and machine\\nlearning imply that in many real-world applications, high dimensional data is\\ndistributed in a union of low dimensional subspaces. Nevertheless, the\\nunderlying structure may be affected by sparse errors and/or outliers. In this\\npaper, we propose a bi-sparse model as a framework to analyze this problem and\\nprovide a novel algorithm to recover the union of subspaces in presence of\\nsparse corruptions. We further show the effectiveness of our method by\\nexperiments on both synthetic data and real-world vision data.\\n</summary>\\n    <author>\\n      <name>Xiao Bian</name>\\n    </author>\\n    <author>\\n      <name>Hamid Krim</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1403.8067v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1403.8067v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1404.0566v1</id>\\n    <updated>2014-02-17T09:42:37Z</updated>\\n    <published>2014-02-17T09:42:37Z</published>\\n    <title>Weyl group orbit functions in image processing</title>\\n    <summary>  We deal with the Fourier-like analysis of functions on discrete grids in\\ntwo-dimensional simplexes using $C-$ and $E-$ Weyl group orbit functions. For\\nthese cases we present the convolution theorem. We provide an example of\\napplication of image processing using the $C-$ functions and the convolutions\\nfor spatial filtering of the treated image.\\n</summary>\\n    <author>\\n      <name>Goce Chadzitaskos</name>\\n    </author>\\n    <author>\\n      <name>Lenka H\\xc3\\xa1kov\\xc3\\xa1</name>\\n    </author>\\n    <author>\\n      <name>Ond\\xc5\\x99ej Kaj\\xc3\\xadnek</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.4236/am.2014.53049.</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.4236/am.2014.53049.\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">12 pages, 5 figures</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Applied Mathematics, Vol. 5 No. 3, 2014, pp. 501-511</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1404.0566v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1404.0566v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1404.1292v1</id>\\n    <updated>2014-03-20T19:47:58Z</updated>\\n    <published>2014-03-20T19:47:58Z</published>\\n    <title>Review of Face Detection Systems Based Artificial Neural Networks\\n  Algorithms</title>\\n    <summary>  Face detection is one of the most relevant applications of image processing\\nand biometric systems. Artificial neural networks (ANN) have been used in the\\nfield of image processing and pattern recognition. There is lack of literature\\nsurveys which give overview about the studies and researches related to the\\nusing of ANN in face detection. Therefore, this research includes a general\\nreview of face detection studies and systems which based on different ANN\\napproaches and algorithms. The strengths and limitations of these literature\\nstudies and systems were included also.\\n</summary>\\n    <author>\\n      <name>Omaima N. A. AL-Allaf</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.5121/ijma.2013.6101</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.5121/ijma.2013.6101\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">16 pages, 12 figures, 1 table, IJMA Journal</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">The International Journal of Multimedia &amp; Its Applications (IJMA)\\n  Vol.6, No.1, February 2014</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1404.1292v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1404.1292v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1404.1869v1</id>\\n    <updated>2014-04-07T18:08:56Z</updated>\\n    <published>2014-04-07T18:08:56Z</published>\\n    <title>DenseNet: Implementing Efficient ConvNet Descriptor Pyramids</title>\\n    <summary>  Convolutional Neural Networks (CNNs) can provide accurate object\\nclassification. They can be extended to perform object detection by iterating\\nover dense or selected proposed object regions. However, the runtime of such\\ndetectors scales as the total number and/or area of regions to examine per\\nimage, and training such detectors may be prohibitively slow. However, for some\\nCNN classifier topologies, it is possible to share significant work among\\noverlapping regions to be classified. This paper presents DenseNet, an open\\nsource system that computes dense, multiscale features from the convolutional\\nlayers of a CNN based object classifier. Future work will involve training\\nefficient object detectors with DenseNet feature descriptors.\\n</summary>\\n    <author>\\n      <name>Forrest Iandola</name>\\n    </author>\\n    <author>\\n      <name>Matt Moskewicz</name>\\n    </author>\\n    <author>\\n      <name>Sergey Karayev</name>\\n    </author>\\n    <author>\\n      <name>Ross Girshick</name>\\n    </author>\\n    <author>\\n      <name>Trevor Darrell</name>\\n    </author>\\n    <author>\\n      <name>Kurt Keutzer</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1404.1869v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1404.1869v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1404.3184v1</id>\\n    <updated>2014-04-11T18:50:34Z</updated>\\n    <published>2014-04-11T18:50:34Z</published>\\n    <title>Decreasing Weighted Sorted $\\\\ell_1$ Regularization</title>\\n    <summary>  We consider a new family of regularizers, termed {\\\\it weighted sorted\\n$\\\\ell_1$ norms} (WSL1), which generalizes the recently introduced {\\\\it\\noctagonal shrinkage and clustering algorithm for regression} (OSCAR) and also\\ncontains the $\\\\ell_1$ and $\\\\ell_{\\\\infty}$ norms as particular instances. We\\nfocus on a special case of the WSL1, the {\\\\sl decreasing WSL1} (DWSL1), where\\nthe elements of the argument vector are sorted in non-increasing order and the\\nweights are also non-increasing. In this paper, after showing that the DWSL1 is\\nindeed a norm, we derive two key tools for its use as a regularizer: the dual\\nnorm and the Moreau proximity operator.\\n</summary>\\n    <author>\\n      <name>Xiangrong Zeng</name>\\n    </author>\\n    <author>\\n      <name>M\\xc3\\xa1rio A. T. Figueiredo</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages, 2 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1404.3184v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1404.3184v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1404.3538v2</id>\\n    <updated>2014-04-30T09:53:00Z</updated>\\n    <published>2014-04-14T11:01:04Z</published>\\n    <title>Proceedings of The 38th Annual Workshop of the Austrian Association for\\n  Pattern Recognition (\\xc3\\x96AGM), 2014</title>\\n    <summary>  The 38th Annual Workshop of the Austrian Association for Pattern Recognition\\n(\\\\\"OAGM) will be held at IST Austria, on May 22-23, 2014. The workshop provides\\na platform for researchers and industry to discuss traditional and new areas of\\ncomputer vision. This year the main topic is: Pattern Recognition:\\ninterdisciplinary challenges and opportunities.\\n</summary>\\n    <author>\\n      <name>Vladimir Kolmogorov</name>\\n    </author>\\n    <author>\\n      <name>Christoph Lampert</name>\\n    </author>\\n    <author>\\n      <name>Emilie Morvant</name>\\n    </author>\\n    <author>\\n      <name>Rustem Takhanov</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1404.3538v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1404.3538v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1404.3991v1</id>\\n    <updated>2014-04-15T17:12:40Z</updated>\\n    <published>2014-04-15T17:12:40Z</published>\\n    <title>Spiralet Sparse Representation</title>\\n    <summary>  This is the first report on Working Paper WP-RFM-14-01. The potential and\\ncapability of sparse representations is well-known. However, their\\n(multivariate variable) vectorial form, which is completely fine in many fields\\nand disciplines, results in removal and filtering of important \"spatial\"\\nrelations that are implicitly carried by two-dimensional [or multi-dimensional]\\nobjects, such as images. In this paper, a new approach, called spiralet sparse\\nrepresentation, is proposed in order to develop an augmented representation and\\ntherefore a modified sparse representation and theory, which is capable to\\npreserve the data associated to the spatial relations.\\n</summary>\\n    <author>\\n      <name>Reza Farrahi Moghaddam</name>\\n    </author>\\n    <author>\\n      <name>Mohamed Cheriet</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages, Working Paper Number: WP-RFM-14-01</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1404.3991v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1404.3991v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1404.4316v1</id>\\n    <updated>2014-04-16T17:23:47Z</updated>\\n    <published>2014-04-16T17:23:47Z</published>\\n    <title>Generic Object Detection With Dense Neural Patterns and Regionlets</title>\\n    <summary>  This paper addresses the challenge of establishing a bridge between deep\\nconvolutional neural networks and conventional object detection frameworks for\\naccurate and efficient generic object detection. We introduce Dense Neural\\nPatterns, short for DNPs, which are dense local features derived from\\ndiscriminatively trained deep convolutional neural networks. DNPs can be easily\\nplugged into conventional detection frameworks in the same way as other dense\\nlocal features(like HOG or LBP). The effectiveness of the proposed approach is\\ndemonstrated with the Regionlets object detection framework. It achieved 46.1%\\nmean average precision on the PASCAL VOC 2007 dataset, and 44.1% on the PASCAL\\nVOC 2010 dataset, which dramatically improves the original Regionlets approach\\nwithout DNPs.\\n</summary>\\n    <author>\\n      <name>Will Y. Zou</name>\\n    </author>\\n    <author>\\n      <name>Xiaoyu Wang</name>\\n    </author>\\n    <author>\\n      <name>Miao Sun</name>\\n    </author>\\n    <author>\\n      <name>Yuanqing Lin</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1404.4316v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1404.4316v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1404.4661v1</id>\\n    <updated>2014-04-17T22:09:16Z</updated>\\n    <published>2014-04-17T22:09:16Z</published>\\n    <title>Learning Fine-grained Image Similarity with Deep Ranking</title>\\n    <summary>  Learning fine-grained image similarity is a challenging task. It needs to\\ncapture between-class and within-class image differences. This paper proposes a\\ndeep ranking model that employs deep learning techniques to learn similarity\\nmetric directly from images.It has higher learning capability than models based\\non hand-crafted features. A novel multiscale network structure has been\\ndeveloped to describe the images effectively. An efficient triplet sampling\\nalgorithm is proposed to learn the model with distributed asynchronized\\nstochastic gradient. Extensive experiments show that the proposed algorithm\\noutperforms models based on hand-crafted visual features and deep\\nclassification models.\\n</summary>\\n    <author>\\n      <name>Jiang Wang</name>\\n    </author>\\n    <author>\\n      <name>Yang song</name>\\n    </author>\\n    <author>\\n      <name>Thomas Leung</name>\\n    </author>\\n    <author>\\n      <name>Chuck Rosenberg</name>\\n    </author>\\n    <author>\\n      <name>Jinbin Wang</name>\\n    </author>\\n    <author>\\n      <name>James Philbin</name>\\n    </author>\\n    <author>\\n      <name>Bo Chen</name>\\n    </author>\\n    <author>\\n      <name>Ying Wu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">CVPR 2014</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1404.4661v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1404.4661v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1404.6071v1</id>\\n    <updated>2014-04-24T10:09:41Z</updated>\\n    <published>2014-04-24T10:09:41Z</published>\\n    <title>Rough Clustering Based Unsupervised Image Change Detection</title>\\n    <summary>  This paper introduces an unsupervised technique to detect the changed region\\nof multitemporal images on a same reference plane with the help of rough\\nclustering. The proposed technique is a soft-computing approach, based on the\\nconcept of rough set with rough clustering and Pawlak\\'s accuracy. It is less\\nnoisy and avoids pre-deterministic knowledge about the distribution of the\\nchanged and unchanged regions. To show the effectiveness, the proposed\\ntechnique is compared with some other approaches.\\n</summary>\\n    <author>\\n      <name>Chandranath Adak</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proc. IEEE Conf. #30853, International Conference on Human Computer\\n  Interactions (ICHCI\\'13), Chennai, India, 23-24 Aug., 2013. (In Press)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1404.6071v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1404.6071v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1404.6075v1</id>\\n    <updated>2014-04-24T10:24:49Z</updated>\\n    <published>2014-04-24T10:24:49Z</published>\\n    <title>Unsupervised Text Extraction from G-Maps</title>\\n    <summary>  This paper represents an text extraction method from Google maps, GIS\\nmaps/images. Due to an unsupervised approach there is no requirement of any\\nprior knowledge or training set about the textual and non-textual parts. Fuzzy\\nCMeans clustering technique is used for image segmentation and Prewitt method\\nis used to detect the edges. Connected component analysis and gridding\\ntechnique enhance the correctness of the results. The proposed method reaches\\n98.5% accuracy level on the basis of experimental data sets.\\n</summary>\\n    <author>\\n      <name>Chandranath Adak</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/ICHCI-IEEE.2013.6887782</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/ICHCI-IEEE.2013.6887782\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proc. IEEE Conf. #30853, International Conference on Human Computer\\n  Interactions (ICHCI\\'13), Chennai, India, 23-24 Aug., 2013</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1404.6075v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1404.6075v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1404.7594v1</id>\\n    <updated>2014-04-30T05:37:44Z</updated>\\n    <published>2014-04-30T05:37:44Z</published>\\n    <title>Selecting a Small Set of Optimal Gestures from an Extensive Lexicon</title>\\n    <summary>  Finding the best set of gestures to use for a given computer recognition\\nproblem is an essential part of optimizing the recognition performance while\\nbeing mindful to those who may articulate the gestures. An objective function,\\ncalled the ellipsoidal distance ratio metric (EDRM), for determining the best\\ngestures from a larger lexicon library is presented, along with a numerical\\nmethod for incorporating subjective preferences. In particular, we demonstrate\\nan efficient algorithm that chooses the best $n$ gestures from a lexicon of $m$\\ngestures where typically $n \\\\ll m$ using a weighting of both subjective and\\nobjective measures.\\n</summary>\\n    <author>\\n      <name>Jacob Grosek</name>\\n    </author>\\n    <author>\\n      <name>J. Nathan Kutz</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.5120/21060-3722</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.5120/21060-3722\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">27 pages, 7 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1404.7594v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1404.7594v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1405.0921v1</id>\\n    <updated>2014-04-30T06:30:19Z</updated>\\n    <published>2014-04-30T06:30:19Z</published>\\n    <title>Gabor Filter and Rough Clustering Based Edge Detection</title>\\n    <summary>  This paper introduces an efficient edge detection method based on Gabor\\nfilter and rough clustering. The input image is smoothed by Gabor function, and\\nthe concept of rough clustering is used to focus on edge detection with soft\\ncomputational approach. Hysteresis thresholding is used to get the actual\\noutput, i.e. edges of the input image. To show the effectiveness, the proposed\\ntechnique is compared with some other edge detection methods.\\n</summary>\\n    <author>\\n      <name>Chandranath Adak</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/ICHCI-IEEE.2013.6887768</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/ICHCI-IEEE.2013.6887768\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proc. IEEE Conf. #30853, International Conference on Human Computer\\n  Interactions (ICHCI\\'13), Chennai, India, 23-24 Aug., 2013</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1405.0921v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1405.0921v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1405.1020v1</id>\\n    <updated>2014-03-19T07:32:37Z</updated>\\n    <published>2014-03-19T07:32:37Z</published>\\n    <title>Study on performance improvement of oil paint image filter algorithm\\n  using parallel pattern library</title>\\n    <summary>  This paper gives a detailed study on the performance of oil paint image\\nfilter algorithm with various parameters applied on an image of RGB model. Oil\\nPaint image processing, being very performance hungry, current research tries\\nto find improvement using parallel pattern library. With increasing\\nkernel-size, the processing time of oil paint image filter algorithm increases\\nexponentially.\\n</summary>\\n    <author>\\n      <name>Siddhartha Mukherjee</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">12 pages, 4 figures, 4 code snippets, 4 tables, 2 graphs, 2 images of\\n  experimental result, Conference: CCSEA 2014</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1405.1020v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1405.1020v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1405.1717v1</id>\\n    <updated>2014-05-07T19:39:10Z</updated>\\n    <published>2014-05-07T19:39:10Z</published>\\n    <title>Entropy Based Cartoon Texture Separation</title>\\n    <summary>  Separating an image into cartoon and texture components comes useful in image\\nprocessing applications, such as image compression, image segmentation, image\\ninpainting. Yves Meyer\\'s influential cartoon texture decomposition model\\ninvolves deriving an energy functional by choosing appropriate spaces and\\nfunctionals. Minimizers of the derived energy functional are cartoon and\\ntexture components of an image. In this study, cartoon part of an image is\\nseparated, by reconstructing it from pixels of multi scale Total-Variation\\nfiltered versions of the original image which is sought to be decomposed into\\ncartoon and texture parts. An information theoretic pixel by pixel selection\\ncriteria is employed to choose the contributing pixels and their scales.\\n</summary>\\n    <author>\\n      <name>Kutlu Emre Yilmaz</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">12 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1405.1717v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1405.1717v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1405.1815v1</id>\\n    <updated>2014-05-08T06:49:41Z</updated>\\n    <published>2014-05-08T06:49:41Z</published>\\n    <title>Implementation And Performance Evaluation Of Background Subtraction\\n  Algorithms</title>\\n    <summary>  The study evaluates three background subtraction techniques. The techniques\\nranges from very basic algorithm to state of the art published techniques\\ncategorized based on speed, memory requirements and accuracy. Such a review can\\neffectively guide the designer to select the most suitable method for a given\\napplication in a principled way. The algorithms used in the study ranges from\\nvarying levels of accuracy and computational complexity. Few of them can also\\ndeal with real time challenges like rain, snow, hails, swaying branches,\\nobjects overlapping, varying light intensity or slow moving objects.\\n</summary>\\n    <author>\\n      <name>Deepjoy Das</name>\\n    </author>\\n    <author>\\n      <name>Dr. Sarat Saharia</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.5121/ijcsa.2014.4206</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.5121/ijcsa.2014.4206\" rel=\"related\"/>\\n    <link href=\"http://arxiv.org/abs/1405.1815v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1405.1815v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1405.1999v1</id>\\n    <updated>2014-03-21T07:42:11Z</updated>\\n    <published>2014-03-21T07:42:11Z</published>\\n    <title>Model-Driven Applications of Fractional Derivatives and Integrals</title>\\n    <summary>  Fractional order derivatives and integrals (differintegrals) are viewed from\\na frequency-domain perspective using the formalism of Riesz, providing a\\ncomputational tool as well as a way to interpret the operations in the\\nfrequency domain. Differintegrals provide a logical extension of current\\ntechniques, generalizing the notion of integral and differential operators and\\nacting as kind of frequency-domain filtering that has many of the advantages of\\na nonlocal linear operator. Several important properties of differintegrals are\\npresented, and sample applications are given to one- and two-dimensional\\nsignals. Computer code to carry out the computations is made available on the\\nauthor\\'s website.\\n</summary>\\n    <author>\\n      <name>William A. Sethares</name>\\n    </author>\\n    <author>\\n      <name>Sel\\xc3\\xa7uk \\xc5\\x9e. Bay\\xc4\\xb1n</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">22 pages, 10 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1405.1999v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1405.1999v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1405.2539v1</id>\\n    <updated>2014-05-11T15:13:56Z</updated>\\n    <published>2014-05-11T15:13:56Z</published>\\n    <title>A Review of Image Mosaicing Techniques</title>\\n    <summary>  Image Mosaicing is a method of constructing multiple images of the same scene\\ninto a larger image. The output of the image mosaic will be the union of two\\ninput images. Image-mosaicing algorithms are used to get mosaiced image. Image\\nMosaicing processed is basically divided in to 5 phases. Which includes;\\nFeature point extraction, Image registration, Homography computation, Warping\\nand Blending if Image. Various corner detection algorithm is being used for\\nFeature extraction. This corner produces an efficient and informative output\\nmosaiced image. Image mosaicing is widely used in creating 3D images, medical\\nimaging, computer vision, data from satellites, and military automatic target\\nrecognition.\\n</summary>\\n    <author>\\n      <name>Dushyant Vaghela</name>\\n    </author>\\n    <author>\\n      <name>Prof. Kapildev Naina</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1405.2539v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1405.2539v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1405.4389v1</id>\\n    <updated>2014-05-17T12:33:29Z</updated>\\n    <published>2014-05-17T12:33:29Z</published>\\n    <title>Efficient Tracking of a Moving Object using Inter-Frame Coding</title>\\n    <summary>  Video surveillance has long been in use to monitor security sensitive areas\\nsuch as banks, department stores, highways, crowded public places and\\nborders.The advance in computing power, availability of large-capacity storage\\ndevices and high speed network infrastructure paved the way for cheaper,\\nmulti-sensor video surveillance systems.Traditionally, the video outputs are\\nprocessed online by human operators and are usually saved to tapes for later\\nuse only after a forensic event.The increase in the number of cameras in\\nordinary surveillance systems overloaded both the human operators and the\\nstorage devices with high volumes of data and made it in-feasible to ensure\\nproper monitoring of sensitive areas for long times.\\n</summary>\\n    <author>\\n      <name>Shraddha Mehta</name>\\n    </author>\\n    <author>\\n      <name>Vaishali Kalariya</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1405.4389v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1405.4389v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1405.6135v1</id>\\n    <updated>2014-02-05T16:14:36Z</updated>\\n    <published>2014-02-05T16:14:36Z</published>\\n    <title>Cellular Automata based adaptive resampling technique for the processing\\n  of remotely sensed imagery</title>\\n    <summary>  Resampling techniques are being widely used at different stages of satellite\\nimage processing. The existing methodologies cannot perfectly recover features\\nfrom a completely under sampled image and hence an intelligent adaptive\\nresampling methodology is required. We address these issues and adopt an error\\nmetric from the available literature to define interpolation quality. We also\\npropose a new resampling scheme that adapts itself with regard to the pixel and\\ntexture variation in the image. The proposed CNN based hybrid method has been\\nfound to perform better than the existing methods as it adapts itself with\\nreference to the image features.\\n</summary>\\n    <author>\\n      <name>S. K. Katiyar</name>\\n    </author>\\n    <author>\\n      <name>P. V. Arun</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1405.6135v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1405.6135v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1405.7032v1</id>\\n    <updated>2014-05-27T19:54:35Z</updated>\\n    <published>2014-05-27T19:54:35Z</published>\\n    <title>An FPGA-based Parallel Architecture for Face Detection using Mixed Color\\n  Models</title>\\n    <summary>  In this paper, a reliable method for detecting human faces in color images is\\nproposed. This system firstly detects skin color in YCgCr and YIQ color space,\\nthen filters binary texture and the result is morphological processed, finally\\nconverts skin tone to the preferred skin color configured by users in YIQ color\\nspace. The real-time adjusting circuit is implemented and some of simulation\\nresults are given out. Experimental results demonstrate that the method has\\nachieved high rates and low false positives, another advantage is its\\nsimplicity and minor computational costs.\\n</summary>\\n    <author>\\n      <name>Luo Tao</name>\\n    </author>\\n    <author>\\n      <name>Shi zaifeng</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages, 7 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1405.7032v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1405.7032v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68U10\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1405.7626v1</id>\\n    <updated>2014-05-29T17:41:06Z</updated>\\n    <published>2014-05-29T17:41:06Z</published>\\n    <title>Classification of Basmati Rice Grain Variety using Image Processing and\\n  Principal Component Analysis</title>\\n    <summary>  All important decisions about the variety of rice grain end product are based\\non the different features of rice grain.There are various methods available for\\nclassification of basmati rice. This paper proposed a new principal component\\nanalysis based approach for classification of different variety of basmati\\nrice. The experimental result shows the effectiveness of the proposed\\nmethodology for various samples of different variety of basmati rice.\\n</summary>\\n    <author>\\n      <name>Rubi Kambo</name>\\n    </author>\\n    <author>\\n      <name>Amit Yerpude</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.14445/22312803/IJCTT-V11P117</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.14445/22312803/IJCTT-V11P117\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages from page no:80-85, 8 Figures</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IJAREEIE, vol. 2, no. 7, pp. 2893-2900, july 2013</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1405.7626v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1405.7626v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1406.0231v1</id>\\n    <updated>2014-06-02T01:44:50Z</updated>\\n    <published>2014-06-02T01:44:50Z</published>\\n    <title>Ambiguous Proximity Distribution</title>\\n    <summary>  Proximity Distribution Kernel is an effective method for bag-of-featues based\\nimage representation. In this paper, we investigate the soft assignment of\\nvisual words to image features for proximity distribution. Visual word\\ncontribution function is proposed to model ambiguous proximity distributions.\\nThree ambiguous proximity distributions is developed by three ambiguous\\ncontribution functions. The experiments are conducted on both classification\\nand retrieval of medical image data sets. The results show that the performance\\nof the proposed methods, Proximity Distribution Kernel (PDK), is better or\\ncomparable to the state-of-the-art bag-of-features based image representation\\nmethods.\\n</summary>\\n    <author>\\n      <name>Quanquan Wang</name>\\n    </author>\\n    <author>\\n      <name>Yongping Li</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1406.0231v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1406.0231v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1406.0289v1</id>\\n    <updated>2014-06-02T08:27:27Z</updated>\\n    <published>2014-06-02T08:27:27Z</published>\\n    <title>The constitution of visual perceptual units in the functional\\n  architecture of V1</title>\\n    <summary>  Scope of this paper is to consider a mean field neural model which takes into\\naccount the functional neurogeometry of the visual cortex modelled as a group\\nof rotations and translations. The model generalizes well known results of\\nBressloff and Cowan which, in absence of input, accounts for hallucination\\npatterns. The main result of our study consists in showing that in presence of\\na visual input, the eigenmodes of the linearized operator which become stable\\nrepresent perceptual units present in the image. The result is strictly related\\nto dimensionality reduction and clustering problems.\\n</summary>\\n    <author>\\n      <name>Alessandro Sarti</name>\\n    </author>\\n    <author>\\n      <name>Giovanna Citti</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1406.0289v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1406.0289v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1406.0588v2</id>\\n    <updated>2014-06-05T02:23:21Z</updated>\\n    <published>2014-06-03T06:32:24Z</published>\\n    <title>Image retrieval with hierarchical matching pursuit</title>\\n    <summary>  A novel representation of images for image retrieval is introduced in this\\npaper, by using a new type of feature with remarkable discriminative power.\\nDespite the multi-scale nature of objects, most existing models perform feature\\nextraction on a fixed scale, which will inevitably degrade the performance of\\nthe whole system. Motivated by this, we introduce a hierarchical sparse coding\\narchitecture for image retrieval to explore multi-scale cues. Sparse codes\\nextracted on lower layers are transmitted to higher layers recursively. With\\nthis mechanism, cues from different scales are fused. Experiments on the\\nHolidays dataset show that the proposed method achieves an excellent retrieval\\nperformance with a small code length.\\n</summary>\\n    <author>\\n      <name>Shasha Bu</name>\\n    </author>\\n    <author>\\n      <name>Yu-Jin Zhang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages, 6 figures, conference</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1406.0588v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1406.0588v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1406.0909v1</id>\\n    <updated>2014-06-04T00:03:51Z</updated>\\n    <published>2014-06-04T00:03:51Z</published>\\n    <title>Improvement Tracking Dynamic Programming using Replication Function for\\n  Continuous Sign Language Recognition</title>\\n    <summary>  In this paper we used a Replication Function (R. F.)for improvement tracking\\nwith dynamic programming. The R. F. transforms values of gray level [0 255] to\\n[0 1]. The resulting images of R. F. are more striking and visible in skin\\nregions. The R. F. improves Dynamic Programming (D. P.) in overlapping hand and\\nface. Results show that Tracking Error Rate 11% and Average Tracked Distance 7%\\nreduced\\n</summary>\\n    <author>\\n      <name>S. Ildarabadi</name>\\n    </author>\\n    <author>\\n      <name>M. Ebrahimi</name>\\n    </author>\\n    <author>\\n      <name>H. R. Pourreza</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.14445/22315381/IJETT-V7P254</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.14445/22315381/IJETT-V7P254\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages, 13 figures, Published with \"International Journal of\\n  Engineering Trends and Technology (IJETT)\"</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1406.0909v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1406.0909v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1406.2984v2</id>\\n    <updated>2014-09-17T22:43:45Z</updated>\\n    <published>2014-06-11T18:16:29Z</published>\\n    <title>Joint Training of a Convolutional Network and a Graphical Model for\\n  Human Pose Estimation</title>\\n    <summary>  This paper proposes a new hybrid architecture that consists of a deep\\nConvolutional Network and a Markov Random Field. We show how this architecture\\nis successfully applied to the challenging problem of articulated human pose\\nestimation in monocular images. The architecture can exploit structural domain\\nconstraints such as geometric relationships between body joint locations. We\\nshow that joint training of these two model paradigms improves performance and\\nallows us to significantly outperform existing state-of-the-art techniques.\\n</summary>\\n    <author>\\n      <name>Jonathan Tompson</name>\\n    </author>\\n    <author>\\n      <name>Arjun Jain</name>\\n    </author>\\n    <author>\\n      <name>Yann LeCun</name>\\n    </author>\\n    <author>\\n      <name>Christoph Bregler</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1406.2984v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1406.2984v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1406.3474v1</id>\\n    <updated>2014-06-13T10:11:18Z</updated>\\n    <published>2014-06-13T10:11:18Z</published>\\n    <title>Heterogeneous Multi-task Learning for Human Pose Estimation with Deep\\n  Convolutional Neural Network</title>\\n    <summary>  We propose an heterogeneous multi-task learning framework for human pose\\nestimation from monocular image with deep convolutional neural network. In\\nparticular, we simultaneously learn a pose-joint regressor and a sliding-window\\nbody-part detector in a deep network architecture. We show that including the\\nbody-part detection task helps to regularize the network, directing it to\\nconverge to a good solution. We report competitive and state-of-art results on\\nseveral data sets. We also empirically show that the learned neurons in the\\nmiddle layer of our network are tuned to localized body parts.\\n</summary>\\n    <author>\\n      <name>Sijin Li</name>\\n    </author>\\n    <author>\\n      <name>Zhi-Qiang Liu</name>\\n    </author>\\n    <author>\\n      <name>Antoni B. Chan</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1406.3474v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1406.3474v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1406.5035v1</id>\\n    <updated>2014-06-19T13:25:47Z</updated>\\n    <published>2014-06-19T13:25:47Z</published>\\n    <title>Why are images smooth?</title>\\n    <summary>  It is a well observed phenomenon that natural images are smooth, in the sense\\nthat nearby pixels tend to have similar values. We describe a mathematical\\nmodel of images that makes no assumptions on the nature of the environment that\\nimages depict. It only assumes that images can be taken at different scales\\n(zoom levels). We provide quantitative bounds on the smoothness of a typical\\nimage in our model, as a function of the number of available scales. These\\nbounds can serve as a baseline against which to compare the observed smoothness\\nof natural images.\\n</summary>\\n    <author>\\n      <name>Uriel Feige</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1406.5035v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1406.5035v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1406.5074v1</id>\\n    <updated>2014-06-19T15:12:49Z</updated>\\n    <published>2014-06-19T15:12:49Z</published>\\n    <title>Robust Outlier Detection Technique in Data Mining: A Univariate Approach</title>\\n    <summary>  Outliers are the points which are different from or inconsistent with the\\nrest of the data. They can be novel, new, abnormal, unusual or noisy\\ninformation. Outliers are sometimes more interesting than the majority of the\\ndata. The main challenges of outlier detection with the increasing complexity,\\nsize and variety of datasets, are how to catch similar outliers as a group, and\\nhow to evaluate the outliers. This paper describes an approach which uses\\nUnivariate outlier detection as a pre-processing step to detect the outlier and\\nthen applies K-means algorithm hence to analyse the effects of the outliers on\\nthe cluster analysis of dataset.\\n</summary>\\n    <author>\\n      <name>Singh Vijendra</name>\\n    </author>\\n    <author>\\n      <name>Pathak Shivani</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">arXiv admin note: text overlap with arXiv:1402.6859 by other authors\\n  without attribution</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1406.5074v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1406.5074v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1406.5212v1</id>\\n    <updated>2014-06-19T20:56:08Z</updated>\\n    <published>2014-06-19T20:56:08Z</published>\\n    <title>R-CNNs for Pose Estimation and Action Detection</title>\\n    <summary>  We present convolutional neural networks for the tasks of keypoint (pose)\\nprediction and action classification of people in unconstrained images. Our\\napproach involves training an R-CNN detector with loss functions depending on\\nthe task being tackled. We evaluate our method on the challenging PASCAL VOC\\ndataset and compare it to previous leading approaches. Our method gives\\nstate-of-the-art results for keypoint and action prediction. Additionally, we\\nintroduce a new dataset for action detection, the task of simultaneously\\nlocalizing people and classifying their actions, and present results using our\\napproach.\\n</summary>\\n    <author>\\n      <name>Georgia Gkioxari</name>\\n    </author>\\n    <author>\\n      <name>Bharath Hariharan</name>\\n    </author>\\n    <author>\\n      <name>Ross Girshick</name>\\n    </author>\\n    <author>\\n      <name>Jitendra Malik</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1406.5212v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1406.5212v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1406.5807v1</id>\\n    <updated>2014-06-23T05:00:31Z</updated>\\n    <published>2014-06-23T05:00:31Z</published>\\n    <title>A Unified Quantitative Model of Vision and Audition</title>\\n    <summary>  We have put forwards a unified quantitative framework of vision and audition,\\nbased on existing data and theories. According to this model, the retina is a\\nfeedforward network self-adaptive to inputs in a specific period. After fully\\ngrown, cells become specialized detectors based on statistics of stimulus\\nhistory. This model has provided explanations for perception mechanisms of\\ncolour, shape, depth and motion. Moreover, based on this ground we have put\\nforwards a bold conjecture that single ear can detect sound direction. This is\\ncomplementary to existing theories and has provided better explanations for\\nsound localization.\\n</summary>\\n    <author>\\n      <name>Peilei Liu</name>\\n    </author>\\n    <author>\\n      <name>Ting Wang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages, 3 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1406.5807v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1406.5807v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-bio.NC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-bio.QM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.5.4; I.5.2\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1406.5947v1</id>\\n    <updated>2014-06-23T15:34:54Z</updated>\\n    <published>2014-06-23T15:34:54Z</published>\\n    <title>Committees of deep feedforward networks trained with few data</title>\\n    <summary>  Deep convolutional neural networks are known to give good results on image\\nclassification tasks. In this paper we present a method to improve the\\nclassification result by combining multiple such networks in a committee. We\\nadopt the STL-10 dataset which has very few training examples and show that our\\nmethod can achieve results that are better than the state of the art. The\\nnetworks are trained layer-wise and no backpropagation is used. We also explore\\nthe effects of dataset augmentation by mirroring, rotation, and scaling.\\n</summary>\\n    <author>\\n      <name>Bogdan Miclut</name>\\n    </author>\\n    <author>\\n      <name>Thomas Kaester</name>\\n    </author>\\n    <author>\\n      <name>Thomas Martinetz</name>\\n    </author>\\n    <author>\\n      <name>Erhardt Barth</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1406.5947v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1406.5947v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1406.6201v1</id>\\n    <updated>2014-06-24T10:57:50Z</updated>\\n    <published>2014-06-24T10:57:50Z</published>\\n    <title>Saccadic Eye Movements and the Generalized Pareto Distribution</title>\\n    <summary>  We describe a statistical analysis of the eye tracker measurements in a\\ndatabase with 15 observers viewing 1003 images under free-viewing conditions.\\nIn contrast to the common approach of investigating the properties of the\\nfixation points we analyze the properties of the transition phases between\\nfixations. We introduce hyperbolic geometry as a tool to measure the step\\nlength between consecutive eye positions. We show that the step lengths,\\nmeasured in hyperbolic and euclidean geometry, follow a generalized Pareto\\ndistribution. The results based on the hyperbolic distance are more robust than\\nthose based on euclidean geometry. We show how the structure of the space of\\ngeneralized Pareto distributions can be used to characterize and identify\\nindividual observers.\\n</summary>\\n    <author>\\n      <name>Reiner Lenz</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1406.6201v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1406.6201v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.5.4\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1406.6507v1</id>\\n    <updated>2014-06-25T09:35:40Z</updated>\\n    <published>2014-06-25T09:35:40Z</published>\\n    <title>Weakly-supervised Discovery of Visual Pattern Configurations</title>\\n    <summary>  The increasing prominence of weakly labeled data nurtures a growing demand\\nfor object detection methods that can cope with minimal supervision. We propose\\nan approach that automatically identifies discriminative configurations of\\nvisual patterns that are characteristic of a given object class. We formulate\\nthe problem as a constrained submodular optimization problem and demonstrate\\nthe benefits of the discovered configurations in remedying mislocalizations and\\nfinding informative positive and negative training examples. Together, these\\nlead to state-of-the-art weakly-supervised detection results on the challenging\\nPASCAL VOC dataset.\\n</summary>\\n    <author>\\n      <name>Hyun Oh Song</name>\\n    </author>\\n    <author>\\n      <name>Yong Jae Lee</name>\\n    </author>\\n    <author>\\n      <name>Stefanie Jegelka</name>\\n    </author>\\n    <author>\\n      <name>Trevor Darrell</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1406.6507v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1406.6507v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1406.6962v2</id>\\n    <updated>2014-07-22T15:11:02Z</updated>\\n    <published>2014-06-26T18:00:56Z</published>\\n    <title>How good are detection proposals, really?</title>\\n    <summary>  Current top performing Pascal VOC object detectors employ detection proposals\\nto guide the search for objects thereby avoiding exhaustive sliding window\\nsearch across images. Despite the popularity of detection proposals, it is\\nunclear which trade-offs are made when using them during object detection. We\\nprovide an in depth analysis of ten object proposal methods along with four\\nbaselines regarding ground truth annotation recall (on Pascal VOC 2007 and\\nImageNet 2013), repeatability, and impact on DPM detector performance. Our\\nfindings show common weaknesses of existing methods, and provide insights to\\nchoose the most adequate method for different settings.\\n</summary>\\n    <author>\\n      <name>Jan Hosang</name>\\n    </author>\\n    <author>\\n      <name>Rodrigo Benenson</name>\\n    </author>\\n    <author>\\n      <name>Bernt Schiele</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1406.6962v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1406.6962v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1406.7444v1</id>\\n    <updated>2014-06-28T21:56:31Z</updated>\\n    <published>2014-06-28T21:56:31Z</published>\\n    <title>Learning to Deblur</title>\\n    <summary>  We describe a learning-based approach to blind image deconvolution. It uses a\\ndeep layered architecture, parts of which are borrowed from recent work on\\nneural network learning, and parts of which incorporate computations that are\\nspecific to image deconvolution. The system is trained end-to-end on a set of\\nartificially generated training examples, enabling competitive performance in\\nblind deconvolution, both with respect to quality and runtime.\\n</summary>\\n    <author>\\n      <name>Christian J. Schuler</name>\\n    </author>\\n    <author>\\n      <name>Michael Hirsch</name>\\n    </author>\\n    <author>\\n      <name>Stefan Harmeling</name>\\n    </author>\\n    <author>\\n      <name>Bernhard Sch\\xc3\\xb6lkopf</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1406.7444v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1406.7444v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1407.0221v1</id>\\n    <updated>2014-07-01T13:06:00Z</updated>\\n    <published>2014-07-01T13:06:00Z</published>\\n    <title>Imaging with Kantorovich-Rubinstein discrepancy</title>\\n    <summary>  We propose the use of the Kantorovich-Rubinstein norm from optimal transport\\nin imaging problems. In particular, we discuss a variational regularisation\\nmodel endowed with a Kantorovich-Rubinstein discrepancy term and total\\nvariation regularization in the context of image denoising and cartoon-texture\\ndecomposition. We point out connections of this approach to several other\\nrecently proposed methods such as total generalized variation and norms\\ncapturing oscillating patterns. We also show that the respective optimization\\nproblem can be turned into a convex-concave saddle point problem with simple\\nconstraints and hence, can be solved by standard tools. Numerical examples\\nexhibit interesting features and favourable performance for denoising and\\ncartoon-texture decomposition.\\n</summary>\\n    <author>\\n      <name>Jan Lellmann</name>\\n    </author>\\n    <author>\\n      <name>Dirk A. Lorenz</name>\\n    </author>\\n    <author>\\n      <name>Carola Sch\\xc3\\xb6nlieb</name>\\n    </author>\\n    <author>\\n      <name>Tuomo Valkonen</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1137/140975528</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1137/140975528\" rel=\"related\"/>\\n    <link href=\"http://arxiv.org/abs/1407.0221v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1407.0221v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.NA\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1407.1610v2</id>\\n    <updated>2014-09-22T17:49:01Z</updated>\\n    <published>2014-07-07T08:00:57Z</published>\\n    <title>Analyzing the Performance of Multilayer Neural Networks for Object\\n  Recognition</title>\\n    <summary>  In the last two years, convolutional neural networks (CNNs) have achieved an\\nimpressive suite of results on standard recognition datasets and tasks.\\nCNN-based features seem poised to quickly replace engineered representations,\\nsuch as SIFT and HOG. However, compared to SIFT and HOG, we understand much\\nless about the nature of the features learned by large CNNs. In this paper, we\\nexperimentally probe several aspects of CNN feature learning in an attempt to\\nhelp practitioners gain useful, evidence-backed intuitions about how to apply\\nCNNs to computer vision problems.\\n</summary>\\n    <author>\\n      <name>Pulkit Agrawal</name>\\n    </author>\\n    <author>\\n      <name>Ross Girshick</name>\\n    </author>\\n    <author>\\n      <name>Jitendra Malik</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Published in European Conference on Computer Vision 2014 (ECCV-2014)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1407.1610v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1407.1610v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1407.2961v1</id>\\n    <updated>2014-07-10T20:55:25Z</updated>\\n    <published>2014-07-10T20:55:25Z</published>\\n    <title>On the Convergence of the Mean Shift Algorithm in the One-Dimensional\\n  Space</title>\\n    <summary>  The mean shift algorithm is a non-parametric and iterative technique that has\\nbeen used for finding modes of an estimated probability density function. It\\nhas been successfully employed in many applications in specific areas of\\nmachine vision, pattern recognition, and image processing. Although the mean\\nshift algorithm has been used in many applications, a rigorous proof of its\\nconvergence is still missing in the literature. In this paper we address the\\nconvergence of the mean shift algorithm in the one-dimensional space and prove\\nthat the sequence generated by the mean shift algorithm is a monotone and\\nconvergent sequence.\\n</summary>\\n    <author>\\n      <name>Youness Aliyari Ghassabeh</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1016/j.patrec.2013.05.004</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1016/j.patrec.2013.05.004\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">13 pages, 10 figures, Published in Pattern Recognition Letters</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Pattern Recognition Letters, 2013, vol. 34(12)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1407.2961v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1407.2961v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1407.3969v1</id>\\n    <updated>2014-07-15T12:56:35Z</updated>\\n    <published>2014-07-15T12:56:35Z</published>\\n    <title>An iterative approach to Hough transform without re-voting</title>\\n    <summary>  Many bone shapes in the human skeleton are characterized by profiles that can\\nbe associated to equations of algebraic curves. Fixing the parameters in the\\ncurve equation, by means of a classical pattern recognition procedure like the\\nHough transform technique, it is then possible to associate an equation to a\\nspecific bone profile. However, most skeleton districts are more accurately\\ndescribed by piecewise defined curves. This paper utilizes an iterative\\napproach of the Hough transform without re-voting, to provide an efficient\\nprocedure for describing the profile of a bone in the human skeleton as a\\ncollection of different but continuously attached curves.\\n</summary>\\n    <author>\\n      <name>Giorgio Ricca</name>\\n    </author>\\n    <author>\\n      <name>Mauro C. Beltrametti</name>\\n    </author>\\n    <author>\\n      <name>Anna Maria Massone</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1407.3969v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1407.3969v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68T45, 68U10\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1407.4739v1</id>\\n    <updated>2014-07-17T17:10:06Z</updated>\\n    <published>2014-07-17T17:10:06Z</published>\\n    <title>An landcover fuzzy logic classification by maximumlikelihood</title>\\n    <summary>  In present days remote sensing is most used application in many sectors. This\\nremote sensing uses different images like multispectral, hyper spectral or\\nultra spectral. The remote sensing image classification is one of the\\nsignificant method to classify image. In this state we classify the maximum\\nlikelihood classification with fuzzy logic. In this we experimenting fuzzy\\nlogic like spatial, spectral texture methods in that different sub methods to\\nbe used for image classification.\\n</summary>\\n    <author>\\n      <name>T. Sarath</name>\\n    </author>\\n    <author>\\n      <name>G. Nagalakshmi</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 Pages, 3 Figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1407.4739v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1407.4739v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1407.4867v2</id>\\n    <updated>2014-08-14T12:10:04Z</updated>\\n    <published>2014-07-18T01:45:17Z</published>\\n    <title>Analysis of Gait Pattern to Recognize the Human Activities</title>\\n    <summary>  Human activity recognition based on the computer vision is the process of\\nlabelling image sequences with action labels. Accurate systems for this problem\\nare applied in areas such as visual surveillance, human computer interaction\\nand video retrieval.\\n</summary>\\n    <author>\\n      <name>Jay Prakash Gupta</name>\\n    </author>\\n    <author>\\n      <name>Pushkar Dixit</name>\\n    </author>\\n    <author>\\n      <name>Nishant Singh</name>\\n    </author>\\n    <author>\\n      <name>Vijay Bhaskar Semwal</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This paper has been withdrawn by the author due to a crucial sign\\n  error in equation 3</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1407.4867v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1407.4867v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1407.6423v1</id>\\n    <updated>2014-07-24T01:39:33Z</updated>\\n    <published>2014-07-24T01:39:33Z</published>\\n    <title>Performance evaluation of wavelet scattering network in image texture\\n  classification in various color spaces</title>\\n    <summary>  Texture plays an important role in many image analysis applications. In this\\npaper, we give a performance evaluation of color texture classification by\\nperforming wavelet scattering network in various color spaces. Experimental\\nresults on the KTH_TIPS_COL database show that opponent RGB based wavelet\\nscattering network outperforms other color spaces. Therefore, when dealing with\\nthe problem of color texture classification, opponent RGB based wavelet\\nscattering network is recommended.\\n</summary>\\n    <author>\\n      <name>Jiasong Wu</name>\\n    </author>\\n    <author>\\n      <name>Longyu Jiang</name>\\n    </author>\\n    <author>\\n      <name>Xu Han</name>\\n    </author>\\n    <author>\\n      <name>Lotfi Senhadji</name>\\n    </author>\\n    <author>\\n      <name>Huazhong Shu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages, 4 figures, 2 tables</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1407.6423v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1407.6423v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1407.7686v1</id>\\n    <updated>2014-07-29T10:29:28Z</updated>\\n    <published>2014-07-29T10:29:28Z</published>\\n    <title>Hyperspectral Imaging and Analysis for Sparse Reconstruction and\\n  Recognition</title>\\n    <summary>  This thesis proposes spatio-spectral techniques for hyperspectral image\\nanalysis. Adaptive spatio-spectral support and variable exposure hyperspectral\\nimaging is demonstrated to improve spectral reflectance recovery from\\nhyperspectral images. Novel spectral dimensionality reduction techniques have\\nbeen proposed from the perspective of spectral only and spatio-spectral\\ninformation preservation. It was found that the joint sparse and joint group\\nsparse hyperspectral image models achieve lower reconstruction error and higher\\nrecognition accuracy using only a small subset of bands. Hyperspectral image\\ndatabases have been developed and made publicly available for further research\\nin compressed hyperspectral imaging, forensic document analysis and spectral\\nreflectance recovery.\\n</summary>\\n    <author>\\n      <name>Zohaib Khan</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">PhD Thesis, School of Computer Science and Software Engineering, The\\n  University of Western Australia</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1407.7686v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1407.7686v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.4.1; I.4.5; I.4.7; I.4.10; I.5.4; I.7.5\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1407.8121v1</id>\\n    <updated>2014-07-30T16:45:00Z</updated>\\n    <published>2014-07-30T16:45:00Z</published>\\n    <title>Clustering Approach Towards Image Segmentation: An Analytical Study</title>\\n    <summary>  Image processing is an important research area in computer vision. Image\\nsegmentation plays the vital rule in image processing research. There exist so\\nmany methods for image segmentation. Clustering is an unsupervised study.\\nClustering can also be used for image segmentation. In this paper, an in-depth\\nstudy is done on different clustering techniques that can be used for image\\nsegmentation with their pros and cons. An experiment for color image\\nsegmentation based on clustering with K-Means algorithm is performed to observe\\nthe accuracy of clustering technique for the segmentation purpose.\\n</summary>\\n    <author>\\n      <name>Dibya Jyoti Bora</name>\\n    </author>\\n    <author>\\n      <name>Anil Kumar Gupta</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages, 3 figures</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">International Journal of Research in Computer Applications and\\n  Robotics, ISSN 2320-7345, Vol.2, Issue.7, Pg.: 115-124 July 2014</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1407.8121v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1407.8121v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1408.2938v1</id>\\n    <updated>2014-08-13T08:27:12Z</updated>\\n    <published>2014-08-13T08:27:12Z</published>\\n    <title>Learning Multi-Scale Representations for Material Classification</title>\\n    <summary>  The recent progress in sparse coding and deep learning has made unsupervised\\nfeature learning methods a strong competitor to hand-crafted descriptors. In\\ncomputer vision, success stories of learned features have been predominantly\\nreported for object recognition tasks. In this paper, we investigate if and how\\nfeature learning can be used for material recognition. We propose two\\nstrategies to incorporate scale information into the learning procedure\\nresulting in a novel multi-scale coding procedure. Our results show that our\\nlearned features for material recognition outperform hand-crafted descriptors\\non the FMD and the KTH-TIPS2 material classification benchmarks.\\n</summary>\\n    <author>\\n      <name>Wenbin Li</name>\\n    </author>\\n    <author>\\n      <name>Mario Fritz</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1408.2938v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1408.2938v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1408.3139v1</id>\\n    <updated>2014-07-10T21:06:02Z</updated>\\n    <published>2014-07-10T21:06:02Z</published>\\n    <title>Real-Time Impulse Noise Suppression from Images Using an Efficient\\n  Weighted-Average Filtering</title>\\n    <summary>  In this paper, we propose a method for real-time high density impulse noise\\nsuppression from images. In our method, we first apply an impulse detector to\\nidentify the corrupted pixels and then employ an innovative weighted-average\\nfilter to restore them. The filter takes the nearest neighboring interpolated\\nimage as the initial image and computes the weights according to the relative\\npositions of the corrupted and uncorrupted pixels. Experimental results show\\nthat the proposed method outperforms the best existing methods in both PSNR\\nmeasure and visual quality and is quite suitable for real-time applications.\\n</summary>\\n    <author>\\n      <name>Hossein Hosseini</name>\\n    </author>\\n    <author>\\n      <name>Farzad Hessar</name>\\n    </author>\\n    <author>\\n      <name>Farokh Marvasti</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/LSP.2014.2381649</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/LSP.2014.2381649\" rel=\"related\"/>\\n    <link href=\"http://arxiv.org/abs/1408.3139v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1408.3139v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1408.3573v1</id>\\n    <updated>2014-08-15T15:51:29Z</updated>\\n    <published>2014-08-15T15:51:29Z</published>\\n    <title>Turkish Presidential Elections TRT Publicity Speech Facial Expression\\n  Analysis</title>\\n    <summary>  In this paper, facial expressions of the three Turkish presidential\\ncandidates Demirtas, Erdogan and Ihsanoglu (in alphabetical order) are analyzed\\nduring the publicity speeches featured at TRT (Turkish Radio and Television) on\\n03.08.2014. FaceReader is used for the analysis where 3D modeling of the face\\nis achieved using the active appearance models (AAM). Over 500 landmark points\\nare tracked and analyzed for obtaining the facial expressions during the whole\\nspeech. All source videos and the data are publicly available for research\\npurposes.\\n</summary>\\n    <author>\\n      <name>H. Emrah Tasli</name>\\n    </author>\\n    <author>\\n      <name>Paul Ivan</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">2 pages 3 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1408.3573v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1408.3573v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1408.3750v1</id>\\n    <updated>2014-08-16T17:11:44Z</updated>\\n    <published>2014-08-16T17:11:44Z</published>\\n    <title>Real-time emotion recognition for gaming using deep convolutional\\n  network features</title>\\n    <summary>  The goal of the present study is to explore the application of deep\\nconvolutional network features to emotion recognition. Results indicate that\\nthey perform similarly to other published models at a best recognition rate of\\n94.4%, and do so with a single still image rather than a video stream. An\\nimplementation of an affective feedback game is also described, where a\\nclassifier using these features tracks the facial expressions of a player in\\nreal-time.\\n</summary>\\n    <author>\\n      <name>S\\xc3\\xa9bastien Ouellet</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages, 8 figures, IEEE style</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1408.3750v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1408.3750v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1408.5552v1</id>\\n    <updated>2014-08-24T04:46:58Z</updated>\\n    <published>2014-08-24T04:46:58Z</published>\\n    <title>Fuzzy and entropy facial recognition</title>\\n    <summary>  This paper suggests an effective method for facial recognition using fuzzy\\ntheory and Shannon entropy. Combination of fuzzy theory and Shannon entropy\\neliminates the complication of other methods. Shannon entropy calculates the\\nratio of an element between faces, and fuzzy theory calculates the member ship\\nof the entropy with 1. More details will be mentioned in Section 3. The\\nlearning performance is better than others as it is very simple, and only need\\ntwo data per learning. By using factors that don\\'t usually change during the\\nlife, the method will have a high accuracy.\\n</summary>\\n    <author>\\n      <name>Jaejun Lee</name>\\n    </author>\\n    <author>\\n      <name>Taeseon Yun</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1408.5552v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1408.5552v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68T10\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1408.6915v1</id>\\n    <updated>2014-08-29T03:34:52Z</updated>\\n    <published>2014-08-29T03:34:52Z</published>\\n    <title>Binary matrices of optimal autocorrelations as alignment marks</title>\\n    <summary>  We define a new class of binary matrices by maximizing the peak-sidelobe\\ndistances in the aperiodic autocorrelations. These matrices can be used as\\nrobust position marks for in-plane spatial alignment. The optimal square\\nmatrices of dimensions up to 7 by 7 and optimal diagonally-symmetric matrices\\nof 8 by 8 and 9 by 9 were found by exhaustive searches.\\n</summary>\\n    <author>\\n      <name>Scott A. Skirlo</name>\\n    </author>\\n    <author>\\n      <name>Ling Lu</name>\\n    </author>\\n    <author>\\n      <name>Marin Solja\\xc4\\x8di\\xc4\\x87</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1116/1.4913316</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1116/1.4913316\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, 6 figures and 1 table</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1408.6915v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1408.6915v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1408.6963v1</id>\\n    <updated>2014-08-29T09:37:56Z</updated>\\n    <published>2014-08-29T09:37:56Z</published>\\n    <title>Comment on \"Ensemble Projection for Semi-supervised Image\\n  Classification\"</title>\\n    <summary>  In a series of papers by Dai and colleagues [1,2], a feature map (or kernel)\\nwas introduced for semi- and unsupervised learning. This feature map is build\\nfrom the output of an ensemble of classifiers trained without using the\\nground-truth class labels. In this critique, we analyze the latest version of\\nthis series of papers, which is called Ensemble Projections [2]. We show that\\nthe results reported in [2] were not well conducted, and that Ensemble\\nProjections performs poorly for semi-supervised learning.\\n</summary>\\n    <author>\\n      <name>Xavier Boix</name>\\n    </author>\\n    <author>\\n      <name>Gemma Roig</name>\\n    </author>\\n    <author>\\n      <name>Luc Van Gool</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1408.6963v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1408.6963v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1409.0347v1</id>\\n    <updated>2014-09-01T09:46:52Z</updated>\\n    <published>2014-09-01T09:46:52Z</published>\\n    <title>Multi-tensor Completion for Estimating Missing Values in Video Data</title>\\n    <summary>  Many tensor-based data completion methods aim to solve image and video\\nin-painting problems. But, all methods were only developed for a single\\ndataset. In most of real applications, we can usually obtain more than one\\ndataset to reflect one phenomenon, and all the datasets are mutually related in\\nsome sense. Thus one question raised whether such the relationship can improve\\nthe performance of data completion or not? In the paper, we proposed a novel\\nand efficient method by exploiting the relationship among datasets for\\nmulti-video data completion. Numerical results show that the proposed method\\nsignificantly improve the performance of video in-painting, particularly in the\\ncase of very high missing percentage.\\n</summary>\\n    <author>\\n      <name>Chao Li</name>\\n    </author>\\n    <author>\\n      <name>Lili Guo</name>\\n    </author>\\n    <author>\\n      <name>Andrzej Cichocki</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1409.0347v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1409.0347v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1409.0908v1</id>\\n    <updated>2014-09-02T22:34:29Z</updated>\\n    <published>2014-09-02T22:34:29Z</published>\\n    <title>Action Recognition in the Frequency Domain</title>\\n    <summary>  In this paper, we describe a simple strategy for mitigating variability in\\ntemporal data series by shifting focus onto long-term, frequency domain\\nfeatures that are less susceptible to variability. We apply this method to the\\nhuman action recognition task and demonstrate how working in the frequency\\ndomain can yield good recognition features for commonly used optical flow and\\narticulated pose features, which are highly sensitive to small differences in\\nmotion, viewpoint, dynamic backgrounds, occlusion and other sources of\\nvariability. We show how these frequency-based features can be used in\\ncombination with a simple forest classifier to achieve good and robust results\\non the popular KTH Actions dataset.\\n</summary>\\n    <author>\\n      <name>Anh Tran</name>\\n    </author>\\n    <author>\\n      <name>Jinyan Guan</name>\\n    </author>\\n    <author>\\n      <name>Thanima Pilantanakitti</name>\\n    </author>\\n    <author>\\n      <name>Paul Cohen</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Keywords: Artificial Intelligence, Computer Vision, Action\\n  Recognition</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1409.0908v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1409.0908v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1409.1789v1</id>\\n    <updated>2014-09-05T13:36:51Z</updated>\\n    <published>2014-09-05T13:36:51Z</published>\\n    <title>Identifying Synapses Using Deep and Wide Multiscale Recursive Networks</title>\\n    <summary>  In this work, we propose a learning framework for identifying synapses using\\na deep and wide multi-scale recursive (DAWMR) network, previously considered in\\nimage segmentation applications. We apply this approach on electron microscopy\\ndata from invertebrate fly brain tissue. By learning features directly from the\\ndata, we are able to achieve considerable improvements over existing techniques\\nthat rely on a small set of hand-designed features. We show that this system\\ncan reduce the amount of manual annotation required, in both acquisition of\\ntraining data as well as verification of inferred detections.\\n</summary>\\n    <author>\\n      <name>Gary B. Huang</name>\\n    </author>\\n    <author>\\n      <name>Stephen Plaza</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1409.1789v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1409.1789v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1409.2413v1</id>\\n    <updated>2014-08-25T03:39:41Z</updated>\\n    <published>2014-08-25T03:39:41Z</published>\\n    <title>Image processing</title>\\n    <summary>  Gabor filters can extract multi-orientation and multiscale features from face\\nimages. Researchers have designed different ways to use the magnitude of the\\nfiltered results for face recognition: Gabor Fisher classifier exploited only\\nthe magnitude information of Gabor magnitude pictures (GMPs); Local Gabor\\nBinary Pattern uses only the gradient information. In this paper, we regard\\nGMPs as smooth surfaces. By completely describing the shape of GMPs, we get a\\nface representation method called Gabor Surface Feature (GSF). First, we\\ncompute the magnitude, 1st and 2nd derivatives of GMPs, then binarize them and\\ntransform them into decimal values. Finally we construct joint histograms and\\nuse subspace methods for classification. Experiments on FERET, ORL and FRGC\\n1.0.4 database show the effectiveness of GSF.\\n</summary>\\n    <author>\\n      <name>Franco Rino</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1409.2413v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1409.2413v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1409.3024v1</id>\\n    <updated>2014-09-10T11:18:08Z</updated>\\n    <published>2014-09-10T11:18:08Z</published>\\n    <title>One-Dimensional Vector based Pattern Matching</title>\\n    <summary>  Template matching is a basic method in image analysis to extract useful\\ninformation from images. In this paper, we suggest a new method for pattern\\nmatching. Our method transform the template image from two dimensional image\\ninto one dimensional vector. Also all sub-windows (same size of template) in\\nthe reference image will transform into one dimensional vectors. The three\\nsimilarity measures SAD, SSD, and Euclidean are used to compute the likeness\\nbetween template and all sub-windows in the reference image to find the best\\nmatch. The experimental results show the superior performance of the proposed\\nmethod over the conventional methods on various template of different sizes.\\n</summary>\\n    <author>\\n      <name>Y. M. Fouda</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1409.3024v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1409.3024v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1409.4205v1</id>\\n    <updated>2014-09-15T10:53:04Z</updated>\\n    <published>2014-09-15T10:53:04Z</published>\\n    <title>Speeding-up Graphical Model Optimization via a Coarse-to-fine Cascade of\\n  Pruning Classifiers</title>\\n    <summary>  We propose a general and versatile framework that significantly speeds-up\\ngraphical model optimization while maintaining an excellent solution accuracy.\\nThe proposed approach relies on a multi-scale pruning scheme that is able to\\nprogressively reduce the solution space by use of a novel strategy based on a\\ncoarse-to-fine cascade of learnt classifiers. We thoroughly experiment with\\nclassic computer vision related MRF problems, where our framework constantly\\nyields a significant time speed-up (with respect to the most efficient\\ninference methods) and obtains a more accurate solution than directly\\noptimizing the MRF.\\n</summary>\\n    <author>\\n      <name>B. Conejo</name>\\n    </author>\\n    <author>\\n      <name>N. Komodakis</name>\\n    </author>\\n    <author>\\n      <name>S. Leprince</name>\\n    </author>\\n    <author>\\n      <name>J. P. Avouac</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1409.4205v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1409.4205v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1409.4326v2</id>\\n    <updated>2015-10-20T15:08:48Z</updated>\\n    <published>2014-09-15T16:54:42Z</published>\\n    <title>Computing the Stereo Matching Cost with a Convolutional Neural Network</title>\\n    <summary>  We present a method for extracting depth information from a rectified image\\npair. We train a convolutional neural network to predict how well two image\\npatches match and use it to compute the stereo matching cost. The cost is\\nrefined by cross-based cost aggregation and semiglobal matching, followed by a\\nleft-right consistency check to eliminate errors in the occluded regions. Our\\nstereo method achieves an error rate of 2.61 % on the KITTI stereo dataset and\\nis currently (August 2014) the top performing method on this dataset.\\n</summary>\\n    <author>\\n      <name>Jure \\xc5\\xbdbontar</name>\\n    </author>\\n    <author>\\n      <name>Yann LeCun</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/CVPR.2015.7298767</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/CVPR.2015.7298767\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Conference on Computer Vision and Pattern Recognition (CVPR), June\\n  2015</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1409.4326v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1409.4326v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1409.4689v2</id>\\n    <updated>2015-05-24T09:16:03Z</updated>\\n    <published>2014-09-16T16:31:07Z</published>\\n    <title>Compute Less to Get More: Using ORC to Improve Sparse Filtering</title>\\n    <summary>  Sparse Filtering is a popular feature learning algorithm for image\\nclassification pipelines. In this paper, we connect the performance of Sparse\\nFiltering with spectral properties of the corresponding feature matrices. This\\nconnection provides new insights into Sparse Filtering; in particular, it\\nsuggests early stopping of Sparse Filtering. We therefore introduce the Optimal\\nRoundness Criterion (ORC), a novel stopping criterion for Sparse Filtering. We\\nshow that this stopping criterion is related with pre-processing procedures\\nsuch as Statistical Whitening and demonstrate that it can make image\\nclassification with Sparse Filtering considerably faster and more accurate.\\n</summary>\\n    <author>\\n      <name>Johannes Lederer</name>\\n    </author>\\n    <author>\\n      <name>Sergio Guadarrama</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1409.4689v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1409.4689v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1409.5114v2</id>\\n    <updated>2014-10-10T13:23:30Z</updated>\\n    <published>2014-09-17T19:55:34Z</published>\\n    <title>A Survey on Heterogeneous Face Recognition: Sketch, Infra-red, 3D and\\n  Low-resolution</title>\\n    <summary>  Heterogeneous face recognition (HFR) refers to matching face imagery across\\ndifferent domains. It has received much interest from the research community as\\na result of its profound implications in law enforcement. A wide variety of new\\ninvariant features, cross-modality matching models and heterogeneous datasets\\nbeing established in recent years. This survey provides a comprehensive review\\nof established techniques and recent developments in HFR. Moreover, we offer a\\ndetailed account of datasets and benchmarks commonly used for evaluation. We\\nfinish by assessing the state of the field and discussing promising directions\\nfor future research.\\n</summary>\\n    <author>\\n      <name>Shuxin Ouyang</name>\\n    </author>\\n    <author>\\n      <name>Timothy Hospedales</name>\\n    </author>\\n    <author>\\n      <name>Yi-Zhe Song</name>\\n    </author>\\n    <author>\\n      <name>Xueming Li</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">survey paper(35 pages)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1409.5114v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1409.5114v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"A.1; I.4.9; I.5.4\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1409.5230v1</id>\\n    <updated>2014-09-18T09:10:28Z</updated>\\n    <published>2014-09-18T09:10:28Z</published>\\n    <title>Deep Regression for Face Alignment</title>\\n    <summary>  In this paper, we present a deep regression approach for face alignment. The\\ndeep architecture consists of a global layer and multi-stage local layers. We\\napply the back-propagation algorithm with the dropout strategy to jointly\\noptimize the regression parameters. We show that the resulting deep regressor\\ngradually and evenly approaches the true facial landmarks stage by stage,\\navoiding the tendency to yield over-strong early stage regressors while\\nover-weak later stage regressors. Experimental results show that our approach\\nachieves the state-of-the-art\\n</summary>\\n    <author>\\n      <name>Baoguang Shi</name>\\n    </author>\\n    <author>\\n      <name>Xiang Bai</name>\\n    </author>\\n    <author>\\n      <name>Wenyu Liu</name>\\n    </author>\\n    <author>\\n      <name>Jingdong Wang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1409.5230v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1409.5230v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1409.5957v2</id>\\n    <updated>2015-02-10T08:24:48Z</updated>\\n    <published>2014-09-21T09:10:24Z</published>\\n    <title>A Global Approach for Solving Edge-Matching Puzzles</title>\\n    <summary>  We consider apictorial edge-matching puzzles, in which the goal is to arrange\\na collection of puzzle pieces with colored edges so that the colors match along\\nthe edges of adjacent pieces. We devise an algebraic representation for this\\nproblem and provide conditions under which it exactly characterizes a puzzle.\\nUsing the new representation, we recast the combinatorial, discrete problem of\\nsolving puzzles as a global, polynomial system of equations with continuous\\nvariables. We further propose new algorithms for generating approximate\\nsolutions to the continuous problem by solving a sequence of convex\\nrelaxations.\\n</summary>\\n    <author>\\n      <name>Shahar Z. Kovalsky</name>\\n    </author>\\n    <author>\\n      <name>Daniel Glasner</name>\\n    </author>\\n    <author>\\n      <name>Ronen Basri</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">SIAM J. Imaging Sciences, Vol. 8, Issue 2, 916--938, 2015</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1409.5957v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1409.5957v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1409.6745v1</id>\\n    <updated>2014-09-23T20:25:46Z</updated>\\n    <published>2014-09-23T20:25:46Z</published>\\n    <title>A Concept Learning Approach to Multisensory Object Perception</title>\\n    <summary>  This paper presents a computational model of concept learning using Bayesian\\ninference for a grammatically structured hypothesis space, and test the model\\non multisensory (visual and haptics) recognition of 3D objects. The study is\\nperformed on a set of artificially generated 3D objects known as fribbles,\\nwhich are complex, multipart objects with categorical structures. The goal of\\nthis work is to develop a working multisensory representational model that\\nintegrates major themes on concepts and concepts learning from the cognitive\\nscience literature. The model combines the representational power of a\\nprobabilistic generative grammar with the inferential power of Bayesian\\ninduction.\\n</summary>\\n    <author>\\n      <name>Ifeoma Nwogu</name>\\n    </author>\\n    <author>\\n      <name>Goker Erdogan</name>\\n    </author>\\n    <author>\\n      <name>Ilker Yildirim</name>\\n    </author>\\n    <author>\\n      <name>Robert Jacobs</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages and 6 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1409.6745v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1409.6745v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1409.7307v1</id>\\n    <updated>2014-09-25T15:52:05Z</updated>\\n    <published>2014-09-25T15:52:05Z</published>\\n    <title>Image Classification with A Deep Network Model based on Compressive\\n  Sensing</title>\\n    <summary>  To simplify the parameter of the deep learning network, a cascaded\\ncompressive sensing model \"CSNet\" is implemented for image classification.\\nFirstly, we use cascaded compressive sensing network to learn feature from the\\ndata. Secondly, CSNet generates the feature by binary hashing and block-wise\\nhistograms. Finally, a linear SVM classifier is used to classify these\\nfeatures. The experiments on the MNIST dataset indicate that higher\\nclassification accuracy can be obtained by this algorithm.\\n</summary>\\n    <author>\\n      <name>Yufei Gan</name>\\n    </author>\\n    <author>\\n      <name>Tong Zhuo</name>\\n    </author>\\n    <author>\\n      <name>Chu He</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1409.7307v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1409.7307v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1409.7313v1</id>\\n    <updated>2014-09-25T16:14:18Z</updated>\\n    <published>2014-09-25T16:14:18Z</published>\\n    <title>A Deep Graph Embedding Network Model for Face Recognition</title>\\n    <summary>  In this paper, we propose a new deep learning network \"GENet\", it combines\\nthe multi-layer network architec- ture and graph embedding framework. Firstly,\\nwe use simplest unsupervised learning PCA/LDA as first layer to generate the\\nlow- level feature. Secondly, many cascaded dimensionality reduction layers\\nbased on graph embedding framework are applied to GENet. Finally, a linear SVM\\nclassifier is used to classify dimension-reduced features. The experiments\\nindicate that higher classification accuracy can be obtained by this algorithm\\non the CMU-PIE, ORL, Extended Yale B dataset.\\n</summary>\\n    <author>\\n      <name>Yufei Gan</name>\\n    </author>\\n    <author>\\n      <name>Teng Yang</name>\\n    </author>\\n    <author>\\n      <name>Chu He</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1409.7313v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1409.7313v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1409.7556v3</id>\\n    <updated>2015-05-26T03:14:19Z</updated>\\n    <published>2014-09-26T12:36:54Z</published>\\n    <title>Location Recognition Over Large Time Lags</title>\\n    <summary>  Would it be possible to automatically associate ancient pictures to modern\\nones and create fancy cultural heritage city maps? We introduce here the task\\nof recognizing the location depicted in an old photo given modern annotated\\nimages collected from the Internet. We present an extensive analysis on\\ndifferent features, looking for the most discriminative and most robust to the\\nimage variability induced by large time lags. Moreover, we show that the\\ndescribed task benefits from domain adaptation.\\n</summary>\\n    <author>\\n      <name>Basura Fernando</name>\\n    </author>\\n    <author>\\n      <name>Tatiana Tommasi</name>\\n    </author>\\n    <author>\\n      <name>Tinne Tuytelaars</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1409.7556v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1409.7556v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1409.7963v1</id>\\n    <updated>2014-09-28T21:32:15Z</updated>\\n    <published>2014-09-28T21:32:15Z</published>\\n    <title>MoDeep: A Deep Learning Framework Using Motion Features for Human Pose\\n  Estimation</title>\\n    <summary>  In this work, we propose a novel and efficient method for articulated human\\npose estimation in videos using a convolutional network architecture, which\\nincorporates both color and motion features. We propose a new human body pose\\ndataset, FLIC-motion, that extends the FLIC dataset with additional motion\\nfeatures. We apply our architecture to this dataset and report significantly\\nbetter performance than current state-of-the-art pose detection systems.\\n</summary>\\n    <author>\\n      <name>Arjun Jain</name>\\n    </author>\\n    <author>\\n      <name>Jonathan Tompson</name>\\n    </author>\\n    <author>\\n      <name>Yann LeCun</name>\\n    </author>\\n    <author>\\n      <name>Christoph Bregler</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1409.7963v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1409.7963v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1410.0243v1</id>\\n    <updated>2014-10-01T14:41:55Z</updated>\\n    <published>2014-10-01T14:41:55Z</published>\\n    <title>Pattern Encoding on the Poincare Sphere</title>\\n    <summary>  This paper presents a convenient graphical tool for encoding visual patterns\\n(such as image patches and image atoms) as point constellations in a space\\nspanned by perceptual features and with a clear geometrical interpretation.\\nGeneral theory and a practical pattern encoding scheme are presented, inspired\\nby encoding polarization states of a light wave on the Poincare sphere. This\\nnew pattern encoding scheme can be useful for many applications in image\\nprocessing and computer vision. Here, three possible applications are\\nillustrated, in clustering perceptually similar patterns, visualizing\\nproperties of learned dictionaries of image atoms and generating new\\ndictionaries of image atoms from spherical codes.\\n</summary>\\n    <author>\\n      <name>Aleksandra Pizurica</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">26 pages, 23 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1410.0243v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1410.0243v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1410.3080v1</id>\\n    <updated>2014-10-12T11:43:37Z</updated>\\n    <published>2014-10-12T11:43:37Z</published>\\n    <title>Tree-Structure Bayesian Compressive Sensing for Video</title>\\n    <summary>  A Bayesian compressive sensing framework is developed for video\\nreconstruction based on the color coded aperture compressive temporal imaging\\n(CACTI) system. By exploiting the three dimension (3D) tree structure of the\\nwavelet and Discrete Cosine Transformation (DCT) coefficients, a Bayesian\\ncompressive sensing inversion algorithm is derived to reconstruct (up to 22)\\ncolor video frames from a single monochromatic compressive measurement. Both\\nsimulated and real datasets are adopted to verify the performance of the\\nproposed algorithm.\\n</summary>\\n    <author>\\n      <name>Xin Yuan</name>\\n    </author>\\n    <author>\\n      <name>Patrick Llull</name>\\n    </author>\\n    <author>\\n      <name>David J. Brady</name>\\n    </author>\\n    <author>\\n      <name>Lawrence Carin</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages, 4 Figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1410.3080v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1410.3080v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1410.3970v1</id>\\n    <updated>2014-10-15T08:42:25Z</updated>\\n    <published>2014-10-15T08:42:25Z</published>\\n    <title>Shape and Color Object Tracking for Real-Time Robotic Navigation</title>\\n    <summary>  This paper presents a real-time approach for single-colored ball detection\\nand tracking. The approach consists of two main phases. In a first offline\\ncalibration phase, the intrinsic parameters of the camera and the radial\\ndistortion are estimated, and a classification of colors is learned from a\\nsample image of colored balls. The second phase consists of four main steps:\\n(1) color segmentation of the input image into several regions based on the\\noffline classification, (2) robust estimation of the circle parameters (3)\\nrefinement of the circle parameters, and (4) ball tracking. The experimental\\nresults showed that the approach presents a good compromise between suitability\\nfor real-time navigation and robustness to occlusions, background congestion\\nand colors interference in the scene.\\n</summary>\\n    <author>\\n      <name>Haythem Ghazouani</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">in French</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1410.3970v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1410.3970v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1410.4017v1</id>\\n    <updated>2014-10-15T11:24:55Z</updated>\\n    <published>2014-10-15T11:24:55Z</published>\\n    <title>Online Tracking of Skin Colour Regions Against a Complex Background</title>\\n    <summary>  Online tracking of human activity against a complex background is a\\nchallenging task for many applications. In this paper, we have developed a\\nrobust technique for localizing skin colour regions from unconstrained image\\nframes. A simple and fast segmentation algorithm is used to train a multiplayer\\nperceptron (MLP) for detection of skin colours. Stepper motors are synchronized\\nwith the MLP to track the movement of the skin colour regions.\\n</summary>\\n    <author>\\n      <name>Subhadip Basu</name>\\n    </author>\\n    <author>\\n      <name>S. Chakraborty</name>\\n    </author>\\n    <author>\\n      <name>K. Mukherjee</name>\\n    </author>\\n    <author>\\n      <name>S. K. Pandit</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/INDICO.2004.1497734</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/INDICO.2004.1497734\" rel=\"related\"/>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proc. of IEEE INDICON, pp. 184-186, Dec-2004, Kharagpur</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1410.4017v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1410.4017v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1410.4393v2</id>\\n    <updated>2014-11-11T07:07:30Z</updated>\\n    <published>2014-10-16T12:25:50Z</published>\\n    <title>The HAWKwood Database</title>\\n    <summary>  We present a database consisting of wood pile images, which can be used as a\\nbenchmark to evaluate the performance of wood pile detection and surveying\\nalgorithms. We distinguish six database cate- gories which can be used for\\ndifferent types of algorithms. Images of real and synthetic scenes are\\nprovided, which consist of 7655 images divided into 354 data sets. Depending on\\nthe category the data sets either include ground truth data or forestry\\nspecific measurements with which algorithms may be compared.\\n</summary>\\n    <author>\\n      <name>Christopher Herbon</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1410.4393v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1410.4393v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1410.4441v1</id>\\n    <updated>2014-10-16T14:17:21Z</updated>\\n    <published>2014-10-16T14:17:21Z</published>\\n    <title>Improve CAPTCHA\\'s Security Using Gaussian Blur Filter</title>\\n    <summary>  Providing security for webservers against unwanted and automated\\nregistrations has become a big concern. To prevent these kinds of false\\nregistrations many websites use CAPTCHAs. Among all kinds of CAPTCHAs OCR-Based\\nor visual CAPTCHAs are very common. Actually visual CAPTCHA is an image\\ncontaining a sequence of characters. So far most of visual CAPTCHAs, in order\\nto resist against OCR programs, use some common implementations such as\\nwrapping the characters, random placement and rotations of characters, etc. In\\nthis paper we applied Gaussian Blur filter, which is an image transformation,\\nto visual CAPTCHAs to reduce their readability by OCR programs. We concluded\\nthat this technique made CAPTCHAs almost unreadable for OCR programs but, their\\nreadability by human users still remained high.\\n</summary>\\n    <author>\\n      <name>Ariyan Zarei</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1410.4441v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1410.4441v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1410.5358v3</id>\\n    <updated>2015-09-01T09:25:50Z</updated>\\n    <published>2014-10-20T17:15:50Z</published>\\n    <title>Remote sensing image classification exploiting multiple kernel learning</title>\\n    <summary>  We propose a strategy for land use classification which exploits Multiple\\nKernel Learning (MKL) to automatically determine a suitable combination of a\\nset of features without requiring any heuristic knowledge about the\\nclassification task. We present a novel procedure that allows MKL to achieve\\ngood performance in the case of small training sets. Experimental results on\\npublicly available datasets demonstrate the feasibility of the proposed\\napproach.\\n</summary>\\n    <author>\\n      <name>Claudio Cusano</name>\\n    </author>\\n    <author>\\n      <name>Paolo Napoletano</name>\\n    </author>\\n    <author>\\n      <name>Raimondo Schettini</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/LGRS.2015.2476365</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/LGRS.2015.2476365\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted for publication on the IEEE Geoscience and Remote Sensing\\n  letters</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1410.5358v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1410.5358v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1410.5894v1</id>\\n    <updated>2014-10-22T01:52:53Z</updated>\\n    <published>2014-10-22T01:52:53Z</published>\\n    <title>Vehicle Detection and Tracking Techniques: A Concise Review</title>\\n    <summary>  Vehicle detection and tracking applications play an important role for\\ncivilian and military applications such as in highway traffic surveillance\\ncontrol, management and urban traffic planning. Vehicle detection process on\\nroad are used for vehicle tracking, counts, average speed of each individual\\nvehicle, traffic analysis and vehicle categorizing objectives and may be\\nimplemented under different environments changes. In this review, we present a\\nconcise overview of image processing methods and analysis tools which used in\\nbuilding these previous mentioned applications that involved developing traffic\\nsurveillance systems. More precisely and in contrast with other reviews, we\\nclassified the processing methods under three categories for more clarification\\nto explain the traffic systems.\\n</summary>\\n    <author>\\n      <name>Raad Ahmed Hadi</name>\\n    </author>\\n    <author>\\n      <name>Ghazali Sulong</name>\\n    </author>\\n    <author>\\n      <name>Loay Edwar George</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.5121/sipij.2013.5101</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.5121/sipij.2013.5101\" rel=\"related\"/>\\n    <link href=\"http://arxiv.org/abs/1410.5894v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1410.5894v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1410.6333v3</id>\\n    <updated>2017-03-24T14:51:09Z</updated>\\n    <published>2014-10-23T12:04:31Z</published>\\n    <title>A Regularization Approach to Blind Deblurring and Denoising of QR\\n  Barcodes</title>\\n    <summary>  QR bar codes are prototypical images for which part of the image is a priori\\nknown (required patterns). Open source bar code readers, such as ZBar, are\\nreadily available. We exploit both these facts to provide and assess purely\\nregularization-based methods for blind deblurring of QR bar codes in the\\npresence of noise.\\n</summary>\\n    <author>\\n      <name>Yves van Gennip</name>\\n    </author>\\n    <author>\\n      <name>Prashant Athavale</name>\\n    </author>\\n    <author>\\n      <name>J\\xc3\\xa9r\\xc3\\xb4me Gilles</name>\\n    </author>\\n    <author>\\n      <name>Rustum Choksi</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/TIP.2015.2432675</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/TIP.2015.2432675\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">14 pages, 19 figures (with a total of 57 subfigures), 1 table; v3:\\n  previously missing reference [35] added</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1410.6333v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1410.6333v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.NA\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68U10, 65K10\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1410.7265v1</id>\\n    <updated>2014-10-27T14:58:28Z</updated>\\n    <published>2014-10-27T14:58:28Z</published>\\n    <title>An Unsupervised Ensemble-based Markov Random Field Approach to\\n  Microscope Cell Image Segmentation</title>\\n    <summary>  In this paper, we propose an approach to the unsupervised segmentation of\\nimages using Markov Random Field. The proposed approach is based on the idea of\\nBit Plane Slicing. We use the planes as initial labellings for an ensemble of\\nsegmentations. With pixelwise voting, a robust segmentation approach can be\\nachieved, which we demonstrate on microscope cell images. We tested our\\napproach on a publicly available database, where it proven to be competitive\\nwith other methods and manual segmentation.\\n</summary>\\n    <author>\\n      <name>Balint Antal</name>\\n    </author>\\n    <author>\\n      <name>Bence Remenyik</name>\\n    </author>\\n    <author>\\n      <name>Andras Hajdu</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.5220/0004612900940099</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.5220/0004612900940099\" rel=\"related\"/>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proceeingds of the 10th International Conference on Signal\\n  Processing and Multimedia Applications (SIGMAP 2013), Reykjavik, Iceland,\\n  2013, pp. 94-99</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1410.7265v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1410.7265v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-bio.QM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1410.7730v1</id>\\n    <updated>2014-10-28T18:55:37Z</updated>\\n    <published>2014-10-28T18:55:37Z</published>\\n    <title>New similarity index based on entropy and group theory</title>\\n    <summary>  In this work, we propose a new similarity index for images considering the\\nentropy function and group theory. This index considers an algebraic group of\\nimages, it is defined by an inner law that provides a novel approach for the\\nsubtraction of images. Through an equivalence relationship in the field of\\nimages, we prove the existence of the quotient group, on which the new\\nsimilarity index is defined. We also present the main properties of the new\\nindex, and the immediate application thereof as a stopping criterion of the\\n\"Mean Shift Iterative Algorithm\".\\n</summary>\\n    <author>\\n      <name>Yasel Garc\\xc3\\xa9s</name>\\n    </author>\\n    <author>\\n      <name>Esley Torres</name>\\n    </author>\\n    <author>\\n      <name>Osvaldo Pereira</name>\\n    </author>\\n    <author>\\n      <name>Roberto Rodr\\xc3\\xadguez</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">in Spanish</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1410.7730v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1410.7730v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1411.0791v1</id>\\n    <updated>2014-11-04T05:53:45Z</updated>\\n    <published>2014-11-04T05:53:45Z</published>\\n    <title>A Robust Point Sets Matching Method</title>\\n    <summary>  Point sets matching method is very important in computer vision, feature\\nextraction, fingerprint matching, motion estimation and so on. This paper\\nproposes a robust point sets matching method. We present an iterative algorithm\\nthat is robust to noise case. Firstly, we calculate all transformations between\\ntwo points. Then similarity matrix are computed to measure the possibility that\\ntwo transformation are both true. We iteratively update the matching score\\nmatrix by using the similarity matrix. By using matching algorithm on graph, we\\nobtain the matching result. Experimental results obtained by our approach show\\nrobustness to outlier and jitter.\\n</summary>\\n    <author>\\n      <name>Xiao Liu</name>\\n    </author>\\n    <author>\\n      <name>Congying Han</name>\\n    </author>\\n    <author>\\n      <name>Tiande Guo</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages, 3 figures, 4 tables</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1411.0791v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1411.0791v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1411.1171v1</id>\\n    <updated>2014-11-05T07:27:08Z</updated>\\n    <published>2014-11-05T07:27:08Z</published>\\n    <title>Multilinear Principal Component Analysis Network for Tensor Object\\n  Classification</title>\\n    <summary>  The recently proposed principal component analysis network (PCANet) has been\\nproved high performance for visual content classification. In this letter, we\\ndevelop a tensorial extension of PCANet, namely, multilinear principal analysis\\ncomponent network (MPCANet), for tensor object classification. Compared to\\nPCANet, the proposed MPCANet uses the spatial structure and the relationship\\nbetween each dimension of tensor objects much more efficiently. Experiments\\nwere conducted on different visual content datasets including UCF sports action\\nvideo sequences database and UCF11 database. The experimental results have\\nrevealed that the proposed MPCANet achieves higher classification accuracy than\\nPCANet for tensor object classification.\\n</summary>\\n    <author>\\n      <name>Rui Zeng</name>\\n    </author>\\n    <author>\\n      <name>Jiasong Wu</name>\\n    </author>\\n    <author>\\n      <name>Zhuhong Shao</name>\\n    </author>\\n    <author>\\n      <name>Lotfi Senhadji</name>\\n    </author>\\n    <author>\\n      <name>Huazhong Shu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, 3 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1411.1171v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1411.1171v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1411.1172v1</id>\\n    <updated>2014-11-05T07:36:04Z</updated>\\n    <published>2014-11-05T07:36:04Z</published>\\n    <title>Tensor object classification via multilinear discriminant analysis\\n  network</title>\\n    <summary>  This paper proposes a multilinear discriminant analysis network (MLDANet) for\\nthe recognition of multidimensional objects, known as tensor objects. The\\nMLDANet is a variation of linear discriminant analysis network (LDANet) and\\nprincipal component analysis network (PCANet), both of which are the recently\\nproposed deep learning algorithms. The MLDANet consists of three parts: 1) The\\nencoder learned by MLDA from tensor data. 2) Features maps ob-tained from\\ndecoder. 3) The use of binary hashing and histogram for feature pooling. A\\nlearning algorithm for MLDANet is described. Evaluations on UCF11 database\\nindicate that the proposed MLDANet outperforms the PCANet, LDANet, MPCA + LDA,\\nand MLDA in terms of classification for tensor objects.\\n</summary>\\n    <author>\\n      <name>Rui Zeng</name>\\n    </author>\\n    <author>\\n      <name>Jiasong Wu</name>\\n    </author>\\n    <author>\\n      <name>Lotfi Senhadji</name>\\n    </author>\\n    <author>\\n      <name>Huazhong Shu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages, 4 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1411.1172v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1411.1172v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1411.2214v1</id>\\n    <updated>2014-11-09T09:51:06Z</updated>\\n    <published>2014-11-09T09:51:06Z</published>\\n    <title>Abnormal Object Recognition: A Comprehensive Study</title>\\n    <summary>  When describing images, humans tend not to talk about the obvious, but rather\\nmention what they find interesting. We argue that abnormalities and deviations\\nfrom typicalities are among the most important components that form what is\\nworth mentioning. In this paper we introduce the abnormality detection as a\\nrecognition problem and show how to model typicalities and, consequently,\\nmeaningful deviations from prototypical properties of categories. Our model can\\nrecognize abnormalities and report the main reasons of any recognized\\nabnormality. We introduce the abnormality detection dataset and show\\ninteresting results on how to reason about abnormalities.\\n</summary>\\n    <author>\\n      <name>Babak Saleh</name>\\n    </author>\\n    <author>\\n      <name>Ali Farhadi</name>\\n    </author>\\n    <author>\\n      <name>Ahmed Elgammal</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1411.2214v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1411.2214v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1411.3041v1</id>\\n    <updated>2014-11-12T01:34:46Z</updated>\\n    <published>2014-11-12T01:34:46Z</published>\\n    <title>Collecting Image Description Datasets using Crowdsourcing</title>\\n    <summary>  We describe our two new datasets with images described by humans. Both the\\ndatasets were collected using Amazon Mechanical Turk, a crowdsourcing platform.\\nThe two datasets contain significantly more descriptions per image than other\\nexisting datasets. One is based on a popular image description dataset called\\nthe UIUC Pascal Sentence Dataset, whereas the other is based on the Abstract\\nScenes dataset con- taining images made from clipart objects. In this paper we\\ndescribe our interfaces, analyze some properties of and show example\\ndescriptions from our two datasets.\\n</summary>\\n    <author>\\n      <name>Ramakrishna Vedantam</name>\\n    </author>\\n    <author>\\n      <name>C. Lawrence Zitnick</name>\\n    </author>\\n    <author>\\n      <name>Devi Parikh</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1411.3041v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1411.3041v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1411.3169v1</id>\\n    <updated>2014-11-12T13:21:48Z</updated>\\n    <published>2014-11-12T13:21:48Z</published>\\n    <title>On Coarse Graining of Information and Its Application to Pattern\\n  Recognition</title>\\n    <summary>  We propose a method based on finite mixture models for classifying a set of\\nobservations into number of different categories. In order to demonstrate the\\nmethod, we show how the component densities for the mixture model can be\\nderived by using the maximum entropy method in conjunction with conservation of\\nPythagorean means. Several examples of distributions belonging to the\\nPythagorean family are derived. A discussion on estimation of model parameters\\nand the number of categories is also given.\\n</summary>\\n    <author>\\n      <name>Ali Ghaderi</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1063/1.4906011</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1063/1.4906011\" rel=\"related\"/>\\n    <link href=\"http://arxiv.org/abs/1411.3169v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1411.3169v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1411.4304v1</id>\\n    <updated>2014-11-16T21:25:53Z</updated>\\n    <published>2014-11-16T21:25:53Z</published>\\n    <title>Ten Years of Pedestrian Detection, What Have We Learned?</title>\\n    <summary>  Paper-by-paper results make it easy to miss the forest for the trees.We\\nanalyse the remarkable progress of the last decade by discussing the main ideas\\nexplored in the 40+ detectors currently present in the Caltech pedestrian\\ndetection benchmark. We observe that there exist three families of approaches,\\nall currently reaching similar detection quality. Based on our analysis, we\\nstudy the complementarity of the most promising ideas by combining multiple\\npublished strategies. This new decision forest detector achieves the current\\nbest known performance on the challenging Caltech-USA dataset.\\n</summary>\\n    <author>\\n      <name>Rodrigo Benenson</name>\\n    </author>\\n    <author>\\n      <name>Mohamed Omran</name>\\n    </author>\\n    <author>\\n      <name>Jan Hosang</name>\\n    </author>\\n    <author>\\n      <name>Bernt Schiele</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">To appear in ECCV 2014 CVRSUAD workshop proceedings</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1411.4304v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1411.4304v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1411.4734v4</id>\\n    <updated>2015-12-17T03:19:36Z</updated>\\n    <published>2014-11-18T04:49:08Z</published>\\n    <title>Predicting Depth, Surface Normals and Semantic Labels with a Common\\n  Multi-Scale Convolutional Architecture</title>\\n    <summary>  In this paper we address three different computer vision tasks using a single\\nbasic architecture: depth prediction, surface normal estimation, and semantic\\nlabeling. We use a multiscale convolutional network that is able to adapt\\neasily to each task using only small modifications, regressing from the input\\nimage to the output map directly. Our method progressively refines predictions\\nusing a sequence of scales, and captures many image details without any\\nsuperpixels or low-level segmentation. We achieve state-of-the-art performance\\non benchmarks for all three tasks.\\n</summary>\\n    <author>\\n      <name>David Eigen</name>\\n    </author>\\n    <author>\\n      <name>Rob Fergus</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1411.4734v4\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1411.4734v4\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1411.5879v2</id>\\n    <updated>2014-12-09T18:00:13Z</updated>\\n    <published>2014-11-18T17:03:20Z</published>\\n    <title>A Unified Semantic Embedding: Relating Taxonomies and Attributes</title>\\n    <summary>  We propose a method that learns a discriminative yet semantic space for\\nobject categorization, where we also embed auxiliary semantic entities such as\\nsupercategories and attributes. Contrary to prior work which only utilized them\\nas side information, we explicitly embed the semantic entities into the same\\nspace where we embed categories, which enables us to represent a category as\\ntheir linear combination. By exploiting such a unified model for semantics, we\\nenforce each category to be represented by a supercategory + sparse combination\\nof attributes, with an additional exclusive regularization to learn\\ndiscriminative composition.\\n</summary>\\n    <author>\\n      <name>Sung Ju Hwang</name>\\n    </author>\\n    <author>\\n      <name>Leonid Sigal</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">To Appear in NIPS 2014 Learning Semantics Workshop</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1411.5879v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1411.5879v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1411.6031v1</id>\\n    <updated>2014-11-21T21:38:15Z</updated>\\n    <published>2014-11-21T21:38:15Z</published>\\n    <title>Finding Action Tubes</title>\\n    <summary>  We address the problem of action detection in videos. Driven by the latest\\nprogress in object detection from 2D images, we build action models using rich\\nfeature hierarchies derived from shape and kinematic cues. We incorporate\\nappearance and motion in two ways. First, starting from image region proposals\\nwe select those that are motion salient and thus are more likely to contain the\\naction. This leads to a significant reduction in the number of regions being\\nprocessed and allows for faster computations. Second, we extract\\nspatio-temporal feature representations to build strong classifiers using\\nConvolutional Neural Networks. We link our predictions to produce detections\\nconsistent in time, which we call action tubes. We show that our approach\\noutperforms other techniques in the task of action detection.\\n</summary>\\n    <author>\\n      <name>Georgia Gkioxari</name>\\n    </author>\\n    <author>\\n      <name>Jitendra Malik</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1411.6031v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1411.6031v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1411.6067v2</id>\\n    <updated>2015-04-26T04:07:17Z</updated>\\n    <published>2014-11-22T03:14:21Z</published>\\n    <title>Viewpoints and Keypoints</title>\\n    <summary>  We characterize the problem of pose estimation for rigid objects in terms of\\ndetermining viewpoint to explain coarse pose and keypoint prediction to capture\\nthe finer details. We address both these tasks in two different settings - the\\nconstrained setting with known bounding boxes and the more challenging\\ndetection setting where the aim is to simultaneously detect and correctly\\nestimate pose of objects. We present Convolutional Neural Network based\\narchitectures for these and demonstrate that leveraging viewpoint estimates can\\nsubstantially improve local appearance based keypoint predictions. In addition\\nto achieving significant improvements over state-of-the-art in the above tasks,\\nwe analyze the error modes and effect of object characteristics on performance\\nto guide future efforts towards this goal.\\n</summary>\\n    <author>\\n      <name>Shubham Tulsiani</name>\\n    </author>\\n    <author>\\n      <name>Jitendra Malik</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1411.6067v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1411.6067v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1411.6206v1</id>\\n    <updated>2014-11-23T08:19:11Z</updated>\\n    <published>2014-11-23T08:19:11Z</published>\\n    <title>Low-Rank and Sparse Matrix Decomposition with a-priori knowledge for\\n  Dynamic 3D MRI reconstruction</title>\\n    <summary>  It has been recently shown that incorporating priori knowledge significantly\\nimproves the performance of basic compressive sensing based approaches. We have\\nmanaged to successfully exploit this idea for recovering a matrix as a\\nsummation of a Low-rank and a Sparse component from compressive measurements.\\nWhen applied to the problem of construction of 4D Cardiac MR image sequences in\\nreal-time from highly under-sampled $k-$space data, our proposed method\\nachieves superior reconstruction quality compared to the other state-of-the-art\\nmethods.\\n</summary>\\n    <author>\\n      <name>Dornoosh Zonoobi</name>\\n    </author>\\n    <author>\\n      <name>Shahrooz Faghih Roohi</name>\\n    </author>\\n    <author>\\n      <name>Ashraf A. Kassim</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">conference</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1411.6206v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1411.6206v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1411.7113v1</id>\\n    <updated>2014-11-26T05:50:02Z</updated>\\n    <published>2014-11-26T05:50:02Z</published>\\n    <title>Real time Detection of Lane Markers in Urban Streets</title>\\n    <summary>  We present a robust and real time approach to lane marker detection in urban\\nstreets. It is based on generating a top view of the road, filtering using\\nselective oriented Gaussian filters, using RANSAC line fitting to give initial\\nguesses to a new and fast RANSAC algorithm for fitting Bezier Splines, which is\\nthen followed by a post-processing step. Our algorithm can detect all lanes in\\nstill images of the street in various conditions, while operating at a rate of\\n50 Hz and achieving comparable results to previous techniques.\\n</summary>\\n    <author>\\n      <name>Mohamed Aly</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/IVS.2008.4621152</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/IVS.2008.4621152\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IEEE Intelligent Vehicles Symposium, Eindhoven, The Netherlands,\\n  June 2008</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1411.7113v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1411.7113v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1411.7682v1</id>\\n    <updated>2014-11-27T19:04:30Z</updated>\\n    <published>2014-11-27T19:04:30Z</published>\\n    <title>On color image quality assessment using natural image statistics</title>\\n    <summary>  Color distortion can introduce a significant damage in visual quality\\nperception, however, most of existing reduced-reference quality measures are\\ndesigned for grayscale images. In this paper, we consider a basic extension of\\nwell-known image-statistics based quality assessment measures to color images.\\nIn order to evaluate the impact of color information on the measures\\nefficiency, two color spaces are investigated: RGB and CIELAB. Results of an\\nextensive evaluation using TID 2013 benchmark demonstrates that significant\\nimprovement can be achieved for a great number of distortion type when the\\nCIELAB color representation is used.\\n</summary>\\n    <author>\\n      <name>Mounir Omari</name>\\n    </author>\\n    <author>\\n      <name>Mohammed El Hassouni</name>\\n    </author>\\n    <author>\\n      <name>Hocine Cherifi</name>\\n    </author>\\n    <author>\\n      <name>Abdelkaher Ait Abdelouahad</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1411.7682v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1411.7682v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1411.7715v1</id>\\n    <updated>2014-11-27T22:39:50Z</updated>\\n    <published>2014-11-27T22:39:50Z</published>\\n    <title>Flying Objects Detection from a Single Moving Camera</title>\\n    <summary>  We propose an approach to detect flying objects such as UAVs and aircrafts\\nwhen they occupy a small portion of the field of view, possibly moving against\\ncomplex backgrounds, and are filmed by a camera that itself moves.\\n  Solving such a difficult problem requires combining both appearance and\\nmotion cues. To this end we propose a regression-based approach to motion\\nstabilization of local image patches that allows us to achieve effective\\nclassification on spatio-temporal image cubes and outperform state-of-the-art\\ntechniques.\\n  As the problem is relatively new, we collected two challenging datasets for\\nUAVs and Aircrafts, which can be used as benchmarks for flying objects\\ndetection and vision-guided collision avoidance.\\n</summary>\\n    <author>\\n      <name>Artem Rozantsev</name>\\n    </author>\\n    <author>\\n      <name>Vincent Lepetit</name>\\n    </author>\\n    <author>\\n      <name>Pascal Fua</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/CVPR.2015.7299040</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/CVPR.2015.7299040\" rel=\"related\"/>\\n    <link href=\"http://arxiv.org/abs/1411.7715v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1411.7715v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1412.0774v1</id>\\n    <updated>2014-12-02T03:31:51Z</updated>\\n    <published>2014-12-02T03:31:51Z</published>\\n    <title>Feedforward semantic segmentation with zoom-out features</title>\\n    <summary>  We introduce a purely feed-forward architecture for semantic segmentation. We\\nmap small image elements (superpixels) to rich feature representations\\nextracted from a sequence of nested regions of increasing extent. These regions\\nare obtained by \"zooming out\" from the superpixel all the way to scene-level\\nresolution. This approach exploits statistical structure in the image and in\\nthe label space without setting up explicit structured prediction mechanisms,\\nand thus avoids complex and expensive inference. Instead superpixels are\\nclassified by a feedforward multilayer network. Our architecture achieves new\\nstate of the art performance in semantic segmentation, obtaining 64.4% average\\naccuracy on the PASCAL VOC 2012 test set.\\n</summary>\\n    <author>\\n      <name>Mohammadreza Mostajabi</name>\\n    </author>\\n    <author>\\n      <name>Payman Yadollahpour</name>\\n    </author>\\n    <author>\\n      <name>Gregory Shakhnarovich</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1412.0774v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1412.0774v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1412.0985v3</id>\\n    <updated>2015-02-11T17:51:12Z</updated>\\n    <published>2014-12-02T17:18:13Z</published>\\n    <title>Covariance estimation using conjugate gradient for 3D classification in\\n  Cryo-EM</title>\\n    <summary>  Classifying structural variability in noisy projections of biological\\nmacromolecules is a central problem in Cryo-EM. In this work, we build on a\\nprevious method for estimating the covariance matrix of the three-dimensional\\nstructure present in the molecules being imaged. Our proposed method allows for\\nincorporation of contrast transfer function and non-uniform distribution of\\nviewing angles, making it more suitable for real-world data. We evaluate its\\nperformance on a synthetic dataset and an experimental dataset obtained by\\nimaging a 70S ribosome complex.\\n</summary>\\n    <author>\\n      <name>Joakim And\\xc3\\xa9n</name>\\n    </author>\\n    <author>\\n      <name>Eugene Katsevich</name>\\n    </author>\\n    <author>\\n      <name>Amit Singer</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/ISBI.2015.7163849</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/ISBI.2015.7163849\" rel=\"related\"/>\\n    <link href=\"http://arxiv.org/abs/1412.0985v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1412.0985v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1412.1194v1</id>\\n    <updated>2014-12-03T05:23:03Z</updated>\\n    <published>2014-12-03T05:23:03Z</published>\\n    <title>Gradient Boundary Histograms for Action Recognition</title>\\n    <summary>  This paper introduces a high efficient local spatiotemporal descriptor,\\ncalled gradient boundary histograms (GBH). The proposed GBH descriptor is built\\non simple spatio-temporal gradients, which are fast to compute. We demonstrate\\nthat it can better represent local structure and motion than other\\ngradient-based descriptors, and significantly outperforms them on large\\nrealistic datasets. A comprehensive evaluation shows that the recognition\\naccuracy is preserved while the spatial resolution is greatly reduced, which\\nyields both high efficiency and low memory usage.\\n</summary>\\n    <author>\\n      <name>Feng Shi</name>\\n    </author>\\n    <author>\\n      <name>Robert Laganiere</name>\\n    </author>\\n    <author>\\n      <name>Emil Petriu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1412.1194v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1412.1194v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1412.2067v1</id>\\n    <updated>2014-11-20T20:13:00Z</updated>\\n    <published>2014-11-20T20:13:00Z</published>\\n    <title>An algorithm for improving Non-Local Means operators via low-rank\\n  approximation</title>\\n    <summary>  We present a method for improving a Non Local Means operator by computing its\\nlow-rank approximation. The low-rank operator is constructed by applying a\\nfilter to the spectrum of the original Non Local Means operator. This results\\nin an operator which is less sensitive to noise while preserving important\\nproperties of the original operator. The method is efficiently implemented\\nbased on Chebyshev polynomials and is demonstrated on the application of\\nnatural images denoising. For this application, we provide a comprehensive\\ncomparison of our method with leading denoising methods.\\n</summary>\\n    <author>\\n      <name>Victor May</name>\\n    </author>\\n    <author>\\n      <name>Yosi Keller</name>\\n    </author>\\n    <author>\\n      <name>Nir Sharon</name>\\n    </author>\\n    <author>\\n      <name>Yoel Shkolnisky</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1412.2067v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1412.2067v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.GM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1412.3684v1</id>\\n    <updated>2014-12-10T18:23:13Z</updated>\\n    <published>2014-12-10T18:23:13Z</published>\\n    <title>Object Recognition Using Deep Neural Networks: A Survey</title>\\n    <summary>  Recognition of objects using Deep Neural Networks is an active area of\\nresearch and many breakthroughs have been made in the last few years. The paper\\nattempts to indicate how far this field has progressed. The paper briefly\\ndescribes the history of research in Neural Networks and describe several of\\nthe recent advances in this field. The performances of recently developed\\nNeural Network Algorithm over benchmark datasets have been tabulated. Finally,\\nsome the applications of this field have been provided.\\n</summary>\\n    <author>\\n      <name>Soren Goyal</name>\\n    </author>\\n    <author>\\n      <name>Paul Benjamin</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1412.3684v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1412.3684v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1412.3717v2</id>\\n    <updated>2015-04-10T12:30:14Z</updated>\\n    <published>2014-11-18T11:09:01Z</published>\\n    <title>Unsupervised Neural Architecture for Saliency Detection: Extended\\n  Version</title>\\n    <summary>  We propose a novel neural network architecture for visual saliency\\ndetections, which utilizes neurophysiologically plausible mechanisms for\\nextraction of salient regions. The model has been significantly inspired by\\nrecent findings from neurophysiology and aimed to simulate the bottom-up\\nprocesses of human selective attention. Two types of features were analyzed:\\ncolor and direction of maximum variance. The mechanism we employ for processing\\nthose features is PCA, implemented by means of normalized Hebbian learning and\\nthe waves of spikes. To evaluate performance of our model we have conducted\\npsychological experiment. Comparison of simulation results with those of\\nexperiment indicates good performance of our model.\\n</summary>\\n    <author>\\n      <name>Natalia Efremova</name>\\n    </author>\\n    <author>\\n      <name>Sergey Tarasenko</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages, 26 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1412.3717v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1412.3717v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1412.3914v1</id>\\n    <updated>2014-12-12T08:17:28Z</updated>\\n    <published>2014-12-12T08:17:28Z</published>\\n    <title>Edge Preserving Multi-Modal Registration Based On Gradient Intensity\\n  Self-Similarity</title>\\n    <summary>  Image registration is a challenging task in the world of medical imaging.\\nParticularly, accurate edge registration plays a central role in a variety of\\nclinical conditions. The Modality Independent Neighbourhood Descriptor (MIND)\\ndemonstrates state of the art alignment, based on the image self-similarity.\\nHowever, this method appears to be less accurate regarding edge registration.\\nIn this work, we propose a new registration method, incorporating gradient\\nintensity and MIND self-similarity metric. Experimental results show the\\nsuperiority of this method in edge registration tasks, while preserving the\\noriginal MIND performance for other image features and textures.\\n</summary>\\n    <author>\\n      <name>Tamar Rott</name>\\n    </author>\\n    <author>\\n      <name>Dorin Shriki</name>\\n    </author>\\n    <author>\\n      <name>Tamir Bendory</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1412.3914v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1412.3914v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1412.3949v1</id>\\n    <updated>2014-12-12T11:11:30Z</updated>\\n    <published>2014-12-12T11:11:30Z</published>\\n    <title>CITlab ARGUS for historical handwritten documents</title>\\n    <summary>  We describe CITlab\\'s recognition system for the HTRtS competition attached to\\nthe 14. International Conference on Frontiers in Handwriting Recognition, ICFHR\\n2014. The task comprises the recognition of historical handwritten documents.\\nThe core algorithms of our system are based on multi-dimensional recurrent\\nneural networks (MDRNN) and connectionist temporal classification (CTC). The\\nsoftware modules behind that as well as the basic utility technologies are\\nessentially powered by PLANET\\'s ARGUS framework for intelligent text\\nrecognition and image processing.\\n</summary>\\n    <author>\\n      <name>Tobias Strau\\xc3\\x9f</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">for the University of Rostock - CITlab</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Tobias Gr\\xc3\\xbcning</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">for the University of Rostock - CITlab</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Gundram Leifert</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">for the University of Rostock - CITlab</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Roger Labahn</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">for the University of Rostock - CITlab</arxiv:affiliation>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1412.3949v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1412.3949v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68T05, 68T10\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1412.4181v2</id>\\n    <updated>2015-06-28T19:37:56Z</updated>\\n    <published>2014-12-13T02:30:59Z</published>\\n    <title>Oriented Edge Forests for Boundary Detection</title>\\n    <summary>  We present a simple, efficient model for learning boundary detection based on\\na random forest classifier. Our approach combines (1) efficient clustering of\\ntraining examples based on simple partitioning of the space of local edge\\norientations and (2) scale-dependent calibration of individual tree output\\nprobabilities prior to multiscale combination. The resulting model outperforms\\npublished results on the challenging BSDS500 boundary detection benchmark.\\nFurther, on large datasets our model requires substantially less memory for\\ntraining and speeds up training time by a factor of 10 over the structured\\nforest model.\\n</summary>\\n    <author>\\n      <name>Sam Hallman</name>\\n    </author>\\n    <author>\\n      <name>Charless C. Fowlkes</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">updated to include contents of CVPR version + new figure showing\\n  example segmentation results</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1412.4181v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1412.4181v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1412.4183v1</id>\\n    <updated>2014-12-13T02:39:09Z</updated>\\n    <published>2014-12-13T02:39:09Z</published>\\n    <title>A survey of modern optical character recognition techniques</title>\\n    <summary>  This report explores the latest advances in the field of digital document\\nrecognition. With the focus on printed document imagery, we discuss the major\\ndevelopments in optical character recognition (OCR) and document image\\nenhancement/restoration in application to Latin and non-Latin scripts. In\\naddition, we review and discuss the available technologies for hand-written\\ndocument recognition. In this report, we also provide some company-accumulated\\nbenchmark results on available OCR engines.\\n</summary>\\n    <author>\\n      <name>Eugene Borovikov</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Technical report surveying OCR/ICR and document understanding methods\\n  as of 2004.It contains 38 pages, numerous figures, 93 references, and\\n  provides a table of contents</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1412.4183v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1412.4183v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"62-04\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.7.5; I.4.1; I.5.4; I.4.1; I.4.3; I.4.6; I.4.7\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1412.4205v1</id>\\n    <updated>2014-12-13T08:57:49Z</updated>\\n    <published>2014-12-13T08:57:49Z</published>\\n    <title>The application of the Bayes Ying Yang harmony based GMMs in on-line\\n  signature verification</title>\\n    <summary>  In this contribution, a Bayes Ying Yang(BYY) harmony based approach for\\non-line signature verification is presented. In the proposed method, a simple\\nbut effective Gaussian Mixture Models(GMMs) is used to represent for each\\nuser\\'s signature model based on the prior information collected. Different from\\nthe early works, in this paper, we use the Bayes Ying Yang machine combined\\nwith the harmony function to achieve Automatic Model Selection(AMS) during the\\nparameter learning for the GMMs, so that a better approximation of the user\\nmodel is assured. Experiments on a database from the First International\\nSignature Verification Competition(SVC 2004) confirm that this combined\\nalgorithm yields quite satisfactory results.\\n</summary>\\n    <author>\\n      <name>Xiaosha Zhao</name>\\n    </author>\\n    <author>\\n      <name>Mandan Liu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1412.4205v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1412.4205v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1412.5322v1</id>\\n    <updated>2014-12-17T10:21:10Z</updated>\\n    <published>2014-12-17T10:21:10Z</published>\\n    <title>An Algebraical Model for Gray Level Images</title>\\n    <summary>  In this paper we propose a new algebraical model for the gray level images.\\nIt can be used for digital image processing. The model adresses to those images\\nwhich are generated in improper light conditions (very low or high level). The\\nvector space structure is able to illustrate some features into the image using\\nmodified level of contrast and luminosity. Also, the defined structure could be\\nused in image enhancement. The general approach is presented with experimental\\nresults to demonstrate image enhancement.\\n</summary>\\n    <author>\\n      <name>Vasile Patrascu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">The 7th International Conference, Exhibition on Optimization of\\n  Electrical and Electronic Equipment, OPTIM 2000, Bra\\\\c{s}ov, Rom\\\\^ania 11-12\\n  May, 2000</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1412.5322v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1412.5322v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1412.5325v1</id>\\n    <updated>2014-12-17T10:32:07Z</updated>\\n    <published>2014-12-17T10:32:07Z</published>\\n    <title>Color Image Enhancement In the Framework of Logarithmic Models</title>\\n    <summary>  In this paper, we propose a mathematical model for color image processing. It\\nis a logarithmical one. We consider the cube (-1,1)x(-1,1)x(-1,1) as the set of\\nvalues for the color space. We define two operations: addition &lt;+&gt; and real\\nscalar multiplication &lt;x&gt;. With these operations the space of colors becomes a\\nreal vector space. Then, defining the scalar product (.|.) and the norm || .\\n||, we obtain a (logarithmic) Euclidean space. We show how we can use this\\nmodel for color image enhancement and we present some experimental results.\\n</summary>\\n    <author>\\n      <name>Vasile Patrascu</name>\\n    </author>\\n    <author>\\n      <name>Vasile Buzuloiu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">The 8th IEEE International Conference on Telecommunications, Vol. 1,\\n  pp. 199-204, IEEE ICT2001, June 4 - 7, 2001, Bucharest,Romania</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1412.5325v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1412.5325v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1412.5328v1</id>\\n    <updated>2014-12-17T10:50:25Z</updated>\\n    <published>2014-12-17T10:50:25Z</published>\\n    <title>A Mathematical Model for Logarithmic Image Processing</title>\\n    <summary>  In this paper, we propose a new mathematical model for image processing. It\\nis a logarithmical one. We consider the bounded interval (-1, 1) as the set of\\ngray levels. Firstly, we define two operations: addition &lt;+&gt; and real scalar\\nmultiplication &lt;x&gt;. With these operations, the set of gray levels becomes a\\nreal vector space. Then, defining the scalar product (.|.) and the norm || .\\n||, we obtain an Euclidean space of the gray levels. Secondly, we extend these\\noperations and functions for color images. We finally show the effect of\\nvarious simple operations on an image.\\n</summary>\\n    <author>\\n      <name>Vasile Patrascu</name>\\n    </author>\\n    <author>\\n      <name>Vasile Buzuloiu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">The 5th World Multi-Conference on Systemics, Cybernetics and\\n  Informatics, Vol 13, pp. 117-122, SCI2001, July 22-25, 2001, Orlando, USA</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1412.5328v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1412.5328v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1412.5764v1</id>\\n    <updated>2014-12-18T09:09:17Z</updated>\\n    <published>2014-12-18T09:09:17Z</published>\\n    <title>Image Dynamic Range Enhancement in the Context of Logarithmic Models</title>\\n    <summary>  Images of a scene observed under a variable illumination or with a variable\\noptical aperture are not identical. Does a privileged representant exist? In\\nwhich mathematical context? How to obtain it? The authors answer to such\\nquestions in the context of logarithmic models for images. After a short\\npresentation of the model, the paper presents two image transforms: one\\nperforms an optimal enhancement of the dynamic range, and the other does the\\nsame for the mean dynamic range. Experimental results are shown.\\n</summary>\\n    <author>\\n      <name>Vasile Patrascu</name>\\n    </author>\\n    <author>\\n      <name>Vasile Buzuloiu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">The 11th European Signal Processing Conference, EUSIPCO 2002,\\n  Toulouse, France, 03-06 september 2002</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1412.5764v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1412.5764v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1412.5787v1</id>\\n    <updated>2014-12-18T10:12:49Z</updated>\\n    <published>2014-12-18T10:12:49Z</published>\\n    <title>Gray Level Image Enhancement Using Polygonal Functions</title>\\n    <summary>  This paper presents a method for enhancing the gray level images. This method\\ntakes part from the category of point transforms and it is based on\\ninterpolation functions. The latter have a graphic represented by polygonal\\nlines. The interpolation nodes of these functions are calculated taking into\\naccount the statistics of gray levels belonging to the image.\\n</summary>\\n    <author>\\n      <name>Vasile Patrascu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">The 13th International Conference on Automation, Quality and Testing,\\n  Robotics, Vol. Robotics, Image and Signal processing, pp. 129-134, May 23-25\\n  2002, Cluj-Napoca, Romania</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1412.5787v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1412.5787v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1412.5802v1</id>\\n    <updated>2014-12-18T10:59:09Z</updated>\\n    <published>2014-12-18T10:59:09Z</published>\\n    <title>Contour Detection Using Contrast Formulas in the Framework of\\n  Logarithmic Models</title>\\n    <summary>  In this paper we use a new logarithmic model of image representation,\\ndeveloped in [1,2], for edge detection. In fact, in the framework of the new\\nmodel we obtain the formulas for computing the \"contrast of a pixel\" and the\\n\"contrast\" image is just the \"contour\" or edge image. In our setting the range\\nof values is preserved and the quality of the contour is good for high as well\\nas for low luminosity regions. We present the comparison of our results with\\nthe results using classical edge detection operators.\\n</summary>\\n    <author>\\n      <name>Vasile Patrascu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">The 8th International Conference, Exhibition on Optimization of\\n  Electrical and Electronic Equipment, OPTIM 2002, Vol III, pp 751-756, 16 - 17\\n  May 2002, Brasov, Romania</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1412.5802v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1412.5802v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1412.6012v1</id>\\n    <updated>2014-12-15T06:54:47Z</updated>\\n    <published>2014-12-15T06:54:47Z</published>\\n    <title>CITlab ARGUS for historical data tables</title>\\n    <summary>  We describe CITlab\\'s recognition system for the ANWRESH-2014 competition\\nattached to the 14. International Conference on Frontiers in Handwriting\\nRecognition, ICFHR 2014. The task comprises word recognition from segmented\\nhistorical documents. The core components of our system are based on\\nmulti-dimensional recurrent neural networks (MDRNN) and connectionist temporal\\nclassification (CTC). The software modules behind that as well as the basic\\nutility technologies are essentially powered by PLANET\\'s ARGUS framework for\\nintelligent text recognition and image processing.\\n</summary>\\n    <author>\\n      <name>Gundram Leifert</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">for the University of Rostock - CITlab</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Tobias Gr\\xc3\\xbcning</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">for the University of Rostock - CITlab</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Tobias Strau\\xc3\\x9f</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">for the University of Rostock - CITlab</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Roger Labahn</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">for the University of Rostock - CITlab</arxiv:affiliation>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">arXiv admin note: text overlap with arXiv:1412.3949</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1412.6012v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1412.6012v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68T05, 68T10\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1412.6574v4</id>\\n    <updated>2016-05-09T08:54:31Z</updated>\\n    <published>2014-12-20T01:32:43Z</published>\\n    <title>Visual Instance Retrieval with Deep Convolutional Networks</title>\\n    <summary>  This paper provides an extensive study on the availability of image\\nrepresentations based on convolutional networks (ConvNets) for the task of\\nvisual instance retrieval. Besides the choice of convolutional layers, we\\npresent an efficient pipeline exploiting multi-scale schemes to extract local\\nfeatures, in particular, by taking geometric invariance into explicit account,\\ni.e. positions, scales and spatial consistency. In our experiments using five\\nstandard image retrieval datasets, we demonstrate that generic ConvNet image\\nrepresentations can outperform other state-of-the-art methods if they are\\nextracted appropriately.\\n</summary>\\n    <author>\\n      <name>Ali Sharif Razavian</name>\\n    </author>\\n    <author>\\n      <name>Josephine Sullivan</name>\\n    </author>\\n    <author>\\n      <name>Stefan Carlsson</name>\\n    </author>\\n    <author>\\n      <name>Atsuto Maki</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1412.6574v4\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1412.6574v4\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1412.6618v3</id>\\n    <updated>2015-05-03T11:26:34Z</updated>\\n    <published>2014-12-20T07:08:54Z</published>\\n    <title>Permutohedral Lattice CNNs</title>\\n    <summary>  This paper presents a convolutional layer that is able to process sparse\\ninput features. As an example, for image recognition problems this allows an\\nefficient filtering of signals that do not lie on a dense grid (like pixel\\nposition), but of more general features (such as color values). The presented\\nalgorithm makes use of the permutohedral lattice data structure. The\\npermutohedral lattice was introduced to efficiently implement a bilateral\\nfilter, a commonly used image processing operation. Its use allows for a\\ngeneralization of the convolution type found in current (spatial) convolutional\\nnetwork architectures.\\n</summary>\\n    <author>\\n      <name>Martin Kiefel</name>\\n    </author>\\n    <author>\\n      <name>Varun Jampani</name>\\n    </author>\\n    <author>\\n      <name>Peter V. Gehler</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1412.6618v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1412.6618v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1412.6847v1</id>\\n    <updated>2014-12-22T00:10:11Z</updated>\\n    <published>2014-12-22T00:10:11Z</published>\\n    <title>A New Way to Factorize Linear Cameras</title>\\n    <summary>  The implementation details of factorizing the 3x4 projection matrices of\\nlinear cameras into their left matrix factors and the 4x4 homogeneous\\ncentral(also parallel for infinite center cases) projection factors are\\npresented in this work. Any full row rank 3x4 real matrix can be factorized\\ninto such basic matrices which will be called LC factors.\\n  A further extension to multiple view midpoint triangulation, for both pinhole\\nand affine camera cases, is also presented based on such camera factorizations.\\n</summary>\\n    <author>\\n      <name>Feng Lu</name>\\n    </author>\\n    <author>\\n      <name>Ziqiang Chen</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1412.6847v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1412.6847v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1412.7851v1</id>\\n    <updated>2014-12-25T18:50:31Z</updated>\\n    <published>2014-12-25T18:50:31Z</published>\\n    <title>Fractal descriptors based on the probability dimension: a texture\\n  analysis and classification approach</title>\\n    <summary>  In this work, we propose a novel technique for obtaining descriptors of\\ngray-level texture images. The descriptors are provided by applying a\\nmultiscale transform to the fractal dimension of the image estimated through\\nthe probability (Voss) method. The effectiveness of the descriptors is verified\\nin a classification task using benchmark over texture datasets. The results\\nobtained demonstrate the efficiency of the proposed method as a tool for the\\ndescription and discrimination of texture images.\\n</summary>\\n    <author>\\n      <name>Jo\\xc3\\xa3o Batista Florindo</name>\\n    </author>\\n    <author>\\n      <name>Odemir Martinez Bruno</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1016/j.patrec.2014.01.009</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1016/j.patrec.2014.01.009\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages, 6 figures. arXiv admin note: text overlap with\\n  arXiv:1205.2821</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Pattern Recognition Letters, Volume 42, Pages 107-114, 2014</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1412.7851v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1412.7851v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1412.7884v1</id>\\n    <updated>2014-12-26T02:46:41Z</updated>\\n    <published>2014-12-26T02:46:41Z</published>\\n    <title>Sparkle Vision: Seeing the World through Random Specular Microfacets</title>\\n    <summary>  In this paper, we study the problem of reproducing the world lighting from a\\nsingle image of an object covered with random specular microfacets on the\\nsurface. We show that such reflectors can be interpreted as a randomized\\nmapping from the lighting to the image. Such specular objects have very\\ndifferent optical properties from both diffuse surfaces and smooth specular\\nobjects like metals, so we design special imaging system to robustly and\\neffectively photograph them. We present simple yet reliable algorithms to\\ncalibrate the proposed system and do the inference. We conduct experiments to\\nverify the correctness of our model assumptions and prove the effectiveness of\\nour pipeline.\\n</summary>\\n    <author>\\n      <name>Zhengdong Zhang</name>\\n    </author>\\n    <author>\\n      <name>Phillip Isola</name>\\n    </author>\\n    <author>\\n      <name>Edward H. Adelson</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1412.7884v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1412.7884v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1412.8070v1</id>\\n    <updated>2014-12-27T17:53:06Z</updated>\\n    <published>2014-12-27T17:53:06Z</published>\\n    <title>Functional correspondence by matrix completion</title>\\n    <summary>  In this paper, we consider the problem of finding dense intrinsic\\ncorrespondence between manifolds using the recently introduced functional\\nframework. We pose the functional correspondence problem as matrix completion\\nwith manifold geometric structure and inducing functional localization with the\\n$L_1$ norm. We discuss efficient numerical procedures for the solution of our\\nproblem. Our method compares favorably to the accuracy of state-of-the-art\\ncorrespondence algorithms on non-rigid shape matching benchmarks, and is\\nespecially advantageous in settings when only scarce data is available.\\n</summary>\\n    <author>\\n      <name>Artiom Kovnatsky</name>\\n    </author>\\n    <author>\\n      <name>Michael M. Bronstein</name>\\n    </author>\\n    <author>\\n      <name>Xavier Bresson</name>\\n    </author>\\n    <author>\\n      <name>Pierre Vandergheynst</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1412.8070v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1412.8070v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1412.8341v1</id>\\n    <updated>2014-12-29T13:47:06Z</updated>\\n    <published>2014-12-29T13:47:06Z</published>\\n    <title>Spectral classification using convolutional neural networks</title>\\n    <summary>  There is a great need for accurate and autonomous spectral classification\\nmethods in astrophysics. This thesis is about training a convolutional neural\\nnetwork (ConvNet) to recognize an object class (quasar, star or galaxy) from\\none-dimension spectra only. Author developed several scripts and C programs for\\ndatasets preparation, preprocessing and postprocessing of the data. EBLearn\\nlibrary (developed by Pierre Sermanet and Yann LeCun) was used to create\\nConvNets. Application on dataset of more than 60000 spectra yielded success\\nrate of nearly 95%. This thesis conclusively proved great potential of\\nconvolutional neural networks and deep learning methods in astrophysics.\\n</summary>\\n    <author>\\n      <name>Pavel H\\xc3\\xa1la</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">71 pages, 50 figures, Master\\'s thesis, Masaryk University</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1412.8341v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1412.8341v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"astro-ph.IM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1412.8556v3</id>\\n    <updated>2015-05-05T05:36:05Z</updated>\\n    <published>2014-12-30T03:49:47Z</published>\\n    <title>Domain-Size Pooling in Local Descriptors: DSP-SIFT</title>\\n    <summary>  We introduce a simple modification of local image descriptors, such as SIFT,\\nbased on pooling gradient orientations across different domain sizes, in\\naddition to spatial locations. The resulting descriptor, which we call\\nDSP-SIFT, outperforms other methods in wide-baseline matching benchmarks,\\nincluding those based on convolutional neural networks, despite having the same\\ndimension of SIFT and requiring no training.\\n</summary>\\n    <author>\\n      <name>Jingming Dong</name>\\n    </author>\\n    <author>\\n      <name>Stefano Soatto</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Extended version of the CVPR 2015 paper. Technical Report UCLA CSD\\n  140022</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1412.8556v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1412.8556v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1412.8659v2</id>\\n    <updated>2015-05-30T16:43:02Z</updated>\\n    <published>2014-12-30T15:32:18Z</published>\\n    <title>Deep Roto-Translation Scattering for Object Classification</title>\\n    <summary>  Dictionary learning algorithms or supervised deep convolution networks have\\nconsiderably improved the efficiency of predefined feature representations such\\nas SIFT. We introduce a deep scattering convolution network, with predefined\\nwavelet filters over spatial and angular variables. This representation brings\\nan important improvement to results previously obtained with predefined\\nfeatures over object image databases such as Caltech and CIFAR. The resulting\\naccuracy is comparable to results obtained with unsupervised deep learning and\\ndictionary based representations. This shows that refining image\\nrepresentations by using geometric priors is a promising direction to improve\\nimage classification and its understanding.\\n</summary>\\n    <author>\\n      <name>Edouard Oyallon</name>\\n    </author>\\n    <author>\\n      <name>St\\xc3\\xa9phane Mallat</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages, 3 figures, CVPR 2015 paper</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1412.8659v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1412.8659v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1501.00108v1</id>\\n    <updated>2014-12-31T10:53:50Z</updated>\\n    <published>2014-12-31T10:53:50Z</published>\\n    <title>HSI based colour image equalization using iterative nth root and nth\\n  power</title>\\n    <summary>  In this paper an equalization technique for colour images is introduced. The\\nmethod is based on nth root and nth power equalization approach but with\\noptimization of the mean of the image in different colour channels such as RGB\\nand HSI. The performance of the proposed method has been measured by the means\\nof peak signal to noise ratio. The proposed algorithm has been compared with\\nconventional histogram equalization and the visual and quantitative\\nexperimental results are showing that the proposed method over perform the\\nhistogram equalization.\\n</summary>\\n    <author>\\n      <name>Gholamreza Anbarjafari</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1501.00108v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1501.00108v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.GR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1501.00834v1</id>\\n    <updated>2015-01-05T12:20:09Z</updated>\\n    <published>2015-01-05T12:20:09Z</published>\\n    <title>Inverse Renormalization Group Transformation in Bayesian Image\\n  Segmentations</title>\\n    <summary>  A new Bayesian image segmentation algorithm is proposed by combining a loopy\\nbelief propagation with an inverse real space renormalization group\\ntransformation to reduce the computational time. In results of our experiment,\\nwe observe that the proposed method can reduce the computational time to less\\nthan one-tenth of that taken by conventional Bayesian approaches.\\n</summary>\\n    <author>\\n      <name>Kazuyuki Tanaka</name>\\n    </author>\\n    <author>\\n      <name>Shun Kataoka</name>\\n    </author>\\n    <author>\\n      <name>Muneki Yasuda</name>\\n    </author>\\n    <author>\\n      <name>Masayuki Ohzeki</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.7566/JPSJ.84.045001</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.7566/JPSJ.84.045001\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages, 2 figures</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Journal of the Physical Society of Japan 84 (2015) 045001</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1501.00834v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1501.00834v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cond-mat.stat-mech\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1501.00901v2</id>\\n    <updated>2015-04-29T06:35:50Z</updated>\\n    <published>2015-01-05T15:53:01Z</published>\\n    <title>Learning to Recognize Pedestrian Attribute</title>\\n    <summary>  Learning to recognize pedestrian attributes at far distance is a challenging\\nproblem in visual surveillance since face and body close-shots are hardly\\navailable; instead, only far-view image frames of pedestrian are given. In this\\nstudy, we present an alternative approach that exploits the context of\\nneighboring pedestrian images for improved attribute inference compared to the\\nconventional SVM-based method. In addition, we conduct extensive experiments to\\nevaluate the informativeness of background and foreground features for\\nattribute recognition. Experiments are based on our newly released pedestrian\\nattribute dataset, which is by far the largest and most diverse of its kind.\\n</summary>\\n    <author>\\n      <name>Yubin Deng</name>\\n    </author>\\n    <author>\\n      <name>Ping Luo</name>\\n    </author>\\n    <author>\\n      <name>Chen Change Loy</name>\\n    </author>\\n    <author>\\n      <name>Xiaoou Tang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1501.00901v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1501.00901v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1501.01548v2</id>\\n    <updated>2015-01-18T04:18:38Z</updated>\\n    <published>2015-01-07T16:40:51Z</published>\\n    <title>Implementation of Auto Monitoring and Short-Message-Service System via\\n  GSM Modem</title>\\n    <summary>  Auto-Monitoring and Short-Messaging-Service System is a real-time monitoring\\nsystem for any critical operational environments. It detects an undesired event\\noccurring in the environment, generates an alert with detailed message and\\nsends it to the user to prevent hazards. This system employs a Friendly ARM as\\nmain controller while, sensors and terminals to interact with the real world. A\\nGSM network is utilized to bridge the communication between monitoring system\\nand user. This paper presents details of prototyping the system.\\n</summary>\\n    <author>\\n      <name>Akilan Thangarajah</name>\\n    </author>\\n    <author>\\n      <name>Buddhapala Wongkaew</name>\\n    </author>\\n    <author>\\n      <name>Mongkol Ekpanyapong</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages, 8 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1501.01548v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1501.01548v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1501.02393v1</id>\\n    <updated>2015-01-10T21:12:09Z</updated>\\n    <published>2015-01-10T21:12:09Z</published>\\n    <title>Riemannian Metric Learning for Symmetric Positive Definite Matrices</title>\\n    <summary>  Over the past few years, symmetric positive definite (SPD) matrices have been\\nreceiving considerable attention from computer vision community. Though various\\ndistance measures have been proposed in the past for comparing SPD matrices,\\nthe two most widely-used measures are affine-invariant distance and\\nlog-Euclidean distance. This is because these two measures are true geodesic\\ndistances induced by Riemannian geometry. In this work, we focus on the\\nlog-Euclidean Riemannian geometry and propose a data-driven approach for\\nlearning Riemannian metrics/geodesic distances for SPD matrices. We show that\\nthe geodesic distance learned using the proposed approach performs better than\\nvarious existing distance measures when evaluated on face matching and\\nclustering tasks.\\n</summary>\\n    <author>\\n      <name>Raviteja Vemulapalli</name>\\n    </author>\\n    <author>\\n      <name>David W. Jacobs</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1501.02393v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1501.02393v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1501.02825v1</id>\\n    <updated>2015-01-12T21:14:56Z</updated>\\n    <published>2015-01-12T21:14:56Z</published>\\n    <title>A Survey on Recent Advances of Computer Vision Algorithms for Egocentric\\n  Video</title>\\n    <summary>  Recent technological advances have made lightweight, head mounted cameras\\nboth practical and affordable and products like Google Glass show first\\napproaches to introduce the idea of egocentric (first-person) video to the\\nmainstream. Interestingly, the computer vision community has only recently\\nstarted to explore this new domain of egocentric vision, where research can\\nroughly be categorized into three areas: Object recognition, activity\\ndetection/recognition, video summarization. In this paper, we try to give a\\nbroad overview about the different problems that have been addressed and\\ncollect and compare evaluation results. Moreover, along with the emergence of\\nthis new domain came the introduction of numerous new and versatile benchmark\\ndatasets, which we summarize and compare as well.\\n</summary>\\n    <author>\\n      <name>Sven Bambach</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1501.02825v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1501.02825v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1501.04782v2</id>\\n    <updated>2015-07-16T14:57:47Z</updated>\\n    <published>2015-01-20T12:38:08Z</published>\\n    <title>Constructing Binary Descriptors with a Stochastic Hill Climbing Search</title>\\n    <summary>  Binary descriptors of image patches provide processing speed advantages and\\nrequire less storage than methods that encode the patch appearance with a\\nvector of real numbers. We provide evidence that, despite its simplicity, a\\nstochastic hill climbing bit selection procedure for descriptor construction\\ndefeats recently proposed alternatives on a standard discriminative power\\nbenchmark. The method is easy to implement and understand, has no free\\nparameters that need fine tuning, and runs fast.\\n</summary>\\n    <author>\\n      <name>Nenad Marku\\xc5\\xa1</name>\\n    </author>\\n    <author>\\n      <name>Igor S. Pand\\xc5\\xbei\\xc4\\x87</name>\\n    </author>\\n    <author>\\n      <name>J\\xc3\\xb6rgen Ahlberg</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1501.04782v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1501.04782v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1501.05759v1</id>\\n    <updated>2015-01-23T10:19:33Z</updated>\\n    <published>2015-01-23T10:19:33Z</published>\\n    <title>Filtered Channel Features for Pedestrian Detection</title>\\n    <summary>  This paper starts from the observation that multiple top performing\\npedestrian detectors can be modelled by using an intermediate layer filtering\\nlow-level features in combination with a boosted decision forest. Based on this\\nobservation we propose a unifying framework and experimentally explore\\ndifferent filter families. We report extensive results enabling a systematic\\nanalysis.\\n  Using filtered channel features we obtain top performance on the challenging\\nCaltech and KITTI datasets, while using only HOG+LUV as low-level features.\\nWhen adding optical flow features we further improve detection quality and\\nreport the best known results on the Caltech dataset, reaching 93% recall at 1\\nFPPI.\\n</summary>\\n    <author>\\n      <name>Shanshan Zhang</name>\\n    </author>\\n    <author>\\n      <name>Rodrigo Benenson</name>\\n    </author>\\n    <author>\\n      <name>Bernt Schiele</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1501.05759v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1501.05759v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1501.05964v1</id>\\n    <updated>2015-01-23T21:36:55Z</updated>\\n    <published>2015-01-23T21:36:55Z</published>\\n    <title>Advances in Human Action Recognition: A Survey</title>\\n    <summary>  Human action recognition has been an important topic in computer vision due\\nto its many applications such as video surveillance, human machine interaction\\nand video retrieval. One core problem behind these applications is\\nautomatically recognizing low-level actions and high-level activities of\\ninterest. The former is usually the basis for the latter. This survey gives an\\noverview of the most recent advances in human action recognition during the\\npast several years, following a well-formed taxonomy proposed by a previous\\nsurvey. From this state-of-the-art survey, researchers can view a panorama of\\nprogress in this area for future research.\\n</summary>\\n    <author>\\n      <name>Guangchun Cheng</name>\\n    </author>\\n    <author>\\n      <name>Yiwen Wan</name>\\n    </author>\\n    <author>\\n      <name>Abdullah N. Saudagar</name>\\n    </author>\\n    <author>\\n      <name>Kamesh Namuduri</name>\\n    </author>\\n    <author>\\n      <name>Bill P. Buckles</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1501.05964v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1501.05964v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1501.06993v1</id>\\n    <updated>2015-01-28T05:41:07Z</updated>\\n    <published>2015-01-28T05:41:07Z</published>\\n    <title>Feature Sampling Strategies for Action Recognition</title>\\n    <summary>  Although dense local spatial-temporal features with bag-of-features\\nrepresentation achieve state-of-the-art performance for action recognition, the\\nhuge feature number and feature size prevent current methods from scaling up to\\nreal size problems. In this work, we investigate different types of feature\\nsampling strategies for action recognition, namely dense sampling, uniformly\\nrandom sampling and selective sampling. We propose two effective selective\\nsampling methods using object proposal techniques. Experiments conducted on a\\nlarge video dataset show that we are able to achieve better average recognition\\naccuracy using 25% less features, through one of proposed selective sampling\\nmethods, and even remain comparable accuracy while discarding 70% features.\\n</summary>\\n    <author>\\n      <name>Youjie Zhou</name>\\n    </author>\\n    <author>\\n      <name>Hongkai Yu</name>\\n    </author>\\n    <author>\\n      <name>Song Wang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1501.06993v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1501.06993v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1501.07645v2</id>\\n    <updated>2015-05-17T03:32:22Z</updated>\\n    <published>2015-01-30T02:08:51Z</published>\\n    <title>Hyper-parameter optimization of Deep Convolutional Networks for object\\n  recognition</title>\\n    <summary>  Recently sequential model based optimization (SMBO) has emerged as a\\npromising hyper-parameter optimization strategy in machine learning. In this\\nwork, we investigate SMBO to identify architecture hyper-parameters of deep\\nconvolution networks (DCNs) object recognition. We propose a simple SMBO\\nstrategy that starts from a set of random initial DCN architectures to generate\\nnew architectures, which on training perform well on a given dataset. Using the\\nproposed SMBO strategy we are able to identify a number of DCN architectures\\nthat produce results that are comparable to state-of-the-art results on object\\nrecognition benchmarks.\\n</summary>\\n    <author>\\n      <name>Sachin S. Talathi</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, 1 figure, 3 tables, Submitted to ICIP 2015</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1501.07645v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1501.07645v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1501.07681v1</id>\\n    <updated>2015-01-30T07:16:50Z</updated>\\n    <published>2015-01-30T07:16:50Z</published>\\n    <title>Vector Quantization by Minimizing Kullback-Leibler Divergence</title>\\n    <summary>  This paper proposes a new method for vector quantization by minimizing the\\nKullback-Leibler Divergence between the class label distributions over the\\nquantization inputs, which are original vectors, and the output, which is the\\nquantization subsets of the vector set. In this way, the vector quantization\\noutput can keep as much information of the class label as possible. An\\nobjective function is constructed and we also developed an iterative algorithm\\nto minimize it. The new method is evaluated on bag-of-features based image\\nclassification problem.\\n</summary>\\n    <author>\\n      <name>Lan Yang</name>\\n    </author>\\n    <author>\\n      <name>Jingbin Wang</name>\\n    </author>\\n    <author>\\n      <name>Yujin Tu</name>\\n    </author>\\n    <author>\\n      <name>Prarthana Mahapatra</name>\\n    </author>\\n    <author>\\n      <name>Nelson Cardoso</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1501.07681v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1501.07681v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1501.07862v1</id>\\n    <updated>2015-01-30T17:50:41Z</updated>\\n    <published>2015-01-30T17:50:41Z</published>\\n    <title>An Analytical Study of different Document Image Binarization Methods</title>\\n    <summary>  Document image has been the area of research for a couple of decades because\\nof its potential application in the area of text recognition, line recognition\\nor any other shape recognition from the image. For most of these purposes\\nbinarization of image becomes mandatory as far as recognition is concerned.\\nThroughout couple decades standard algorithms have already been developed for\\nthis purpose. Some of these algorithms are applicable to degraded image also.\\nOur objective behind this work is to study the existing techniques, compare\\nthem in view of advantages and disadvantages and modify some of these\\nalgorithms to optimize time or performance.\\n</summary>\\n    <author>\\n      <name>Mahua Nandy</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Pal</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Satadal Saha</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">National Conference on Computing and Communication Systems\\n  (COCOSYS-09), UIT, Burdwan, January 02-04, 2009, pp. 71-76</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1501.07862v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1501.07862v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1502.00558v2</id>\\n    <updated>2015-02-09T19:38:58Z</updated>\\n    <published>2015-02-02T17:22:26Z</published>\\n    <title>Complex-Valued Hough Transforms for Circles</title>\\n    <summary>  This paper advocates the use of complex variables to represent votes in the\\nHough transform for circle detection. Replacing the positive numbers\\nclassically used in the parameter space of the Hough transforms by complex\\nnumbers allows cancellation effects when adding up the votes. Cancellation and\\nthe computation of shape likelihood via a complex number\\'s magnitude square\\nlead to more robust solutions than the \"classic\" algorithms, as shown by\\ncomputational experiments on synthetic and real datasets.\\n</summary>\\n    <author>\\n      <name>Marcelo Cicconet</name>\\n    </author>\\n    <author>\\n      <name>Davi Geiger</name>\\n    </author>\\n    <author>\\n      <name>Michael Werman</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">The paper has been withdrawn since the authors concluded a more\\n  comprehensive study on the choice of parameters needs to be performed</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1502.00558v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1502.00558v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1502.00561v2</id>\\n    <updated>2015-02-09T19:38:39Z</updated>\\n    <published>2015-02-02T17:33:55Z</published>\\n    <title>Quantum Pairwise Symmetry: Applications in 2D Shape Analysis</title>\\n    <summary>  A pair of rooted tangents -- defining a quantum triangle -- with an\\nassociated quantum wave of spin 1/2 is proposed as the primitive to represent\\nand compute symmetry. Measures of the spin characterize how \"isosceles\" or how\\n\"degenerate\" these triangles are -- which corresponds to their mirror or\\nparallel symmetry. We also introduce a complex-valued kernel to model\\nprobability errors in the parameter space, which is more robust to noise and\\nclutter than the classical model.\\n</summary>\\n    <author>\\n      <name>Marcelo Cicconet</name>\\n    </author>\\n    <author>\\n      <name>Davi Geiger</name>\\n    </author>\\n    <author>\\n      <name>Michael Werman</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">The paper has been withdrawn since the authors concluded a more\\n  comprehensive study on the choice of parameters needs to be performed</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1502.00561v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1502.00561v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1502.02506v1</id>\\n    <updated>2015-02-09T14:46:40Z</updated>\\n    <published>2015-02-09T14:46:40Z</published>\\n    <title>Predicting Alzheimer\\'s disease: a neuroimaging study with 3D\\n  convolutional neural networks</title>\\n    <summary>  Pattern recognition methods using neuroimaging data for the diagnosis of\\nAlzheimer\\'s disease have been the subject of extensive research in recent\\nyears. In this paper, we use deep learning methods, and in particular sparse\\nautoencoders and 3D convolutional neural networks, to build an algorithm that\\ncan predict the disease status of a patient, based on an MRI scan of the brain.\\nWe report on experiments using the ADNI data set involving 2,265 historical\\nscans. We demonstrate that 3D convolutional neural networks outperform several\\nother classifiers reported in the literature and produce state-of-art results.\\n</summary>\\n    <author>\\n      <name>Adrien Payan</name>\\n    </author>\\n    <author>\\n      <name>Giovanni Montana</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1502.02506v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1502.02506v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.AP\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1502.02905v1</id>\\n    <updated>2015-02-10T13:37:49Z</updated>\\n    <published>2015-02-10T13:37:49Z</published>\\n    <title>Real Time Implementation of Spatial Filtering On FPGA</title>\\n    <summary>  Field Programmable Gate Array (FPGA) technology has gained vital importance\\nmainly because of its parallel processing hardware which makes it ideal for\\nimage and video processing. In this paper, a step by step approach to apply a\\nlinear spatial filter on real time video frame sent by Omnivision OV7670 camera\\nusing Zynq Evaluation and Development board based on Xilinx XC7Z020 has been\\ndiscussed. Face detection application was chosen to explain above procedure.\\nThis procedure is applicable to most of the complex image processing algorithms\\nwhich needs to be implemented using FPGA.\\n</summary>\\n    <author>\\n      <name>Chaitannya Supe</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, 7 figures, 1 table</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1502.02905v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1502.02905v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1502.04623v2</id>\\n    <updated>2015-05-20T15:29:42Z</updated>\\n    <published>2015-02-16T16:48:56Z</published>\\n    <title>DRAW: A Recurrent Neural Network For Image Generation</title>\\n    <summary>  This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural\\nnetwork architecture for image generation. DRAW networks combine a novel\\nspatial attention mechanism that mimics the foveation of the human eye, with a\\nsequential variational auto-encoding framework that allows for the iterative\\nconstruction of complex images. The system substantially improves on the state\\nof the art for generative models on MNIST, and, when trained on the Street View\\nHouse Numbers dataset, it generates images that cannot be distinguished from\\nreal data with the naked eye.\\n</summary>\\n    <author>\\n      <name>Karol Gregor</name>\\n    </author>\\n    <author>\\n      <name>Ivo Danihelka</name>\\n    </author>\\n    <author>\\n      <name>Alex Graves</name>\\n    </author>\\n    <author>\\n      <name>Danilo Jimenez Rezende</name>\\n    </author>\\n    <author>\\n      <name>Daan Wierstra</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1502.04623v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1502.04623v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1502.05212v1</id>\\n    <updated>2015-02-18T13:11:46Z</updated>\\n    <published>2015-02-18T13:11:46Z</published>\\n    <title>IAT - Image Annotation Tool: Manual</title>\\n    <summary>  The annotation of image and video data of large datasets is a fundamental\\ntask in multimedia information retrieval and computer vision applications. In\\norder to support the users during the image and video annotation process,\\nseveral software tools have been developed to provide them with a graphical\\nenvironment which helps drawing object contours, handling tracking information\\nand specifying object metadata. Here we introduce a preliminary version of the\\nimage annotation tools developed at the Imaging and Vision Laboratory.\\n</summary>\\n    <author>\\n      <name>Gianluigi Ciocca</name>\\n    </author>\\n    <author>\\n      <name>Paolo Napoletano</name>\\n    </author>\\n    <author>\\n      <name>Raimondo Schettini</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1502.05212v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1502.05212v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1502.05224v2</id>\\n    <updated>2015-02-25T12:13:47Z</updated>\\n    <published>2015-02-18T13:41:23Z</published>\\n    <title>Cross-Modality Hashing with Partial Correspondence</title>\\n    <summary>  Learning a hashing function for cross-media search is very desirable due to\\nits low storage cost and fast query speed. However, the data crawled from\\nInternet cannot always guarantee good correspondence among different modalities\\nwhich affects the learning for hashing function. In this paper, we focus on\\ncross-modal hashing with partially corresponded data. The data without full\\ncorrespondence are made in use to enhance the hashing performance. The\\nexperiments on Wiki and NUS-WIDE datasets demonstrates that the proposed method\\noutperforms some state-of-the-art hashing approaches with fewer correspondence\\ninformation.\\n</summary>\\n    <author>\\n      <name>Yun Gu</name>\\n    </author>\\n    <author>\\n      <name>Haoyang Xue</name>\\n    </author>\\n    <author>\\n      <name>Jie Yang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1502.05224v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1502.05224v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1502.06807v2</id>\\n    <updated>2016-12-02T15:41:25Z</updated>\\n    <published>2015-02-24T13:39:55Z</published>\\n    <title>Hands Deep in Deep Learning for Hand Pose Estimation</title>\\n    <summary>  We introduce and evaluate several architectures for Convolutional Neural\\nNetworks to predict the 3D joint locations of a hand given a depth map. We\\nfirst show that a prior on the 3D pose can be easily introduced and\\nsignificantly improves the accuracy and reliability of the predictions. We also\\nshow how to use context efficiently to deal with ambiguities between fingers.\\nThese two contributions allow us to significantly outperform the\\nstate-of-the-art on several challenging benchmarks, both in terms of accuracy\\nand computation times.\\n</summary>\\n    <author>\\n      <name>Markus Oberweger</name>\\n    </author>\\n    <author>\\n      <name>Paul Wohlhart</name>\\n    </author>\\n    <author>\\n      <name>Vincent Lepetit</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">added link to source https://github.com/moberweger/deep-prior</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">In Proceedings of 20th Computer Vision Winter Workshop (CVWW)\\n  2015, pp. 21-30</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1502.06807v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1502.06807v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1503.01804v1</id>\\n    <updated>2015-03-05T22:15:33Z</updated>\\n    <published>2015-03-05T22:15:33Z</published>\\n    <title>Frequency Domain TOF: Encoding Object Depth in Modulation Frequency</title>\\n    <summary>  Time of flight cameras may emerge as the 3-D sensor of choice. Today, time of\\nflight sensors use phase-based sampling, where the phase delay between emitted\\nand received, high-frequency signals encodes distance. In this paper, we\\npresent a new time of flight architecture that relies only on frequency---we\\nrefer to this technique as frequency-domain time of flight (FD-TOF). Inspired\\nby optical coherence tomography (OCT), FD-TOF excels when frequency bandwidth\\nis high. With the increasing frequency of TOF sensors, new challenges to time\\nof flight sensing continue to emerge. At high frequencies, FD-TOF offers\\nseveral potential benefits over phase-based time of flight methods.\\n</summary>\\n    <author>\\n      <name>Achuta Kadambi</name>\\n    </author>\\n    <author>\\n      <name>Vage Taamazyan</name>\\n    </author>\\n    <author>\\n      <name>Suren Jayasuriya</name>\\n    </author>\\n    <author>\\n      <name>Ramesh Raskar</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1503.01804v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1503.01804v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.GR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1503.01986v2</id>\\n    <updated>2015-03-16T09:23:19Z</updated>\\n    <published>2015-03-06T15:19:19Z</published>\\n    <title>Convex Color Image Segmentation with Optimal Transport Distances</title>\\n    <summary>  This work is about the use of regularized optimal-transport distances for\\nconvex, histogram-based image segmentation. In the considered framework, fixed\\nexemplar histograms define a prior on the statistical features of the two\\nregions in competition. In this paper, we investigate the use of various\\ntransport-based cost functions as discrepancy measures and rely on a\\nprimal-dual algorithm to solve the obtained convex optimization problem.\\n</summary>\\n    <author>\\n      <name>Julien Rabin</name>\\n    </author>\\n    <author>\\n      <name>Nicolas Papadakis</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">A short version of this report has been submitted to the Fifth\\n  International Conference on Scale Space and Variational Methods in Computer\\n  Vision (SSVM) 2015</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1503.01986v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1503.01986v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1503.02351v1</id>\\n    <updated>2015-03-09T01:08:00Z</updated>\\n    <published>2015-03-09T01:08:00Z</published>\\n    <title>Fully Connected Deep Structured Networks</title>\\n    <summary>  Convolutional neural networks with many layers have recently been shown to\\nachieve excellent results on many high-level tasks such as image\\nclassification, object detection and more recently also semantic segmentation.\\nParticularly for semantic segmentation, a two-stage procedure is often\\nemployed. Hereby, convolutional networks are trained to provide good local\\npixel-wise features for the second step being traditionally a more global\\ngraphical model. In this work we unify this two-stage process into a single\\njoint training algorithm. We demonstrate our method on the semantic image\\nsegmentation task and show encouraging results on the challenging PASCAL VOC\\n2012 dataset.\\n</summary>\\n    <author>\\n      <name>Alexander G. Schwing</name>\\n    </author>\\n    <author>\\n      <name>Raquel Urtasun</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1503.02351v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1503.02351v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1503.03621v2</id>\\n    <updated>2015-09-08T19:04:30Z</updated>\\n    <published>2015-03-12T08:09:13Z</published>\\n    <title>Designing A Composite Dictionary Adaptively From Joint Examples</title>\\n    <summary>  We study the complementary behaviors of external and internal examples in\\nimage restoration, and are motivated to formulate a composite dictionary design\\nframework. The composite dictionary consists of the global part learned from\\nexternal examples, and the sample-specific part learned from internal examples.\\nThe dictionary atoms in both parts are further adaptively weighted to emphasize\\ntheir model statistics. Experiments demonstrate that the joint utilization of\\nexternal and internal examples leads to substantial improvements, with\\nsuccessful applications in image denoising and super resolution.\\n</summary>\\n    <author>\\n      <name>Zhangyang Wang</name>\\n    </author>\\n    <author>\\n      <name>Yingzhen Yang</name>\\n    </author>\\n    <author>\\n      <name>Jianchao Yang</name>\\n    </author>\\n    <author>\\n      <name>Thomas S. Huang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1503.03621v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1503.03621v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1503.03637v3</id>\\n    <updated>2015-09-04T11:33:39Z</updated>\\n    <published>2015-03-12T09:16:11Z</published>\\n    <title>On Computing the Translations Norm in the Epipolar Graph</title>\\n    <summary>  This paper deals with the problem of recovering the unknown norm of relative\\ntranslations between cameras based on the knowledge of relative rotations and\\ntranslation directions. We provide theoretical conditions for the solvability\\nof such a problem, and we propose a two-stage method to solve it. First, a\\ncycle basis for the epipolar graph is computed, then all the scaling factors\\nare recovered simultaneously by solving a homogeneous linear system. We\\ndemonstrate the accuracy of our solution by means of synthetic and real\\nexperiments.\\n</summary>\\n    <author>\\n      <name>Federica Arrigoni</name>\\n    </author>\\n    <author>\\n      <name>Beatrice Rossi</name>\\n    </author>\\n    <author>\\n      <name>Andrea Fusiello</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/3DV.2015.41</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/3DV.2015.41\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted at 3DV 2015</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proceedings of the 2015 International Conference on 3D Vision, pp.\\n  300-308</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1503.03637v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1503.03637v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1503.03771v1</id>\\n    <updated>2015-03-12T15:33:09Z</updated>\\n    <published>2015-03-12T15:33:09Z</published>\\n    <title>Learning to Detect Vehicles by Clustering Appearance Patterns</title>\\n    <summary>  This paper studies efficient means for dealing with intra-category diversity\\nin object detection. Strategies for occlusion and orientation handling are\\nexplored by learning an ensemble of detection models from visual and\\ngeometrical clusters of object instances. An AdaBoost detection scheme is\\nemployed with pixel lookup features for fast detection. The analysis provides\\ninsight into the design of a robust vehicle detection system, showing promise\\nin terms of detection performance and orientation estimation accuracy.\\n</summary>\\n    <author>\\n      <name>Eshed Ohn-Bar</name>\\n    </author>\\n    <author>\\n      <name>Mohan M. Trivedi</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Preprint version of our T-ITS 2015 paper</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1503.03771v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1503.03771v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1503.04036v1</id>\\n    <updated>2015-03-13T12:30:34Z</updated>\\n    <published>2015-03-13T12:30:34Z</published>\\n    <title>Characterizing driving behavior using automatic visual analysis</title>\\n    <summary>  In this work, we present the problem of rash driving detection algorithm\\nusing a single wide angle camera sensor, particularly useful in the Indian\\ncontext. To our knowledge this rash driving problem has not been addressed\\nusing Image processing techniques (existing works use other sensors such as\\naccelerometer). Car Image processing literature, though rich and mature, does\\nnot address the rash driving problem. In this work-in-progress paper, we\\npresent the need to address this problem, our approach and our future plans to\\nbuild a rash driving detector.\\n</summary>\\n    <author>\\n      <name>Mrinal Haloi</name>\\n    </author>\\n    <author>\\n      <name>Dinesh Babu Jayagopi</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1145/2662117.2662126</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1145/2662117.2662126\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages,7 figures, IBM-ICARE2014</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1503.04036v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1503.04036v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"H.4.3\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1503.04444v2</id>\\n    <updated>2015-11-30T15:04:15Z</updated>\\n    <published>2015-03-15T16:05:24Z</published>\\n    <title>Pattern Recognition of Bearing Faults using Smoother Statistical\\n  Features</title>\\n    <summary>  A pattern recognition (PR) based diagnostic scheme is presented to identify\\nbearing faults, using time domain features. Vibration data is acquired from\\nfaulty bearings using a test rig. The features are extracted from the data, and\\nprocessed prior to utilize in the PR process. The processing involves smoothing\\nof feature distributions. This reduces the undesired impact of vibration\\nrandomness on the PR process, and thus enhances the diagnostic accuracy of the\\nmodel.\\n</summary>\\n    <author>\\n      <name>Muhammad Masood Tahir</name>\\n    </author>\\n    <author>\\n      <name>Ayyaz Hussain</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This paper has been withdrawn by the author due to a crucial errors</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1503.04444v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1503.04444v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1503.07274v1</id>\\n    <updated>2015-03-25T03:41:47Z</updated>\\n    <published>2015-03-25T03:41:47Z</published>\\n    <title>Initialization Strategies of Spatio-Temporal Convolutional Neural\\n  Networks</title>\\n    <summary>  We propose a new way of incorporating temporal information present in videos\\ninto Spatial Convolutional Neural Networks (ConvNets) trained on images, that\\navoids training Spatio-Temporal ConvNets from scratch. We describe several\\ninitializations of weights in 3D Convolutional Layers of Spatio-Temporal\\nConvNet using 2D Convolutional Weights learned from ImageNet. We show that it\\nis important to initialize 3D Convolutional Weights judiciously in order to\\nlearn temporal representations of videos. We evaluate our methods on the\\nUCF-101 dataset and demonstrate improvement over Spatial ConvNets.\\n</summary>\\n    <author>\\n      <name>Elman Mansimov</name>\\n    </author>\\n    <author>\\n      <name>Nitish Srivastava</name>\\n    </author>\\n    <author>\\n      <name>Ruslan Salakhutdinov</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Technical Report</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1503.07274v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1503.07274v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1503.07460v1</id>\\n    <updated>2015-01-30T15:05:12Z</updated>\\n    <published>2015-01-30T15:05:12Z</published>\\n    <title>RANSAC based three points algorithm for ellipse fitting of spherical\\n  object\\'s projection</title>\\n    <summary>  As the spherical object can be seen everywhere, we should extract the ellipse\\nimage accurately and fit it by implicit algebraic curve in order to finish the\\n3D reconstruction. In this paper, we propose a new ellipse fitting algorithm\\nwhich only needs three points to fit the projection of spherical object and is\\ndifferent from the traditional algorithms that need at least five point. The\\nfitting procedure is just similar as the estimation of Fundamental Matrix\\nestimation by seven points, and the RANSAC algorithm has also been used to\\nexclude the interference of noise and scattered points.\\n</summary>\\n    <author>\\n      <name>Shenghui Xu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1503.07460v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1503.07460v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1505.00066v2</id>\\n    <updated>2015-09-28T18:17:20Z</updated>\\n    <published>2015-05-01T01:45:52Z</published>\\n    <title>Pose Induction for Novel Object Categories</title>\\n    <summary>  We address the task of predicting pose for objects of unannotated object\\ncategories from a small seed set of annotated object classes. We present a\\ngeneralized classifier that can reliably induce pose given a single instance of\\na novel category. In case of availability of a large collection of novel\\ninstances, our approach then jointly reasons over all instances to improve the\\ninitial estimates. We empirically validate the various components of our\\nalgorithm and quantitatively show that our method produces reliable pose\\nestimates. We also show qualitative results on a diverse set of classes and\\nfurther demonstrate the applicability of our system for learning shape models\\nof novel object classes.\\n</summary>\\n    <author>\\n      <name>Shubham Tulsiani</name>\\n    </author>\\n    <author>\\n      <name>Jo\\xc3\\xa3o Carreira</name>\\n    </author>\\n    <author>\\n      <name>Jitendra Malik</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1505.00066v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1505.00066v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1505.00171v1</id>\\n    <updated>2015-05-01T12:55:32Z</updated>\\n    <published>2015-05-01T12:55:32Z</published>\\n    <title>SynthCam3D: Semantic Understanding With Synthetic Indoor Scenes</title>\\n    <summary>  We are interested in automatic scene understanding from geometric cues. To\\nthis end, we aim to bring semantic segmentation in the loop of real-time\\nreconstruction. Our semantic segmentation is built on a deep autoencoder stack\\ntrained exclusively on synthetic depth data generated from our novel 3D scene\\nlibrary, SynthCam3D. Importantly, our network is able to segment real world\\nscenes without any noise modelling. We present encouraging preliminary results.\\n</summary>\\n    <author>\\n      <name>Ankur Handa</name>\\n    </author>\\n    <author>\\n      <name>Viorica Patraucean</name>\\n    </author>\\n    <author>\\n      <name>Vijay Badrinarayanan</name>\\n    </author>\\n    <author>\\n      <name>Simon Stent</name>\\n    </author>\\n    <author>\\n      <name>Roberto Cipolla</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1505.00171v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1505.00171v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1505.00249v1</id>\\n    <updated>2015-05-01T19:14:39Z</updated>\\n    <published>2015-05-01T19:14:39Z</published>\\n    <title>Image Segmentation by Size-Dependent Single Linkage Clustering of a\\n  Watershed Basin Graph</title>\\n    <summary>  We present a method for hierarchical image segmentation that defines a\\ndisaffinity graph on the image, over-segments it into watershed basins, defines\\na new graph on the basins, and then merges basins with a modified,\\nsize-dependent version of single linkage clustering. The quasilinear runtime of\\nthe method makes it suitable for segmenting large images. We illustrate the\\nmethod on the challenging problem of segmenting 3D electron microscopic brain\\nimages.\\n</summary>\\n    <author>\\n      <name>Aleksandar Zlateski</name>\\n    </author>\\n    <author>\\n      <name>H. Sebastian Seung</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1505.00249v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1505.00249v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1505.00432v1</id>\\n    <updated>2015-05-03T14:06:08Z</updated>\\n    <published>2015-05-03T14:06:08Z</published>\\n    <title>Object Class Detection and Classification using Multi Scale Gradient and\\n  Corner Point based Shape Descriptors</title>\\n    <summary>  This paper presents a novel multi scale gradient and a corner point based\\nshape descriptors. The novel multi scale gradient based shape descriptor is\\ncombined with generic Fourier descriptors to extract contour and region based\\nshape information. Shape information based object class detection and\\nclassification technique with a random forest classifier has been optimized.\\nProposed integrated descriptor in this paper is robust to rotation, scale,\\ntranslation, affine deformations, noisy contours and noisy shapes. The new\\ncorner point based interpolated shape descriptor has been exploited for fast\\nobject detection and classification with higher accuracy.\\n</summary>\\n    <author>\\n      <name>Basura Fernando</name>\\n    </author>\\n    <author>\\n      <name>Sezer Karaoglu</name>\\n    </author>\\n    <author>\\n      <name>Sajib Kumar Saha</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1505.00432v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1505.00432v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1505.00737v1</id>\\n    <updated>2015-05-04T18:05:52Z</updated>\\n    <published>2015-05-04T18:05:52Z</published>\\n    <title>A Gaussian Scale Space Approach For Exudates Detection, Classification\\n  And Severity Prediction</title>\\n    <summary>  In the context of Computer Aided Diagnosis system for diabetic retinopathy,\\nwe present a novel method for detection of exudates and their classification\\nfor disease severity prediction. The method is based on Gaussian scale space\\nbased interest map and mathematical morphology. It makes use of support vector\\nmachine for classification and location information of the optic disc and the\\nmacula region for severity prediction. It can efficiently handle luminance\\nvariation and it is suitable for varied sized exudates. The method has been\\nprobed in publicly available DIARETDB1V2 and e-ophthaEX databases. For exudate\\ndetection the proposed method achieved a sensitivity of 96.54% and prediction\\nof 98.35% in DIARETDB1V2 database.\\n</summary>\\n    <author>\\n      <name>Mrinal Haloi</name>\\n    </author>\\n    <author>\\n      <name>Samarendra Dandapat</name>\\n    </author>\\n    <author>\\n      <name>Rohit Sinha</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted in ICIP 2015, Quebec city, Canada</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1505.00737v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1505.00737v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68T45\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1505.00996v1</id>\\n    <updated>2015-05-05T13:10:53Z</updated>\\n    <published>2015-05-05T13:10:53Z</published>\\n    <title>Fast Guided Filter</title>\\n    <summary>  The guided filter is a technique for edge-aware image filtering. Because of\\nits nice visual quality, fast speed, and ease of implementation, the guided\\nfilter has witnessed various applications in real products, such as image\\nediting apps in phones and stereo reconstruction, and has been included in\\nofficial MATLAB and OpenCV. In this note, we remind that the guided filter can\\nbe simply sped up from O(N) time to O(N/s^2) time for a subsampling ratio s. In\\na variety of applications, this leads to a speedup of &gt;10x with almost no\\nvisible degradation. We hope this acceleration will improve performance of\\ncurrent applications and further popularize this filter. Code is released.\\n</summary>\\n    <author>\\n      <name>Kaiming He</name>\\n    </author>\\n    <author>\\n      <name>Jian Sun</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Technical report</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1505.00996v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1505.00996v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1505.01936v1</id>\\n    <updated>2015-05-08T06:15:42Z</updated>\\n    <published>2015-05-08T06:15:42Z</published>\\n    <title>Noise in Structured-Light Stereo Depth Cameras: Modeling and its\\n  Applications</title>\\n    <summary>  Depth maps obtained from commercially available structured-light stereo based\\ndepth cameras, such as the Kinect, are easy to use but are affected by\\nsignificant amounts of noise. This paper is devoted to a study of the intrinsic\\nnoise characteristics of such depth maps, i.e. the standard deviation of noise\\nin estimated depth varies quadratically with the distance of the object from\\nthe depth camera. We validate this theoretical model against empirical\\nobservations and demonstrate the utility of this noise model in three popular\\napplications: depth map denoising, volumetric scan merging for 3D modeling, and\\nidentification of 3D planes in depth maps.\\n</summary>\\n    <author>\\n      <name>Avishek Chatterjee</name>\\n    </author>\\n    <author>\\n      <name>Venu Madhav Govindu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1505.01936v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1505.01936v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1505.02247v1</id>\\n    <updated>2015-05-09T09:07:08Z</updated>\\n    <published>2015-05-09T09:07:08Z</published>\\n    <title>Performance Evaluation of Vision-Based Algorithms for MAVs</title>\\n    <summary>  An important focus of current research in the field of Micro Aerial Vehicles\\n(MAVs) is to increase the safety of their operation in general unstructured\\nenvironments. Especially indoors, where GPS cannot be used for localization,\\nreliable algorithms for localization and mapping of the environment are\\nnecessary in order to keep an MAV airborne safely. In this paper, we compare\\nvision-based real-time capable methods for localization and mapping and point\\nout their strengths and weaknesses. Additionally, we describe algorithms for\\nstate estimation, control and navigation, which use the localization and\\nmapping results of our vision-based algorithms as input.\\n</summary>\\n    <author>\\n      <name>T. Holzmann</name>\\n    </author>\\n    <author>\\n      <name>R. Prettenthaler</name>\\n    </author>\\n    <author>\\n      <name>J. Pestana</name>\\n    </author>\\n    <author>\\n      <name>D. Muschick</name>\\n    </author>\\n    <author>\\n      <name>G. Graber</name>\\n    </author>\\n    <author>\\n      <name>C. Mostegel</name>\\n    </author>\\n    <author>\\n      <name>F. Fraundorfer</name>\\n    </author>\\n    <author>\\n      <name>H. Bischof</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Presented at OAGM Workshop, 2015 (arXiv:1505.01065)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1505.02247v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1505.02247v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1505.02496v1</id>\\n    <updated>2015-05-11T06:26:46Z</updated>\\n    <published>2015-05-11T06:26:46Z</published>\\n    <title>Training Deeper Convolutional Networks with Deep Supervision</title>\\n    <summary>  One of the most promising ways of improving the performance of deep\\nconvolutional neural networks is by increasing the number of convolutional\\nlayers. However, adding layers makes training more difficult and\\ncomputationally expensive. In order to train deeper networks, we propose to add\\nauxiliary supervision branches after certain intermediate layers during\\ntraining. We formulate a simple rule of thumb to determine where these branches\\nshould be added. The resulting deeply supervised structure makes the training\\nmuch easier and also produces better classification results on ImageNet and the\\nrecently released, larger MIT Places dataset\\n</summary>\\n    <author>\\n      <name>Liwei Wang</name>\\n    </author>\\n    <author>\\n      <name>Chen-Yu Lee</name>\\n    </author>\\n    <author>\\n      <name>Zhuowen Tu</name>\\n    </author>\\n    <author>\\n      <name>Svetlana Lazebnik</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1505.02496v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1505.02496v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1505.03344v1</id>\\n    <updated>2015-05-13T11:56:01Z</updated>\\n    <published>2015-05-13T11:56:01Z</published>\\n    <title>A Framework for Fast Face and Eye Detection</title>\\n    <summary>  Face detection is an essential step in many computer vision applications like\\nsurveillance, tracking, medical analysis, facial expression analysis etc.\\nSeveral approaches have been made in the direction of face detection. Among\\nthem, Haar-like features based method is a robust method. In spite of the\\nrobustness, Haar - like features work with some limitations. However, with some\\nsimple modifications in the algorithm, its performance can be made faster and\\nmore robust. The present work refers to the increase in speed of operation of\\nthe original algorithm by down sampling the frames and its analysis with\\ndifferent scale factors. It also discusses the detection of tilted faces using\\nan affine transformation of the input image.\\n</summary>\\n    <author>\\n      <name>Anjith George</name>\\n    </author>\\n    <author>\\n      <name>Anirban Dasgupta</name>\\n    </author>\\n    <author>\\n      <name>Aurobinda Routray</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages , 10 figures,</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1505.03344v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1505.03344v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1505.03493v2</id>\\n    <updated>2015-05-14T13:08:15Z</updated>\\n    <published>2015-05-13T18:45:07Z</published>\\n    <title>Modified Hausdorff Fractal Dimension (MHFD)</title>\\n    <summary>  The Hausdorff fractal dimension has been a fast-to-calculate method to\\nestimate complexity of fractal shapes. In this work, a modified version of this\\nfractal dimension is presented in order to make it more robust when applied in\\nestimating complexity of non-fractal images. The modified Hausdorff fractal\\ndimension stands on two features that weaken the requirement of presence of a\\nshape and also reduce the impact of the noise possibly presented in the input\\nimage. The new algorithm has been evaluated on a set of images of different\\ncharacter with promising performance.\\n</summary>\\n    <author>\\n      <name>Reza Farrahi Moghaddam</name>\\n    </author>\\n    <author>\\n      <name>Mohamed Cheriet</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">15 pages, 4 figures, 2 algorithms. Working Paper WP-RFM-15-02,\\n  (version: 150507)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1505.03493v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1505.03493v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1505.03505v3</id>\\n    <updated>2017-01-28T20:57:56Z</updated>\\n    <published>2015-05-13T19:38:57Z</published>\\n    <title>On a spatial-temporal decomposition of the optical flow</title>\\n    <summary>  In this paper we present a decomposition algorithm for computation of the\\nspatial-temporal optical flow of a dynamic image sequence. We consider several\\napplications, such as the extraction of temporal motion features and motion\\ndetection in dynamic sequences under varying illumination conditions, such as\\nthey appear for instance in psychological flickering experiments. For the\\nnumerical implementation we are solving an integro-differential equation by a\\nfixed point iteration. For comparison purposes we use a standard time dependent\\noptical flow algorithm, which in contrast to our method, constitutes in solving\\na spatial-temporal differential equation.\\n</summary>\\n    <author>\\n      <name>Aniello Raffale Patrone</name>\\n    </author>\\n    <author>\\n      <name>Otmar Scherzer</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1505.03505v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1505.03505v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1505.03578v1</id>\\n    <updated>2015-05-14T00:22:35Z</updated>\\n    <published>2015-05-14T00:22:35Z</published>\\n    <title>Vanishing Point Attracts Eye Movements in Scene Free-viewing</title>\\n    <summary>  Eye movements are crucial in understanding complex scenes. By predicting\\nwhere humans look in natural scenes, we can understand how they percieve scenes\\nand priotriaze information for further high-level processing. Here, we study\\nthe effect of a particular type of scene structural information known as\\nvanishing point and show that human gaze is attracted to vanishing point\\nregions. We then build a combined model of traditional saliency and vanishing\\npoint channel that outperforms state of the art saliency models.\\n</summary>\\n    <author>\\n      <name>Ali Borji</name>\\n    </author>\\n    <author>\\n      <name>Mengyang Feng</name>\\n    </author>\\n    <author>\\n      <name>Huchuan Lu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1505.03578v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1505.03578v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1505.03932v1</id>\\n    <updated>2015-05-15T00:59:48Z</updated>\\n    <published>2015-05-15T00:59:48Z</published>\\n    <title>Using Ensemble Models in the Histological Examination of Tissue\\n  Abnormalities</title>\\n    <summary>  Classification models for the automatic detection of abnormalities on\\nhistological samples do exists, with an active debate on the cost associated\\nwith false negative diagnosis (underdiagnosis) and false positive diagnosis\\n(overdiagnosis). Current models tend to underdiagnose, failing to recognize a\\npotentially fatal disease.\\n  The objective of this study is to investigate the possibility of\\nautomatically identifying abnormalities in tissue samples through the use of an\\nensemble model on data generated by histological examination and to minimize\\nthe number of false negative cases.\\n</summary>\\n    <author>\\n      <name>Giancarlo Crocetti</name>\\n    </author>\\n    <author>\\n      <name>Michael Coakley</name>\\n    </author>\\n    <author>\\n      <name>Phil Dressner</name>\\n    </author>\\n    <author>\\n      <name>Wanda Kellum</name>\\n    </author>\\n    <author>\\n      <name>Tamba Lamin</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, 4 tables, 3 figures. Proceedings of 12th Annual Research\\n  Day, 2014 - Pace University</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1505.03932v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1505.03932v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"H.2.8; I.5.3; J.3\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1505.04260v1</id>\\n    <updated>2015-05-16T10:28:51Z</updated>\\n    <published>2015-05-16T10:28:51Z</published>\\n    <title>The color of smiling: computational synaesthesia of facial expressions</title>\\n    <summary>  This note gives a preliminary account of the transcoding or rechanneling\\nproblem between different stimuli as it is of interest for the natural\\ninteraction or affective computing fields. By the consideration of a simple\\nexample, namely the color response of an affective lamp to a sensed facial\\nexpression, we frame the problem within an information- theoretic perspective.\\nA full justification in terms of the Information Bottleneck principle promotes\\na latent affective space, hitherto surmised as an appealing and intuitive\\nsolution, as a suitable mediator between the different stimuli.\\n</summary>\\n    <author>\\n      <name>Vittorio Cuculo</name>\\n    </author>\\n    <author>\\n      <name>Raffaella Lanzarotti</name>\\n    </author>\\n    <author>\\n      <name>Giuseppe Boccignone</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Submitted to: 18th International Conference on Image Analysis and\\n  Processing (ICIAP 2015), 7-11 September 2015, Genova, Italy</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1505.04260v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1505.04260v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1505.04424v2</id>\\n    <updated>2016-07-17T10:07:40Z</updated>\\n    <published>2015-05-17T17:37:14Z</published>\\n    <title>Improved Microaneurysm Detection using Deep Neural Networks</title>\\n    <summary>  In this work, we propose a novel microaneurysm (MA) detection for early\\ndiabetic retinopathy screening using color fundus images. Since MA usually the\\nfirst lesions to appear as an indicator of diabetic retinopathy, accurate\\ndetection of MA is necessary for treatment. Each pixel of the image is\\nclassified as either MA or non-MA using a deep neural network with dropout\\ntraining procedure using maxout activation function. No preprocessing step or\\nmanual feature extraction is required. Substantial improvements over standard\\nMA detection method based on the pipeline of preprocessing, feature extraction,\\nclassification followed by post processing is achieved. The presented method is\\nevaluated in publicly available Retinopathy Online Challenge (ROC) and\\nDiaretdb1v2 database and achieved state-of-the-art accuracy.\\n</summary>\\n    <author>\\n      <name>Mrinal Haloi</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1505.04424v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1505.04424v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68T45\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1505.04467v1</id>\\n    <updated>2015-05-17T22:14:27Z</updated>\\n    <published>2015-05-17T22:14:27Z</published>\\n    <title>Exploring Nearest Neighbor Approaches for Image Captioning</title>\\n    <summary>  We explore a variety of nearest neighbor baseline approaches for image\\ncaptioning. These approaches find a set of nearest neighbor images in the\\ntraining set from which a caption may be borrowed for the query image. We\\nselect a caption for the query image by finding the caption that best\\nrepresents the \"consensus\" of the set of candidate captions gathered from the\\nnearest neighbor images. When measured by automatic evaluation metrics on the\\nMS COCO caption evaluation server, these approaches perform as well as many\\nrecent approaches that generate novel captions. However, human studies show\\nthat a method that generates novel captions is still preferred over the nearest\\nneighbor approach.\\n</summary>\\n    <author>\\n      <name>Jacob Devlin</name>\\n    </author>\\n    <author>\\n      <name>Saurabh Gupta</name>\\n    </author>\\n    <author>\\n      <name>Ross Girshick</name>\\n    </author>\\n    <author>\\n      <name>Margaret Mitchell</name>\\n    </author>\\n    <author>\\n      <name>C. Lawrence Zitnick</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1505.04467v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1505.04467v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1505.05601v1</id>\\n    <updated>2015-05-21T04:24:48Z</updated>\\n    <published>2015-05-21T04:24:48Z</published>\\n    <title>Unsupervised Segmentation of Overlapping Cervical Cell Cytoplasm</title>\\n    <summary>  Overlapping of cervical cells and poor contrast of cell cytoplasm are the\\nmajor issues in accurate detection and segmentation of cervical cells. An\\nunsupervised cell segmentation approach is presented here. Cell clump\\nsegmentation was carried out using the extended depth of field (EDF) image\\ncreated from the images of different focal planes. A modified Otsu method with\\nprior class weights is proposed for accurate segmentation of nuclei from the\\ncell clumps. The cell cytoplasm was further segmented from cell clump depending\\nupon the number of nucleus detected in that cell clump. Level set model was\\nused for cytoplasm segmentation.\\n</summary>\\n    <author>\\n      <name>S L Happy</name>\\n    </author>\\n    <author>\\n      <name>Swarnadip Chatterjee</name>\\n    </author>\\n    <author>\\n      <name>Debdoot Sheet</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">2 pages, 2 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1505.05601v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1505.05601v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1505.05819v1</id>\\n    <updated>2015-02-24T13:23:58Z</updated>\\n    <published>2015-02-24T13:23:58Z</published>\\n    <title>New HSL Distance Based Colour Clustering Algorithm</title>\\n    <summary>  In this paper, we define a distance for the HSL colour system. Next, the\\nproposed distance is used for a fuzzy colour clustering algorithm construction.\\nThe presented algorithm is related to the well-known fuzzy c-means algorithm.\\nFinally, the clustering algorithm is used as colour reduction method. The\\nobtained experimental results are presented to demonstrate the effectiveness of\\nour approach.\\n</summary>\\n    <author>\\n      <name>Vasile Patrascu</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.13140/2.1.4990.8007</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.13140/2.1.4990.8007\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">The 24th Midwest Artificial Intelligence and Cognitive Sciences\\n  Conference (MAICS 2013), pp. 85-92, New Albany, Indiana. USA, April 13-14,\\n  2013</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1505.05819v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1505.05819v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1505.06079v2</id>\\n    <updated>2017-07-12T13:58:54Z</updated>\\n    <published>2015-05-22T13:48:10Z</published>\\n    <title>Robust Rotation Synchronization via Low-rank and Sparse Matrix\\n  Decomposition</title>\\n    <summary>  This paper deals with the rotation synchronization problem, which arises in\\nglobal registration of 3D point-sets and in structure from motion. The problem\\nis formulated in an unprecedented way as a \"low-rank and sparse\" matrix\\ndecomposition that handles both outliers and missing data. A minimization\\nstrategy, dubbed R-GoDec, is also proposed and evaluated experimentally against\\nstate-of-the-art algorithms on simulated and real data. The results show that\\nR-GoDec is the fastest among the robust algorithms.\\n</summary>\\n    <author>\\n      <name>Federica Arrigoni</name>\\n    </author>\\n    <author>\\n      <name>Andrea Fusiello</name>\\n    </author>\\n    <author>\\n      <name>Beatrice Rossi</name>\\n    </author>\\n    <author>\\n      <name>Pasqualina Fragneto</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1016/j.cviu.2018.08.001</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1016/j.cviu.2018.08.001\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">The material contained in this paper is part of a manuscript\\n  submitted to CVIU</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">In Computer Vision and Image Understanding, 174: 95-113, 2018</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1505.06079v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1505.06079v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1505.06219v1</id>\\n    <updated>2015-04-07T01:26:06Z</updated>\\n    <published>2015-04-07T01:26:06Z</published>\\n    <title>A comparative study between proposed Hyper Kurtosis based Modified\\n  Duo-Histogram Equalization (HKMDHE) and Contrast Limited Adaptive Histogram\\n  Equalization (CLAHE) for Contrast Enhancement Purpose of Low Contrast Human\\n  Brain CT scan images</title>\\n    <summary>  In this paper, a comparative study between proposed hyper kurtosis based\\nmodified duo-histogram equalization (HKMDHE) algorithm and contrast limited\\nadaptive histogram enhancement (CLAHE) has been presented for the\\nimplementation of contrast enhancement and brightness preservation of low\\ncontrast human brain CT scan images. In HKMDHE algorithm, contrast enhancement\\nis done on the hyper-kurtosis based application. The results are very promising\\nof proposed HKMDHE technique with improved PSNR values and lesser AMMBE values\\nthan CLAHE technique.\\n</summary>\\n    <author>\\n      <name>Sabyasachi Mukhopadhyay</name>\\n    </author>\\n    <author>\\n      <name>Soham Mandal</name>\\n    </author>\\n    <author>\\n      <name>Sawon Pratiher</name>\\n    </author>\\n    <author>\\n      <name>Satyasaran Changdar</name>\\n    </author>\\n    <author>\\n      <name>Ritwik Burman</name>\\n    </author>\\n    <author>\\n      <name>Nirmalya Ghosh</name>\\n    </author>\\n    <author>\\n      <name>Prasanta K. Panigrahi</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1505.06219v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1505.06219v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1505.06800v1</id>\\n    <updated>2015-05-26T03:52:52Z</updated>\\n    <published>2015-05-26T03:52:52Z</published>\\n    <title>Boosting-like Deep Learning For Pedestrian Detection</title>\\n    <summary>  This paper proposes boosting-like deep learning (BDL) framework for\\npedestrian detection. Due to overtraining on the limited training samples,\\noverfitting is a major problem of deep learning. We incorporate a boosting-like\\ntechnique into deep learning to weigh the training samples, and thus prevent\\novertraining in the iterative process. We theoretically give the details of\\nderivation of our algorithm, and report the experimental results on open data\\nsets showing that BDL achieves a better stable performance than the\\nstate-of-the-arts. Our approach achieves 15.85% and 3.81% reduction in the\\naverage miss rate compared with ACF and JointDeep on the largest Caltech\\nbenchmark dataset, respectively.\\n</summary>\\n    <author>\\n      <name>Lei Wang</name>\\n    </author>\\n    <author>\\n      <name>Baochang Zhang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages,7 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1505.06800v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1505.06800v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1505.06814v1</id>\\n    <updated>2015-05-26T06:02:05Z</updated>\\n    <published>2015-05-26T06:02:05Z</published>\\n    <title>Discrete Independent Component Analysis (DICA) with Belief Propagation</title>\\n    <summary>  We apply belief propagation to a Bayesian bipartite graph composed of\\ndiscrete independent hidden variables and discrete visible variables. The\\nnetwork is the Discrete counterpart of Independent Component Analysis (DICA)\\nand it is manipulated in a factor graph form for inference and learning. A full\\nset of simulations is reported for character images from the MNIST dataset. The\\nresults show that the factorial code implemented by the sources contributes to\\nbuild a good generative model for the data that can be used in various\\ninference modes.\\n</summary>\\n    <author>\\n      <name>Francesco A. N. Palmieri</name>\\n    </author>\\n    <author>\\n      <name>Amedeo Buonanno</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Sumbitted for publication (May 2015)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1505.06814v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1505.06814v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1505.07934v1</id>\\n    <updated>2015-05-29T06:18:31Z</updated>\\n    <published>2015-05-29T06:18:31Z</published>\\n    <title>Symbolic Segmentation Using Algorithm Selection</title>\\n    <summary>  In this paper we present an alternative approach to symbolic segmentation;\\ninstead of implementing a new method we approach symbolic segmentation as an\\nalgorithm selection problem. That is, let there be $n$ available algorithms for\\nsymbolic segmentation, a selection mechanism forms a set of input features and\\nimage attributes and selects on a case by case basis the best algorithm. The\\nselection mechanism is demonstrated from within an algorithm framework where\\nthe selection is done in a set of various algorithm networks. Two sets of\\nexperiments are performed and in both cases we demonstrate that the algorithm\\nselection allows to increase the result of the symbolic segmentation by a\\nconsiderable amount.\\n</summary>\\n    <author>\\n      <name>Martin Lukac</name>\\n    </author>\\n    <author>\\n      <name>Kamila Abdiyeva</name>\\n    </author>\\n    <author>\\n      <name>Michitaka Kameyama</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1505.07934v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1505.07934v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1505.08070v2</id>\\n    <updated>2022-01-09T19:51:44Z</updated>\\n    <published>2015-05-29T14:51:18Z</published>\\n    <title>General Deformations of Point Configurations Viewed By a Pinhole Model\\n  Camera</title>\\n    <summary>  This paper is a theoretical study of the following Non-Rigid Structure from\\nMotion problem. What can be computed from a monocular view of a parametrically\\ndeforming set of points? We treat various variations of this problem for affine\\nand polynomial deformations with calibrated and uncalibrated cameras. We show\\nthat in general at least three images with quasi-identical two deformations are\\nneeded in order to have a finite set of solutions of the points\\' structure and\\ncalculate some simple examples.\\n</summary>\\n    <author>\\n      <name>Yirmeyahu Kaminski</name>\\n    </author>\\n    <author>\\n      <name>Michael Werman</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Journal of Mathematical Imaging and Vision, 65, 631-643 (2023)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1505.08070v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1505.08070v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.AG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"53Z99, 53A07, 68T45, 14P05\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1505.08071v1</id>\\n    <updated>2015-05-29T14:51:23Z</updated>\\n    <published>2015-05-29T14:51:23Z</published>\\n    <title>Geometry of Graph Edit Distance Spaces</title>\\n    <summary>  In this paper we study the geometry of graph spaces endowed with a special\\nclass of graph edit distances. The focus is on geometrical results useful for\\nstatistical pattern recognition. The main result is the Graph Representation\\nTheorem. It states that a graph is a point in some geometrical space, called\\norbit space. Orbit spaces are well investigated and easier to explore than the\\noriginal graph space. We derive a number of geometrical results from the orbit\\nspace representation, translate them to the graph space, and indicate their\\nsignificance and usefulness in statistical pattern recognition.\\n</summary>\\n    <author>\\n      <name>Brijnesh J. Jain</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1505.08071v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1505.08071v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.MG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1505.08153v1</id>\\n    <updated>2015-05-29T19:09:02Z</updated>\\n    <published>2015-05-29T19:09:02Z</published>\\n    <title>Feature Representation for Online Signature Verification</title>\\n    <summary>  Biometrics systems have been used in a wide range of applications and have\\nimproved people authentication. Signature verification is one of the most\\ncommon biometric methods with techniques that employ various specifications of\\na signature. Recently, deep learning has achieved great success in many fields,\\nsuch as image, sounds and text processing. In this paper, deep learning method\\nhas been used for feature extraction and feature selection.\\n</summary>\\n    <author>\\n      <name>Mohsen Fayyaz</name>\\n    </author>\\n    <author>\\n      <name>Mohammad Hajizadeh_Saffar</name>\\n    </author>\\n    <author>\\n      <name>Mohammad Sabokrou</name>\\n    </author>\\n    <author>\\n      <name>Mahmood Fathy</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/AISP.2015.7123528</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/AISP.2015.7123528\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages, 10 figures, Submitted to IEEE Transactions on Information\\n  Forensics and Security</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1505.08153v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1505.08153v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1507.00302v1</id>\\n    <updated>2015-07-01T17:59:21Z</updated>\\n    <published>2015-07-01T17:59:21Z</published>\\n    <title>Pose Embeddings: A Deep Architecture for Learning to Match Human Poses</title>\\n    <summary>  We present a method for learning an embedding that places images of humans in\\nsimilar poses nearby. This embedding can be used as a direct method of\\ncomparing images based on human pose, avoiding potential challenges of\\nestimating body joint positions. Pose embedding learning is formulated under a\\ntriplet-based distance criterion. A deep architecture is used to allow learning\\nof a representation capable of making distinctions between different poses.\\nExperiments on human pose matching and retrieval from video data demonstrate\\nthe potential of the method.\\n</summary>\\n    <author>\\n      <name>Greg Mori</name>\\n    </author>\\n    <author>\\n      <name>Caroline Pantofaru</name>\\n    </author>\\n    <author>\\n      <name>Nisarg Kothari</name>\\n    </author>\\n    <author>\\n      <name>Thomas Leung</name>\\n    </author>\\n    <author>\\n      <name>George Toderici</name>\\n    </author>\\n    <author>\\n      <name>Alexander Toshev</name>\\n    </author>\\n    <author>\\n      <name>Weilong Yang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1507.00302v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1507.00302v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1507.00410v2</id>\\n    <updated>2015-09-18T18:01:47Z</updated>\\n    <published>2015-07-02T02:16:42Z</published>\\n    <title>Convolutional Color Constancy</title>\\n    <summary>  Color constancy is the problem of inferring the color of the light that\\nilluminated a scene, usually so that the illumination color can be removed.\\nBecause this problem is underconstrained, it is often solved by modeling the\\nstatistical regularities of the colors of natural objects and illumination. In\\ncontrast, in this paper we reformulate the problem of color constancy as a 2D\\nspatial localization task in a log-chrominance space, thereby allowing us to\\napply techniques from object detection and structured prediction to the color\\nconstancy problem. By directly learning how to discriminate between correctly\\nwhite-balanced images and poorly white-balanced images, our model is able to\\nimprove performance on standard benchmarks by nearly 40%.\\n</summary>\\n    <author>\\n      <name>Jonathan T. Barron</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1507.00410v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1507.00410v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1507.00913v1</id>\\n    <updated>2015-07-03T13:53:26Z</updated>\\n    <published>2015-07-03T13:53:26Z</published>\\n    <title>Fine-grained Recognition Datasets for Biodiversity Analysis</title>\\n    <summary>  In the following paper, we present and discuss challenging applications for\\nfine-grained visual classification (FGVC): biodiversity and species analysis.\\nWe not only give details about two challenging new datasets suitable for\\ncomputer vision research with up to 675 highly similar classes, but also\\npresent first results with localized features using convolutional neural\\nnetworks (CNN). We conclude with a list of challenging new research directions\\nin the area of visual classification for biodiversity research.\\n</summary>\\n    <author>\\n      <name>Erik Rodner</name>\\n    </author>\\n    <author>\\n      <name>Marcel Simon</name>\\n    </author>\\n    <author>\\n      <name>Gunnar Brehm</name>\\n    </author>\\n    <author>\\n      <name>Stephanie Pietsch</name>\\n    </author>\\n    <author>\\n      <name>J. Wolfgang W\\xc3\\xa4gele</name>\\n    </author>\\n    <author>\\n      <name>Joachim Denzler</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">CVPR FGVC Workshop 2015; dataset available</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1507.00913v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1507.00913v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1507.03751v1</id>\\n    <updated>2015-07-14T07:57:39Z</updated>\\n    <published>2015-07-14T07:57:39Z</published>\\n    <title>Closed Curves and Elementary Visual Object Identification</title>\\n    <summary>  For two closed curves on a plane (discrete version) and local criteria for\\nsimilarity of points on the curves one gets a potential, which describes the\\nsimilarity between curve points. This is the base for a global similarity\\nmeasure of closed curves (Fr\\\\\\'echet distance). I use borderlines of handwritten\\ndigits to demonstrate an area of application. I imagine, measuring the\\nsimilarity of closed curves is an essential and elementary task performed by a\\nvisual system. This approach to similarity measures may be used by visual\\nsystems.\\n</summary>\\n    <author>\\n      <name>Manfred Harringer</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">13 pages, 10 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1507.03751v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1507.03751v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-bio.NC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1507.04835v1</id>\\n    <updated>2015-07-17T05:24:45Z</updated>\\n    <published>2015-07-17T05:24:45Z</published>\\n    <title>Multiscale Adaptive Representation of Signals: I. The Basic Framework</title>\\n    <summary>  We introduce a framework for designing multi-scale, adaptive, shift-invariant\\nframes and bi-frames for representing signals. The new framework, called\\nAdaFrame, improves over dictionary learning-based techniques in terms of\\ncomputational efficiency at inference time. It improves classical multi-scale\\nbasis such as wavelet frames in terms of coding efficiency. It provides an\\nattractive alternative to dictionary learning-based techniques for low level\\nsignal processing tasks, such as compression and denoising, as well as high\\nlevel tasks, such as feature extraction for object recognition. Connections\\nwith deep convolutional networks are also discussed. In particular, the\\nproposed framework reveals a drawback in the commonly used approach for\\nvisualizing the activations of the intermediate layers in convolutional\\nnetworks, and suggests a natural alternative.\\n</summary>\\n    <author>\\n      <name>Cheng Tai</name>\\n    </author>\\n    <author>\\n      <name>Weinan E</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1507.04835v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1507.04835v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1507.04844v1</id>\\n    <updated>2015-07-17T06:21:31Z</updated>\\n    <published>2015-07-17T06:21:31Z</published>\\n    <title>Learning Robust Deep Face Representation</title>\\n    <summary>  With the development of convolution neural network, more and more researchers\\nfocus their attention on the advantage of CNN for face recognition task. In\\nthis paper, we propose a deep convolution network for learning a robust face\\nrepresentation. The deep convolution net is constructed by 4 convolution\\nlayers, 4 max pooling layers and 2 fully connected layers, which totally\\ncontains about 4M parameters. The Max-Feature-Map activation function is used\\ninstead of ReLU because the ReLU might lead to the loss of information due to\\nthe sparsity while the Max-Feature-Map can get the compact and discriminative\\nfeature vectors. The model is trained on CASIA-WebFace dataset and evaluated on\\nLFW dataset. The result on LFW achieves 97.77% on unsupervised setting for\\nsingle net.\\n</summary>\\n    <author>\\n      <name>Xiang Wu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1507.04844v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1507.04844v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1507.05243v1</id>\\n    <updated>2015-07-19T03:10:28Z</updated>\\n    <published>2015-07-19T03:10:28Z</published>\\n    <title>Hand Gesture Recognition Library</title>\\n    <summary>  In this paper we have presented a hand gesture recognition library. Various\\nfunctions include detecting cluster count, cluster orientation, finger pointing\\ndirection, etc. To use these functions first the input image needs to be\\nprocessed into a logical array for which a function has been developed. The\\nlibrary has been developed keeping flexibility in mind and thus provides\\napplication developers a wide range of options to develop custom gestures.\\n</summary>\\n    <author>\\n      <name>Jonathan Fidelis Paul</name>\\n    </author>\\n    <author>\\n      <name>Dibyabiva Seth</name>\\n    </author>\\n    <author>\\n      <name>Cijo Paul</name>\\n    </author>\\n    <author>\\n      <name>Jayati Ghosh Dastidar</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">International Journal of Science and Applied Information\\n  Technology, Volume 3, No.2, March - April 2014</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1507.05243v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1507.05243v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1507.06332v1</id>\\n    <updated>2015-07-22T20:21:59Z</updated>\\n    <published>2015-07-22T20:21:59Z</published>\\n    <title>Part Localization using Multi-Proposal Consensus for Fine-Grained\\n  Categorization</title>\\n    <summary>  We present a simple deep learning framework to simultaneously predict\\nkeypoint locations and their respective visibilities and use those to achieve\\nstate-of-the-art performance for fine-grained classification. We show that by\\nconditioning the predictions on object proposals with sufficient image support,\\nour method can do well without complicated spatial reasoning. Instead,\\ninference methods with robustness to outliers, yield state-of-the-art for\\nkeypoint localization. We demonstrate the effectiveness of our accurate\\nkeypoint localization and visibility prediction on the fine-grained bird\\nrecognition task with and without ground truth bird bounding boxes, and\\noutperform existing state-of-the-art methods by over 2%.\\n</summary>\\n    <author>\\n      <name>Kevin J. Shih</name>\\n    </author>\\n    <author>\\n      <name>Arun Mallya</name>\\n    </author>\\n    <author>\\n      <name>Saurabh Singh</name>\\n    </author>\\n    <author>\\n      <name>Derek Hoiem</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">BMVC 2015</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1507.06332v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1507.06332v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1507.08958v1</id>\\n    <updated>2015-07-31T17:41:55Z</updated>\\n    <published>2015-07-31T17:41:55Z</published>\\n    <title>SnowWatch: Snow Monitoring through Acquisition and Analysis of\\n  User-Generated Content</title>\\n    <summary>  We present a system for complementing snow phenomena monitoring with virtual\\nmeasurements extracted from public visual content. The proposed system\\nintegrates an automatic acquisition and analysis of photographs and webcam\\nimages depicting Alpine mountains. In particular, the technical demonstration\\nconsists in a web portal that interfaces the whole system with the population.\\nIt acts as an entertaining photo-sharing social web site, acquiring at the same\\ntime visual content necessary for environmental monitoring.\\n</summary>\\n    <author>\\n      <name>Roman Fedorov</name>\\n    </author>\\n    <author>\\n      <name>Piero Fraternali</name>\\n    </author>\\n    <author>\\n      <name>Chiara Pasini</name>\\n    </author>\\n    <author>\\n      <name>Marco Tagliasacchi</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IEEE International Conference on Multimedia and Expo, ICME 2015.\\n  Accepted and presented technical demo proposal</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1507.08958v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1507.08958v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1508.00092v1</id>\\n    <updated>2015-08-01T07:15:19Z</updated>\\n    <published>2015-08-01T07:15:19Z</published>\\n    <title>Land Use Classification in Remote Sensing Images by Convolutional Neural\\n  Networks</title>\\n    <summary>  We explore the use of convolutional neural networks for the semantic\\nclassification of remote sensing scenes. Two recently proposed architectures,\\nCaffeNet and GoogLeNet, are adopted, with three different learning modalities.\\nBesides conventional training from scratch, we resort to pre-trained networks\\nthat are only fine-tuned on the target data, so as to avoid overfitting\\nproblems and reduce design time. Experiments on two remote sensing datasets,\\nwith markedly different characteristics, testify on the effectiveness and wide\\napplicability of the proposed solution, which guarantees a significant\\nperformance improvement over all state-of-the-art references.\\n</summary>\\n    <author>\\n      <name>Marco Castelluccio</name>\\n    </author>\\n    <author>\\n      <name>Giovanni Poggi</name>\\n    </author>\\n    <author>\\n      <name>Carlo Sansone</name>\\n    </author>\\n    <author>\\n      <name>Luisa Verdoliva</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1508.00092v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1508.00092v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1508.00239v1</id>\\n    <updated>2015-08-02T14:09:02Z</updated>\\n    <published>2015-08-02T14:09:02Z</published>\\n    <title>Partial matching face recognition method for rehabilitation nursing\\n  robots beds</title>\\n    <summary>  In order to establish face recognition system in rehabilitation nursing\\nrobots beds and achieve real-time monitor the patient on the bed. We propose a\\nface recognition method based on partial matching Hu moments which apply for\\nrehabilitation nursing robots beds. Firstly we using Haar classifier to detect\\nhuman faces automatically in dynamic video frames. Secondly we using Otsu\\nthreshold method to extract facial features (eyebrows, eyes, mouth) in the face\\nimage and its Hu moments. Finally, we using Hu moment feature set to achieve\\nthe automatic face recognition. Experimental results show that this method can\\nefficiently identify face in a dynamic video and it has high practical value\\n(the accuracy rate is 91% and the average recognition time is 4.3s).\\n</summary>\\n    <author>\\n      <name>Dongmei Liang</name>\\n    </author>\\n    <author>\\n      <name>Wushan Cheng</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1508.00239v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1508.00239v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1508.00282v1</id>\\n    <updated>2015-08-02T20:40:21Z</updated>\\n    <published>2015-08-02T20:40:21Z</published>\\n    <title>On Hyperspectral Classification in the Compressed Domain</title>\\n    <summary>  In this paper, we study the problem of hyperspectral pixel classification\\nbased on the recently proposed architectures for compressive whisk-broom\\nhyperspectral imagers without the need to reconstruct the complete data cube. A\\nclear advantage of classification in the compressed domain is its suitability\\nfor real-time on-site processing of the sensed data. Moreover, it is assumed\\nthat the training process also takes place in the compressed domain, thus,\\nisolating the classification unit from the recovery unit at the receiver\\'s\\nside. We show that, perhaps surprisingly, using distinct measurement matrices\\nfor different pixels results in more accuracy of the learned classifier and\\nconsistent classification performance, supporting the role of information\\ndiversity in learning.\\n</summary>\\n    <author>\\n      <name>Mohammad Aghagolzadeh</name>\\n    </author>\\n    <author>\\n      <name>Hayder Radha</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1508.00282v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1508.00282v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1508.00998v2</id>\\n    <updated>2015-12-11T14:35:20Z</updated>\\n    <published>2015-08-05T08:25:27Z</published>\\n    <title>Single and Multiple Illuminant Estimation Using Convolutional Neural\\n  Networks</title>\\n    <summary>  In this paper we present a method for the estimation of the color of the\\nilluminant in RAW images. The method includes a Convolutional Neural Network\\nthat has been specially designed to produce multiple local estimates. A\\nmultiple illuminant detector determines whether or not the local outputs of the\\nnetwork must be aggregated into a single estimate. We evaluated our method on\\nstandard datasets with single and multiple illuminants, obtaining lower\\nestimation errors with respect to those obtained by other general purpose\\nmethods in the state of the art.\\n</summary>\\n    <author>\\n      <name>Simone Bianco</name>\\n    </author>\\n    <author>\\n      <name>Claudio Cusano</name>\\n    </author>\\n    <author>\\n      <name>Raimondo Schettini</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Submitted to IEEE Transactions on Pattern Analysis and Machine\\n  Intelligence</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1508.00998v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1508.00998v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1508.01057v2</id>\\n    <updated>2017-04-19T08:19:29Z</updated>\\n    <published>2015-08-05T13:02:48Z</published>\\n    <title>On the convergence of the sparse possibilistic c-means algorithm</title>\\n    <summary>  In this paper, a convergence proof for the recently proposed sparse\\npossibilistic c-means (SPCM) algorithm is provided, utilizing the celebrated\\nZangwill convergence theorem. It is shown that the iterative sequence generated\\nby SPCM converges to a stationary point or there exists a subsequence of it\\nthat converges to a stationary point of the cost function of the algorithm.\\n</summary>\\n    <author>\\n      <name>Spyridoula D. Xenaki</name>\\n    </author>\\n    <author>\\n      <name>Konstantinos D. Koutroumbas</name>\\n    </author>\\n    <author>\\n      <name>Athanasios A. Rontogiannis</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1508.01057v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1508.01057v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1508.01667v1</id>\\n    <updated>2015-08-07T12:11:06Z</updated>\\n    <published>2015-08-07T12:11:06Z</published>\\n    <title>Places205-VGGNet Models for Scene Recognition</title>\\n    <summary>  VGGNets have turned out to be effective for object recognition in still\\nimages. However, it is unable to yield good performance by directly adapting\\nthe VGGNet models trained on the ImageNet dataset for scene recognition. This\\nreport describes our implementation of training the VGGNets on the large-scale\\nPlaces205 dataset. Specifically, we train three VGGNet models, namely\\nVGGNet-11, VGGNet-13, and VGGNet-16, by using a Multi-GPU extension of Caffe\\ntoolbox with high computational efficiency. We verify the performance of\\ntrained Places205-VGGNet models on three datasets: MIT67, SUN397, and\\nPlaces205. Our trained models achieve the state-of-the-art performance on these\\ndatasets and are made public available.\\n</summary>\\n    <author>\\n      <name>Limin Wang</name>\\n    </author>\\n    <author>\\n      <name>Sheng Guo</name>\\n    </author>\\n    <author>\\n      <name>Weilin Huang</name>\\n    </author>\\n    <author>\\n      <name>Yu Qiao</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">2 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1508.01667v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1508.01667v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1508.01722v2</id>\\n    <updated>2016-03-02T19:41:42Z</updated>\\n    <published>2015-08-07T15:21:19Z</published>\\n    <title>Unconstrained Face Verification using Deep CNN Features</title>\\n    <summary>  In this paper, we present an algorithm for unconstrained face verification\\nbased on deep convolutional features and evaluate it on the newly released\\nIARPA Janus Benchmark A (IJB-A) dataset. The IJB-A dataset includes real-world\\nunconstrained faces from 500 subjects with full pose and illumination\\nvariations which are much harder than the traditional Labeled Face in the Wild\\n(LFW) and Youtube Face (YTF) datasets. The deep convolutional neural network\\n(DCNN) is trained using the CASIA-WebFace dataset. Extensive experiments on the\\nIJB-A dataset are provided.\\n</summary>\\n    <author>\\n      <name>Jun-Cheng Chen</name>\\n    </author>\\n    <author>\\n      <name>Vishal M. Patel</name>\\n    </author>\\n    <author>\\n      <name>Rama Chellappa</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1508.01722v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1508.01722v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1508.01859v1</id>\\n    <updated>2015-08-08T06:40:55Z</updated>\\n    <published>2015-08-08T06:40:55Z</published>\\n    <title>Simulation of optical flow and fuzzy based obstacle avoidance system for\\n  mobile robots</title>\\n    <summary>  Honey bees use optical flow to avoid obstacles effectively. In this research\\nwork similar methodology was tested on a simulated mobile robot. Simulation\\nframework was based on VRML and Simulink in a 3D world. Optical flow vectors\\nwere calculated from a video scene captured by a virtual camera which was used\\nas inputs to a fuzzy logic controller. Fuzzy logic controller decided the\\nlocomotion of the robot. Different fuzzy logic rules were evaluated. The robot\\nwas able to navigate through complex static and dynamic environments\\neffectively, avoiding obstacles on its path.\\n</summary>\\n    <author>\\n      <name>G. D. Illeperuma</name>\\n    </author>\\n    <author>\\n      <name>D. U. J. Sonnadara</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, Published in 30 April 2015</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">International Journal of Artificial Intelligence and Neural\\n  Networks, 5-1 (2015) 53-56</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1508.01859v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1508.01859v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1508.02606v1</id>\\n    <updated>2015-08-11T14:17:28Z</updated>\\n    <published>2015-08-11T14:17:28Z</published>\\n    <title>InAR:Inverse Augmented Reality</title>\\n    <summary>  Augmented reality is the art to seamlessly fuse virtual objects into real\\nones. In this short note, we address the opposite problem, the inverse\\naugmented reality, that is, given a perfectly augmented reality scene where\\nhuman is unable to distinguish real objects from virtual ones, how the machine\\ncould help do the job. We show by structure from motion (SFM), a simple 3D\\nreconstruction technique from images in computer vision, the real and virtual\\nobjects can be easily separated in the reconstructed 3D scene.\\n</summary>\\n    <author>\\n      <name>Hao Hu</name>\\n    </author>\\n    <author>\\n      <name>Hainan Cui</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">2 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1508.02606v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1508.02606v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1508.02977v1</id>\\n    <updated>2015-08-12T16:16:10Z</updated>\\n    <published>2015-08-12T16:16:10Z</published>\\n    <title>A massively parallel multi-level approach to a domain decomposition\\n  method for the optical flow estimation with varying illumination</title>\\n    <summary>  We consider a variational method to solve the optical flow problem with\\nvarying illumination. We apply an adaptive control of the regularization\\nparameter which allows us to preserve the edges and fine features of the\\ncomputed flow. To reduce the complexity of the estimation for high resolution\\nimages and the time of computations, we implement a multi-level parallel\\napproach based on the domain decomposition with the Schwarz overlapping method.\\nThe second level of parallelism uses the massively parallel solver MUMPS. We\\nperform some numerical simulations to show the efficiency of our approach and\\nto validate it on classical and real-world image sequences.\\n</summary>\\n    <author>\\n      <name>Diane Gilliocq-Hirtz</name>\\n    </author>\\n    <author>\\n      <name>Zakaria Belhachmi</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1508.02977v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1508.02977v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1508.03710v1</id>\\n    <updated>2015-08-15T09:03:19Z</updated>\\n    <published>2015-08-15T09:03:19Z</published>\\n    <title>A Novel Approach For Finger Vein Verification Based on Self-Taught\\n  Learning</title>\\n    <summary>  In this paper, we propose a method for user Finger Vein Authentication (FVA)\\nas a biometric system. Using the discriminative features for classifying theses\\nfinger veins is one of the main tips that make difference in related works,\\nThus we propose to learn a set of representative features, based on\\nautoencoders. We model the user finger vein using a Gaussian distribution.\\nExperimental results show that our algorithm perform like a state-of-the-art on\\nSDUMLA-HMT benchmark.\\n</summary>\\n    <author>\\n      <name>Mohsen Fayyaz</name>\\n    </author>\\n    <author>\\n      <name>Masoud PourReza</name>\\n    </author>\\n    <author>\\n      <name>Mohammad Hajizadeh Saffar</name>\\n    </author>\\n    <author>\\n      <name>Mohammad Sabokrou</name>\\n    </author>\\n    <author>\\n      <name>Mahmood Fathy</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, 4 figures, Submitted Iranian Conference on Machine Vision\\n  and Image Processing</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1508.03710v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1508.03710v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1508.04198v1</id>\\n    <updated>2015-08-18T02:33:30Z</updated>\\n    <published>2015-08-18T02:33:30Z</published>\\n    <title>Low Rank Representation on Riemannian Manifold of Square Root Densities</title>\\n    <summary>  In this paper, we present a novel low rank representation (LRR) algorithm for\\ndata lying on the manifold of square root densities. Unlike traditional LRR\\nmethods which rely on the assumption that the data points are vectors in the\\nEuclidean space, our new algorithm is designed to incorporate the intrinsic\\ngeometric structure and geodesic distance of the manifold. Experiments on\\nseveral computer vision datasets showcase its noise robustness and superior\\nperformance on classification and subspace clustering compared to other\\nstate-of-the-art approaches.\\n</summary>\\n    <author>\\n      <name>Yifan Fu</name>\\n    </author>\\n    <author>\\n      <name>Junbin Gao</name>\\n    </author>\\n    <author>\\n      <name>Xia Hong</name>\\n    </author>\\n    <author>\\n      <name>David Tien</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1508.04198v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1508.04198v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1508.04238v1</id>\\n    <updated>2015-08-18T08:18:55Z</updated>\\n    <published>2015-08-18T08:18:55Z</published>\\n    <title>Preprint ARPPS Augmented Reality Pipeline Prospect System</title>\\n    <summary>  This is the preprint version of our paper on ICONIP. Outdoor augmented\\nreality geographic information system (ARGIS) is the hot application of\\naugmented reality over recent years. This paper concludes the key solutions of\\nARGIS, designs the mobile augmented reality pipeline prospect system (ARPPS),\\nand respectively realizes the machine vision based pipeline prospect system\\n(MVBPPS) and the sensor based pipeline prospect system (SBPPS). With the\\nMVBPPS\\'s realization, this paper studies the neural network based 3D features\\nmatching method.\\n</summary>\\n    <author>\\n      <name>Xiaolei Zhang</name>\\n    </author>\\n    <author>\\n      <name>Yong Han</name>\\n    </author>\\n    <author>\\n      <name>DongSheng Hao</name>\\n    </author>\\n    <author>\\n      <name>Zhihan Lv</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This is the preprint version of our paper on ICONIP</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1508.04238v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1508.04238v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1508.04389v1</id>\\n    <updated>2015-08-18T17:24:09Z</updated>\\n    <published>2015-08-18T17:24:09Z</published>\\n    <title>A Deep Pyramid Deformable Part Model for Face Detection</title>\\n    <summary>  We present a face detection algorithm based on Deformable Part Models and\\ndeep pyramidal features. The proposed method called DP2MFD is able to detect\\nfaces of various sizes and poses in unconstrained conditions. It reduces the\\ngap in training and testing of DPM on deep features by adding a normalization\\nlayer to the deep convolutional neural network (CNN). Extensive experiments on\\nfour publicly available unconstrained face detection datasets show that our\\nmethod is able to capture the meaningful structure of faces and performs\\nsignificantly better than many competitive face detection algorithms.\\n</summary>\\n    <author>\\n      <name>Rajeev Ranjan</name>\\n    </author>\\n    <author>\\n      <name>Vishal M. Patel</name>\\n    </author>\\n    <author>\\n      <name>Rama Chellappa</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1508.04389v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1508.04389v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n</feed>\\n'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Проверка уникальности заголовков"
      ],
      "metadata": {
        "id": "t6gfvM_9pcJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# unique titles\n",
        "\n",
        "docs = feedparser.parse(r)\n",
        "titles = [d[\"title\"] for d in docs[\"entries\"]]\n",
        "len(set(titles))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJ45CPNAH7vI",
        "outputId": "dab84ce5-e261-4420-ed70-0b9f263f64c9"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Извлечение ключевых слов (Transformers)\n",
        "\n",
        "Модель: `ilsilfverskiold/tech-keywords-extractor`.\n"
      ],
      "metadata": {
        "id": "USko2qW9JZjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# keywords extraction\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text2text-generation\", model=\"ilsilfverskiold/tech-keywords-extractor\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lgmfzge1H7qR",
        "outputId": "20a95c4b-15b2-49d6-f95e-28eb47a8246d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.09 s, sys: 173 ms, total: 1.26 s\n",
            "Wall time: 2.11 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%time\n",
        "# data extraction example\n",
        "\n",
        "doc = feedparser.parse(r)\n",
        "title = doc['entries'][0]['title']\n",
        "abstract = doc['entries'][0]['summary']\n",
        "authors = [author['name'] for author in doc['entries'][0]['authors']]\n",
        "tags = [tag['term'] for tag in doc['entries'][0]['tags']]\n",
        "\n",
        "keywords = pipe(abstract)[0][\"generated_text\"].split(\", \")\n",
        "\n",
        "\n",
        "print(f\"Title: {title}\\n\\nAuthors: {authors}\\n\\nAbstract: {abstract}\\n\\nTags: {tags}\\n\\nKeywords: {keywords}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4qZTtW5H7nq",
        "outputId": "e828ed78-8486-4ce6-e127-b51d7cd498d5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 9 µs, sys: 1 µs, total: 10 µs\n",
            "Wall time: 17.9 µs\n",
            "Title: Recognition of Regular Shapes in Satelite Images\n",
            "\n",
            "Authors: ['Ahmad Reza Eskandari', 'Ali Pourmohammad']\n",
            "\n",
            "Abstract: This paper has been withdrawn by the author ali pourmohammad.\n",
            "\n",
            "Tags: ['cs.CV']\n",
            "\n",
            "Keywords: ['Paper', 'Ali Pourmohammad']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%time\n",
        "\n",
        "pubs = []\n",
        "for entry in docs[\"entries\"]:\n",
        "  data = {\"title\": entry['title'],\n",
        "          \"abstract\": entry['summary'],\n",
        "          \"authors\": [author['name'] for author in entry['authors']],\n",
        "          \"tags\": [tag['term'] for tag in entry['tags']]}\n",
        "  pubs.append(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dhYVDKJH7lG",
        "outputId": "511b6855-b734-4ac8-c339-99d881b05da5"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
            "Wall time: 6.2 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Формирование DataFrame"
      ],
      "metadata": {
        "id": "ROyZ4y-QpxA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(pubs)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "VsZ76w5jJwSH",
        "outputId": "084cef6b-87b8-4f28-b2f2-08d4205c3fb3"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 title  \\\n",
              "0     Recognition of Regular Shapes in Satelite Images   \n",
              "1    Convolutional Matching Pursuit and Dictionary ...   \n",
              "2    Template Matching based Object Detection Using...   \n",
              "3    Exploration of object recognition from 3D poin...   \n",
              "4    Brain MRI Image Super Resolution using Phase S...   \n",
              "..                                                 ...   \n",
              "995  A massively parallel multi-level approach to a...   \n",
              "996  A Novel Approach For Finger Vein Verification ...   \n",
              "997  Low Rank Representation on Riemannian Manifold...   \n",
              "998  Preprint ARPPS Augmented Reality Pipeline Pros...   \n",
              "999  A Deep Pyramid Deformable Part Model for Face ...   \n",
              "\n",
              "                                              abstract  \\\n",
              "0    This paper has been withdrawn by the author al...   \n",
              "1    Matching pursuit and K-SVD is demonstrated in ...   \n",
              "2    This article provides a step by step developme...   \n",
              "3    We present our latest experiment results of ob...   \n",
              "4    A hallucination-free and computationally effic...   \n",
              "..                                                 ...   \n",
              "995  We consider a variational method to solve the ...   \n",
              "996  In this paper, we propose a method for user Fi...   \n",
              "997  In this paper, we present a novel low rank rep...   \n",
              "998  This is the preprint version of our paper on I...   \n",
              "999  We present a face detection algorithm based on...   \n",
              "\n",
              "                                               authors     tags  \n",
              "0             [Ahmad Reza Eskandari, Ali Pourmohammad]  [cs.CV]  \n",
              "1        [Arthur Szlam, Koray Kavukcuoglu, Yann LeCun]  [cs.CV]  \n",
              "2                                      [Anish Acharya]  [cs.CV]  \n",
              "3                                           [Lin Duan]  [cs.CV]  \n",
              "4                           [Sifeng He, Bahram Jalali]  [cs.CV]  \n",
              "..                                                 ...      ...  \n",
              "995          [Diane Gilliocq-Hirtz, Zakaria Belhachmi]  [cs.CV]  \n",
              "996  [Mohsen Fayyaz, Masoud PourReza, Mohammad Haji...  [cs.CV]  \n",
              "997       [Yifan Fu, Junbin Gao, Xia Hong, David Tien]  [cs.CV]  \n",
              "998  [Xiaolei Zhang, Yong Han, DongSheng Hao, Zhiha...  [cs.CV]  \n",
              "999   [Rajeev Ranjan, Vishal M. Patel, Rama Chellappa]  [cs.CV]  \n",
              "\n",
              "[1000 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e7679f39-3df1-4bf9-ba0f-5412ae818815\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>authors</th>\n",
              "      <th>tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Recognition of Regular Shapes in Satelite Images</td>\n",
              "      <td>This paper has been withdrawn by the author al...</td>\n",
              "      <td>[Ahmad Reza Eskandari, Ali Pourmohammad]</td>\n",
              "      <td>[cs.CV]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Convolutional Matching Pursuit and Dictionary ...</td>\n",
              "      <td>Matching pursuit and K-SVD is demonstrated in ...</td>\n",
              "      <td>[Arthur Szlam, Koray Kavukcuoglu, Yann LeCun]</td>\n",
              "      <td>[cs.CV]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Template Matching based Object Detection Using...</td>\n",
              "      <td>This article provides a step by step developme...</td>\n",
              "      <td>[Anish Acharya]</td>\n",
              "      <td>[cs.CV]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Exploration of object recognition from 3D poin...</td>\n",
              "      <td>We present our latest experiment results of ob...</td>\n",
              "      <td>[Lin Duan]</td>\n",
              "      <td>[cs.CV]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Brain MRI Image Super Resolution using Phase S...</td>\n",
              "      <td>A hallucination-free and computationally effic...</td>\n",
              "      <td>[Sifeng He, Bahram Jalali]</td>\n",
              "      <td>[cs.CV]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>A massively parallel multi-level approach to a...</td>\n",
              "      <td>We consider a variational method to solve the ...</td>\n",
              "      <td>[Diane Gilliocq-Hirtz, Zakaria Belhachmi]</td>\n",
              "      <td>[cs.CV]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>A Novel Approach For Finger Vein Verification ...</td>\n",
              "      <td>In this paper, we propose a method for user Fi...</td>\n",
              "      <td>[Mohsen Fayyaz, Masoud PourReza, Mohammad Haji...</td>\n",
              "      <td>[cs.CV]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>Low Rank Representation on Riemannian Manifold...</td>\n",
              "      <td>In this paper, we present a novel low rank rep...</td>\n",
              "      <td>[Yifan Fu, Junbin Gao, Xia Hong, David Tien]</td>\n",
              "      <td>[cs.CV]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>Preprint ARPPS Augmented Reality Pipeline Pros...</td>\n",
              "      <td>This is the preprint version of our paper on I...</td>\n",
              "      <td>[Xiaolei Zhang, Yong Han, DongSheng Hao, Zhiha...</td>\n",
              "      <td>[cs.CV]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>A Deep Pyramid Deformable Part Model for Face ...</td>\n",
              "      <td>We present a face detection algorithm based on...</td>\n",
              "      <td>[Rajeev Ranjan, Vishal M. Patel, Rama Chellappa]</td>\n",
              "      <td>[cs.CV]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e7679f39-3df1-4bf9-ba0f-5412ae818815')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e7679f39-3df1-4bf9-ba0f-5412ae818815 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e7679f39-3df1-4bf9-ba0f-5412ae818815');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-1d166b03-bb67-426c-b3ce-3d59d25f9dfb\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1d166b03-bb67-426c-b3ce-3d59d25f9dfb')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-1d166b03-bb67-426c-b3ce-3d59d25f9dfb button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_aca9849a-4648-4f2a-abcb-77af7c5d15cc\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_aca9849a-4648-4f2a-abcb-77af7c5d15cc button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"An Iterative Fingerprint Enhancement Algorithm Based on Accurate\\n  Determination of Orientation Flow\",\n          \"Reduced egomotion estimation drift using omnidirectional views\",\n          \"Who and Where: People and Location Co-Clustering\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 998,\n        \"samples\": [\n          \"A brief description of tree structured sparse coding on the binary cube.\",\n          \"We first establish a law of large numbers and a convergence theorem in\\ndistribution to show the rate of convergence of the non-local means filter for\\nremoving Gaussian noise. We then introduce the notion of degree of similarity\\nto measure the role of similarity for the non-local means filter. Based on the\\nconvergence theorems, we propose a patch-based weighted means filter for\\nremoving impulse noise and its mixture with Gaussian noise by combining the\\nessential idea of the trilateral filter and that of the non-local means filter.\\nOur experiments show that our filter is competitive compared to recently\\nproposed methods.\",\n          \"This paper has been removed from arXiv as the submitter did not have\\nownership of the data presented in this work.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authors\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tags\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Генерация ключевых слов для всех абстрактов"
      ],
      "metadata": {
        "id": "CvXrp9edqTzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "df[\"keywords\"] = df[\"abstract\"].apply(lambda x: pipe(x)[0][\"generated_text\"].split(\", \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1uOPhVPH7id",
        "outputId": "3c354662-7940-4037-96ec-b3a297abe8a7"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 5min 32s, sys: 502 ms, total: 5min 32s\n",
            "Wall time: 6min 32s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"authors\"] = df[\"authors\"].apply(lambda x: \", \".join(x))\n",
        "df[\"tags\"] = df[\"tags\"].apply(lambda x: \", \".join(x))\n",
        "df[\"keywords\"] = df[\"keywords\"].apply(lambda x: \", \".join(x))\n",
        "\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "sGt_qGDbH7fs",
        "outputId": "d9afd463-54ff-49fb-f99d-861094c92983"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 title  \\\n",
              "0     Recognition of Regular Shapes in Satelite Images   \n",
              "1    Convolutional Matching Pursuit and Dictionary ...   \n",
              "2    Template Matching based Object Detection Using...   \n",
              "3    Exploration of object recognition from 3D poin...   \n",
              "4    Brain MRI Image Super Resolution using Phase S...   \n",
              "..                                                 ...   \n",
              "995  A massively parallel multi-level approach to a...   \n",
              "996  A Novel Approach For Finger Vein Verification ...   \n",
              "997  Low Rank Representation on Riemannian Manifold...   \n",
              "998  Preprint ARPPS Augmented Reality Pipeline Pros...   \n",
              "999  A Deep Pyramid Deformable Part Model for Face ...   \n",
              "\n",
              "                                              abstract  \\\n",
              "0    This paper has been withdrawn by the author al...   \n",
              "1    Matching pursuit and K-SVD is demonstrated in ...   \n",
              "2    This article provides a step by step developme...   \n",
              "3    We present our latest experiment results of ob...   \n",
              "4    A hallucination-free and computationally effic...   \n",
              "..                                                 ...   \n",
              "995  We consider a variational method to solve the ...   \n",
              "996  In this paper, we propose a method for user Fi...   \n",
              "997  In this paper, we present a novel low rank rep...   \n",
              "998  This is the preprint version of our paper on I...   \n",
              "999  We present a face detection algorithm based on...   \n",
              "\n",
              "                                               authors   tags  \\\n",
              "0               Ahmad Reza Eskandari, Ali Pourmohammad  cs.CV   \n",
              "1          Arthur Szlam, Koray Kavukcuoglu, Yann LeCun  cs.CV   \n",
              "2                                        Anish Acharya  cs.CV   \n",
              "3                                             Lin Duan  cs.CV   \n",
              "4                             Sifeng He, Bahram Jalali  cs.CV   \n",
              "..                                                 ...    ...   \n",
              "995            Diane Gilliocq-Hirtz, Zakaria Belhachmi  cs.CV   \n",
              "996  Mohsen Fayyaz, Masoud PourReza, Mohammad Hajiz...  cs.CV   \n",
              "997         Yifan Fu, Junbin Gao, Xia Hong, David Tien  cs.CV   \n",
              "998  Xiaolei Zhang, Yong Han, DongSheng Hao, Zhihan Lv  cs.CV   \n",
              "999     Rajeev Ranjan, Vishal M. Patel, Rama Chellappa  cs.CV   \n",
              "\n",
              "                                              keywords  \n",
              "0                              Paper, Ali Pourmohammad  \n",
              "1       Matching Pursuit, K-SVD, Translation invariant  \n",
              "2    ObjectDetection, HOG, Feature Pyramid, Templat...  \n",
              "3       Object Recognition, 3D Point Cloud, Moving Car  \n",
              "4         Brain MRI, Algorithm, Resolution Enhancement  \n",
              "..                                                 ...  \n",
              "995  Vilational Method, Optical Flow, MUMPS, Domain...  \n",
              "996  Finger Vein Authentication, FVA, Gaussian Dist...  \n",
              "997  Low Rank Representation, LRR, Computer Vision,...  \n",
              "998  ICONIP, ARGIS, Machine Vision, Sensor, Neural ...  \n",
              "999  DP2MFD, Deformable Part Models, Pyramidal Feat...  \n",
              "\n",
              "[1000 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4800c6e3-3d0d-4cfa-95e6-9522029a2724\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>authors</th>\n",
              "      <th>tags</th>\n",
              "      <th>keywords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Recognition of Regular Shapes in Satelite Images</td>\n",
              "      <td>This paper has been withdrawn by the author al...</td>\n",
              "      <td>Ahmad Reza Eskandari, Ali Pourmohammad</td>\n",
              "      <td>cs.CV</td>\n",
              "      <td>Paper, Ali Pourmohammad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Convolutional Matching Pursuit and Dictionary ...</td>\n",
              "      <td>Matching pursuit and K-SVD is demonstrated in ...</td>\n",
              "      <td>Arthur Szlam, Koray Kavukcuoglu, Yann LeCun</td>\n",
              "      <td>cs.CV</td>\n",
              "      <td>Matching Pursuit, K-SVD, Translation invariant</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Template Matching based Object Detection Using...</td>\n",
              "      <td>This article provides a step by step developme...</td>\n",
              "      <td>Anish Acharya</td>\n",
              "      <td>cs.CV</td>\n",
              "      <td>ObjectDetection, HOG, Feature Pyramid, Templat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Exploration of object recognition from 3D poin...</td>\n",
              "      <td>We present our latest experiment results of ob...</td>\n",
              "      <td>Lin Duan</td>\n",
              "      <td>cs.CV</td>\n",
              "      <td>Object Recognition, 3D Point Cloud, Moving Car</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Brain MRI Image Super Resolution using Phase S...</td>\n",
              "      <td>A hallucination-free and computationally effic...</td>\n",
              "      <td>Sifeng He, Bahram Jalali</td>\n",
              "      <td>cs.CV</td>\n",
              "      <td>Brain MRI, Algorithm, Resolution Enhancement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>A massively parallel multi-level approach to a...</td>\n",
              "      <td>We consider a variational method to solve the ...</td>\n",
              "      <td>Diane Gilliocq-Hirtz, Zakaria Belhachmi</td>\n",
              "      <td>cs.CV</td>\n",
              "      <td>Vilational Method, Optical Flow, MUMPS, Domain...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>A Novel Approach For Finger Vein Verification ...</td>\n",
              "      <td>In this paper, we propose a method for user Fi...</td>\n",
              "      <td>Mohsen Fayyaz, Masoud PourReza, Mohammad Hajiz...</td>\n",
              "      <td>cs.CV</td>\n",
              "      <td>Finger Vein Authentication, FVA, Gaussian Dist...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>Low Rank Representation on Riemannian Manifold...</td>\n",
              "      <td>In this paper, we present a novel low rank rep...</td>\n",
              "      <td>Yifan Fu, Junbin Gao, Xia Hong, David Tien</td>\n",
              "      <td>cs.CV</td>\n",
              "      <td>Low Rank Representation, LRR, Computer Vision,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>Preprint ARPPS Augmented Reality Pipeline Pros...</td>\n",
              "      <td>This is the preprint version of our paper on I...</td>\n",
              "      <td>Xiaolei Zhang, Yong Han, DongSheng Hao, Zhihan Lv</td>\n",
              "      <td>cs.CV</td>\n",
              "      <td>ICONIP, ARGIS, Machine Vision, Sensor, Neural ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>A Deep Pyramid Deformable Part Model for Face ...</td>\n",
              "      <td>We present a face detection algorithm based on...</td>\n",
              "      <td>Rajeev Ranjan, Vishal M. Patel, Rama Chellappa</td>\n",
              "      <td>cs.CV</td>\n",
              "      <td>DP2MFD, Deformable Part Models, Pyramidal Feat...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 5 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4800c6e3-3d0d-4cfa-95e6-9522029a2724')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4800c6e3-3d0d-4cfa-95e6-9522029a2724 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4800c6e3-3d0d-4cfa-95e6-9522029a2724');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-75fa0c1b-9aa4-4c5a-91ba-809d9d77e152\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-75fa0c1b-9aa4-4c5a-91ba-809d9d77e152')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-75fa0c1b-9aa4-4c5a-91ba-809d9d77e152 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_32aa37ba-8204-4db2-8f7e-2d18b1d1525e\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_32aa37ba-8204-4db2-8f7e-2d18b1d1525e button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"An Iterative Fingerprint Enhancement Algorithm Based on Accurate\\n  Determination of Orientation Flow\",\n          \"Reduced egomotion estimation drift using omnidirectional views\",\n          \"Who and Where: People and Location Co-Clustering\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 998,\n        \"samples\": [\n          \"A brief description of tree structured sparse coding on the binary cube.\",\n          \"We first establish a law of large numbers and a convergence theorem in\\ndistribution to show the rate of convergence of the non-local means filter for\\nremoving Gaussian noise. We then introduce the notion of degree of similarity\\nto measure the role of similarity for the non-local means filter. Based on the\\nconvergence theorems, we propose a patch-based weighted means filter for\\nremoving impulse noise and its mixture with Gaussian noise by combining the\\nessential idea of the trilateral filter and that of the non-local means filter.\\nOur experiments show that our filter is competitive compared to recently\\nproposed methods.\",\n          \"This paper has been removed from arXiv as the submitter did not have\\nownership of the data presented in this work.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authors\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 922,\n        \"samples\": [\n          \"Luca Patarnello, Marco Celin, Loris Nanni\",\n          \"Maciej Wielgosz, Antonio M. L\\u00f3pez, Muhammad Naveed Riaz\",\n          \"N. S. Nikolaidis, I. N. Nikolaidis, C. C. Tsouros\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tags\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 180,\n        \"samples\": [\n          \"cs.CV, I.2.10, I.4.7, I.4.8\",\n          \"cs.CV, cs.IR, I.4.7; I.5.4; I.4.8; I.4.9; I.5.2\",\n          \"cs.CV, cs.CE\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"keywords\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 997,\n        \"samples\": [\n          \"Tree Structured Scaling, Sparse Coding, Binary Cube\",\n          \"Orthonormal Polynomials, Image Reconstruction, Simulation\",\n          \"ArXiv, Data Science\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Кол-во публикаций:\", len(df))\n",
        "print(\"Средняя длина аннотации:\", df[\"abstract\"].apply(lambda x: len(x.split())).mean())\n",
        "all_keywords = [kw for kws in df[\"keywords\"].str.split(\",\") for kw in kws]\n",
        "print(\"Уникальных ключевых слов:\", len(set(all_keywords)))\n",
        "pd.Series(all_keywords).value_counts().head(20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "id": "m4Kl-yZsTnvr",
        "outputId": "d8a2c651-52ee-4782-cf83-b76b81acbd1f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Кол-во публикаций: 1000\n",
            "Средняя длина аннотации: 69.327\n",
            "Уникальных ключевых слов: 3209\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " Computer Vision                  52\n",
              " Algorithms                       33\n",
              " Image Processing                 21\n",
              " CNN                              19\n",
              " Algorithm                        15\n",
              " Deep Learning                    14\n",
              " Pattern Recognition              14\n",
              " Image Segmentation               13\n",
              "Deep Learning                     13\n",
              " Machine Learning                 13\n",
              " Convolutional Neural Networks    12\n",
              " Neural Networks                  11\n",
              " SVM                              11\n",
              " GPU                              11\n",
              " Object Detection                 11\n",
              " Object Recognition               10\n",
              " Image Classification             10\n",
              " MNIST                            10\n",
              " Face Recognition                 10\n",
              " Neural Network                   10\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Computer Vision</th>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Algorithms</th>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Image Processing</th>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CNN</th>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Algorithm</th>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Deep Learning</th>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Pattern Recognition</th>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Image Segmentation</th>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Deep Learning</th>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Machine Learning</th>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Convolutional Neural Networks</th>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Neural Networks</th>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SVM</th>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GPU</th>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Object Detection</th>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Object Recognition</th>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Image Classification</th>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MNIST</th>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Face Recognition</th>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Neural Network</th>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Сохранение результата"
      ],
      "metadata": {
        "id": "PUyD3L04qXrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"multimedia_250.csv\", index=False)"
      ],
      "metadata": {
        "id": "2_hGURplH7dK"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Построение графа ключевых слов\n",
        "\n",
        "Граф: узлы = ключевые слова, ребро между двумя словами если они встречаются в одной статье.\n",
        "\n",
        "Вес ребра = количество совместных появлений.\n",
        "\n",
        "Далее строим взвешенный граф и выполняем кластеризацию.\n"
      ],
      "metadata": {
        "id": "j_NTlLkHJ7uo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from itertools import combinations, chain\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "from operator import itemgetter\n",
        "import plotly.graph_objects as go"
      ],
      "metadata": {
        "id": "JJRByA7KH7al"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"multimedia_250.csv\")\n",
        "df.sample(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "01WONuBDH7YC",
        "outputId": "17fe2d06-c13b-4287-846b-d5515cd76e0c"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 title  \\\n",
              "132  Head Gesture Recognition using Optical Flow ba...   \n",
              "437  A Short Note on Evaluating RepNet for Temporal...   \n",
              "591      Edge detection based on morphological amoebas   \n",
              "55   A Modified Convolutional Network for Auto-enco...   \n",
              "554  Cost-Effective Implementation of Order-Statist...   \n",
              "\n",
              "                                              abstract  \\\n",
              "132  This paper describes a technique of real time ...   \n",
              "437  We discuss some consistent issues on how RepNe...   \n",
              "591  Detecting the edges of objects within images i...   \n",
              "55   This brief paper reports the shortcoming of a ...   \n",
              "554  Vector operators based on robust order statist...   \n",
              "\n",
              "                                               authors                   tags  \\\n",
              "132                         Parimita Saikia, Karen Das                  cs.CV   \n",
              "437  Debidatta Dwibedi, Yusuf Aytar, Jonathan Tomps...    cs.CV, cs.AI, cs.LG   \n",
              "591  Won Yeol Lee, Young Woo Kim, Se Yun Kim, Jae Y...                  cs.CV   \n",
              "55                                          Erico Tjoa  cs.CV, cs.LG, eess.IV   \n",
              "554  M. Emre Celebi, Hassan A. Kingravi, Rastislav ...           cs.CV, I.4.3   \n",
              "\n",
              "                                              keywords  \n",
              "132  Real Time Head Gesture Recognition, Gaussian M...  \n",
              "437  RepNet, Performance Evaluation, Google, Google...  \n",
              "591  Neoeba, Image Processing, Image Detection, Man...  \n",
              "55         Convolutional Neuralnetwork, Pattern Theory  \n",
              "554  Vector Filters, Robust Order Statistics, Multi...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c0abeb1b-d8ba-4cca-9400-b431249b3a1f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>authors</th>\n",
              "      <th>tags</th>\n",
              "      <th>keywords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>Head Gesture Recognition using Optical Flow ba...</td>\n",
              "      <td>This paper describes a technique of real time ...</td>\n",
              "      <td>Parimita Saikia, Karen Das</td>\n",
              "      <td>cs.CV</td>\n",
              "      <td>Real Time Head Gesture Recognition, Gaussian M...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>437</th>\n",
              "      <td>A Short Note on Evaluating RepNet for Temporal...</td>\n",
              "      <td>We discuss some consistent issues on how RepNe...</td>\n",
              "      <td>Debidatta Dwibedi, Yusuf Aytar, Jonathan Tomps...</td>\n",
              "      <td>cs.CV, cs.AI, cs.LG</td>\n",
              "      <td>RepNet, Performance Evaluation, Google, Google...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591</th>\n",
              "      <td>Edge detection based on morphological amoebas</td>\n",
              "      <td>Detecting the edges of objects within images i...</td>\n",
              "      <td>Won Yeol Lee, Young Woo Kim, Se Yun Kim, Jae Y...</td>\n",
              "      <td>cs.CV</td>\n",
              "      <td>Neoeba, Image Processing, Image Detection, Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>A Modified Convolutional Network for Auto-enco...</td>\n",
              "      <td>This brief paper reports the shortcoming of a ...</td>\n",
              "      <td>Erico Tjoa</td>\n",
              "      <td>cs.CV, cs.LG, eess.IV</td>\n",
              "      <td>Convolutional Neuralnetwork, Pattern Theory</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>554</th>\n",
              "      <td>Cost-Effective Implementation of Order-Statist...</td>\n",
              "      <td>Vector operators based on robust order statist...</td>\n",
              "      <td>M. Emre Celebi, Hassan A. Kingravi, Rastislav ...</td>\n",
              "      <td>cs.CV, I.4.3</td>\n",
              "      <td>Vector Filters, Robust Order Statistics, Multi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c0abeb1b-d8ba-4cca-9400-b431249b3a1f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c0abeb1b-d8ba-4cca-9400-b431249b3a1f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c0abeb1b-d8ba-4cca-9400-b431249b3a1f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-c9f7fff6-d545-44de-82c0-ed29f99df23b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c9f7fff6-d545-44de-82c0-ed29f99df23b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-c9f7fff6-d545-44de-82c0-ed29f99df23b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"A Short Note on Evaluating RepNet for Temporal Repetition Counting in\\n  Videos\",\n          \"Cost-Effective Implementation of Order-Statistics Based Vector Filters\\n  Using Minimax Approximations\",\n          \"Edge detection based on morphological amoebas\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"We discuss some consistent issues on how RepNet has been evaluated in various\\npapers. As a way to mitigate these issues, we report RepNet performance results\\non different datasets, and release evaluation code and the RepNet checkpoint to\\nobtain these results. Code URL:\\nhttps://github.com/google-research/google-research/blob/master/repnet/\",\n          \"Vector operators based on robust order statistics have proved successful in\\ndigital multichannel imaging applications, particularly color image filtering\\nand enhancement, in dealing with impulsive noise while preserving edges and\\nfine image details. These operators often have very high computational\\nrequirements which limits their use in time-critical applications. This paper\\nintroduces techniques to speed up vector filters using the minimax\\napproximation theory. Extensive experiments on a large and diverse set of color\\nimages show that proposed approximations achieve an excellent balance among\\nease of implementation, accuracy, and computational speed.\",\n          \"Detecting the edges of objects within images is critical for quality image\\nprocessing. We present an edge-detecting technique that uses morphological\\namoebas that adjust their shape based on variation in image contours. We\\nevaluate the method both quantitatively and qualitatively for edge detection of\\nimages, and compare it to classic morphological methods. Our amoeba-based\\nedge-detection system performed better than the classic edge detectors.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authors\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, Andrew Zisserman\",\n          \"M. Emre Celebi, Hassan A. Kingravi, Rastislav Lukac, Fatih Celiker\",\n          \"Won Yeol Lee, Young Woo Kim, Se Yun Kim, Jae Young Lim, Dong Hoon Lim\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tags\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"cs.CV, cs.AI, cs.LG\",\n          \"cs.CV, I.4.3\",\n          \"cs.CV\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"keywords\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"RepNet, Performance Evaluation, Google, Google Research\",\n          \"Vector Filters, Robust Order Statistics, Multichannel Imaging\",\n          \"Neoeba, Image Processing, Image Detection, Manipulation\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "klist = [[word.strip() for word in keywords.split(\",\") if word]\n",
        "         for keywords in df[\"keywords\"].tolist()]\n",
        "klist[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KA9LRyJEH7Vh",
        "outputId": "92e7e724-a23d-4c23-e780-01843745b83e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Paper', 'Ali Pourmohammad'],\n",
              " ['Matching Pursuit', 'K-SVD', 'Translation invariant'],\n",
              " ['ObjectDetection', 'HOG', 'Feature Pyramid', 'Template Matching']]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "edges = list(chain(*[list(combinations(words, 2)) for words in klist]))\n",
        "edges = [tuple(sorted(edge)) for edge in edges]\n",
        "weighted_edges = [(edge[0], edge[1], {\"weight\": edges.count(edge)}) for edge in set(edges)]\n",
        "weighted_edges[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxRMXoT0H7S-",
        "outputId": "c75618bc-a552-4471-b1b2-8a9b8d4215e3"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('DCT', 'Mixture of Gaussians', {'weight': 1}),\n",
              " ('Clarkson University', 'YFA', {'weight': 1}),\n",
              " ('Accuracy', 'Stereo Matching Algorithms', {'weight': 1}),\n",
              " ('EAR', 'MAR', {'weight': 1}),\n",
              " ('Feature Extraction', 'Skin Filtering', {'weight': 1}),\n",
              " ('FuzzyCMeans', 'Text Extraction', {'weight': 1}),\n",
              " ('Humeprimal-dual algorithm', 'Optimal-transport distances', {'weight': 1}),\n",
              " ('Bayes Ying Yang', 'SIGNATURE Verification', {'weight': 1}),\n",
              " ('Aesthetic Gradients', 'Quantitative Experiments', {'weight': 1}),\n",
              " ('Algorithms', 'organization', {'weight': 1})]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(set(edges)), len(weighted_edges)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQKsTSxcH7Qa",
        "outputId": "93d89363-dfc9-4135-931f-5a28d2e67f60"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6543, 6543)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "G = nx.Graph()\n",
        "G.add_edges_from(weighted_edges)\n",
        "nx.draw(G, with_labels=False, font_weight='bold', node_size=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "-2LDZLkTH7N8",
        "outputId": "a8969b93-09c5-4209-975f-3367c37767b0"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Кластеризация (Louvain) и визуализация\n",
        "\n",
        "- Находим сообщества Louvain.\n",
        "- Выводим число кластеров и модульность.\n",
        "- Визуализируем топ-3 сообщества статично (matplotlib) и интерактивно (Plotly).\n"
      ],
      "metadata": {
        "id": "0Gnq08q3qyjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "communities = nx.community.louvain_communities(G, resolution=0.9)\n",
        "print(f\"# of clusters: {len(communities)}, Modularity: {nx.community.modularity(G, communities)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5wxtVlQH7La",
        "outputId": "5e7143de-a0b2-4cf7-c05e-72cf6c1b8590"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of clusters: 341, Modularity: 0.8885584595562667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "comm_data = [{\"n_of_nodes\": len(comm), \"nodes\": comm} for comm in communities]\n",
        "cdf = pd.DataFrame(comm_data)\n",
        "cdf.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "Hcs7vVwQH7I6",
        "outputId": "c50ba903-9685-4422-a71d-143b1172e4b1"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       n_of_nodes\n",
              "count  341.000000\n",
              "mean     9.046921\n",
              "std     20.434922\n",
              "min      2.000000\n",
              "25%      3.000000\n",
              "50%      4.000000\n",
              "75%      5.000000\n",
              "max    178.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ccea73f4-9192-45e2-befc-d600cb4c2216\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>n_of_nodes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>341.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>9.046921</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>20.434922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>5.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>178.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ccea73f4-9192-45e2-befc-d600cb4c2216')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ccea73f4-9192-45e2-befc-d600cb4c2216 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ccea73f4-9192-45e2-befc-d600cb4c2216');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-578da625-ecf6-48eb-b229-aa4fcdcd8c02\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-578da625-ecf6-48eb-b229-aa4fcdcd8c02')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-578da625-ecf6-48eb-b229-aa4fcdcd8c02 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"cdf\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"n_of_nodes\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 124.76847409713945,\n        \"min\": 2.0,\n        \"max\": 341.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          9.04692082111437,\n          4.0,\n          341.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top3_comm = cdf.nlargest(3, \"n_of_nodes\")\n",
        "top3_comm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "XxD8qwrwH7Gj",
        "outputId": "91deb443-1a28-49ac-924d-248240c2c2a8"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     n_of_nodes                                              nodes\n",
              "11          178  {Object Identification, Makkah, ISIC Challenge...\n",
              "12          126  {SVM, CLIP, Patterned Textures, Histogram of O...\n",
              "125         126  {Laplacian Eigenbases, Low Rank Representation..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3abf32dd-a94e-4123-8fc0-154eeec9dcd3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>n_of_nodes</th>\n",
              "      <th>nodes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>178</td>\n",
              "      <td>{Object Identification, Makkah, ISIC Challenge...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>126</td>\n",
              "      <td>{SVM, CLIP, Patterned Textures, Histogram of O...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>126</td>\n",
              "      <td>{Laplacian Eigenbases, Low Rank Representation...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3abf32dd-a94e-4123-8fc0-154eeec9dcd3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3abf32dd-a94e-4123-8fc0-154eeec9dcd3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3abf32dd-a94e-4123-8fc0-154eeec9dcd3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-9bb92df5-35c2-4f24-b0a6-9948e0473d9c\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9bb92df5-35c2-4f24-b0a6-9948e0473d9c')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-9bb92df5-35c2-4f24-b0a6-9948e0473d9c button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_bd728e43-3c74-447a-9f87-3676df23f6b2\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('top3_comm')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_bd728e43-3c74-447a-9f87-3676df23f6b2 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('top3_comm');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "top3_comm",
              "summary": "{\n  \"name\": \"top3_comm\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"n_of_nodes\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 30,\n        \"min\": 126,\n        \"max\": 178,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          126,\n          178\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"nodes\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nodes = list(chain(*top3_comm[\"nodes\"].tolist()))\n",
        "S = G.subgraph(nodes)"
      ],
      "metadata": {
        "id": "NUDV0c76H7ER"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, nodes in enumerate(top3_comm[\"nodes\"], start=1):\n",
        "    sub_keywords = [kw for kw in nodes]\n",
        "    print(f\"\\nКластер {i}, размер: {len(nodes)}\")\n",
        "    print(pd.Series(sub_keywords).value_counts().head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKRzDEMsT5Uq",
        "outputId": "f1f84603-f088-4a43-e3ab-168a0003dad5"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Кластер 1, размер: 178\n",
            "Object Identification             1\n",
            "Makkah                            1\n",
            "ISIC Challenge                    1\n",
            "Video Data                        1\n",
            "Yale B                            1\n",
            "Analysis of Space-Time Objects    1\n",
            "Cognitive Science                 1\n",
            "Action Recognition                1\n",
            "Bag of Words                      1\n",
            "Pytorch                           1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Кластер 2, размер: 126\n",
            "SVM                                      1\n",
            "CLIP                                     1\n",
            "Patterned Textures                       1\n",
            "Histogram of Oriented Gradients (HOG)    1\n",
            "Ladars                                   1\n",
            "Convolutional Neural Networks            1\n",
            "HR Grid                                  1\n",
            "Species Analysis                         1\n",
            "Storage Devices                          1\n",
            "CNNs                                     1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Кластер 3, размер: 126\n",
            "Laplacian Eigenbases                1\n",
            "Low Rank Representation             1\n",
            "Cheirality Constraint               1\n",
            "Mobile Object Tracking              1\n",
            "Human Computer Interaction          1\n",
            "M multimodal spectral geometry      1\n",
            "Motion Capture                      1\n",
            "DTW                                 1\n",
            "Multimedia Information Retrieval    1\n",
            "Aerial View                         1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_community_node_colors(graph, communities):\n",
        "    colors = list(set(mcolors.TABLEAU_COLORS.values()))\n",
        "    node_colors = []\n",
        "    for node in graph:\n",
        "        current_community_index = 0\n",
        "        for community in communities:\n",
        "            if node in community:\n",
        "                node_colors.append(colors[current_community_index])\n",
        "                break\n",
        "            current_community_index += 1\n",
        "    return node_colors"
      ],
      "metadata": {
        "id": "jWe1L1D7H7CA"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_communities(graph, communities, i):\n",
        "    node_colors = create_community_node_colors(graph, communities)\n",
        "    title = f\"Visualization of Top-3 Communities\"\n",
        "    pos = nx.spring_layout(graph, iterations=100, seed=23)\n",
        "    plt.title(title)\n",
        "    nx.draw(\n",
        "        graph,\n",
        "        pos=pos,\n",
        "        node_size=50,\n",
        "        node_color=node_colors,\n",
        "        edge_color=\"gray\",\n",
        "        with_labels=False\n",
        "    )\n",
        "    plt.text(0.05, 0.95, \"Цвет = кластер\", transform=plt.gca().transAxes)\n",
        "\n",
        "\n",
        "visualize_communities(S, top3_comm[\"nodes\"].tolist(), 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "4ZlSgMrFKM7l",
        "outputId": "b9ef8e56-6b4e-498d-8fc2-f72bd3d4acae"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "communities = top3_comm[\"nodes\"].tolist()\n",
        "pos = nx.spring_layout(S, iterations=100, seed=23)\n",
        "\n",
        "# edges coordinates\n",
        "x_nodes, y_nodes = zip(*pos.values())\n",
        "edge_x, edge_y = [], []\n",
        "for edge in S.edges():\n",
        "    x0, y0 = pos[edge[0]]\n",
        "    x1, y1 = pos[edge[1]]\n",
        "    edge_x.extend([x0, x1, None])\n",
        "    edge_y.extend([y0, y1, None])\n",
        "\n",
        "# degree labels\n",
        "node_labels = [f\"Node {n}<br>Degree: {S.degree(n)}\" for n in S.nodes()]\n",
        "node_degrees = [S.degree(n) for n in S.nodes()]\n",
        "node_colors_list = create_community_node_colors(S, communities)\n",
        "\n",
        "fig = go.Figure()\n",
        "# add edges\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=edge_x, y=edge_y,\n",
        "    line=dict(width=1, color=\"gray\"),\n",
        "))\n",
        "\n",
        "# add nodes\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=x_nodes, y=y_nodes, mode=\"markers\",\n",
        "    marker=dict(size=[deg*1.1+5 for deg in node_degrees], color=node_colors_list),\n",
        "    hoverinfo=\"text\",\n",
        "    text=node_labels\n",
        "))\n",
        "\n",
        "# background settings\n",
        "fig.update_layout(\n",
        "    title=f\"Interactive Visualization of Top-3 Communities\",\n",
        "    showlegend=False,\n",
        "    plot_bgcolor=\"white\",\n",
        "    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "oVFPoMHAKM4n",
        "outputId": "d3ca3cff-f22a-4231-b00a-a020efd422f1"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"8252a310-19db-4718-ba40-54383b3f6899\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"8252a310-19db-4718-ba40-54383b3f6899\")) {                    Plotly.newPlot(                        \"8252a310-19db-4718-ba40-54383b3f6899\",                        [{\"line\":{\"color\":\"gray\",\"width\":1},\"x\":[0.35802146325715956,0.35481653666982,null,0.35802146325715956,0.24793127610873328,null,-0.12728901874657475,-0.12876266091326669,null,-0.12728901874657475,-0.1495739063216861,null,-0.2313883184489302,-0.2469484386894806,null,-0.2313883184489302,-0.22670041266116028,null,-0.2313883184489302,-0.09402260018557479,null,-0.2313883184489302,-0.3189380237812923,null,0.37932148692863255,0.36690844037313647,null,0.37932148692863255,0.3901658860838829,null,0.37932148692863255,0.24793127610873328,null,0.37932148692863255,0.35973951436128465,null,-0.336627990610311,-0.1495739063216861,null,-0.336627990610311,-0.5054384017048197,null,-0.336627990610311,-0.30278858348574605,null,-0.336627990610311,-0.2723753581795727,null,0.06937191545962786,0.057334084694421625,null,0.06937191545962786,0.05344183832425574,null,0.06937191545962786,0.05238279382659552,null,0.1795869014914811,0.1842235820026506,null,0.1795869014914811,0.16251134383437724,null,0.1795869014914811,0.057334084694421625,null,-0.07859666409593062,-0.07829613842283081,null,-0.07859666409593062,-0.1990101795123446,null,-0.07859666409593062,0.030175564405072793,null,0.3097766873483381,0.32678542837044966,null,0.3097766873483381,0.30607930557133256,null,0.3097766873483381,0.24793127610873328,null,0.3097766873483381,0.3294125077776689,null,0.06352176126627909,0.057334084694421625,null,0.06352176126627909,0.08005275191518312,null,0.06352176126627909,0.04681872113616281,null,0.35467737095478785,0.359962953341593,null,0.35467737095478785,0.24793127610873328,null,0.35467737095478785,0.3752376086201889,null,0.35467737095478785,0.3814752508259371,null,-0.2508561289227041,-0.18657760563145134,null,-0.2508561289227041,-0.38575174347229135,null,-0.2508561289227041,-0.1792167006492603,null,-0.2508561289227041,-0.16665091533742601,null,-0.2508561289227041,-0.17152550671702288,null,-0.2508561289227041,-0.144686749814993,null,-0.2508561289227041,-0.19523393094487043,null,-0.2508561289227041,-0.3755624460982769,null,-0.2508561289227041,-0.10652205133493349,null,-0.2508561289227041,-0.33180714124855976,null,0.1470565039835934,0.1523153183089973,null,0.1470565039835934,0.057334084694421625,null,0.1470565039835934,0.1706630634898999,null,-0.16665091533742601,-0.144686749814993,null,-0.16665091533742601,-0.19523393094487043,null,-0.16665091533742601,-0.10652205133493349,null,-0.16665091533742601,-0.17152550671702288,null,-0.16665091533742601,-0.18657760563145134,null,-0.16665091533742601,-0.1792167006492603,null,-0.3420990961487803,-0.3184832378126542,null,-0.3420990961487803,-0.30893512627664627,null,-0.3420990961487803,-0.33325379630751584,null,-0.3420990961487803,-0.1836505333207323,null,0.20633381098486714,0.1487834908852371,null,0.20633381098486714,0.2150016907648118,null,-0.3443249446410901,-0.45057978119332287,null,-0.3443249446410901,-0.40352085698302403,null,-0.3443249446410901,-0.1495739063216861,null,-0.3443249446410901,-0.2896957306775164,null,-0.3443249446410901,-0.3189380237812923,null,-0.3443249446410901,-0.4095024600014184,null,-0.3443249446410901,-0.39695777512934566,null,-0.3443249446410901,-0.29965137180678686,null,-0.3443249446410901,-0.3685271366320466,null,-0.3443249446410901,-0.1990101795123446,null,-0.3443249446410901,-0.41052348335075456,null,-0.3443249446410901,-0.29804063625835997,null,-0.3443249446410901,-0.28789257980080607,null,-0.3443249446410901,-0.4434411916247953,null,0.29481836665416755,0.24793127610873328,null,0.29481836665416755,0.31758394433330916,null,0.5136799542604107,0.644682038075124,null,0.5136799542604107,0.6168625462231521,null,0.5136799542604107,0.6554322070713565,null,0.5136799542604107,0.6019867637806753,null,0.5136799542604107,0.5572920587787685,null,0.5136799542604107,0.24793127610873328,null,0.5136799542604107,0.5990671011450727,null,0.5136799542604107,0.6265708269309338,null,0.5136799542604107,0.41053910172059754,null,0.5136799542604107,0.42210454820204046,null,0.5136799542604107,0.5846603069932143,null,0.5136799542604107,0.6292907642573967,null,0.5136799542604107,0.5412752971026304,null,0.5136799542604107,0.6447492779973604,null,0.5136799542604107,0.652699643779598,null,0.5136799542604107,0.6653851540788173,null,0.5136799542604107,0.4422536566566158,null,-0.30601511342197146,-0.144686749814993,null,-0.30601511342197146,-0.2929541265930888,null,-0.30601511342197146,-0.3189380237812923,null,-0.30601511342197146,-0.33180714124855976,null,-0.37752878069259493,-0.2615337288865682,null,0.14291440227544103,0.04089838824983995,null,0.14291440227544103,0.16482404617764268,null,0.14291440227544103,0.14558640763898786,null,0.4444640942907851,0.46387374622357397,null,0.4444640942907851,0.37887442378253694,null,0.1608965907188315,0.1487834908852371,null,0.1608965907188315,0.17448262792336158,null,0.1608965907188315,0.18321757063648267,null,-0.27522077501197767,-0.2849325763170544,null,-0.27522077501197767,-0.26074112055363974,null,-0.27522077501197767,-0.2923860576960361,null,-0.27522077501197767,-0.1836505333207323,null,-0.6452977219649615,-0.6572413154250697,null,-0.6452977219649615,-0.6642943383952763,null,-0.6452977219649615,-0.5054384017048197,null,-0.6452977219649615,-0.6436347608996995,null,0.0055930358038620825,-0.006885414194733128,null,0.0055930358038620825,-0.00493005919142411,null,0.0055930358038620825,0.16958236960938491,null,0.0055930358038620825,0.057334084694421625,null,0.0055930358038620825,-0.1836505333207323,null,-0.5324586439154924,-0.38389722149376426,null,-0.5324586439154924,-0.5347475809279216,null,-0.5324586439154924,-0.5167114869660626,null,0.3008929768895503,0.3790387836282906,null,0.3008929768895503,0.29640515821314634,null,0.3008929768895503,0.3178707894002735,null,0.3008929768895503,0.31382444575974944,null,0.3008929768895503,0.16958236960938491,null,-0.5330680644759629,-0.5299450733889067,null,-0.5330680644759629,-0.3685271366320466,null,-0.5330680644759629,-0.5415048957640967,null,-0.17798893261201718,-0.144686749814993,null,-0.17798893261201718,-0.19507660191462672,null,-0.17798893261201718,-0.17559766123972692,null,0.3440784506403394,0.3383665933113078,null,0.3440784506403394,0.24793127610873328,null,-0.003096762211667722,-0.2723753581795727,null,-0.003096762211667722,-0.09402260018557479,null,-0.003096762211667722,0.09087402068867718,null,-0.003096762211667722,0.1395106533602813,null,-0.003096762211667722,-0.018290606821115094,null,-0.003096762211667722,0.24793127610873328,null,-0.003096762211667722,0.07854287120897296,null,-0.003096762211667722,-0.09110604816279191,null,-0.43326286272812614,-0.42081542953672807,null,-0.43326286272812614,-0.29629988432854404,null,-0.43326286272812614,-0.4428693315091862,null,0.2995847258538968,0.2731667728753765,null,0.2995847258538968,0.30901432651209554,null,0.2995847258538968,0.3334973209636406,null,0.2995847258538968,0.16958236960938491,null,0.2995847258538968,0.2919034835040413,null,0.2995847258538968,0.34782804731496286,null,0.2995847258538968,0.3630233891422448,null,-0.26522542619625966,-0.2853489430024099,null,-0.26522542619625966,-0.2583431024501166,null,-0.26522542619625966,-0.23687880135753614,null,-0.26522542619625966,-0.2615337288865682,null,-0.26522542619625966,-0.2718184307114589,null,0.34782804731496286,0.30901432651209554,null,0.34782804731496286,0.2919034835040413,null,0.34782804731496286,0.3334973209636406,null,0.34782804731496286,0.3630233891422448,null,0.049328522508159806,0.057334084694421625,null,0.049328522508159806,0.02610275324395288,null,0.04681872113616281,0.08005275191518312,null,0.04681872113616281,0.057334084694421625,null,0.39911558766455274,0.40421041765369375,null,0.39911558766455274,0.38312191126574846,null,0.39911558766455274,0.24793127610873328,null,0.13123306239294708,0.1487834908852371,null,0.13123306239294708,0.11384096381898817,null,0.13123306239294708,0.057334084694421625,null,0.16281018653965207,0.16369988644385897,null,0.16281018653965207,0.17940852423182896,null,0.16281018653965207,0.14194894105793918,null,-0.4529557510135941,-0.4334716660933204,null,-0.4529557510135941,-0.4567480324717053,null,-0.4529557510135941,-0.3374232405718896,null,0.05238279382659552,0.057334084694421625,null,0.05238279382659552,0.05344183832425574,null,0.7578317135354431,0.8396411855098973,null,0.7578317135354431,0.7560242176572916,null,0.7578317135354431,0.5859485874111335,null,-0.12697604169589957,-0.17644192235806314,null,-0.12697604169589957,-0.11952935068819252,null,-0.12697604169589957,-0.14890401981238952,null,-0.8975157052560349,-0.8948474605608381,null,-0.8975157052560349,-0.8853133311253923,null,-0.8975157052560349,-0.7537293611153583,null,-0.31041990494555693,-0.3332341397234369,null,-0.31041990494555693,-0.24141094975240646,null,-0.21050276567264867,-0.24141094975240646,null,-0.21050276567264867,-0.1836505333207323,null,0.20717024273643025,0.20817803395698897,null,0.20717024273643025,0.057334084694421625,null,0.20717024273643025,0.1883944039782016,null,0.13613075671146055,0.13785619470043894,null,0.13613075671146055,0.057334084694421625,null,0.13613075671146055,0.11205902299391327,null,0.13613075671146055,0.11399466396032104,null,-0.2853489430024099,-0.2583431024501166,null,-0.2853489430024099,-0.2615337288865682,null,-0.2853489430024099,-0.2718184307114589,null,-0.2853489430024099,-0.23687880135753614,null,-0.46397922854759394,-0.48846553395784154,null,-0.46397922854759394,-0.33180714124855976,null,-0.46397922854759394,-0.48062921463594915,null,0.20356106326693688,0.21185040164655008,null,0.20356106326693688,0.1890902961594707,null,0.20356106326693688,0.1853649318823246,null,0.20356106326693688,0.14194894105793918,null,0.20356106326693688,0.24793127610873328,null,0.16054627857154377,0.08769698842219174,null,0.16054627857154377,0.08940521575722862,null,0.16054627857154377,0.24793127610873328,null,0.030175564405072793,0.15067194039703355,null,0.030175564405072793,-0.07829613842283081,null,0.030175564405072793,-0.1990101795123446,null,0.030175564405072793,0.24793127610873328,null,0.030175564405072793,0.13992448703460458,null,-0.17288422515389917,-0.18671272321451288,null,-0.17288422515389917,-0.24141094975240646,null,-0.17288422515389917,-0.09442219372406183,null,-0.4904701255608735,-0.3189380237812923,null,-0.4904701255608735,-0.4939680017345201,null,-0.4904701255608735,-0.4756606814966038,null,0.2066875189072768,0.24793127610873328,null,0.2066875189072768,0.14194894105793918,null,0.2066875189072768,0.23463510980425722,null,0.2066875189072768,0.21817281240266825,null,0.2066875189072768,0.22944271326723042,null,0.3961876717337006,0.41226186919680397,null,0.3961876717337006,0.24793127610873328,null,0.3961876717337006,0.4040553680416552,null,0.3961876717337006,0.38349562863158315,null,0.35481653666982,0.24793127610873328,null,0.41053910172059754,0.24793127610873328,null,-0.3371387795492324,-0.3215109553880422,null,-0.3371387795492324,-0.3792302693245371,null,-0.3371387795492324,-0.24141094975240646,null,-0.15114890331709083,-0.1507048737986401,null,-0.15114890331709083,-0.13243327461895996,null,-0.15114890331709083,-0.144686749814993,null,-0.18671272321451288,-0.09442219372406183,null,-0.18671272321451288,-0.24141094975240646,null,-0.44206492012847737,-0.3792302693245371,null,-0.44206492012847737,-0.46953462631619036,null,-0.44206492012847737,-0.4620598855371922,null,-0.44206492012847737,-0.4469873318740338,null,-0.05663653832461424,-0.09110604816279191,null,-0.05663653832461424,-0.03542711989110437,null,0.5990671011450727,0.6168625462231521,null,0.5990671011450727,0.6019867637806753,null,0.5990671011450727,0.5846603069932143,null,-0.29965137180678686,-0.3189380237812923,null,-0.29965137180678686,-0.3685271366320466,null,-0.29965137180678686,-0.29804063625835997,null,-0.29965137180678686,-0.28789257980080607,null,-0.29965137180678686,-0.1495739063216861,null,-0.29965137180678686,-0.1990101795123446,null,-0.29965137180678686,-0.2896957306775164,null,0.16369988644385897,0.14194894105793918,null,0.16369988644385897,0.17940852423182896,null,0.5846603069932143,0.6168625462231521,null,0.5846603069932143,0.6019867637806753,null,0.3294125077776689,0.32678542837044966,null,0.3294125077776689,0.30607930557133256,null,0.3294125077776689,0.24793127610873328,null,0.3902574969423922,0.24793127610873328,null,0.3902574969423922,0.3753392139893115,null,0.3902574969423922,0.4422536566566158,null,-0.3189380237812923,-0.4756606814966038,null,-0.3189380237812923,-0.3685271366320466,null,-0.3189380237812923,-0.4728855558167171,null,-0.3189380237812923,-0.33180714124855976,null,-0.3189380237812923,-0.27603064263977073,null,-0.3189380237812923,-0.41052348335075456,null,-0.3189380237812923,-0.29804063625835997,null,-0.3189380237812923,-0.28789257980080607,null,-0.3189380237812923,-0.456108545418915,null,-0.3189380237812923,-0.1836505333207323,null,-0.3189380237812923,-0.4095024600014184,null,-0.3189380237812923,-0.39695777512934566,null,-0.3189380237812923,-0.45057978119332287,null,-0.3189380237812923,-0.40352085698302403,null,-0.3189380237812923,-0.1495739063216861,null,-0.3189380237812923,-0.2929541265930888,null,-0.3189380237812923,-0.09402260018557479,null,-0.3189380237812923,-0.1990101795123446,null,-0.3189380237812923,-0.2469484386894806,null,-0.3189380237812923,-0.144686749814993,null,-0.3189380237812923,-0.2896957306775164,null,-0.3189380237812923,-0.4434411916247953,null,-0.3189380237812923,-0.22670041266116028,null,-0.3189380237812923,-0.2615337288865682,null,-0.3189380237812923,-0.4939680017345201,null,-0.2187414062552595,-0.144686749814993,null,-0.1495739063216861,-0.21382953176850616,null,-0.1495739063216861,-0.2461872482370283,null,-0.1495739063216861,-0.30278858348574605,null,-0.1495739063216861,-0.24435265202250844,null,-0.1495739063216861,-0.28789257980080607,null,-0.1495739063216861,-0.26334457784291787,null,-0.1495739063216861,0.06642535210665855,null,-0.1495739063216861,-0.13003583831081006,null,-0.1495739063216861,-0.5054384017048197,null,-0.1495739063216861,-0.2723753581795727,null,-0.1495739063216861,-0.2615337288865682,null,-0.1495739063216861,-0.04611542283153123,null,-0.1495739063216861,0.24793127610873328,null,-0.1495739063216861,-0.25097815792824024,null,-0.1495739063216861,-0.1836505333207323,null,-0.1495739063216861,-0.11199051836344989,null,-0.1495739063216861,0.057334084694421625,null,-0.1495739063216861,-0.2089898630849497,null,-0.1495739063216861,-0.0811046258787826,null,-0.1495739063216861,-0.2356170238850429,null,-0.1495739063216861,0.04439718693558544,null,-0.1495739063216861,-0.08904274807422541,null,-0.1495739063216861,-0.1904785676111387,null,-0.1495739063216861,-0.13232578990474772,null,-0.1495739063216861,-0.3685271366320466,null,-0.1495739063216861,-0.2764873500848596,null,-0.1495739063216861,-0.1990101795123446,null,-0.1495739063216861,-0.02670321717437541,null,-0.1495739063216861,-0.29629988432854404,null,-0.1495739063216861,-0.2896957306775164,null,-0.1495739063216861,-0.04336858531685759,null,-0.1495739063216861,-0.24962714915267492,null,-0.1495739063216861,-0.24156551376006546,null,-0.1495739063216861,0.04089838824983995,null,-0.1495739063216861,0.07311632108879322,null,-0.1495739063216861,-0.19387517514263394,null,-0.1495739063216861,-0.2682955881388224,null,-0.1495739063216861,-0.1864588572644126,null,-0.1495739063216861,0.14194894105793918,null,-0.1495739063216861,-0.21245442217735583,null,-0.1495739063216861,-0.12876266091326669,null,-0.1495739063216861,-0.09322760846944347,null,-0.1495739063216861,-0.1604148598201767,null,-0.1495739063216861,-0.04055091301155076,null,-0.1495739063216861,-0.29804063625835997,null,-0.1495739063216861,-0.027688258194792974,null,-0.1495739063216861,-0.2292534555429686,null,-0.1495739063216861,-0.04698230570917894,null,-0.1495739063216861,-0.22297643194709293,null,-0.1495739063216861,-0.1757443483642554,null,0.06550835442767675,0.08241191770775588,null,0.06550835442767675,0.05914784709403482,null,0.06550835442767675,0.05344183832425574,null,0.6447492779973604,0.652699643779598,null,0.6447492779973604,0.6265708269309338,null,0.6447492779973604,0.6292907642573967,null,-0.3532035297627773,-0.1990101795123446,null,-0.3532035297627773,-0.36676844222740407,null,-0.3532035297627773,-0.35202536416758207,null,0.31382444575974944,0.29640515821314634,null,0.31382444575974944,0.3178707894002735,null,0.31382444575974944,0.3790387836282906,null,0.31382444575974944,0.16958236960938491,null,-0.04525649422358554,-0.024947729861626466,null,-0.04525649422358554,-0.0499352311458284,null,-0.04525649422358554,-0.03860274572188954,null,-0.04525649422358554,-0.05625098916420061,null,-0.04525649422358554,0.057334084694421625,null,-0.04525649422358554,-0.03131020025571709,null,-0.04525649422358554,-0.10652205133493349,null,-0.5347475809279216,-0.5167114869660626,null,-0.5347475809279216,-0.38389722149376426,null,0.20817803395698897,0.057334084694421625,null,0.20817803395698897,0.1883944039782016,null,0.9839684558484487,0.9727688532838321,null,0.9839684558484487,0.9762208561537624,null,0.9839684558484487,0.8396411855098973,null,0.09087402068867718,0.07854287120897296,null,0.11963495831973896,0.057334084694421625,null,0.11963495831973896,0.1182357569735683,null,-0.456108545418915,-0.4728855558167171,null,0.11150918344956297,0.14194894105793918,null,0.11150918344956297,0.1024211626106067,null,0.4422536566566158,0.5572920587787685,null,0.4422536566566158,0.24793127610873328,null,0.4422536566566158,0.5412752971026304,null,0.4422536566566158,0.3753392139893115,null,-0.4434411916247953,-0.45057978119332287,null,-0.28195728372459283,-0.24141094975240646,null,-0.28195728372459283,-0.2974401388998378,null,-0.28195728372459283,-0.26979617963088337,null,-0.28195728372459283,-0.2980396251674858,null,-0.1864588572644126,-0.21382953176850616,null,-0.04055091301155076,0.04439718693558544,null,-0.04055091301155076,-0.04336858531685759,null,-0.04055091301155076,-0.04611542283153123,null,-0.22297643194709293,-0.1990101795123446,null,-0.22297643194709293,-0.24435265202250844,null,0.31940429439000473,0.2940367893379515,null,0.31940429439000473,0.31740307368494936,null,0.31940429439000473,0.3285570414267141,null,0.31940429439000473,0.24793127610873328,null,0.17453265194861656,0.18667681403372977,null,0.17453265194861656,0.04089838824983995,null,-0.24962714915267492,-0.21245442217735583,null,-0.24962714915267492,-0.2292534555429686,null,-0.24962714915267492,-0.3127004645936037,null,-0.24962714915267492,-0.1836505333207323,null,-0.24962714915267492,-0.28702363465327146,null,0.3752376086201889,0.3814752508259371,null,0.3752376086201889,0.359962953341593,null,0.3752376086201889,0.24793127610873328,null,-0.32702809587560167,-0.34181627347178384,null,-0.32702809587560167,-0.24141094975240646,null,-0.32702809587560167,-0.31838535923286315,null,-0.32702809587560167,-0.29629988432854404,null,0.6019867637806753,0.6168625462231521,null,-0.41076459971282414,-0.4144616082784801,null,-0.41076459971282414,-0.391269072491725,null,-0.41076459971282414,-0.3792302693245371,null,0.07841579937842513,0.24793127610873328,null,0.07841579937842513,-0.09402260018557479,null,-0.13003583831081006,-0.13232578990474772,null,-0.13003583831081006,-0.11199051836344989,null,-0.09110604816279191,-0.08273244370801588,null,-0.09110604816279191,-0.03542711989110437,null,-0.09110604816279191,-0.2723753581795727,null,0.1890902961594707,0.1853649318823246,null,0.1890902961594707,0.14194894105793918,null,0.1890902961594707,0.24793127610873328,null,0.1890902961594707,0.21185040164655008,null,-0.19523393094487043,-0.17152550671702288,null,-0.19523393094487043,-0.144686749814993,null,-0.19523393094487043,-0.18657760563145134,null,-0.19523393094487043,-0.1792167006492603,null,-0.19523393094487043,-0.10652205133493349,null,0.37887442378253694,0.24793127610873328,null,0.37887442378253694,0.34065069034019135,null,0.37887442378253694,0.46387374622357397,null,0.9727688532838321,0.8396411855098973,null,0.9727688532838321,0.9762208561537624,null,-0.4620598855371922,-0.46953462631619036,null,-0.4620598855371922,-0.4469873318740338,null,-0.4620598855371922,-0.3792302693245371,null,0.30901432651209554,0.16958236960938491,null,0.30901432651209554,0.3630233891422448,null,0.30901432651209554,0.3334973209636406,null,0.30901432651209554,0.2919034835040413,null,0.30901432651209554,0.2731667728753765,null,-0.48846553395784154,-0.48062921463594915,null,-0.48846553395784154,-0.33180714124855976,null,0.29640515821314634,0.3178707894002735,null,0.29640515821314634,0.3790387836282906,null,0.29640515821314634,0.16958236960938491,null,0.08940521575722862,0.24793127610873328,null,0.08940521575722862,0.08769698842219174,null,0.08940521575722862,-0.144686749814993,null,0.08940521575722862,0.053137722331690666,null,0.08940521575722862,0.05361689161600848,null,0.36002385632264805,0.3833601469938797,null,0.36002385632264805,0.36252340605732686,null,0.36002385632264805,0.3780644033648565,null,0.36002385632264805,0.24793127610873328,null,-0.35895954063272634,-0.2615337288865682,null,-0.35895954063272634,-0.3406428068741444,null,0.27619869573704353,0.24793127610873328,null,0.27619869573704353,0.2950075103704614,null,0.27619869573704353,0.3099201225646745,null,0.27619869573704353,0.2916485853018193,null,-0.24156551376006546,-0.26334457784291787,null,-0.24156551376006546,-0.2682955881388224,null,-0.24156551376006546,-0.2461872482370283,null,-0.08904274807422541,0.04089838824983995,null,-0.08904274807422541,-0.1990101795123446,null,-0.08904274807422541,-0.0811046258787826,null,-0.6572413154250697,-0.6642943383952763,null,-0.6572413154250697,-0.5054384017048197,null,-0.6572413154250697,-0.6436347608996995,null,-0.38389722149376426,-0.1836505333207323,null,-0.38389722149376426,-0.26953468484437026,null,-0.38389722149376426,-0.5167114869660626,null,-0.38389722149376426,-0.1707402344599611,null,-0.38389722149376426,-0.5089160627921744,null,0.41172380176257367,0.39921730961513135,null,0.41172380176257367,0.24793127610873328,null,-0.027688258194792974,-0.04698230570917894,null,-0.027688258194792974,-0.02670321717437541,null,-0.027688258194792974,0.057334084694421625,null,0.2731667728753765,0.3334973209636406,null,0.2731667728753765,0.2919034835040413,null,0.2731667728753765,0.16958236960938491,null,-0.27603064263977073,-0.33180714124855976,null,-0.27603064263977073,-0.1836505333207323,null,-0.27603064263977073,-0.2615337288865682,null,0.2919034835040413,0.16958236960938491,null,0.2919034835040413,0.3334973209636406,null,0.2919034835040413,0.3630233891422448,null,0.08003922710033808,0.057334084694421625,null,0.08003922710033808,0.0877521067971623,null,0.23463510980425722,0.14194894105793918,null,0.23463510980425722,0.24793127610873328,null,0.23463510980425722,0.21817281240266825,null,0.23463510980425722,0.22944271326723042,null,-0.19507660191462672,-0.144686749814993,null,-0.19507660191462672,-0.17559766123972692,null,0.21068533430161263,0.20133526129273188,null,0.21068533430161263,0.057334084694421625,null,0.706383504533662,0.6941823288213782,null,0.706383504533662,0.7137485613074239,null,0.706383504533662,0.7001794223369159,null,0.706383504533662,0.5535941155957321,null,-0.2652618334936792,-0.24785072743817574,null,-0.2652618334936792,-0.144686749814993,null,-0.2652618334936792,-0.26556354325152703,null,-0.2652618334936792,-0.24417091013762704,null,-0.23373042976057667,-0.24170688735741838,null,-0.23373042976057667,-0.2243655011031012,null,-0.23373042976057667,-0.09402260018557479,null,0.2916485853018193,0.24793127610873328,null,0.2916485853018193,0.2950075103704614,null,0.2916485853018193,0.3099201225646745,null,-0.5299450733889067,-0.3685271366320466,null,-0.5299450733889067,-0.5415048957640967,null,-0.2100976511773641,-0.22055460878200367,null,-0.2100976511773641,-0.17644192235806314,null,-0.2100976511773641,-0.2359433423418696,null,-0.2100976511773641,-0.23687880135753614,null,0.08769698842219174,0.24793127610873328,null,0.08769698842219174,0.05361689161600848,null,0.08769698842219174,-0.144686749814993,null,0.08769698842219174,0.053137722331690666,null,-0.1757443483642554,-0.1604148598201767,null,-0.1757443483642554,-0.1904785676111387,null,0.38312191126574846,0.24793127610873328,null,0.38312191126574846,0.40421041765369375,null,0.1487834908852371,0.11384096381898817,null,0.1487834908852371,0.17448262792336158,null,0.1487834908852371,0.057334084694421625,null,0.1487834908852371,0.2150016907648118,null,0.1487834908852371,0.18321757063648267,null,0.22944271326723042,0.14194894105793918,null,0.22944271326723042,0.24793127610873328,null,0.22944271326723042,0.21817281240266825,null,0.11384096381898817,0.057334084694421625,null,-0.2243655011031012,-0.24170688735741838,null,-0.2243655011031012,-0.09402260018557479,null,-0.26556354325152703,-0.24785072743817574,null,-0.26556354325152703,-0.24417091013762704,null,-0.26556354325152703,-0.144686749814993,null,-0.23438686665116693,-0.29629988432854404,null,-0.23438686665116693,-0.2472570330106062,null,-0.23438686665116693,-0.1836505333207323,null,-0.2723753581795727,-0.3883830499648306,null,-0.2723753581795727,-0.32636092908642256,null,-0.2723753581795727,-0.36231955840191604,null,-0.2723753581795727,-0.3561897980492358,null,-0.2723753581795727,-0.40212747931893505,null,-0.2723753581795727,-0.34026147827652853,null,-0.2723753581795727,-0.30278858348574605,null,-0.2723753581795727,-0.5054384017048197,null,-0.24435265202250844,-0.1990101795123446,null,-0.21245442217735583,-0.1836505333207323,null,-0.21245442217735583,-0.2292534555429686,null,0.1900362921676404,0.057334084694421625,null,0.1900362921676404,0.17970780832667052,null,0.1900362921676404,0.24793127610873328,null,0.1900362921676404,0.16581441848574047,null,-0.24141094975240646,-0.3374232405718896,null,-0.24141094975240646,-0.34181627347178384,null,-0.24141094975240646,-0.2547710386737485,null,-0.24141094975240646,-0.2980396251674858,null,-0.24141094975240646,-0.26979617963088337,null,-0.24141094975240646,-0.09442219372406183,null,-0.24141094975240646,-0.26719625390059504,null,-0.24141094975240646,-0.31838535923286315,null,-0.24141094975240646,-0.29629988432854404,null,-0.24141094975240646,-0.3792302693245371,null,-0.24141094975240646,-0.1836505333207323,null,-0.24141094975240646,-0.2974401388998378,null,-0.24141094975240646,-0.3215109553880422,null,-0.24141094975240646,-0.22805009965548856,null,-0.24141094975240646,0.057334084694421625,null,-0.24141094975240646,-0.3332341397234369,null,0.3780644033648565,0.24793127610873328,null,0.3780644033648565,0.3833601469938797,null,0.3780644033648565,0.36252340605732686,null,0.20301957564304668,0.20489771711205892,null,0.20301957564304668,0.18962833325795828,null,0.20301957564304668,0.14194894105793918,null,0.20301957564304668,0.2193518199824058,null,0.2725955813838193,0.2500974731006018,null,0.2725955813838193,0.24793127610873328,null,0.2725955813838193,0.2620548937568489,null,-0.23687880135753614,-0.17644192235806314,null,-0.23687880135753614,-0.2359433423418696,null,-0.23687880135753614,-0.2583431024501166,null,-0.23687880135753614,-0.22055460878200367,null,-0.23687880135753614,-0.2615337288865682,null,-0.23687880135753614,-0.2718184307114589,null,0.24793127610873328,0.30607930557133256,null,0.24793127610873328,0.40421041765369375,null,0.24793127610873328,0.07311632108879322,null,0.24793127610873328,-0.09402260018557479,null,0.24793127610873328,0.45175934543539276,null,0.24793127610873328,0.14194894105793918,null,0.24793127610873328,0.1574609173018652,null,0.24793127610873328,0.44040724094224254,null,0.24793127610873328,0.5859485874111335,null,0.24793127610873328,0.4165804572631586,null,0.24793127610873328,0.36252340605732686,null,0.24793127610873328,0.41226186919680397,null,0.24793127610873328,0.057334084694421625,null,0.24793127610873328,0.3088416552061604,null,0.24793127610873328,0.2620548937568489,null,0.24793127610873328,0.5535941155957321,null,0.24793127610873328,0.21185040164655008,null,0.24793127610873328,0.32678542837044966,null,0.24793127610873328,0.3814752508259371,null,0.24793127610873328,0.15067194039703355,null,0.24793127610873328,0.3307339418375218,null,0.24793127610873328,0.2950075103704614,null,0.24793127610873328,0.3379588916709153,null,0.24793127610873328,0.39921730961513135,null,0.24793127610873328,0.4645533205826636,null,0.24793127610873328,0.36690844037313647,null,0.24793127610873328,0.359962953341593,null,0.24793127610873328,-0.1836505333207323,null,0.24793127610873328,0.4489634541441082,null,0.24793127610873328,0.17970780832667052,null,0.24793127610873328,0.3383665933113078,null,0.24793127610873328,0.38349562863158315,null,0.24793127610873328,0.05361689161600848,null,0.24793127610873328,0.1395106533602813,null,0.24793127610873328,0.3901658860838829,null,0.24793127610873328,0.3099201225646745,null,0.24793127610873328,0.35973951436128465,null,0.24793127610873328,0.2940367893379515,null,0.24793127610873328,0.4619080469980777,null,0.24793127610873328,0.4040553680416552,null,0.24793127610873328,0.04439718693558544,null,0.24793127610873328,0.31758394433330916,null,0.24793127610873328,0.4140408635038966,null,0.24793127610873328,0.13992448703460458,null,0.24793127610873328,0.42210454820204046,null,0.24793127610873328,0.3285570414267141,null,0.24793127610873328,0.31414515220677697,null,0.24793127610873328,0.21817281240266825,null,0.24793127610873328,0.3833601469938797,null,0.24793127610873328,0.31740307368494936,null,0.24793127610873328,0.16581441848574047,null,0.24793127610873328,0.1853649318823246,null,0.24793127610873328,0.2500974731006018,null,0.24793127610873328,0.053137722331690666,null,0.24793127610873328,0.531659483070264,null,0.24793127610873328,0.34065069034019135,null,0.24793127610873328,0.06642535210665855,null,0.24793127610873328,-0.144686749814993,null,0.24793127610873328,0.3753392139893115,null,0.1706630634898999,0.1523153183089973,null,0.1706630634898999,0.057334084694421625,null,0.053137722331690666,-0.144686749814993,null,0.053137722331690666,0.04414693753220777,null,0.053137722331690666,0.05361689161600848,null,0.053137722331690666,0.02290323808003787,null,0.20133526129273188,0.057334084694421625,null,-0.3173777198161389,-0.1836505333207323,null,-0.3173777198161389,-0.30705829483340735,null,-0.3173777198161389,-0.28789257980080607,null,-0.2849325763170544,-0.1836505333207323,null,-0.2849325763170544,-0.26074112055363974,null,-0.2849325763170544,-0.2923860576960361,null,-0.39695777512934566,-0.41052348335075456,null,-0.39695777512934566,-0.4095024600014184,null,-0.39695777512934566,-0.40352085698302403,null,-0.3561897980492358,-0.36231955840191604,null,-0.3561897980492358,-0.34026147827652853,null,-0.3561897980492358,-0.32636092908642256,null,0.6941823288213782,0.5535941155957321,null,0.6941823288213782,0.7001794223369159,null,0.6941823288213782,0.7137485613074239,null,0.4079781251683783,0.3334973209636406,null,0.4079781251683783,0.4270851889189868,null,0.4079781251683783,0.4353067119261765,null,0.4619080469980777,0.4645533205826636,null,0.4619080469980777,0.5859485874111335,null,0.08241191770775588,0.05914784709403482,null,0.08241191770775588,0.05344183832425574,null,0.31740307368494936,0.3285570414267141,null,0.31740307368494936,0.2940367893379515,null,0.3630233891422448,0.3334973209636406,null,-0.3127004645936037,-0.28702363465327146,null,-0.2615337288865682,-0.2718184307114589,null,-0.2615337288865682,-0.1836505333207323,null,-0.2615337288865682,-0.33180714124855976,null,-0.2615337288865682,-0.2089898630849497,null,-0.2615337288865682,-0.3406428068741444,null,-0.2615337288865682,-0.2583431024501166,null,-0.2615337288865682,-0.19387517514263394,null,0.3833601469938797,0.36252340605732686,null,-0.0811046258787826,-0.1990101795123446,null,-0.0811046258787826,0.04089838824983995,null,-0.3256437401553685,-0.341232963957206,null,-0.3256437401553685,-0.3089586970196594,null,-0.3256437401553685,-0.2974401388998378,null,-0.41052348335075456,-0.4095024600014184,null,-0.41052348335075456,-0.40352085698302403,null,-0.2547710386737485,-0.26719625390059504,null,-0.2547710386737485,-0.3374232405718896,null,-0.2547710386737485,-0.1836505333207323,null,-0.03131020025571709,-0.024947729861626466,null,-0.03131020025571709,-0.0499352311458284,null,-0.03131020025571709,-0.05625098916420061,null,-0.03131020025571709,-0.03860274572188954,null,-0.03131020025571709,0.057334084694421625,null,-0.03131020025571709,-0.10652205133493349,null,0.14194894105793918,0.14341842730052223,null,0.14194894105793918,0.17940852423182896,null,0.14194894105793918,0.1853649318823246,null,0.14194894105793918,0.21185040164655008,null,0.14194894105793918,0.2193518199824058,null,0.14194894105793918,0.18962833325795828,null,0.14194894105793918,0.21817281240266825,null,0.14194894105793918,0.20489771711205892,null,0.14194894105793918,0.13236420791083955,null,0.14194894105793918,0.1024211626106067,null,0.14194894105793918,0.1345310293546687,null,0.6265708269309338,0.6292907642573967,null,0.6265708269309338,0.652699643779598,null,-0.6642943383952763,-0.5054384017048197,null,-0.6642943383952763,-0.6436347608996995,null,-0.144686749814993,-0.1836505333207323,null,-0.144686749814993,-0.18657760563145134,null,-0.144686749814993,-0.17559766123972692,null,-0.144686749814993,-0.13243327461895996,null,-0.144686749814993,-0.22963198675309393,null,-0.144686749814993,-0.2929541265930888,null,-0.144686749814993,-0.1792167006492603,null,-0.144686749814993,-0.24417091013762704,null,-0.144686749814993,-0.20164554872031096,null,-0.144686749814993,-0.21228882667771784,null,-0.144686749814993,-0.10652205133493349,null,-0.144686749814993,-0.1507048737986401,null,-0.144686749814993,-0.17152550671702288,null,-0.144686749814993,0.05361689161600848,null,-0.144686749814993,-0.33180714124855976,null,-0.144686749814993,-0.24785072743817574,null,0.1182357569735683,0.057334084694421625,null,0.652699643779598,0.6292907642573967,null,-0.32636092908642256,-0.34026147827652853,null,-0.32636092908642256,-0.36231955840191604,null,-0.3755624460982769,-0.38575174347229135,null,-0.3755624460982769,-0.33180714124855976,null,0.4040553680416552,0.41226186919680397,null,0.4040553680416552,0.38349562863158315,null,0.7137485613074239,0.7001794223369159,null,0.7137485613074239,0.5535941155957321,null,-0.29629988432854404,-0.34181627347178384,null,-0.29629988432854404,-0.2356170238850429,null,-0.29629988432854404,-0.42081542953672807,null,-0.29629988432854404,-0.4428693315091862,null,-0.29629988432854404,-0.31838535923286315,null,-0.29629988432854404,-0.2472570330106062,null,-0.29629988432854404,-0.25097815792824024,null,-0.29629988432854404,-0.1836505333207323,null,-0.05625098916420061,-0.0499352311458284,null,-0.05625098916420061,-0.024947729861626466,null,-0.05625098916420061,0.057334084694421625,null,-0.05625098916420061,-0.03860274572188954,null,-0.05625098916420061,-0.10652205133493349,null,-0.3089586970196594,-0.341232963957206,null,-0.3089586970196594,-0.2974401388998378,null,0.41226186919680397,0.38349562863158315,null,-0.25097815792824024,-0.2356170238850429,null,-0.40212747931893505,-0.3883830499648306,null,0.2193518199824058,0.20489771711205892,null,0.2193518199824058,0.18962833325795828,null,0.32678542837044966,0.30607930557133256,null,-0.1836505333207323,-0.30705829483340735,null,-0.1836505333207323,0.16958236960938491,null,-0.1836505333207323,-0.09322760846944347,null,-0.1836505333207323,-0.33325379630751584,null,-0.1836505333207323,-0.26719625390059504,null,-0.1836505333207323,0.057334084694421625,null,-0.1836505333207323,-0.332172199275611,null,-0.1836505333207323,-0.3374232405718896,null,-0.1836505333207323,-0.09125897197569359,null,-0.1836505333207323,-0.09258786841158448,null,-0.1836505333207323,-0.314255494127538,null,-0.1836505333207323,-0.00493005919142411,null,-0.1836505333207323,-0.33874929577940854,null,-0.1836505333207323,-0.26074112055363974,null,-0.1836505333207323,-0.31949532406570397,null,-0.1836505333207323,-0.3184832378126542,null,-0.1836505333207323,-0.2472570330106062,null,-0.1836505333207323,-0.2923860576960361,null,-0.1836505333207323,-0.35137546983663787,null,-0.1836505333207323,-0.33180714124855976,null,-0.1836505333207323,-0.22805009965548856,null,-0.1836505333207323,-0.2292534555429686,null,-0.1836505333207323,-0.1707402344599611,null,-0.1836505333207323,-0.26953468484437026,null,-0.1836505333207323,-0.28789257980080607,null,-0.1836505333207323,-0.30893512627664627,null,-0.1836505333207323,-0.006885414194733128,null,0.2196304275472683,0.16958236960938491,null,0.2196304275472683,0.2379545845260003,null,-0.341232963957206,-0.2974401388998378,null,-0.2461872482370283,-0.2682955881388224,null,-0.2461872482370283,-0.26334457784291787,null,-0.00493005919142411,0.057334084694421625,null,-0.00493005919142411,-0.006885414194733128,null,-0.00493005919142411,0.16958236960938491,null,0.16581441848574047,0.057334084694421625,null,0.16581441848574047,0.17970780832667052,null,0.6713067263576877,0.6846786709157807,null,0.6713067263576877,0.531659483070264,null,0.6713067263576877,0.6787023823798306,null,0.04439718693558544,-0.04336858531685759,null,0.04439718693558544,0.1574609173018652,null,0.04439718693558544,-0.04611542283153123,null,-0.36231955840191604,-0.34026147827652853,null,-0.314255494127538,-0.332172199275611,null,0.3790387836282906,0.506318065890254,null,0.3790387836282906,0.48881548200266917,null,0.3790387836282906,0.5068511863823153,null,0.3790387836282906,0.16958236960938491,null,0.3790387836282906,0.3178707894002735,null,0.17970780832667052,0.057334084694421625,null,0.05914784709403482,0.05344183832425574,null,0.3099201225646745,0.2950075103704614,null,-0.34181627347178384,-0.31838535923286315,null,0.48881548200266917,0.5068511863823153,null,0.48881548200266917,0.506318065890254,null,-0.46953462631619036,-0.3792302693245371,null,-0.46953462631619036,-0.4469873318740338,null,0.02290323808003787,0.04414693753220777,null,0.2940367893379515,0.3285570414267141,null,0.20489771711205892,0.18962833325795828,null,-0.00930595779806558,-0.02108142461924934,null,-0.00930595779806558,0.057334084694421625,null,-0.00930595779806558,-0.09442219372406183,null,-0.3374232405718896,-0.26719625390059504,null,-0.3374232405718896,-0.4334716660933204,null,-0.3374232405718896,-0.4567480324717053,null,0.2379545845260003,0.16958236960938491,null,-0.24785072743817574,-0.24417091013762704,null,-0.2583431024501166,-0.2718184307114589,null,-0.7537293611153583,-0.6704667391493794,null,-0.7537293611153583,-0.6754853648670038,null,-0.7537293611153583,-0.5054384017048197,null,-0.7537293611153583,-0.8948474605608381,null,-0.7537293611153583,-0.8853133311253923,null,-0.7537293611153583,-0.6824709878651782,null,-0.48062921463594915,-0.33180714124855976,null,-0.4939680017345201,-0.4756606814966038,null,-0.07829613842283081,-0.1990101795123446,null,0.14558640763898786,0.16482404617764268,null,0.14558640763898786,0.04089838824983995,null,0.13992448703460458,0.15067194039703355,null,0.5535941155957321,0.7001794223369159,null,0.5535941155957321,0.45175934543539276,null,0.5535941155957321,0.4489634541441082,null,-0.8948474605608381,-0.8853133311253923,null,0.18135772178870596,0.16202457842932785,null,0.18135772178870596,0.057334084694421625,null,-0.03860274572188954,-0.10652205133493349,null,-0.03860274572188954,-0.024947729861626466,null,-0.03860274572188954,-0.0499352311458284,null,-0.03860274572188954,0.057334084694421625,null,-0.09402260018557479,-0.24170688735741838,null,-0.09402260018557479,-0.018290606821115094,null,-0.09402260018557479,-0.22670041266116028,null,-0.09402260018557479,-0.2469484386894806,null,0.5068511863823153,0.506318065890254,null,0.17448262792336158,0.18321757063648267,null,-0.4567480324717053,-0.4334716660933204,null,-0.2896957306775164,-0.29804063625835997,null,-0.2896957306775164,-0.28789257980080607,null,-0.2896957306775164,-0.3685271366320466,null,-0.2896957306775164,-0.1990101795123446,null,-0.1904785676111387,-0.1604148598201767,null,-0.4428693315091862,-0.42081542953672807,null,-0.2980396251674858,-0.2974401388998378,null,-0.2980396251674858,-0.26979617963088337,null,0.8396411855098973,0.7560242176572916,null,0.8396411855098973,0.9762208561537624,null,0.8396411855098973,0.5859485874111335,null,0.1345310293546687,0.13236420791083955,null,0.1345310293546687,0.14341842730052223,null,0.11205902299391327,0.057334084694421625,null,0.11205902299391327,0.11399466396032104,null,0.11205902299391327,0.13785619470043894,null,-0.38575174347229135,-0.33180714124855976,null,0.6787023823798306,0.531659483070264,null,0.6787023823798306,0.6846786709157807,null,-0.09322760846944347,-0.032914440909998774,null,-0.09322760846944347,-0.015797701573913855,null,-0.09322760846944347,-0.0015688344355572715,null,-0.26074112055363974,-0.2923860576960361,null,-0.024947729861626466,-0.10652205133493349,null,-0.024947729861626466,0.057334084694421625,null,-0.024947729861626466,-0.0499352311458284,null,-0.2469484386894806,-0.22670041266116028,null,0.14341842730052223,0.13236420791083955,null,0.6653851540788173,0.644682038075124,null,0.6653851540788173,0.6554322070713565,null,-0.04611542283153123,-0.04336858531685759,null,0.45175934543539276,0.4489634541441082,null,-0.032914440909998774,-0.015797701573913855,null,-0.032914440909998774,-0.0015688344355572715,null,-0.0015688344355572715,-0.015797701573913855,null,-0.1507048737986401,-0.13243327461895996,null,-0.35137546983663787,-0.33874929577940854,null,-0.35137546983663787,-0.31949532406570397,null,0.16958236960938491,-0.006885414194733128,null,0.16958236960938491,0.3178707894002735,null,0.16958236960938491,0.3334973209636406,null,0.16958236960938491,0.057334084694421625,null,-0.1707402344599611,-0.09125897197569359,null,-0.1707402344599611,0.057334084694421625,null,-0.1707402344599611,-0.09258786841158448,null,-0.1707402344599611,-0.26953468484437026,null,0.1883944039782016,0.057334084694421625,null,-0.22963198675309393,-0.20164554872031096,null,-0.22963198675309393,-0.21228882667771784,null,0.44040724094224254,0.531659483070264,null,-0.1792167006492603,-0.17152550671702288,null,-0.1792167006492603,-0.18657760563145134,null,-0.1792167006492603,-0.10652205133493349,null,0.18667681403372977,0.04089838824983995,null,0.4645533205826636,0.5859485874111335,null,0.057334084694421625,0.16251134383437724,null,0.057334084694421625,-0.09442219372406183,null,0.057334084694421625,0.08005275191518312,null,0.057334084694421625,-0.09258786841158448,null,0.057334084694421625,0.0877521067971623,null,0.057334084694421625,-0.10652205133493349,null,0.057334084694421625,-0.04698230570917894,null,0.057334084694421625,0.1842235820026506,null,0.057334084694421625,0.02610275324395288,null,0.057334084694421625,-0.0499352311458284,null,0.057334084694421625,0.05344183832425574,null,0.057334084694421625,-0.02670321717437541,null,0.057334084694421625,0.13785619470043894,null,0.057334084694421625,-0.09125897197569359,null,0.057334084694421625,0.06642535210665855,null,0.057334084694421625,-0.006885414194733128,null,0.057334084694421625,0.1523153183089973,null,0.057334084694421625,0.07311632108879322,null,0.057334084694421625,-0.02108142461924934,null,0.057334084694421625,0.11399466396032104,null,0.057334084694421625,0.16202457842932785,null,-0.17644192235806314,-0.14890401981238952,null,-0.17644192235806314,-0.22055460878200367,null,-0.17644192235806314,-0.2359433423418696,null,-0.17644192235806314,-0.11952935068819252,null,-0.02670321717437541,-0.04698230570917894,null,0.2620548937568489,0.2500974731006018,null,-0.35202536416758207,-0.1990101795123446,null,-0.35202536416758207,-0.36676844222740407,null,0.1722507872877681,0.1916246687135562,null,0.1722507872877681,0.04089838824983995,null,-0.09258786841158448,-0.09125897197569359,null,0.05344183832425574,0.038126884768340766,null,0.05344183832425574,0.021856445723820242,null,-0.02108142461924934,-0.09442219372406183,null,0.3334973209636406,0.4270851889189868,null,0.3334973209636406,0.4353067119261765,null,-0.13232578990474772,-0.11199051836344989,null,-0.33874929577940854,-0.31949532406570397,null,-0.6824709878651782,-0.5054384017048197,null,-0.6824709878651782,-0.6754853648670038,null,-0.6824709878651782,-0.6704667391493794,null,-0.33180714124855976,-0.2929541265930888,null,-0.6436347608996995,-0.5054384017048197,null,0.31414515220677697,0.3307339418375218,null,0.31414515220677697,0.3379588916709153,null,0.31414515220677697,0.3088416552061604,null,-0.10652205133493349,-0.18657760563145134,null,-0.10652205133493349,-0.17152550671702288,null,-0.10652205133493349,-0.0499352311458284,null,-0.2764873500848596,-0.30278858348574605,null,-0.11952935068819252,-0.14890401981238952,null,-0.2089898630849497,-0.19387517514263394,null,-0.28789257980080607,-0.30705829483340735,null,-0.28789257980080607,-0.3685271366320466,null,-0.28789257980080607,-0.1990101795123446,null,-0.28789257980080607,-0.29804063625835997,null,0.1853649318823246,0.21185040164655008,null,-0.17152550671702288,-0.18657760563145134,null,0.13785619470043894,0.11399466396032104,null,-0.33325379630751584,-0.3184832378126542,null,-0.33325379630751584,-0.30893512627664627,null,0.7560242176572916,0.5859485874111335,null,-0.26979617963088337,-0.2974401388998378,null,0.07311632108879322,0.06642535210665855,null,0.644682038075124,0.6554322070713565,null,-0.6704667391493794,-0.5054384017048197,null,-0.6704667391493794,-0.6754853648670038,null,-0.3184832378126542,-0.30893512627664627,null,0.4140408635038966,0.4165804572631586,null,-0.29804063625835997,-0.1990101795123446,null,-0.29804063625835997,-0.3685271366320466,null,-0.20164554872031096,-0.21228882667771784,null,-0.1990101795123446,-0.3685271366320466,null,-0.1990101795123446,0.04089838824983995,null,-0.1990101795123446,-0.36676844222740407,null,0.531659483070264,0.6846786709157807,null,0.4270851889189868,0.4353067119261765,null,0.3379588916709153,0.3307339418375218,null,0.3379588916709153,0.3088416552061604,null,-0.40352085698302403,-0.4095024600014184,null,0.3901658860838829,0.36690844037313647,null,0.3901658860838829,0.35973951436128465,null,0.3307339418375218,0.3088416552061604,null,0.038126884768340766,0.021856445723820242,null,0.5412752971026304,0.5572920587787685,null,-0.5415048957640967,-0.3685271366320466,null,0.36690844037313647,0.35973951436128465,null,0.16251134383437724,0.1842235820026506,null,-0.5054384017048197,-0.6754853648670038,null,-0.5054384017048197,-0.30278858348574605,null,-0.22055460878200367,-0.2359433423418696,null,-0.4469873318740338,-0.3792302693245371,null,0.04089838824983995,0.1916246687135562,null,0.04089838824983995,0.16482404617764268,null,-0.391269072491725,-0.4144616082784801,null,-0.391269072491725,-0.3792302693245371,null,-0.4144616082784801,-0.3792302693245371,null,-0.26334457784291787,-0.2682955881388224,null,-0.3215109553880422,-0.3792302693245371,null,0.3814752508259371,0.359962953341593,null],\"y\":[-0.09473908054505915,-0.07423880547523237,null,-0.09473908054505915,-0.18619444411252264,null,-0.06082007419998915,-0.08580695485443018,null,-0.06082007419998915,-0.03224720983741994,null,-0.24258385757215384,-0.23676427856980825,null,-0.24258385757215384,-0.2563194451866121,null,-0.24258385757215384,-0.1833406496335737,null,-0.24258385757215384,-0.15988802912569328,null,-0.28420673827069143,-0.2629302652147024,null,-0.28420673827069143,-0.26959520084182204,null,-0.28420673827069143,-0.18619444411252264,null,-0.28420673827069143,-0.28499666387946154,null,0.07076787941052783,-0.03224720983741994,null,0.07076787941052783,0.09915528791016963,null,0.07076787941052783,0.047393329650638825,null,0.07076787941052783,0.16739073915131036,null,0.3951549430380065,0.1886028543500137,null,0.3951549430380065,0.5221126583114559,null,0.3951549430380065,0.3942813706415176,null,0.3141020725811047,0.2939437129939946,null,0.3141020725811047,0.3060347413794392,null,0.3141020725811047,0.1886028543500137,null,-0.12274799089163504,-0.14599749971722017,null,-0.12274799089163504,-0.06609687589514415,null,-0.12274799089163504,-0.149092400476577,null,-0.11955193531236262,-0.08778813364244716,null,-0.11955193531236262,-0.09512268355096269,null,-0.11955193531236262,-0.18619444411252264,null,-0.11955193531236262,-0.11138942615615467,null,0.3431749038863376,0.1886028543500137,null,0.3431749038863376,0.33406777440450847,null,0.3431749038863376,0.33559018541953684,null,-0.1291038296578684,-0.14882561163665764,null,-0.1291038296578684,-0.18619444411252264,null,-0.1291038296578684,-0.12146979378652997,null,-0.1291038296578684,-0.1406472191013985,null,-0.18523445970736696,-0.16797292313505285,null,-0.18523445970736696,-0.2573360959957922,null,-0.18523445970736696,-0.15363859377901015,null,-0.18523445970736696,-0.16432945296348467,null,-0.18523445970736696,-0.17693417457559646,null,-0.18523445970736696,-0.23295063620578352,null,-0.18523445970736696,-0.15401501457085723,null,-0.18523445970736696,-0.2729992278294885,null,-0.18523445970736696,-0.029584526978025834,null,-0.18523445970736696,-0.19072842206004084,null,0.254615132898204,0.27954542884156214,null,0.254615132898204,0.1886028543500137,null,0.254615132898204,0.26710866766898106,null,-0.16432945296348467,-0.23295063620578352,null,-0.16432945296348467,-0.15401501457085723,null,-0.16432945296348467,-0.029584526978025834,null,-0.16432945296348467,-0.17693417457559646,null,-0.16432945296348467,-0.16797292313505285,null,-0.16432945296348467,-0.15363859377901015,null,0.17370039481023547,0.17752001827538338,null,0.17370039481023547,0.15974965178049294,null,0.17370039481023547,0.15750947573299953,null,0.17370039481023547,0.1212639111459302,null,0.6546017394087829,0.503358480185972,null,0.6546017394087829,0.6376086499875915,null,-0.12233588290712585,-0.1594006090732356,null,-0.12233588290712585,-0.15887028102092338,null,-0.12233588290712585,-0.03224720983741994,null,-0.12233588290712585,-0.08967916906965047,null,-0.12233588290712585,-0.15988802912569328,null,-0.12233588290712585,-0.1717537206387508,null,-0.12233588290712585,-0.17569245652442902,null,-0.12233588290712585,-0.08426985966657831,null,-0.12233588290712585,-0.08410587306394082,null,-0.12233588290712585,-0.06609687589514415,null,-0.12233588290712585,-0.1490487361713442,null,-0.12233588290712585,-0.07291821986559399,null,-0.12233588290712585,-0.025648300008902995,null,-0.12233588290712585,-0.18127835200762124,null,-0.30366419714311954,-0.18619444411252264,null,-0.30366419714311954,-0.309397667143193,null,-0.41263598048119,-0.4755206106804721,null,-0.41263598048119,-0.541320567250647,null,-0.41263598048119,-0.45466075901141983,null,-0.41263598048119,-0.5541772559348288,null,-0.41263598048119,-0.43988845167183815,null,-0.41263598048119,-0.18619444411252264,null,-0.41263598048119,-0.528428217263766,null,-0.41263598048119,-0.5171286673489337,null,-0.41263598048119,-0.3354095993759736,null,-0.41263598048119,-0.31641033915165484,null,-0.41263598048119,-0.5465194355149727,null,-0.41263598048119,-0.4977716600081553,null,-0.41263598048119,-0.45820637160626865,null,-0.41263598048119,-0.5207149603943795,null,-0.41263598048119,-0.5024162905092944,null,-0.41263598048119,-0.4734834205993846,null,-0.41263598048119,-0.35791354719913365,null,-0.2586971650787668,-0.23295063620578352,null,-0.2586971650787668,-0.2619641311660235,null,-0.2586971650787668,-0.15988802912569328,null,-0.2586971650787668,-0.19072842206004084,null,-0.4274051946433234,-0.3049620133078615,null,0.11236105301195334,0.039148380299295654,null,0.11236105301195334,0.09662585050243096,null,0.11236105301195334,0.08560768557096833,null,-0.5620521284734261,-0.5506125182696979,null,-0.5620521284734261,-0.41904369022875587,null,0.6597607598730224,0.503358480185972,null,0.6597607598730224,0.6415580880236341,null,0.6597607598730224,0.6609805333929283,null,0.18835058194077514,0.21394867572571552,null,0.18835058194077514,0.20780692008662935,null,0.18835058194077514,0.1956563171328315,null,0.18835058194077514,0.1212639111459302,null,0.12592559255430438,0.08602485722027778,null,0.12592559255430438,0.10521408321066524,null,0.12592559255430438,0.09915528791016963,null,0.12592559255430438,0.10419531045106283,null,0.2718365825839619,0.2849445660598289,null,0.2718365825839619,0.27415841801808505,null,0.2718365825839619,0.425001856679912,null,0.2718365825839619,0.1886028543500137,null,0.2718365825839619,0.1212639111459302,null,0.35369556781065925,0.2951019654709796,null,0.35369556781065925,0.37451399347830866,null,0.35369556781065925,0.37695327074156576,null,0.506262691980272,0.5530246130869145,null,0.506262691980272,0.5244992806762039,null,0.506262691980272,0.49943140444975304,null,0.506262691980272,0.518265287641295,null,0.506262691980272,0.425001856679912,null,-0.11482663470450603,-0.08320674279248072,null,-0.11482663470450603,-0.08410587306394082,null,-0.11482663470450603,-0.09751548399638116,null,-0.3770264968912099,-0.23295063620578352,null,-0.3770264968912099,-0.40024016499261583,null,-0.3770264968912099,-0.40021342415754274,null,-0.30794849802697655,-0.32766176601098457,null,-0.30794849802697655,-0.18619444411252264,null,0.01468944905554162,0.16739073915131036,null,0.01468944905554162,-0.1833406496335737,null,0.01468944905554162,0.052649666840792954,null,0.01468944905554162,-0.08203170140299136,null,0.01468944905554162,-0.08774310123087792,null,0.01468944905554162,-0.18619444411252264,null,0.01468944905554162,0.07690136371217947,null,0.01468944905554162,0.2843988726533322,null,0.33498539482890716,0.3498643543890353,null,0.33498539482890716,0.24701501139127843,null,0.33498539482890716,0.318219656918684,null,0.579577193174421,0.5603881322583837,null,0.579577193174421,0.5733737388384338,null,0.579577193174421,0.6195368610878262,null,0.579577193174421,0.425001856679912,null,0.579577193174421,0.5880525395555292,null,0.579577193174421,0.6533078942993944,null,0.579577193174421,0.6391017781762831,null,-0.46716096085448283,-0.50326941880636,null,-0.46716096085448283,-0.5033085609723988,null,-0.46716096085448283,-0.6197217261102363,null,-0.46716096085448283,-0.3049620133078615,null,-0.46716096085448283,-0.5061353006305839,null,0.6533078942993944,0.5733737388384338,null,0.6533078942993944,0.5880525395555292,null,0.6533078942993944,0.6195368610878262,null,0.6533078942993944,0.6391017781762831,null,0.3065688157047326,0.1886028543500137,null,0.3065688157047326,0.318922025205062,null,0.33559018541953684,0.33406777440450847,null,0.33559018541953684,0.1886028543500137,null,-0.09958251283662231,-0.11735359751284857,null,-0.09958251283662231,-0.09044651229440363,null,-0.09958251283662231,-0.18619444411252264,null,0.3804093953128879,0.503358480185972,null,0.3804093953128879,0.385318759069059,null,0.3804093953128879,0.1886028543500137,null,-0.5581548831803843,-0.5369972624122011,null,-0.5581548831803843,-0.5653139543165341,null,-0.5581548831803843,-0.39634162035385134,null,0.47961927862615317,0.4836782572866589,null,0.47961927862615317,0.4616782612452741,null,0.47961927862615317,0.3597555982236658,null,0.3942813706415176,0.1886028543500137,null,0.3942813706415176,0.5221126583114559,null,-0.22721869560542146,-0.24582740485687266,null,-0.22721869560542146,-0.2459278200545944,null,-0.22721869560542146,-0.21804434281335933,null,-1.0,-0.8582642246862597,null,-1.0,-0.9829656234714816,null,-1.0,-0.9940605143642297,null,0.19440032776866975,0.17474549952034865,null,0.19440032776866975,0.20889158696368113,null,0.19440032776866975,0.16073225673217204,null,0.5310158325874729,0.5275680954238605,null,0.5310158325874729,0.40512099193755413,null,0.29878873457655236,0.40512099193755413,null,0.29878873457655236,0.1212639111459302,null,0.23778259648591643,0.21735276335940829,null,0.23778259648591643,0.1886028543500137,null,0.23778259648591643,0.21540927585792963,null,0.3132392970495637,0.33162186363018303,null,0.3132392970495637,0.1886028543500137,null,0.3132392970495637,0.3172362351302249,null,0.3132392970495637,0.33570124808116836,null,-0.50326941880636,-0.5033085609723988,null,-0.50326941880636,-0.3049620133078615,null,-0.50326941880636,-0.5061353006305839,null,-0.50326941880636,-0.6197217261102363,null,-0.3055838737635453,-0.28222981032767774,null,-0.3055838737635453,-0.19072842206004084,null,-0.3055838737635453,-0.29924112560559535,null,-0.3179027491094956,-0.33115114853832545,null,-0.3179027491094956,-0.3349392610969492,null,-0.3179027491094956,-0.317820369700927,null,-0.3179027491094956,-0.39634162035385134,null,-0.3179027491094956,-0.18619444411252264,null,-0.2607777315002243,-0.24745149731883143,null,-0.2607777315002243,-0.23887250688075642,null,-0.2607777315002243,-0.18619444411252264,null,-0.149092400476577,-0.17013989765121723,null,-0.149092400476577,-0.14599749971722017,null,-0.149092400476577,-0.06609687589514415,null,-0.149092400476577,-0.18619444411252264,null,-0.149092400476577,-0.18823801006385443,null,0.48813248042244944,0.47959182577550863,null,0.48813248042244944,0.40512099193755413,null,0.48813248042244944,0.37843692938560886,null,-0.19694404797122134,-0.15988802912569328,null,-0.19694404797122134,-0.21785857280854873,null,-0.19694404797122134,-0.2189904982597784,null,-0.3494543011162388,-0.18619444411252264,null,-0.3494543011162388,-0.39634162035385134,null,-0.3494543011162388,-0.3615740065449596,null,-0.3494543011162388,-0.360236873171928,null,-0.3494543011162388,-0.3438704698833398,null,-0.22219483872893078,-0.22926406759548967,null,-0.22219483872893078,-0.18619444411252264,null,-0.22219483872893078,-0.24831201015029286,null,-0.22219483872893078,-0.23945112261865942,null,-0.07423880547523237,-0.18619444411252264,null,-0.3354095993759736,-0.18619444411252264,null,0.5941155381969414,0.6004349393456646,null,0.5941155381969414,0.7284613981228892,null,0.5941155381969414,0.40512099193755413,null,-0.39730755900042214,-0.3728646615254483,null,-0.39730755900042214,-0.3847103358879526,null,-0.39730755900042214,-0.23295063620578352,null,0.47959182577550863,0.37843692938560886,null,0.47959182577550863,0.40512099193755413,null,0.8622150066205894,0.7284613981228892,null,0.8622150066205894,0.8387218736722715,null,0.8622150066205894,0.8588965393840657,null,0.8622150066205894,0.8406738436433908,null,0.4493120540293686,0.2843988726533322,null,0.4493120540293686,0.44604187208914126,null,-0.528428217263766,-0.541320567250647,null,-0.528428217263766,-0.5541772559348288,null,-0.528428217263766,-0.5465194355149727,null,-0.08426985966657831,-0.15988802912569328,null,-0.08426985966657831,-0.08410587306394082,null,-0.08426985966657831,-0.07291821986559399,null,-0.08426985966657831,-0.025648300008902995,null,-0.08426985966657831,-0.03224720983741994,null,-0.08426985966657831,-0.06609687589514415,null,-0.08426985966657831,-0.08967916906965047,null,-0.5369972624122011,-0.39634162035385134,null,-0.5369972624122011,-0.5653139543165341,null,-0.5465194355149727,-0.541320567250647,null,-0.5465194355149727,-0.5541772559348288,null,-0.11138942615615467,-0.08778813364244716,null,-0.11138942615615467,-0.09512268355096269,null,-0.11138942615615467,-0.18619444411252264,null,-0.3087611695043734,-0.18619444411252264,null,-0.3087611695043734,-0.32458218646632025,null,-0.3087611695043734,-0.35791354719913365,null,-0.15988802912569328,-0.2189904982597784,null,-0.15988802912569328,-0.08410587306394082,null,-0.15988802912569328,-0.24248646465676577,null,-0.15988802912569328,-0.19072842206004084,null,-0.15988802912569328,-0.1127593069977425,null,-0.15988802912569328,-0.1490487361713442,null,-0.15988802912569328,-0.07291821986559399,null,-0.15988802912569328,-0.025648300008902995,null,-0.15988802912569328,-0.2554792055879162,null,-0.15988802912569328,0.1212639111459302,null,-0.15988802912569328,-0.1717537206387508,null,-0.15988802912569328,-0.17569245652442902,null,-0.15988802912569328,-0.1594006090732356,null,-0.15988802912569328,-0.15887028102092338,null,-0.15988802912569328,-0.03224720983741994,null,-0.15988802912569328,-0.2619641311660235,null,-0.15988802912569328,-0.1833406496335737,null,-0.15988802912569328,-0.06609687589514415,null,-0.15988802912569328,-0.23676427856980825,null,-0.15988802912569328,-0.23295063620578352,null,-0.15988802912569328,-0.08967916906965047,null,-0.15988802912569328,-0.18127835200762124,null,-0.15988802912569328,-0.2563194451866121,null,-0.15988802912569328,-0.3049620133078615,null,-0.15988802912569328,-0.21785857280854873,null,-0.35521836373331095,-0.23295063620578352,null,-0.03224720983741994,-0.07084393033765604,null,-0.03224720983741994,-0.06642716051546609,null,-0.03224720983741994,0.047393329650638825,null,-0.03224720983741994,-0.01598159204432383,null,-0.03224720983741994,-0.025648300008902995,null,-0.03224720983741994,-0.07996548652059092,null,-0.03224720983741994,-0.025141016094927415,null,-0.03224720983741994,-0.13066228146212297,null,-0.03224720983741994,0.09915528791016963,null,-0.03224720983741994,0.16739073915131036,null,-0.03224720983741994,-0.3049620133078615,null,-0.03224720983741994,-0.11944601209031362,null,-0.03224720983741994,-0.18619444411252264,null,-0.03224720983741994,0.1085929171523956,null,-0.03224720983741994,0.1212639111459302,null,-0.03224720983741994,-0.14095356694899133,null,-0.03224720983741994,0.1886028543500137,null,-0.03224720983741994,-0.20055101774792666,null,-0.03224720983741994,-0.015402010014341454,null,-0.03224720983741994,0.11606574454120086,null,-0.03224720983741994,-0.12739979661233738,null,-0.03224720983741994,0.00025612200150601744,null,-0.03224720983741994,-0.11688158779776744,null,-0.03224720983741994,-0.15461997176419787,null,-0.03224720983741994,-0.08410587306394082,null,-0.03224720983741994,0.014441149754839167,null,-0.03224720983741994,-0.06609687589514415,null,-0.03224720983741994,0.07588138015509364,null,-0.03224720983741994,0.24701501139127843,null,-0.03224720983741994,-0.08967916906965047,null,-0.03224720983741994,-0.08769625905626616,null,-0.03224720983741994,-0.050399974127980966,null,-0.03224720983741994,-0.09200102558546301,null,-0.03224720983741994,0.039148380299295654,null,-0.03224720983741994,-0.018776757671173976,null,-0.03224720983741994,-0.20981528519832734,null,-0.03224720983741994,-0.05250722528903687,null,-0.03224720983741994,-0.07010313110333175,null,-0.03224720983741994,-0.39634162035385134,null,-0.03224720983741994,0.03501815974823906,null,-0.03224720983741994,-0.08580695485443018,null,-0.03224720983741994,0.13349566988592435,null,-0.03224720983741994,-0.12278814593361007,null,-0.03224720983741994,-0.10536589546082997,null,-0.03224720983741994,-0.07291821986559399,null,-0.03224720983741994,0.06325564080489869,null,-0.03224720983741994,0.035929639045516215,null,-0.03224720983741994,0.07069867077690756,null,-0.03224720983741994,-0.008583710418954723,null,-0.03224720983741994,-0.09909122076099709,null,0.6861005384827137,0.6723726360448772,null,0.6861005384827137,0.6655440508353427,null,0.6861005384827137,0.5221126583114559,null,-0.5207149603943795,-0.5024162905092944,null,-0.5207149603943795,-0.5171286673489337,null,-0.5207149603943795,-0.4977716600081553,null,-0.04111667540525816,-0.06609687589514415,null,-0.04111667540525816,-0.05499295045622741,null,-0.04111667540525816,-0.07028601118818568,null,0.518265287641295,0.5244992806762039,null,0.518265287641295,0.49943140444975304,null,0.518265287641295,0.5530246130869145,null,0.518265287641295,0.425001856679912,null,0.09222520204066263,0.10540746857225657,null,0.09222520204066263,0.11669612500080291,null,0.09222520204066263,0.10877648537691828,null,0.09222520204066263,0.10292076687032965,null,0.09222520204066263,0.1886028543500137,null,0.09222520204066263,0.09175699714192388,null,0.09222520204066263,-0.029584526978025834,null,0.37451399347830866,0.37695327074156576,null,0.37451399347830866,0.2951019654709796,null,0.21735276335940829,0.1886028543500137,null,0.21735276335940829,0.21540927585792963,null,-0.263591124755981,-0.27996868292741217,null,-0.263591124755981,-0.24573382045342956,null,-0.263591124755981,-0.24582740485687266,null,0.052649666840792954,0.07690136371217947,null,0.266023747764293,0.1886028543500137,null,0.266023747764293,0.28990418153785036,null,-0.2554792055879162,-0.24248646465676577,null,-0.5624230063059268,-0.39634162035385134,null,-0.5624230063059268,-0.5424432011849858,null,-0.35791354719913365,-0.43988845167183815,null,-0.35791354719913365,-0.18619444411252264,null,-0.35791354719913365,-0.45820637160626865,null,-0.35791354719913365,-0.32458218646632025,null,-0.18127835200762124,-0.1594006090732356,null,0.5663542233806376,0.40512099193755413,null,0.5663542233806376,0.6516218795677106,null,0.5663542233806376,0.575613665108678,null,0.5663542233806376,0.5667672134842375,null,-0.07010313110333175,-0.07084393033765604,null,-0.10536589546082997,-0.12739979661233738,null,-0.10536589546082997,-0.08769625905626616,null,-0.10536589546082997,-0.11944601209031362,null,-0.008583710418954723,-0.06609687589514415,null,-0.008583710418954723,-0.01598159204432383,null,-0.17235945649930776,-0.1919979571571069,null,-0.17235945649930776,-0.21191043533431878,null,-0.17235945649930776,-0.19268966830034723,null,-0.17235945649930776,-0.18619444411252264,null,0.11918769638471641,0.0981341191941886,null,0.11918769638471641,0.039148380299295654,null,-0.050399974127980966,0.03501815974823906,null,-0.050399974127980966,0.035929639045516215,null,-0.050399974127980966,-0.20196141404205742,null,-0.050399974127980966,0.1212639111459302,null,-0.050399974127980966,-0.21413781468010457,null,-0.12146979378652997,-0.1406472191013985,null,-0.12146979378652997,-0.14882561163665764,null,-0.12146979378652997,-0.18619444411252264,null,0.38761342108081936,0.39593031318845046,null,0.38761342108081936,0.40512099193755413,null,0.38761342108081936,0.4009207345818591,null,0.38761342108081936,0.24701501139127843,null,-0.5541772559348288,-0.541320567250647,null,0.8592776505394563,0.8796470292172429,null,0.8592776505394563,0.8737295511080929,null,0.8592776505394563,0.7284613981228892,null,-0.20846962646702585,-0.18619444411252264,null,-0.20846962646702585,-0.1833406496335737,null,-0.13066228146212297,-0.15461997176419787,null,-0.13066228146212297,-0.14095356694899133,null,0.2843988726533322,0.44845638310596414,null,0.2843988726533322,0.44604187208914126,null,0.2843988726533322,0.16739073915131036,null,-0.3349392610969492,-0.317820369700927,null,-0.3349392610969492,-0.39634162035385134,null,-0.3349392610969492,-0.18619444411252264,null,-0.3349392610969492,-0.33115114853832545,null,-0.15401501457085723,-0.17693417457559646,null,-0.15401501457085723,-0.23295063620578352,null,-0.15401501457085723,-0.16797292313505285,null,-0.15401501457085723,-0.15363859377901015,null,-0.15401501457085723,-0.029584526978025834,null,-0.41904369022875587,-0.18619444411252264,null,-0.41904369022875587,-0.35698018824925504,null,-0.41904369022875587,-0.5506125182696979,null,-0.27996868292741217,-0.24582740485687266,null,-0.27996868292741217,-0.24573382045342956,null,0.8588965393840657,0.8387218736722715,null,0.8588965393840657,0.8406738436433908,null,0.8588965393840657,0.7284613981228892,null,0.5733737388384338,0.425001856679912,null,0.5733737388384338,0.6391017781762831,null,0.5733737388384338,0.6195368610878262,null,0.5733737388384338,0.5880525395555292,null,0.5733737388384338,0.5603881322583837,null,-0.28222981032767774,-0.29924112560559535,null,-0.28222981032767774,-0.19072842206004084,null,0.5244992806762039,0.49943140444975304,null,0.5244992806762039,0.5530246130869145,null,0.5244992806762039,0.425001856679912,null,-0.23887250688075642,-0.18619444411252264,null,-0.23887250688075642,-0.24745149731883143,null,-0.23887250688075642,-0.23295063620578352,null,-0.23887250688075642,-0.31539317321703486,null,-0.23887250688075642,-0.2549894052920728,null,-0.18942984184687633,-0.2003527609540083,null,-0.18942984184687633,-0.2131560247888121,null,-0.18942984184687633,-0.18003340801785414,null,-0.18942984184687633,-0.18619444411252264,null,-0.443199296871594,-0.3049620133078615,null,-0.443199296871594,-0.4530630683994992,null,-0.3361548289161914,-0.18619444411252264,null,-0.3361548289161914,-0.3285659796201382,null,-0.3361548289161914,-0.3408794941935949,null,-0.3361548289161914,-0.3517239904887218,null,-0.09200102558546301,-0.07996548652059092,null,-0.09200102558546301,-0.05250722528903687,null,-0.09200102558546301,-0.06642716051546609,null,0.00025612200150601744,0.039148380299295654,null,0.00025612200150601744,-0.06609687589514415,null,0.00025612200150601744,-0.015402010014341454,null,0.08602485722027778,0.10521408321066524,null,0.08602485722027778,0.09915528791016963,null,0.08602485722027778,0.10419531045106283,null,0.2951019654709796,0.1212639111459302,null,0.2951019654709796,0.2514239849304563,null,0.2951019654709796,0.37695327074156576,null,0.2951019654709796,0.2238684081605373,null,0.2951019654709796,0.4014135031936428,null,-0.1439789199736273,-0.1605923937704407,null,-0.1439789199736273,-0.18619444411252264,null,0.06325564080489869,0.07069867077690756,null,0.06325564080489869,0.07588138015509364,null,0.06325564080489869,0.1886028543500137,null,0.5603881322583837,0.6195368610878262,null,0.5603881322583837,0.5880525395555292,null,0.5603881322583837,0.425001856679912,null,-0.1127593069977425,-0.19072842206004084,null,-0.1127593069977425,0.1212639111459302,null,-0.1127593069977425,-0.3049620133078615,null,0.5880525395555292,0.425001856679912,null,0.5880525395555292,0.6195368610878262,null,0.5880525395555292,0.6391017781762831,null,0.3057573173510207,0.1886028543500137,null,0.3057573173510207,0.28348276948518963,null,-0.3615740065449596,-0.39634162035385134,null,-0.3615740065449596,-0.18619444411252264,null,-0.3615740065449596,-0.360236873171928,null,-0.3615740065449596,-0.3438704698833398,null,-0.40024016499261583,-0.23295063620578352,null,-0.40024016499261583,-0.40021342415754274,null,0.26160188857133465,0.2789016591966121,null,0.26160188857133465,0.1886028543500137,null,-0.14374444529798758,-0.12653258859693792,null,-0.14374444529798758,-0.12195105897445904,null,-0.14374444529798758,-0.10479584645712924,null,-0.14374444529798758,-0.1428074375587611,null,-0.3473607165809309,-0.3730988460220396,null,-0.3473607165809309,-0.23295063620578352,null,-0.3473607165809309,-0.36583053229955315,null,-0.3473607165809309,-0.3524958249757812,null,-0.18983859252456353,-0.2073009583986664,null,-0.18983859252456353,-0.21844932421191654,null,-0.18983859252456353,-0.1833406496335737,null,-0.3517239904887218,-0.18619444411252264,null,-0.3517239904887218,-0.3285659796201382,null,-0.3517239904887218,-0.3408794941935949,null,-0.08320674279248072,-0.08410587306394082,null,-0.08320674279248072,-0.09751548399638116,null,-0.7782674490851623,-0.7897003552318688,null,-0.7782674490851623,-0.8582642246862597,null,-0.7782674490851623,-0.7841655875793423,null,-0.7782674490851623,-0.6197217261102363,null,-0.24745149731883143,-0.18619444411252264,null,-0.24745149731883143,-0.2549894052920728,null,-0.24745149731883143,-0.23295063620578352,null,-0.24745149731883143,-0.31539317321703486,null,-0.09909122076099709,-0.12278814593361007,null,-0.09909122076099709,-0.11688158779776744,null,-0.09044651229440363,-0.18619444411252264,null,-0.09044651229440363,-0.11735359751284857,null,0.503358480185972,0.385318759069059,null,0.503358480185972,0.6415580880236341,null,0.503358480185972,0.1886028543500137,null,0.503358480185972,0.6376086499875915,null,0.503358480185972,0.6609805333929283,null,-0.3438704698833398,-0.39634162035385134,null,-0.3438704698833398,-0.18619444411252264,null,-0.3438704698833398,-0.360236873171928,null,0.385318759069059,0.1886028543500137,null,-0.21844932421191654,-0.2073009583986664,null,-0.21844932421191654,-0.1833406496335737,null,-0.36583053229955315,-0.3730988460220396,null,-0.36583053229955315,-0.3524958249757812,null,-0.36583053229955315,-0.23295063620578352,null,0.24265224161802026,0.24701501139127843,null,0.24265224161802026,0.23016917731583136,null,0.24265224161802026,0.1212639111459302,null,0.16739073915131036,0.2617417492897248,null,0.16739073915131036,0.3004791095447427,null,0.16739073915131036,0.28085655268921983,null,0.16739073915131036,0.29739027270763907,null,0.16739073915131036,0.2452836542230963,null,0.16739073915131036,0.30894671355136255,null,0.16739073915131036,0.047393329650638825,null,0.16739073915131036,0.09915528791016963,null,-0.01598159204432383,-0.06609687589514415,null,0.03501815974823906,0.1212639111459302,null,0.03501815974823906,0.035929639045516215,null,0.01408884526820556,0.1886028543500137,null,0.01408884526820556,0.006149652292808575,null,0.01408884526820556,-0.18619444411252264,null,0.01408884526820556,0.005702495627258031,null,0.40512099193755413,0.3597555982236658,null,0.40512099193755413,0.39593031318845046,null,0.40512099193755413,0.308033277786985,null,0.40512099193755413,0.5667672134842375,null,0.40512099193755413,0.575613665108678,null,0.40512099193755413,0.37843692938560886,null,0.40512099193755413,0.29929500596893654,null,0.40512099193755413,0.4009207345818591,null,0.40512099193755413,0.24701501139127843,null,0.40512099193755413,0.7284613981228892,null,0.40512099193755413,0.1212639111459302,null,0.40512099193755413,0.6516218795677106,null,0.40512099193755413,0.6004349393456646,null,0.40512099193755413,0.2938682600004126,null,0.40512099193755413,0.1886028543500137,null,0.40512099193755413,0.5275680954238605,null,-0.18003340801785414,-0.18619444411252264,null,-0.18003340801785414,-0.2003527609540083,null,-0.18003340801785414,-0.2131560247888121,null,-0.5264419572020417,-0.5537058465216298,null,-0.5264419572020417,-0.5398906122194267,null,-0.5264419572020417,-0.39634162035385134,null,-0.5264419572020417,-0.5383100084616208,null,-0.2905153570996774,-0.2907322999340731,null,-0.2905153570996774,-0.18619444411252264,null,-0.2905153570996774,-0.26722440058438873,null,-0.6197217261102363,-0.8582642246862597,null,-0.6197217261102363,-0.7841655875793423,null,-0.6197217261102363,-0.5033085609723988,null,-0.6197217261102363,-0.7897003552318688,null,-0.6197217261102363,-0.3049620133078615,null,-0.6197217261102363,-0.5061353006305839,null,-0.18619444411252264,-0.09512268355096269,null,-0.18619444411252264,-0.11735359751284857,null,-0.18619444411252264,-0.018776757671173976,null,-0.18619444411252264,-0.1833406496335737,null,-0.18619444411252264,-0.16116816684656904,null,-0.18619444411252264,-0.39634162035385134,null,-0.18619444411252264,-0.1912794137233783,null,-0.18619444411252264,-0.2509636185152038,null,-0.18619444411252264,-0.21804434281335933,null,-0.18619444411252264,-0.1993033227970437,null,-0.18619444411252264,-0.2131560247888121,null,-0.18619444411252264,-0.22926406759548967,null,-0.18619444411252264,0.1886028543500137,null,-0.18619444411252264,-0.27209373589842234,null,-0.18619444411252264,-0.26722440058438873,null,-0.18619444411252264,-0.1428074375587611,null,-0.18619444411252264,-0.33115114853832545,null,-0.18619444411252264,-0.08778813364244716,null,-0.18619444411252264,-0.1406472191013985,null,-0.18619444411252264,-0.17013989765121723,null,-0.18619444411252264,-0.2754966802088814,null,-0.18619444411252264,-0.3285659796201382,null,-0.18619444411252264,-0.25338950384913905,null,-0.18619444411252264,-0.1605923937704407,null,-0.18619444411252264,-0.19868126119735063,null,-0.18619444411252264,-0.2629302652147024,null,-0.18619444411252264,-0.14882561163665764,null,-0.18619444411252264,0.1212639111459302,null,-0.18619444411252264,-0.14269091117895255,null,-0.18619444411252264,0.006149652292808575,null,-0.18619444411252264,-0.32766176601098457,null,-0.18619444411252264,-0.23945112261865942,null,-0.18619444411252264,-0.2549894052920728,null,-0.18619444411252264,-0.08203170140299136,null,-0.18619444411252264,-0.26959520084182204,null,-0.18619444411252264,-0.3408794941935949,null,-0.18619444411252264,-0.28499666387946154,null,-0.18619444411252264,-0.1919979571571069,null,-0.18619444411252264,-0.21493607388453514,null,-0.18619444411252264,-0.24831201015029286,null,-0.18619444411252264,-0.12739979661233738,null,-0.18619444411252264,-0.309397667143193,null,-0.18619444411252264,-0.17893381660834062,null,-0.18619444411252264,-0.18823801006385443,null,-0.18619444411252264,-0.31641033915165484,null,-0.18619444411252264,-0.19268966830034723,null,-0.18619444411252264,-0.25040972511142157,null,-0.18619444411252264,-0.360236873171928,null,-0.18619444411252264,-0.2003527609540083,null,-0.18619444411252264,-0.21191043533431878,null,-0.18619444411252264,0.005702495627258031,null,-0.18619444411252264,-0.317820369700927,null,-0.18619444411252264,-0.2907322999340731,null,-0.18619444411252264,-0.31539317321703486,null,-0.18619444411252264,-0.27053688795599407,null,-0.18619444411252264,-0.35698018824925504,null,-0.18619444411252264,-0.025141016094927415,null,-0.18619444411252264,-0.23295063620578352,null,-0.18619444411252264,-0.32458218646632025,null,0.26710866766898106,0.27954542884156214,null,0.26710866766898106,0.1886028543500137,null,-0.31539317321703486,-0.23295063620578352,null,-0.31539317321703486,-0.4633522773668628,null,-0.31539317321703486,-0.2549894052920728,null,-0.31539317321703486,-0.46428446534308687,null,0.2789016591966121,0.1886028543500137,null,0.06297236244816794,0.1212639111459302,null,0.06297236244816794,0.07548617014643645,null,0.06297236244816794,-0.025648300008902995,null,0.21394867572571552,0.1212639111459302,null,0.21394867572571552,0.20780692008662935,null,0.21394867572571552,0.1956563171328315,null,-0.17569245652442902,-0.1490487361713442,null,-0.17569245652442902,-0.1717537206387508,null,-0.17569245652442902,-0.15887028102092338,null,0.29739027270763907,0.28085655268921983,null,0.29739027270763907,0.30894671355136255,null,0.29739027270763907,0.3004791095447427,null,-0.12653258859693792,-0.1428074375587611,null,-0.12653258859693792,-0.10479584645712924,null,-0.12653258859693792,-0.12195105897445904,null,0.745872733958323,0.6195368610878262,null,0.745872733958323,0.7409472282323009,null,0.745872733958323,0.723750950241385,null,-0.21493607388453514,-0.19868126119735063,null,-0.21493607388453514,-0.21804434281335933,null,0.6723726360448772,0.6655440508353427,null,0.6723726360448772,0.5221126583114559,null,-0.21191043533431878,-0.19268966830034723,null,-0.21191043533431878,-0.1919979571571069,null,0.6391017781762831,0.6195368610878262,null,-0.20196141404205742,-0.21413781468010457,null,-0.3049620133078615,-0.5061353006305839,null,-0.3049620133078615,0.1212639111459302,null,-0.3049620133078615,-0.19072842206004084,null,-0.3049620133078615,-0.20055101774792666,null,-0.3049620133078615,-0.4530630683994992,null,-0.3049620133078615,-0.5033085609723988,null,-0.3049620133078615,-0.20981528519832734,null,-0.2003527609540083,-0.2131560247888121,null,-0.015402010014341454,-0.06609687589514415,null,-0.015402010014341454,0.039148380299295654,null,0.7982780026909637,0.7839455107548775,null,0.7982780026909637,0.79045574517452,null,0.7982780026909637,0.6516218795677106,null,-0.1490487361713442,-0.1717537206387508,null,-0.1490487361713442,-0.15887028102092338,null,0.308033277786985,0.29929500596893654,null,0.308033277786985,0.3597555982236658,null,0.308033277786985,0.1212639111459302,null,0.09175699714192388,0.10540746857225657,null,0.09175699714192388,0.11669612500080291,null,0.09175699714192388,0.10292076687032965,null,0.09175699714192388,0.10877648537691828,null,0.09175699714192388,0.1886028543500137,null,0.09175699714192388,-0.029584526978025834,null,-0.39634162035385134,-0.5688892520783643,null,-0.39634162035385134,-0.5653139543165341,null,-0.39634162035385134,-0.317820369700927,null,-0.39634162035385134,-0.33115114853832545,null,-0.39634162035385134,-0.5383100084616208,null,-0.39634162035385134,-0.5398906122194267,null,-0.39634162035385134,-0.360236873171928,null,-0.39634162035385134,-0.5537058465216298,null,-0.39634162035385134,-0.5535395370454727,null,-0.39634162035385134,-0.5424432011849858,null,-0.39634162035385134,-0.5352352554220844,null,-0.5171286673489337,-0.4977716600081553,null,-0.5171286673489337,-0.5024162905092944,null,0.10521408321066524,0.09915528791016963,null,0.10521408321066524,0.10419531045106283,null,-0.23295063620578352,0.1212639111459302,null,-0.23295063620578352,-0.16797292313505285,null,-0.23295063620578352,-0.40021342415754274,null,-0.23295063620578352,-0.3847103358879526,null,-0.23295063620578352,-0.3865686063792784,null,-0.23295063620578352,-0.2619641311660235,null,-0.23295063620578352,-0.15363859377901015,null,-0.23295063620578352,-0.3524958249757812,null,-0.23295063620578352,-0.37030126941167885,null,-0.23295063620578352,-0.38984150531609296,null,-0.23295063620578352,-0.029584526978025834,null,-0.23295063620578352,-0.3728646615254483,null,-0.23295063620578352,-0.17693417457559646,null,-0.23295063620578352,-0.2549894052920728,null,-0.23295063620578352,-0.19072842206004084,null,-0.23295063620578352,-0.3730988460220396,null,0.28990418153785036,0.1886028543500137,null,-0.5024162905092944,-0.4977716600081553,null,0.3004791095447427,0.30894671355136255,null,0.3004791095447427,0.28085655268921983,null,-0.2729992278294885,-0.2573360959957922,null,-0.2729992278294885,-0.19072842206004084,null,-0.24831201015029286,-0.22926406759548967,null,-0.24831201015029286,-0.23945112261865942,null,-0.12195105897445904,-0.10479584645712924,null,-0.12195105897445904,-0.1428074375587611,null,0.24701501139127843,0.39593031318845046,null,0.24701501139127843,0.11606574454120086,null,0.24701501139127843,0.3498643543890353,null,0.24701501139127843,0.318219656918684,null,0.24701501139127843,0.4009207345818591,null,0.24701501139127843,0.23016917731583136,null,0.24701501139127843,0.1085929171523956,null,0.24701501139127843,0.1212639111459302,null,0.10292076687032965,0.11669612500080291,null,0.10292076687032965,0.10540746857225657,null,0.10292076687032965,0.1886028543500137,null,0.10292076687032965,0.10877648537691828,null,0.10292076687032965,-0.029584526978025834,null,0.79045574517452,0.7839455107548775,null,0.79045574517452,0.6516218795677106,null,-0.22926406759548967,-0.23945112261865942,null,0.1085929171523956,0.11606574454120086,null,0.2452836542230963,0.2617417492897248,null,-0.5383100084616208,-0.5537058465216298,null,-0.5383100084616208,-0.5398906122194267,null,-0.08778813364244716,-0.09512268355096269,null,0.1212639111459302,0.07548617014643645,null,0.1212639111459302,0.425001856679912,null,0.1212639111459302,0.13349566988592435,null,0.1212639111459302,0.15750947573299953,null,0.1212639111459302,0.29929500596893654,null,0.1212639111459302,0.1886028543500137,null,0.1212639111459302,0.1980625658620864,null,0.1212639111459302,0.3597555982236658,null,0.1212639111459302,0.20097900949511968,null,0.1212639111459302,0.21577549245314886,null,0.1212639111459302,0.2121067696568312,null,0.1212639111459302,0.27415841801808505,null,0.1212639111459302,0.124883029616991,null,0.1212639111459302,0.20780692008662935,null,0.1212639111459302,0.13491862699461749,null,0.1212639111459302,0.17752001827538338,null,0.1212639111459302,0.23016917731583136,null,0.1212639111459302,0.1956563171328315,null,0.1212639111459302,0.13998980537333214,null,0.1212639111459302,-0.19072842206004084,null,0.1212639111459302,0.2938682600004126,null,0.1212639111459302,0.035929639045516215,null,0.1212639111459302,0.2238684081605373,null,0.1212639111459302,0.2514239849304563,null,0.1212639111459302,-0.025648300008902995,null,0.1212639111459302,0.15974965178049294,null,0.1212639111459302,0.2849445660598289,null,0.5557997537489542,0.425001856679912,null,0.5557997537489542,0.5389110475017207,null,0.7839455107548775,0.6516218795677106,null,-0.06642716051546609,-0.05250722528903687,null,-0.06642716051546609,-0.07996548652059092,null,0.27415841801808505,0.1886028543500137,null,0.27415841801808505,0.2849445660598289,null,0.27415841801808505,0.425001856679912,null,0.005702495627258031,0.1886028543500137,null,0.005702495627258031,0.006149652292808575,null,-0.32827514178451705,-0.3144553106404237,null,-0.32827514178451705,-0.27053688795599407,null,-0.32827514178451705,-0.29648274283588866,null,-0.12739979661233738,-0.08769625905626616,null,-0.12739979661233738,-0.1912794137233783,null,-0.12739979661233738,-0.11944601209031362,null,0.28085655268921983,0.30894671355136255,null,0.2121067696568312,0.1980625658620864,null,0.5530246130869145,0.6207718033590545,null,0.5530246130869145,0.647746579709601,null,0.5530246130869145,0.640287735166148,null,0.5530246130869145,0.425001856679912,null,0.5530246130869145,0.49943140444975304,null,0.006149652292808575,0.1886028543500137,null,0.6655440508353427,0.5221126583114559,null,-0.3408794941935949,-0.3285659796201382,null,0.39593031318845046,0.4009207345818591,null,0.647746579709601,0.640287735166148,null,0.647746579709601,0.6207718033590545,null,0.8387218736722715,0.7284613981228892,null,0.8387218736722715,0.8406738436433908,null,-0.46428446534308687,-0.4633522773668628,null,-0.1919979571571069,-0.19268966830034723,null,-0.5537058465216298,-0.5398906122194267,null,0.34410597769029616,0.3309733858067373,null,0.34410597769029616,0.1886028543500137,null,0.34410597769029616,0.37843692938560886,null,0.3597555982236658,0.29929500596893654,null,0.3597555982236658,0.4836782572866589,null,0.3597555982236658,0.4616782612452741,null,0.5389110475017207,0.425001856679912,null,-0.3730988460220396,-0.3524958249757812,null,-0.5033085609723988,-0.5061353006305839,null,0.16073225673217204,0.1602522709663446,null,0.16073225673217204,0.14589954402870595,null,0.16073225673217204,0.09915528791016963,null,0.16073225673217204,0.17474549952034865,null,0.16073225673217204,0.20889158696368113,null,0.16073225673217204,0.13132351058790828,null,-0.29924112560559535,-0.19072842206004084,null,-0.21785857280854873,-0.2189904982597784,null,-0.14599749971722017,-0.06609687589514415,null,0.08560768557096833,0.09662585050243096,null,0.08560768557096833,0.039148380299295654,null,-0.18823801006385443,-0.17013989765121723,null,-0.1428074375587611,-0.10479584645712924,null,-0.1428074375587611,-0.16116816684656904,null,-0.1428074375587611,-0.14269091117895255,null,0.17474549952034865,0.20889158696368113,null,0.24412493693926027,0.22675600821213018,null,0.24412493693926027,0.1886028543500137,null,0.10877648537691828,-0.029584526978025834,null,0.10877648537691828,0.10540746857225657,null,0.10877648537691828,0.11669612500080291,null,0.10877648537691828,0.1886028543500137,null,-0.1833406496335737,-0.2073009583986664,null,-0.1833406496335737,-0.08774310123087792,null,-0.1833406496335737,-0.2563194451866121,null,-0.1833406496335737,-0.23676427856980825,null,0.640287735166148,0.6207718033590545,null,0.6415580880236341,0.6609805333929283,null,0.4616782612452741,0.4836782572866589,null,-0.08967916906965047,-0.07291821986559399,null,-0.08967916906965047,-0.025648300008902995,null,-0.08967916906965047,-0.08410587306394082,null,-0.08967916906965047,-0.06609687589514415,null,-0.11688158779776744,-0.12278814593361007,null,0.318219656918684,0.3498643543890353,null,0.5667672134842375,0.6516218795677106,null,0.5667672134842375,0.575613665108678,null,-0.24582740485687266,-0.2459278200545944,null,-0.24582740485687266,-0.24573382045342956,null,-0.24582740485687266,-0.21804434281335933,null,-0.5352352554220844,-0.5535395370454727,null,-0.5352352554220844,-0.5688892520783643,null,0.3172362351302249,0.1886028543500137,null,0.3172362351302249,0.33570124808116836,null,0.3172362351302249,0.33162186363018303,null,-0.2573360959957922,-0.19072842206004084,null,-0.29648274283588866,-0.27053688795599407,null,-0.29648274283588866,-0.3144553106404237,null,0.13349566988592435,0.2145491517159694,null,0.13349566988592435,0.18824116219835,null,0.13349566988592435,0.1997562469747687,null,0.20780692008662935,0.1956563171328315,null,0.10540746857225657,-0.029584526978025834,null,0.10540746857225657,0.1886028543500137,null,0.10540746857225657,0.11669612500080291,null,-0.23676427856980825,-0.2563194451866121,null,-0.5688892520783643,-0.5535395370454727,null,-0.4734834205993846,-0.4755206106804721,null,-0.4734834205993846,-0.45466075901141983,null,-0.11944601209031362,-0.08769625905626616,null,-0.16116816684656904,-0.14269091117895255,null,0.2145491517159694,0.18824116219835,null,0.2145491517159694,0.1997562469747687,null,0.1997562469747687,0.18824116219835,null,-0.3728646615254483,-0.3847103358879526,null,0.13998980537333214,0.124883029616991,null,0.13998980537333214,0.13491862699461749,null,0.425001856679912,0.2849445660598289,null,0.425001856679912,0.49943140444975304,null,0.425001856679912,0.6195368610878262,null,0.425001856679912,0.1886028543500137,null,0.2238684081605373,0.20097900949511968,null,0.2238684081605373,0.1886028543500137,null,0.2238684081605373,0.21577549245314886,null,0.2238684081605373,0.2514239849304563,null,0.21540927585792963,0.1886028543500137,null,-0.3865686063792784,-0.37030126941167885,null,-0.3865686063792784,-0.38984150531609296,null,-0.2509636185152038,-0.27053688795599407,null,-0.15363859377901015,-0.17693417457559646,null,-0.15363859377901015,-0.16797292313505285,null,-0.15363859377901015,-0.029584526978025834,null,0.0981341191941886,0.039148380299295654,null,-0.19868126119735063,-0.21804434281335933,null,0.1886028543500137,0.3060347413794392,null,0.1886028543500137,0.37843692938560886,null,0.1886028543500137,0.33406777440450847,null,0.1886028543500137,0.21577549245314886,null,0.1886028543500137,0.28348276948518963,null,0.1886028543500137,-0.029584526978025834,null,0.1886028543500137,0.07069867077690756,null,0.1886028543500137,0.2939437129939946,null,0.1886028543500137,0.318922025205062,null,0.1886028543500137,0.11669612500080291,null,0.1886028543500137,0.5221126583114559,null,0.1886028543500137,0.07588138015509364,null,0.1886028543500137,0.33162186363018303,null,0.1886028543500137,0.20097900949511968,null,0.1886028543500137,-0.025141016094927415,null,0.1886028543500137,0.2849445660598289,null,0.1886028543500137,0.27954542884156214,null,0.1886028543500137,-0.018776757671173976,null,0.1886028543500137,0.3309733858067373,null,0.1886028543500137,0.33570124808116836,null,0.1886028543500137,0.22675600821213018,null,-0.8582642246862597,-0.9940605143642297,null,-0.8582642246862597,-0.7897003552318688,null,-0.8582642246862597,-0.7841655875793423,null,-0.8582642246862597,-0.9829656234714816,null,0.07588138015509364,0.07069867077690756,null,-0.26722440058438873,-0.2907322999340731,null,-0.07028601118818568,-0.06609687589514415,null,-0.07028601118818568,-0.05499295045622741,null,0.06180501724584363,0.0728193174884413,null,0.06180501724584363,0.039148380299295654,null,0.21577549245314886,0.20097900949511968,null,0.5221126583114559,0.6824995656981749,null,0.5221126583114559,0.6704270004850482,null,0.3309733858067373,0.37843692938560886,null,0.6195368610878262,0.7409472282323009,null,0.6195368610878262,0.723750950241385,null,-0.15461997176419787,-0.14095356694899133,null,0.124883029616991,0.13491862699461749,null,0.13132351058790828,0.09915528791016963,null,0.13132351058790828,0.14589954402870595,null,0.13132351058790828,0.1602522709663446,null,-0.19072842206004084,-0.2619641311660235,null,0.10419531045106283,0.09915528791016963,null,-0.25040972511142157,-0.2754966802088814,null,-0.25040972511142157,-0.25338950384913905,null,-0.25040972511142157,-0.27209373589842234,null,-0.029584526978025834,-0.16797292313505285,null,-0.029584526978025834,-0.17693417457559646,null,-0.029584526978025834,0.11669612500080291,null,0.014441149754839167,0.047393329650638825,null,-0.9829656234714816,-0.9940605143642297,null,-0.20055101774792666,-0.20981528519832734,null,-0.025648300008902995,0.07548617014643645,null,-0.025648300008902995,-0.08410587306394082,null,-0.025648300008902995,-0.06609687589514415,null,-0.025648300008902995,-0.07291821986559399,null,-0.317820369700927,-0.33115114853832545,null,-0.17693417457559646,-0.16797292313505285,null,0.33162186363018303,0.33570124808116836,null,0.15750947573299953,0.17752001827538338,null,0.15750947573299953,0.15974965178049294,null,-0.2459278200545944,-0.21804434281335933,null,0.575613665108678,0.6516218795677106,null,-0.018776757671173976,-0.025141016094927415,null,-0.4755206106804721,-0.45466075901141983,null,0.1602522709663446,0.09915528791016963,null,0.1602522709663446,0.14589954402870595,null,0.17752001827538338,0.15974965178049294,null,-0.17893381660834062,-0.1993033227970437,null,-0.07291821986559399,-0.06609687589514415,null,-0.07291821986559399,-0.08410587306394082,null,-0.37030126941167885,-0.38984150531609296,null,-0.06609687589514415,-0.08410587306394082,null,-0.06609687589514415,0.039148380299295654,null,-0.06609687589514415,-0.05499295045622741,null,-0.27053688795599407,-0.3144553106404237,null,0.7409472282323009,0.723750950241385,null,-0.25338950384913905,-0.2754966802088814,null,-0.25338950384913905,-0.27209373589842234,null,-0.15887028102092338,-0.1717537206387508,null,-0.26959520084182204,-0.2629302652147024,null,-0.26959520084182204,-0.28499666387946154,null,-0.2754966802088814,-0.27209373589842234,null,0.6824995656981749,0.6704270004850482,null,-0.45820637160626865,-0.43988845167183815,null,-0.09751548399638116,-0.08410587306394082,null,-0.2629302652147024,-0.28499666387946154,null,0.3060347413794392,0.2939437129939946,null,0.09915528791016963,0.14589954402870595,null,0.09915528791016963,0.047393329650638825,null,-0.7897003552318688,-0.7841655875793423,null,0.8406738436433908,0.7284613981228892,null,0.039148380299295654,0.0728193174884413,null,0.039148380299295654,0.09662585050243096,null,0.8737295511080929,0.8796470292172429,null,0.8737295511080929,0.7284613981228892,null,0.8796470292172429,0.7284613981228892,null,-0.07996548652059092,-0.05250722528903687,null,0.6004349393456646,0.7284613981228892,null,-0.1406472191013985,-0.14882561163665764,null],\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":[\"#9467bd\",\"#bcbd22\",\"#bcbd22\",\"#9467bd\",\"#bcbd22\",\"#17becf\",\"#17becf\",\"#9467bd\",\"#9467bd\",\"#17becf\",\"#9467bd\",\"#17becf\",\"#17becf\",\"#17becf\",\"#17becf\",\"#17becf\",\"#bcbd22\",\"#9467bd\",\"#9467bd\",\"#bcbd22\",\"#bcbd22\",\"#bcbd22\",\"#9467bd\",\"#17becf\",\"#17becf\",\"#bcbd22\",\"#17becf\",\"#17becf\",\"#17becf\",\"#bcbd22\",\"#17becf\",\"#9467bd\",\"#bcbd22\",\"#17becf\",\"#17becf\",\"#bcbd22\",\"#17becf\",\"#17becf\",\"#17becf\",\"#9467bd\",\"#17becf\",\"#9467bd\",\"#17becf\",\"#17becf\",\"#9467bd\",\"#bcbd22\",\"#bcbd22\",\"#17becf\",\"#17becf\",\"#17becf\",\"#17becf\",\"#bcbd22\",\"#bcbd22\",\"#9467bd\",\"#9467bd\",\"#9467bd\",\"#17becf\",\"#bcbd22\",\"#9467bd\",\"#9467bd\",\"#9467bd\",\"#9467bd\",\"#17becf\",\"#17becf\",\"#17becf\",\"#17becf\",\"#bcbd22\",\"#9467bd\",\"#bcbd22\",\"#9467bd\",\"#9467bd\",\"#9467bd\",\"#9467bd\",\"#bcbd22\",\"#17becf\",\"#bcbd22\",\"#17becf\",\"#9467bd\",\"#bcbd22\",\"#17becf\",\"#17becf\",\"#17becf\",\"#17becf\",\"#9467bd\",\"#bcbd22\",\"#17becf\",\"#bcbd22\",\"#9467bd\",\"#9467bd\",\"#bcbd22\",\"#17becf\",\"#bcbd22\",\"#bcbd22\",\"#bcbd22\",\"#9467bd\",\"#bcbd22\",\"#bcbd22\",\"#9467bd\",\"#17becf\",\"#9467bd\",\"#17becf\",\"#bcbd22\",\"#bcbd22\",\"#bcbd22\",\"#9467bd\",\"#17becf\",\"#9467bd\",\"#9467bd\",\"#17becf\",\"#17becf\",\"#bcbd22\",\"#17becf\",\"#9467bd\",\"#9467bd\",\"#bcbd22\",\"#9467bd\",\"#bcbd22\",\"#bcbd22\",\"#bcbd22\",\"#17becf\",\"#9467bd\",\"#17becf\",\"#17becf\",\"#bcbd22\",\"#17becf\",\"#17becf\",\"#9467bd\",\"#17becf\",\"#17becf\",\"#9467bd\",\"#17becf\",\"#bcbd22\",\"#9467bd\",\"#bcbd22\",\"#bcbd22\",\"#9467bd\",\"#bcbd22\",\"#9467bd\",\"#17becf\",\"#9467bd\",\"#17becf\",\"#bcbd22\",\"#17becf\",\"#17becf\",\"#bcbd22\",\"#bcbd22\",\"#bcbd22\",\"#17becf\",\"#17becf\",\"#9467bd\",\"#9467bd\",\"#9467bd\",\"#bcbd22\",\"#9467bd\",\"#17becf\",\"#9467bd\",\"#17becf\",\"#17becf\",\"#17becf\",\"#bcbd22\",\"#bcbd22\",\"#9467bd\",\"#17becf\",\"#9467bd\",\"#17becf\",\"#9467bd\",\"#17becf\",\"#bcbd22\",\"#bcbd22\",\"#9467bd\",\"#bcbd22\",\"#17becf\",\"#bcbd22\",\"#17becf\",\"#9467bd\",\"#17becf\",\"#9467bd\",\"#9467bd\",\"#bcbd22\",\"#17becf\",\"#17becf\",\"#9467bd\",\"#bcbd22\",\"#9467bd\",\"#17becf\",\"#bcbd22\",\"#bcbd22\",\"#9467bd\",\"#9467bd\",\"#17becf\",\"#17becf\",\"#17becf\",\"#9467bd\",\"#17becf\",\"#bcbd22\",\"#9467bd\",\"#9467bd\",\"#17becf\",\"#17becf\",\"#17becf\",\"#bcbd22\",\"#17becf\",\"#17becf\",\"#9467bd\",\"#bcbd22\",\"#bcbd22\",\"#17becf\",\"#17becf\",\"#17becf\",\"#17becf\",\"#17becf\",\"#17becf\",\"#9467bd\",\"#17becf\",\"#17becf\",\"#17becf\",\"#9467bd\",\"#bcbd22\",\"#9467bd\",\"#9467bd\",\"#17becf\",\"#17becf\",\"#17becf\",\"#bcbd22\",\"#17becf\",\"#bcbd22\",\"#bcbd22\",\"#17becf\",\"#bcbd22\",\"#bcbd22\",\"#9467bd\",\"#17becf\",\"#9467bd\",\"#bcbd22\",\"#9467bd\",\"#9467bd\",\"#bcbd22\",\"#17becf\",\"#17becf\",\"#17becf\",\"#bcbd22\",\"#17becf\",\"#17becf\",\"#17becf\",\"#bcbd22\",\"#bcbd22\",\"#17becf\",\"#bcbd22\",\"#17becf\",\"#bcbd22\",\"#9467bd\",\"#9467bd\",\"#17becf\",\"#17becf\",\"#9467bd\",\"#17becf\",\"#17becf\",\"#17becf\",\"#bcbd22\",\"#bcbd22\",\"#9467bd\",\"#9467bd\",\"#bcbd22\",\"#9467bd\",\"#17becf\",\"#9467bd\",\"#17becf\",\"#17becf\",\"#9467bd\",\"#9467bd\",\"#17becf\",\"#9467bd\",\"#17becf\",\"#17becf\",\"#17becf\",\"#17becf\",\"#9467bd\",\"#17becf\",\"#17becf\",\"#17becf\",\"#bcbd22\",\"#9467bd\",\"#9467bd\",\"#17becf\",\"#bcbd22\",\"#17becf\",\"#9467bd\",\"#9467bd\",\"#bcbd22\",\"#bcbd22\",\"#17becf\",\"#17becf\",\"#17becf\",\"#17becf\",\"#17becf\",\"#bcbd22\",\"#17becf\",\"#17becf\",\"#bcbd22\",\"#bcbd22\",\"#bcbd22\",\"#bcbd22\",\"#9467bd\",\"#17becf\",\"#17becf\",\"#9467bd\",\"#bcbd22\",\"#bcbd22\",\"#bcbd22\",\"#9467bd\",\"#bcbd22\",\"#9467bd\",\"#9467bd\",\"#9467bd\",\"#17becf\",\"#9467bd\",\"#bcbd22\",\"#17becf\",\"#9467bd\",\"#17becf\",\"#17becf\",\"#9467bd\",\"#9467bd\",\"#17becf\",\"#17becf\",\"#17becf\",\"#17becf\",\"#9467bd\",\"#17becf\",\"#bcbd22\",\"#17becf\",\"#bcbd22\",\"#17becf\",\"#bcbd22\",\"#9467bd\",\"#bcbd22\",\"#17becf\",\"#17becf\",\"#bcbd22\",\"#9467bd\",\"#17becf\",\"#9467bd\",\"#17becf\",\"#9467bd\",\"#9467bd\",\"#bcbd22\",\"#9467bd\",\"#bcbd22\",\"#9467bd\",\"#9467bd\",\"#17becf\",\"#bcbd22\",\"#17becf\",\"#17becf\",\"#bcbd22\",\"#9467bd\",\"#bcbd22\",\"#17becf\",\"#17becf\",\"#9467bd\",\"#17becf\",\"#9467bd\",\"#17becf\",\"#bcbd22\",\"#9467bd\",\"#9467bd\",\"#bcbd22\",\"#9467bd\",\"#17becf\",\"#9467bd\",\"#17becf\",\"#bcbd22\",\"#17becf\",\"#17becf\",\"#17becf\",\"#bcbd22\",\"#9467bd\",\"#17becf\",\"#17becf\",\"#17becf\",\"#bcbd22\",\"#9467bd\",\"#bcbd22\",\"#17becf\",\"#17becf\",\"#9467bd\",\"#bcbd22\",\"#bcbd22\",\"#bcbd22\",\"#bcbd22\",\"#9467bd\",\"#17becf\",\"#9467bd\",\"#17becf\",\"#17becf\",\"#9467bd\",\"#9467bd\",\"#bcbd22\",\"#bcbd22\",\"#9467bd\",\"#bcbd22\",\"#17becf\",\"#17becf\",\"#17becf\",\"#bcbd22\",\"#17becf\",\"#bcbd22\",\"#9467bd\",\"#bcbd22\",\"#9467bd\",\"#bcbd22\",\"#17becf\",\"#bcbd22\",\"#17becf\",\"#17becf\",\"#bcbd22\",\"#9467bd\",\"#17becf\",\"#bcbd22\",\"#bcbd22\",\"#17becf\",\"#17becf\",\"#bcbd22\",\"#9467bd\",\"#17becf\",\"#9467bd\",\"#17becf\",\"#bcbd22\",\"#bcbd22\",\"#bcbd22\"],\"size\":[7.2,7.2,9.4,9.4,9.4,8.3,8.3,8.3,9.4,8.3,9.4,16.0,8.3,12.700000000000001,9.4,7.2,20.400000000000002,7.2,23.700000000000003,9.4,6.1,8.3,7.2,8.3,9.4,9.4,10.5,8.3,10.5,8.3,8.3,7.2,13.8,8.3,12.700000000000001,10.5,10.5,7.2,8.3,8.3,8.3,8.3,8.3,8.3,8.3,8.3,8.3,7.2,7.2,8.3,9.4,10.5,8.3,10.5,8.3,11.600000000000001,8.3,8.3,10.5,9.4,7.2,7.2,8.3,8.3,8.3,9.4,7.2,9.4,13.8,8.3,9.4,9.4,8.3,38.0,6.1,65.5,8.3,9.4,8.3,10.5,12.700000000000001,8.3,8.3,8.3,7.2,7.2,7.2,7.2,11.600000000000001,8.3,9.4,7.2,9.4,8.3,9.4,7.2,11.600000000000001,9.4,9.4,9.4,8.3,7.2,8.3,10.5,10.5,12.700000000000001,9.4,8.3,9.4,12.700000000000001,8.3,10.5,11.600000000000001,9.4,7.2,9.4,9.4,9.4,9.4,12.700000000000001,7.2,9.4,10.5,9.4,12.700000000000001,7.2,10.5,8.3,7.2,9.4,9.4,8.3,9.4,8.3,9.4,11.600000000000001,8.3,8.3,13.8,10.5,8.3,8.3,9.4,8.3,18.200000000000003,8.3,9.4,9.4,30.3,9.4,9.4,8.3,14.9,110.60000000000001,8.3,12.700000000000001,7.2,8.3,9.4,10.5,9.4,9.4,8.3,8.3,8.3,9.4,10.5,7.2,21.5,9.4,9.4,8.3,10.5,9.4,9.4,12.700000000000001,29.200000000000003,9.4,9.4,39.1,7.2,9.4,9.4,7.2,8.3,8.3,7.2,9.4,9.4,19.3,12.700000000000001,8.3,9.4,8.3,7.2,9.4,9.4,55.6,7.2,8.3,9.4,10.5,9.4,8.3,11.600000000000001,9.4,7.2,7.2,13.8,9.4,7.2,8.3,9.4,9.4,8.3,9.4,7.2,7.2,9.4,9.4,8.3,12.700000000000001,7.2,7.2,9.4,10.5,12.700000000000001,8.3,8.3,8.3,8.3,6.1,9.4,8.3,8.3,12.700000000000001,8.3,7.2,12.700000000000001,7.2,17.1,8.3,8.3,8.3,13.8,8.3,8.3,7.2,9.4,8.3,11.600000000000001,8.3,9.4,8.3,8.3,10.5,9.4,12.700000000000001,9.4,9.4,8.3,8.3,9.4,8.3,8.3,9.4,8.3,8.3,8.3,9.4,8.3,9.4,23.700000000000003,11.600000000000001,8.3,8.3,7.2,8.3,8.3,12.700000000000001,7.2,8.3,8.3,72.10000000000001,12.700000000000001,9.4,7.2,8.3,8.3,7.2,9.4,13.8,10.5,8.3,16.0,8.3,8.3,8.3,9.4,19.3,9.4,9.4,9.4,10.5,20.400000000000002,9.4,7.2,8.3,8.3,8.3,17.1,8.3,10.5,9.4,11.600000000000001,10.5,7.2,12.700000000000001,7.2,9.4,9.4,8.3,8.3,7.2,9.4,9.4,8.3,8.3,7.2,9.4,9.4,7.2,9.4,8.3,7.2,13.8,8.3,9.4,25.900000000000002,10.5,9.4,10.5,8.3,9.4,8.3,7.2,9.4,9.4,10.5,8.3,9.4,8.3,7.2,8.3,8.3,7.2,10.5,8.3,8.3,9.4,8.3,9.4,7.2,8.3,9.4,8.3,8.3,9.4,9.4,8.3,8.3,18.200000000000003,8.3,8.3,7.2,9.4,8.3,8.3,9.4,12.700000000000001,9.4,8.3,9.4,9.4,12.700000000000001,7.2,6.1,17.1,10.5,8.3,9.4,9.4,7.2,8.3,8.3,7.2,7.2,8.3,17.1,8.3,9.4,8.3,8.3,8.3,8.3,8.3,9.4,11.600000000000001,8.3,9.4,9.4,8.3,7.2,9.4,9.4,7.2,7.2,12.700000000000001,7.2,7.2,7.2,16.0,9.4,9.4,8.3,9.4,8.3,10.5,7.2,10.5]},\"mode\":\"markers\",\"text\":[\"Node Cheirality Constraint\\u003cbr\\u003eDegree: 2\",\"Node Forgeries\\u003cbr\\u003eDegree: 2\",\"Node 3D Video\\u003cbr\\u003eDegree: 4\",\"Node Trust Region\\u003cbr\\u003eDegree: 4\",\"Node Shape Detection\\u003cbr\\u003eDegree: 4\",\"Node Breast Cancer\\u003cbr\\u003eDegree: 3\",\"Node Regression\\u003cbr\\u003eDegree: 3\",\"Node Hand Pose Recognition\\u003cbr\\u003eDegree: 3\",\"Node Pattern Encoding\\u003cbr\\u003eDegree: 4\",\"Node Asynchronized Stochastic Gradient\\u003cbr\\u003eDegree: 3\",\"Node Android\\u003cbr\\u003eDegree: 4\",\"Node FERET\\u003cbr\\u003eDegree: 10\",\"Node Cornea Scans\\u003cbr\\u003eDegree: 3\",\"Node Gabor\\u003cbr\\u003eDegree: 7\",\"Node Image\\u003cbr\\u003eDegree: 4\",\"Node HueSTIP\\u003cbr\\u003eDegree: 2\",\"Node Support Vector Machines\\u003cbr\\u003eDegree: 14\",\"Node Omnidirectional Video\\u003cbr\\u003eDegree: 2\",\"Node Clustering\\u003cbr\\u003eDegree: 17\",\"Node 2D Principal Component Analysis\\u003cbr\\u003eDegree: 4\",\"Node Edges\\u003cbr\\u003eDegree: 1\",\"Node Convolutional Layer\\u003cbr\\u003eDegree: 3\",\"Node Artificial Visual Attention Systems\\u003cbr\\u003eDegree: 2\",\"Node UCF101\\u003cbr\\u003eDegree: 3\",\"Node Rotation invariance\\u003cbr\\u003eDegree: 4\",\"Node System Disruptions\\u003cbr\\u003eDegree: 4\",\"Node Gyroscopic Sensor\\u003cbr\\u003eDegree: 5\",\"Node Evaluation Measures\\u003cbr\\u003eDegree: 3\",\"Node Convolutional Automoencoder Networks\\u003cbr\\u003eDegree: 5\",\"Node Gradient Space\\u003cbr\\u003eDegree: 3\",\"Node Haar Classifier\\u003cbr\\u003eDegree: 3\",\"Node Rotation Matrix\\u003cbr\\u003eDegree: 2\",\"Node Video Surveillance\\u003cbr\\u003eDegree: 8\",\"Node Minkowski Engine\\u003cbr\\u003eDegree: 3\",\"Node PLANET\\u003cbr\\u003eDegree: 7\",\"Node Entropy\\u003cbr\\u003eDegree: 5\",\"Node ANWRESH-2014\\u003cbr\\u003eDegree: 5\",\"Node Optimization Techniques\\u003cbr\\u003eDegree: 2\",\"Node Image Clarity\\u003cbr\\u003eDegree: 3\",\"Node affine-invariant distance\\u003cbr\\u003eDegree: 3\",\"Node Hand-crafted Features\\u003cbr\\u003eDegree: 3\",\"Node Dense Fusion Classmate Network\\u003cbr\\u003eDegree: 3\",\"Node L1WGAN-GP\\u003cbr\\u003eDegree: 3\",\"Node Histopathology\\u003cbr\\u003eDegree: 3\",\"Node Image-to-Image Translation\\u003cbr\\u003eDegree: 3\",\"Node Anisotropic artifacts\\u003cbr\\u003eDegree: 3\",\"Node Sparse Samples\\u003cbr\\u003eDegree: 3\",\"Node Colouring\\u003cbr\\u003eDegree: 2\",\"Node Surgical Cuttings\\u003cbr\\u003eDegree: 2\",\"Node Representational Dissimilarity Matrices\\u003cbr\\u003eDegree: 3\",\"Node visibility prediction\\u003cbr\\u003eDegree: 4\",\"Node Tsalli's\\u003cbr\\u003eDegree: 5\",\"Node Bottleneck Distance\\u003cbr\\u003eDegree: 3\",\"Node PSPNet\\u003cbr\\u003eDegree: 5\",\"Node Eye-Gaze Tracking\\u003cbr\\u003eDegree: 3\",\"Node Hand Tracking\\u003cbr\\u003eDegree: 6\",\"Node Inbetweening\\u003cbr\\u003eDegree: 3\",\"Node Wavelet Marginals\\u003cbr\\u003eDegree: 3\",\"Node Hyperspectral Imagery\\u003cbr\\u003eDegree: 5\",\"Node RGB-D Sensors\\u003cbr\\u003eDegree: 4\",\"Node Feature Clustering\\u003cbr\\u003eDegree: 2\",\"Node K-Means Algorithm\\u003cbr\\u003eDegree: 2\",\"Node Local Appearance\\u003cbr\\u003eDegree: 3\",\"Node Linear Discriminant Analysis\\u003cbr\\u003eDegree: 3\",\"Node Filter Based\\u003cbr\\u003eDegree: 3\",\"Node Autoencoding\\u003cbr\\u003eDegree: 4\",\"Node Electronic Microscopes\\u003cbr\\u003eDegree: 2\",\"Node Facebook\\u003cbr\\u003eDegree: 4\",\"Node Pyramid VectorQuantization\\u003cbr\\u003eDegree: 8\",\"Node Satellite Image\\u003cbr\\u003eDegree: 3\",\"Node Semi-Supervised\\u003cbr\\u003eDegree: 4\",\"Node Poincare Sphere\\u003cbr\\u003eDegree: 4\",\"Node Selenium\\u003cbr\\u003eDegree: 3\",\"Node SVM\\u003cbr\\u003eDegree: 30\",\"Node Image Augmentation\\u003cbr\\u003eDegree: 1\",\"Node Convolutional Neural Networks\\u003cbr\\u003eDegree: 55\",\"Node Gradientintensity\\u003cbr\\u003eDegree: 3\",\"Node DTW\\u003cbr\\u003eDegree: 4\",\"Node Fast-scan cyclic voltammetry\\u003cbr\\u003eDegree: 3\",\"Node 4D Objects\\u003cbr\\u003eDegree: 5\",\"Node GENet\\u003cbr\\u003eDegree: 7\",\"Node Biomedical Images\\u003cbr\\u003eDegree: 3\",\"Node Algonauts\\u003cbr\\u003eDegree: 3\",\"Node Image Synthesis\\u003cbr\\u003eDegree: 3\",\"Node Network Infrastructure\\u003cbr\\u003eDegree: 2\",\"Node Ape-specific\\u003cbr\\u003eDegree: 2\",\"Node Visual Surveillance\\u003cbr\\u003eDegree: 2\",\"Node Feed-forward Architecture\\u003cbr\\u003eDegree: 2\",\"Node Data Mining\\u003cbr\\u003eDegree: 6\",\"Node Vowels\\u003cbr\\u003eDegree: 3\",\"Node Drishti\\u003cbr\\u003eDegree: 4\",\"Node Kidney Function Prediction\\u003cbr\\u003eDegree: 2\",\"Node Hexagonal Grids\\u003cbr\\u003eDegree: 4\",\"Node FlowNet\\u003cbr\\u003eDegree: 3\",\"Node Gaming\\u003cbr\\u003eDegree: 4\",\"Node PVRC\\u003cbr\\u003eDegree: 2\",\"Node Biodiversity\\u003cbr\\u003eDegree: 6\",\"Node Single-view Metrology\\u003cbr\\u003eDegree: 4\",\"Node Jaccard Index\\u003cbr\\u003eDegree: 4\",\"Node Flickr\\u003cbr\\u003eDegree: 4\",\"Node 4-Dimensional Deformation\\u003cbr\\u003eDegree: 3\",\"Node Action Labels\\u003cbr\\u003eDegree: 2\",\"Node Convexgeometry\\u003cbr\\u003eDegree: 3\",\"Node Video Analysis\\u003cbr\\u003eDegree: 5\",\"Node FCN\\u003cbr\\u003eDegree: 5\",\"Node GSF\\u003cbr\\u003eDegree: 7\",\"Node Robotics\\u003cbr\\u003eDegree: 4\",\"Node NeRF\\u003cbr\\u003eDegree: 3\",\"Node ARCANEframework\\u003cbr\\u003eDegree: 4\",\"Node CCTC\\u003cbr\\u003eDegree: 7\",\"Node Hepatic Lesions\\u003cbr\\u003eDegree: 3\",\"Node Spatially-sparse\\u003cbr\\u003eDegree: 5\",\"Node HCI\\u003cbr\\u003eDegree: 6\",\"Node Distributed Computing\\u003cbr\\u003eDegree: 4\",\"Node Image Fusion\\u003cbr\\u003eDegree: 2\",\"Node Noise Basis\\u003cbr\\u003eDegree: 4\",\"Node Counter-harmonic Mean\\u003cbr\\u003eDegree: 4\",\"Node Self-supervised Training\\u003cbr\\u003eDegree: 4\",\"Node Ad-hoc\\u003cbr\\u003eDegree: 4\",\"Node Image Captioning\\u003cbr\\u003eDegree: 7\",\"Node Machine Thinking\\u003cbr\\u003eDegree: 2\",\"Node Astrophysics\\u003cbr\\u003eDegree: 4\",\"Node HTRtS\\u003cbr\\u003eDegree: 5\",\"Node Forest Species Recognition\\u003cbr\\u003eDegree: 4\",\"Node CITlab\\u003cbr\\u003eDegree: 7\",\"Node DeSTIN\\u003cbr\\u003eDegree: 2\",\"Node Varroa\\u003cbr\\u003eDegree: 5\",\"Node Rehabilitation Nursing Robots\\u003cbr\\u003eDegree: 3\",\"Node Mobile Robotics\\u003cbr\\u003eDegree: 2\",\"Node Laplacian Eigenbases\\u003cbr\\u003eDegree: 4\",\"Node Object Identification\\u003cbr\\u003eDegree: 4\",\"Node CLIP\\u003cbr\\u003eDegree: 3\",\"Node Low Rank Representation\\u003cbr\\u003eDegree: 4\",\"Node Patterned Textures\\u003cbr\\u003eDegree: 3\",\"Node Ladars\\u003cbr\\u003eDegree: 4\",\"Node Human Computer Interaction\\u003cbr\\u003eDegree: 6\",\"Node 3D Joint Locations\\u003cbr\\u003eDegree: 3\",\"Node log-Euclidean distance\\u003cbr\\u003eDegree: 3\",\"Node Action Recognition\\u003cbr\\u003eDegree: 8\",\"Node U-net\\u003cbr\\u003eDegree: 5\",\"Node Shallow Learning\\u003cbr\\u003eDegree: 3\",\"Node Contrastive Vision Language\\u003cbr\\u003eDegree: 3\",\"Node Hajj\\u003cbr\\u003eDegree: 4\",\"Node Iterative Algorithms\\u003cbr\\u003eDegree: 3\",\"Node Motion Detection\\u003cbr\\u003eDegree: 12\",\"Node Per-pixel Regression\\u003cbr\\u003eDegree: 3\",\"Node FGVC\\u003cbr\\u003eDegree: 4\",\"Node Video Analytics\\u003cbr\\u003eDegree: 4\",\"Node Convolutional Neural Network\\u003cbr\\u003eDegree: 23\",\"Node Real-time Imaging\\u003cbr\\u003eDegree: 4\",\"Node Image-level Classification\\u003cbr\\u003eDegree: 4\",\"Node image annotation\\u003cbr\\u003eDegree: 3\",\"Node Matlab\\u003cbr\\u003eDegree: 9\",\"Node Computer Vision\\u003cbr\\u003eDegree: 96\",\"Node Ophtamologist\\u003cbr\\u003eDegree: 3\",\"Node Thresholding\\u003cbr\\u003eDegree: 7\",\"Node Artificial Neural Networks\\u003cbr\\u003eDegree: 2\",\"Node Tiny Yolo v3\\u003cbr\\u003eDegree: 3\",\"Node Translation Invariance\\u003cbr\\u003eDegree: 4\",\"Node Mahalanobis distance based classifiers\\u003cbr\\u003eDegree: 5\",\"Node Psychological Flashing\\u003cbr\\u003eDegree: 4\",\"Node Harmonic Analysis\\u003cbr\\u003eDegree: 4\",\"Node ARGUS\\u003cbr\\u003eDegree: 3\",\"Node Hair Translation\\u003cbr\\u003eDegree: 3\",\"Node Image Registration\\u003cbr\\u003eDegree: 3\",\"Node RGB-D Sensor\\u003cbr\\u003eDegree: 4\",\"Node Handwriting\\u003cbr\\u003eDegree: 5\",\"Node Binary Blob\\u003cbr\\u003eDegree: 2\",\"Node Remote Sensing\\u003cbr\\u003eDegree: 15\",\"Node Parallel Computing\\u003cbr\\u003eDegree: 4\",\"Node Transformer Networks\\u003cbr\\u003eDegree: 4\",\"Node Self-supervised Learning\\u003cbr\\u003eDegree: 3\",\"Node Gaussian empirical rule\\u003cbr\\u003eDegree: 5\",\"Node Generative Adversarial Network\\u003cbr\\u003eDegree: 4\",\"Node Co-clustering Algorithm\\u003cbr\\u003eDegree: 4\",\"Node PCA\\u002fLDA\\u003cbr\\u003eDegree: 7\",\"Node Semantic Segmentation\\u003cbr\\u003eDegree: 22\",\"Node Archaeology\\u003cbr\\u003eDegree: 4\",\"Node Multi-view 3D Reconstruction\\u003cbr\\u003eDegree: 4\",\"Node Face Recognition\\u003cbr\\u003eDegree: 31\",\"Node Visual Behaviour Recognition\\u003cbr\\u003eDegree: 2\",\"Node Dynamic Time Warping\\u003cbr\\u003eDegree: 4\",\"Node Spatial-temporal Optical Flow\\u003cbr\\u003eDegree: 4\",\"Node Attention-Guided Vision Systems\\u003cbr\\u003eDegree: 2\",\"Node Gender Classification\\u003cbr\\u003eDegree: 3\",\"Node Numeric Recognition\\u003cbr\\u003eDegree: 3\",\"Node Storage Devices\\u003cbr\\u003eDegree: 2\",\"Node Aerial View\\u003cbr\\u003eDegree: 4\",\"Node Synthesis\\u003cbr\\u003eDegree: 4\",\"Node U-Net\\u003cbr\\u003eDegree: 13\",\"Node Graph Embedding Framework\\u003cbr\\u003eDegree: 7\",\"Node ImageSimilarity Challenge\\u003cbr\\u003eDegree: 3\",\"Node Pedestrian Surveillance\\u003cbr\\u003eDegree: 4\",\"Node Attention Architecture\\u003cbr\\u003eDegree: 3\",\"Node SCC\\u003cbr\\u003eDegree: 2\",\"Node SLAMs\\u003cbr\\u003eDegree: 4\",\"Node Image Atoms\\u003cbr\\u003eDegree: 4\",\"Node CNN\\u003cbr\\u003eDegree: 46\",\"Node Printed Format\\u003cbr\\u003eDegree: 2\",\"Node Facebook AI\\u003cbr\\u003eDegree: 3\",\"Node Top-hat Transform\\u003cbr\\u003eDegree: 4\",\"Node Acceleration Sensor\\u003cbr\\u003eDegree: 5\",\"Node Edge2Train\\u003cbr\\u003eDegree: 4\",\"Node Multi-Scale Fully Convolutional Network\\u003cbr\\u003eDegree: 3\",\"Node PyTorch\\u003cbr\\u003eDegree: 6\",\"Node Dictation Algorithm\\u003cbr\\u003eDegree: 4\",\"Node Globetarget Tracking\\u003cbr\\u003eDegree: 2\",\"Node Spatiotemporal Local Features\\u003cbr\\u003eDegree: 2\",\"Node 3D Objects\\u003cbr\\u003eDegree: 8\",\"Node DL\\u003cbr\\u003eDegree: 4\",\"Node Retrieval\\u003cbr\\u003eDegree: 2\",\"Node MIND\\u003cbr\\u003eDegree: 3\",\"Node LRR\\u003cbr\\u003eDegree: 4\",\"Node Skin Lesion Segmentation\\u003cbr\\u003eDegree: 4\",\"Node Multisensory Representational Model\\u003cbr\\u003eDegree: 3\",\"Node CARLA\\u003cbr\\u003eDegree: 4\",\"Node Perception\\u003cbr\\u003eDegree: 2\",\"Node Spectral Curvature Clustering\\u003cbr\\u003eDegree: 2\",\"Node Microsoft Kinect\\u003cbr\\u003eDegree: 4\",\"Node Activation Map\\u003cbr\\u003eDegree: 4\",\"Node Triangle Mesh\\u003cbr\\u003eDegree: 3\",\"Node GAN\\u003cbr\\u003eDegree: 7\",\"Node Natural Handwriting\\u003cbr\\u003eDegree: 2\",\"Node Microtubule Growth\\u003cbr\\u003eDegree: 2\",\"Node CrowdSensing\\u003cbr\\u003eDegree: 4\",\"Node Renyi's\\u003cbr\\u003eDegree: 5\",\"Node Super-resolution\\u003cbr\\u003eDegree: 7\",\"Node Light Field Synthesis\\u003cbr\\u003eDegree: 3\",\"Node Multidimensional Persistent Homology\\u003cbr\\u003eDegree: 3\",\"Node Covariance Matrices\\u003cbr\\u003eDegree: 3\",\"Node Embedded Computers\\u003cbr\\u003eDegree: 3\",\"Node Narest Neighbor\\u003cbr\\u003eDegree: 1\",\"Node RGB Cameras\\u003cbr\\u003eDegree: 4\",\"Node Perutohedral lattice\\u003cbr\\u003eDegree: 3\",\"Node Hand Gestures\\u003cbr\\u003eDegree: 3\",\"Node Computer Graphics\\u003cbr\\u003eDegree: 7\",\"Node HR Grid\\u003cbr\\u003eDegree: 3\",\"Node Video Data\\u003cbr\\u003eDegree: 2\",\"Node Yale B\\u003cbr\\u003eDegree: 7\",\"Node Analysis of Space-Time Objects\\u003cbr\\u003eDegree: 2\",\"Node Human Activity Recognition\\u003cbr\\u003eDegree: 11\",\"Node Cognitive Science\\u003cbr\\u003eDegree: 3\",\"Node Bag of Words\\u003cbr\\u003eDegree: 3\",\"Node Pytorch\\u003cbr\\u003eDegree: 3\",\"Node Hardware Acceleration\\u003cbr\\u003eDegree: 8\",\"Node Context Management\\u003cbr\\u003eDegree: 3\",\"Node Person Detection Network\\u003cbr\\u003eDegree: 3\",\"Node ALwassaiProcess\\u003cbr\\u003eDegree: 2\",\"Node ACRIMA\\u003cbr\\u003eDegree: 4\",\"Node Depth Map\\u003cbr\\u003eDegree: 3\",\"Node Artifact Reduction\\u003cbr\\u003eDegree: 6\",\"Node SynthCam3D\\u003cbr\\u003eDegree: 3\",\"Node fine-grained classification\\u003cbr\\u003eDegree: 4\",\"Node GDP histogram\\u003cbr\\u003eDegree: 3\",\"Node MSFCN\\u003cbr\\u003eDegree: 3\",\"Node DenseNet\\u003cbr\\u003eDegree: 5\",\"Node Cyclic Convolutional Layer\\u003cbr\\u003eDegree: 4\",\"Node Linear SVM\\u003cbr\\u003eDegree: 7\",\"Node Temporal Motion Features\\u003cbr\\u003eDegree: 4\",\"Node Bag-of-visual-word\\u003cbr\\u003eDegree: 4\",\"Node Synthetic Depth Data\\u003cbr\\u003eDegree: 3\",\"Node Laplacians\\u003cbr\\u003eDegree: 3\",\"Node HexagDLy\\u003cbr\\u003eDegree: 4\",\"Node Catmull Romspline\\u003cbr\\u003eDegree: 3\",\"Node MIDOG 2021\\u003cbr\\u003eDegree: 3\",\"Node Complex Networks\\u003cbr\\u003eDegree: 4\",\"Node Mitosis Detection\\u003cbr\\u003eDegree: 3\",\"Node Kernel Analysis\\u003cbr\\u003eDegree: 3\",\"Node Molybdenum\\u003cbr\\u003eDegree: 3\",\"Node Image Patching\\u003cbr\\u003eDegree: 4\",\"Node Error Metric\\u003cbr\\u003eDegree: 3\",\"Node Consumer Electronics\\u003cbr\\u003eDegree: 4\",\"Node Handwriting Recognition\\u003cbr\\u003eDegree: 17\",\"Node LSTM\\u003cbr\\u003eDegree: 6\",\"Node Group Convolutions\\u003cbr\\u003eDegree: 3\",\"Node HUFRD\\u003cbr\\u003eDegree: 3\",\"Node Face-like Structures\\u003cbr\\u003eDegree: 2\",\"Node Bayesianinference\\u003cbr\\u003eDegree: 3\",\"Node Feature Maps\\u003cbr\\u003eDegree: 3\",\"Node Fisher\\u003cbr\\u003eDegree: 7\",\"Node Polyhedron Volume Ratio Classification\\u003cbr\\u003eDegree: 2\",\"Node Spherical Positive definite (SPD) Matrices\\u003cbr\\u003eDegree: 3\",\"Node Perceptual Loss\\u003cbr\\u003eDegree: 3\",\"Node Deep Learning\\u003cbr\\u003eDegree: 61\",\"Node Mathematica\\u003cbr\\u003eDegree: 7\",\"Node EBLearn\\u003cbr\\u003eDegree: 4\",\"Node Motion Capture\\u003cbr\\u003eDegree: 2\",\"Node Multimedia Information Retrieval\\u003cbr\\u003eDegree: 3\",\"Node Dopamine\\u003cbr\\u003eDegree: 3\",\"Node Iridology\\u003cbr\\u003eDegree: 2\",\"Node GRU\\u003cbr\\u003eDegree: 4\",\"Node Medical Imaging\\u003cbr\\u003eDegree: 8\",\"Node Spatial-Sparse\\u003cbr\\u003eDegree: 5\",\"Node Motion Implantation\\u003cbr\\u003eDegree: 3\",\"Node MDRNN\\u003cbr\\u003eDegree: 10\",\"Node Inference\\u003cbr\\u003eDegree: 3\",\"Node CNN+Transformer\\u003cbr\\u003eDegree: 3\",\"Node Resampling\\u003cbr\\u003eDegree: 3\",\"Node Local Linear Embedding\\u003cbr\\u003eDegree: 4\",\"Node Support Vector Machine\\u003cbr\\u003eDegree: 13\",\"Node Portfolio Theory\\u003cbr\\u003eDegree: 4\",\"Node 2DPCA\\u003cbr\\u003eDegree: 4\",\"Node Multiscale Convolutional Network\\u003cbr\\u003eDegree: 4\",\"Node RNNs\\u003cbr\\u003eDegree: 5\",\"Node ORL\\u003cbr\\u003eDegree: 14\",\"Node Clarity\\u003cbr\\u003eDegree: 4\",\"Node R-CNN\\u003cbr\\u003eDegree: 2\",\"Node Łąkernels\\u003cbr\\u003eDegree: 3\",\"Node CaffeNet\\u003cbr\\u003eDegree: 3\",\"Node video annotation\\u003cbr\\u003eDegree: 3\",\"Node PVQ\\u003cbr\\u003eDegree: 11\",\"Node DFCNet\\u003cbr\\u003eDegree: 3\",\"Node Beehive Monitoring Device\\u003cbr\\u003eDegree: 5\",\"Node WSSS\\u003cbr\\u003eDegree: 4\",\"Node Animation\\u003cbr\\u003eDegree: 6\",\"Node SegNet\\u003cbr\\u003eDegree: 5\",\"Node Human Machine Interaction\\u003cbr\\u003eDegree: 2\",\"Node FRGC\\u003cbr\\u003eDegree: 7\",\"Node Trace(A'R)\\u003cbr\\u003eDegree: 2\",\"Node Hierarchical Deep Learning Framework\\u003cbr\\u003eDegree: 4\",\"Node Multiple-frame\\u003cbr\\u003eDegree: 4\",\"Node Pixel Level Cycle Consistency\\u003cbr\\u003eDegree: 3\",\"Node Vectorization\\u003cbr\\u003eDegree: 3\",\"Node ConvolutionalNeuralNetworks\\u003cbr\\u003eDegree: 2\",\"Node Glaucoma Detection\\u003cbr\\u003eDegree: 4\",\"Node Vision Transformers\\u003cbr\\u003eDegree: 4\",\"Node Otsuthreshold\\u003cbr\\u003eDegree: 3\",\"Node Diffusion Maps\\u003cbr\\u003eDegree: 3\",\"Node MNIST Classification\\u003cbr\\u003eDegree: 2\",\"Node LLE\\u003cbr\\u003eDegree: 4\",\"Node Noise Estimator\\u003cbr\\u003eDegree: 4\",\"Node Ledbetter et al.\\u003cbr\\u003eDegree: 2\",\"Node Yann LeCun\\u003cbr\\u003eDegree: 4\",\"Node MSCOCO\\u003cbr\\u003eDegree: 3\",\"Node Mobile Object Tracking\\u003cbr\\u003eDegree: 2\",\"Node Histogram of Oriented Gradients (HOG)\\u003cbr\\u003eDegree: 8\",\"Node Makkah\\u003cbr\\u003eDegree: 3\",\"Node ISIC Challenge\\u003cbr\\u003eDegree: 4\",\"Node CNNs\\u003cbr\\u003eDegree: 19\",\"Node Satellite Images\\u003cbr\\u003eDegree: 5\",\"Node Umrah\\u003cbr\\u003eDegree: 4\",\"Node Skin Color Segmentation\\u003cbr\\u003eDegree: 5\",\"Node OpenHaRT\\u003cbr\\u003eDegree: 3\",\"Node Procrustesanalysis\\u003cbr\\u003eDegree: 4\",\"Node Autoencoder\\u003cbr\\u003eDegree: 3\",\"Node Pedestrian Attributes\\u003cbr\\u003eDegree: 2\",\"Node Semantic Labeling\\u003cbr\\u003eDegree: 4\",\"Node RGBD Cameras\\u003cbr\\u003eDegree: 4\",\"Node UNet\\u003cbr\\u003eDegree: 5\",\"Node Neural Radiance Field\\u003cbr\\u003eDegree: 3\",\"Node Convolutional Models\\u003cbr\\u003eDegree: 4\",\"Node Grid Pattern\\u003cbr\\u003eDegree: 3\",\"Node Photorealistic Style Transfer\\u003cbr\\u003eDegree: 2\",\"Node Triplet Sampling\\u003cbr\\u003eDegree: 3\",\"Node Image Interpolation\\u003cbr\\u003eDegree: 3\",\"Node PASCAL VOC 2012\\u003cbr\\u003eDegree: 2\",\"Node Offline Signature System\\u003cbr\\u003eDegree: 5\",\"Node Crowdsensing\\u003cbr\\u003eDegree: 3\",\"Node Diagnosis\\u003cbr\\u003eDegree: 3\",\"Node Non-linear Functionals\\u003cbr\\u003eDegree: 4\",\"Node Satellite Image Processing\\u003cbr\\u003eDegree: 3\",\"Node Surface Normal Estimation\\u003cbr\\u003eDegree: 4\",\"Node Example-based super-resolution\\u003cbr\\u003eDegree: 2\",\"Node IEEE-ISBIon Bone Texture Characterization\\u003cbr\\u003eDegree: 3\",\"Node Depth Prediction\\u003cbr\\u003eDegree: 4\",\"Node Kolmogorov complexity\\u003cbr\\u003eDegree: 3\",\"Node Fabric Defect Detection\\u003cbr\\u003eDegree: 3\",\"Node Level Sets\\u003cbr\\u003eDegree: 4\",\"Node Fast FourierTransform\\u003cbr\\u003eDegree: 4\",\"Node Information Distance\\u003cbr\\u003eDegree: 3\",\"Node MR-Images\\u003cbr\\u003eDegree: 3\",\"Node Video Processing\\u003cbr\\u003eDegree: 12\",\"Node Faster RCNN\\u003cbr\\u003eDegree: 3\",\"Node Parkinson Disease\\u003cbr\\u003eDegree: 3\",\"Node Pose Embedding\\u003cbr\\u003eDegree: 2\",\"Node Python\\u003cbr\\u003eDegree: 4\",\"Node Jarvis Algorithm\\u003cbr\\u003eDegree: 3\",\"Node Human Actions\\u003cbr\\u003eDegree: 3\",\"Node Texture Independent\\u003cbr\\u003eDegree: 4\",\"Node CMU-PIE\\u003cbr\\u003eDegree: 7\",\"Node Species Analysis\\u003cbr\\u003eDegree: 4\",\"Node M multimodal spectral geometry\\u003cbr\\u003eDegree: 3\",\"Node Parzen\\u003cbr\\u003eDegree: 4\",\"Node Pose Lifting\\u003cbr\\u003eDegree: 4\",\"Node Vision Transformer\\u003cbr\\u003eDegree: 7\",\"Node Color Distance\\u003cbr\\u003eDegree: 2\",\"Node Key Frame Extraction Algorithms\\u003cbr\\u003eDegree: 1\",\"Node Image Recognition\\u003cbr\\u003eDegree: 11\",\"Node Euclidean\\u003cbr\\u003eDegree: 5\",\"Node Bilateral Filter\\u003cbr\\u003eDegree: 3\",\"Node Geodesic Distance\\u003cbr\\u003eDegree: 4\",\"Node X-ray computed tomography\\u003cbr\\u003eDegree: 4\",\"Node Equirectangular Projection\\u003cbr\\u003eDegree: 2\",\"Node Pyramid Vector Quantization\\u003cbr\\u003eDegree: 3\",\"Node Super Resolution Network\\u003cbr\\u003eDegree: 3\",\"Node Tracking Taxonomy\\u003cbr\\u003eDegree: 2\",\"Node Unsupervised Dictionary Learning\\u003cbr\\u003eDegree: 2\",\"Node GoogLeNet\\u003cbr\\u003eDegree: 3\",\"Node Machine Vision\\u003cbr\\u003eDegree: 11\",\"Node discriminativerepresentations\\u003cbr\\u003eDegree: 3\",\"Node Adaptive Linear Interpolation\\u003cbr\\u003eDegree: 4\",\"Node Lip Synchronization\\u003cbr\\u003eDegree: 3\",\"Node Parametric Component Analysis\\u003cbr\\u003eDegree: 3\",\"Node Kalman Filter\\u003cbr\\u003eDegree: 3\",\"Node Phasic dopamine Release\\u003cbr\\u003eDegree: 3\",\"Node Partial Solutions\\u003cbr\\u003eDegree: 3\",\"Node Morphological Operators\\u003cbr\\u003eDegree: 4\",\"Node CycleGAN\\u003cbr\\u003eDegree: 6\",\"Node Range Images\\u003cbr\\u003eDegree: 3\",\"Node High-order\\u003cbr\\u003eDegree: 4\",\"Node Stochastic Gradient Descent\\u003cbr\\u003eDegree: 4\",\"Node Keypoint Prediction\\u003cbr\\u003eDegree: 3\",\"Node YOLO v1\\u003cbr\\u003eDegree: 2\",\"Node 2-D Symbol Recognition\\u003cbr\\u003eDegree: 4\",\"Node Glacial localization\\u003cbr\\u003eDegree: 4\",\"Node Assignments\\u003cbr\\u003eDegree: 2\",\"Node Neuroscience\\u003cbr\\u003eDegree: 2\",\"Node Gabor Surface Feature\\u003cbr\\u003eDegree: 7\",\"Node Behaviour Understanding\\u003cbr\\u003eDegree: 2\",\"Node Safety System\\u003cbr\\u003eDegree: 2\",\"Node EBSR\\u003cbr\\u003eDegree: 2\",\"Node Pose Estimation\\u003cbr\\u003eDegree: 10\",\"Node Multi-target Tracking\\u003cbr\\u003eDegree: 4\",\"Node Android App\\u003cbr\\u003eDegree: 4\",\"Node Person-MinkUNet\\u003cbr\\u003eDegree: 3\",\"Node Line Detection\\u003cbr\\u003eDegree: 4\",\"Node Offline Handwriting Recognition\\u003cbr\\u003eDegree: 3\",\"Node Action Detection\\u003cbr\\u003eDegree: 5\",\"Node Signal Curvature\\u003cbr\\u003eDegree: 2\",\"Node Shanon's\\u003cbr\\u003eDegree: 5\"],\"x\":[0.35802146325715956,-0.12728901874657475,-0.2313883184489302,0.37932148692863255,-0.336627990610311,0.06937191545962786,0.1795869014914811,-0.07859666409593062,0.3097766873483381,0.06352176126627909,0.35467737095478785,-0.2508561289227041,0.1470565039835934,-0.16665091533742601,-0.3420990961487803,0.20633381098486714,-0.3443249446410901,0.29481836665416755,0.5136799542604107,-0.30601511342197146,-0.37752878069259493,0.14291440227544103,0.4444640942907851,0.1608965907188315,-0.27522077501197767,-0.6452977219649615,0.0055930358038620825,-0.5324586439154924,0.3008929768895503,-0.5330680644759629,-0.17798893261201718,0.3440784506403394,-0.003096762211667722,-0.43326286272812614,0.2995847258538968,-0.26522542619625966,0.34782804731496286,0.049328522508159806,0.04681872113616281,0.39911558766455274,0.13123306239294708,0.16281018653965207,-0.4529557510135941,0.05238279382659552,0.7578317135354431,-0.12697604169589957,-0.8975157052560349,-0.31041990494555693,-0.21050276567264867,0.20717024273643025,0.13613075671146055,-0.2853489430024099,-0.46397922854759394,0.20356106326693688,0.16054627857154377,0.030175564405072793,-0.17288422515389917,-0.4904701255608735,0.2066875189072768,0.3961876717337006,0.35481653666982,0.41053910172059754,-0.3371387795492324,-0.15114890331709083,-0.18671272321451288,-0.44206492012847737,-0.05663653832461424,0.5990671011450727,-0.29965137180678686,0.16369988644385897,0.5846603069932143,0.3294125077776689,0.3902574969423922,-0.3189380237812923,-0.2187414062552595,-0.1495739063216861,0.06550835442767675,0.6447492779973604,-0.3532035297627773,0.31382444575974944,-0.04525649422358554,-0.5347475809279216,0.20817803395698897,0.9839684558484487,0.09087402068867718,0.11963495831973896,-0.456108545418915,0.11150918344956297,0.4422536566566158,-0.4434411916247953,-0.28195728372459283,-0.1864588572644126,-0.04055091301155076,-0.22297643194709293,0.31940429439000473,0.17453265194861656,-0.24962714915267492,0.3752376086201889,-0.32702809587560167,0.6019867637806753,-0.41076459971282414,0.07841579937842513,-0.13003583831081006,-0.09110604816279191,0.1890902961594707,-0.19523393094487043,0.37887442378253694,0.9727688532838321,-0.4620598855371922,0.30901432651209554,-0.48846553395784154,0.29640515821314634,0.08940521575722862,0.36002385632264805,-0.35895954063272634,0.27619869573704353,-0.24156551376006546,-0.08904274807422541,-0.6572413154250697,-0.38389722149376426,0.41172380176257367,-0.027688258194792974,0.2731667728753765,-0.27603064263977073,0.2919034835040413,0.08003922710033808,0.23463510980425722,-0.19507660191462672,0.21068533430161263,0.706383504533662,-0.2652618334936792,-0.23373042976057667,0.2916485853018193,-0.5299450733889067,-0.2100976511773641,0.08769698842219174,-0.1757443483642554,0.38312191126574846,0.1487834908852371,0.22944271326723042,0.11384096381898817,-0.2243655011031012,-0.26556354325152703,-0.23438686665116693,-0.2723753581795727,-0.24435265202250844,-0.21245442217735583,0.1900362921676404,-0.24141094975240646,0.3780644033648565,0.20301957564304668,0.2725955813838193,-0.23687880135753614,0.24793127610873328,0.1706630634898999,0.053137722331690666,0.20133526129273188,-0.3173777198161389,-0.2849325763170544,-0.39695777512934566,-0.3561897980492358,0.6941823288213782,0.4079781251683783,0.4619080469980777,0.08241191770775588,0.31740307368494936,0.3630233891422448,-0.3127004645936037,-0.2615337288865682,0.3833601469938797,-0.0811046258787826,-0.3256437401553685,-0.41052348335075456,-0.2547710386737485,0.6168625462231521,-0.03131020025571709,0.14194894105793918,0.6265708269309338,-0.6642943383952763,-0.144686749814993,0.1182357569735683,0.652699643779598,-0.32636092908642256,0.46387374622357397,-0.3755624460982769,-0.45057978119332287,0.07854287120897296,0.4040553680416552,0.7137485613074239,-0.29629988432854404,-0.05625098916420061,-0.3089586970196594,0.41226186919680397,-0.25097815792824024,-0.40212747931893505,0.2193518199824058,0.32678542837044966,-0.1836505333207323,0.2196304275472683,-0.341232963957206,-0.2461872482370283,-0.00493005919142411,0.16581441848574047,0.6713067263576877,0.04439718693558544,-0.36231955840191604,-0.314255494127538,0.2150016907648118,0.3790387836282906,0.17970780832667052,-0.3332341397234369,0.05914784709403482,0.3099201225646745,-0.34181627347178384,0.48881548200266917,-0.46953462631619036,0.02290323808003787,-0.3883830499648306,0.2940367893379515,0.20489771711205892,-0.00930595779806558,-0.3374232405718896,0.2379545845260003,-0.03542711989110437,-0.24785072743817574,-0.2583431024501166,-0.7537293611153583,-0.2472570330106062,-0.48062921463594915,-0.4939680017345201,-0.07829613842283081,-0.5089160627921744,0.38349562863158315,0.14558640763898786,0.13992448703460458,0.5535941155957321,-0.8948474605608381,0.18135772178870596,-0.03860274572188954,-0.22805009965548856,-0.09402260018557479,0.5068511863823153,0.17448262792336158,-0.4567480324717053,-0.2896957306775164,-0.1904785676111387,-0.4428693315091862,-0.3406428068741444,-0.2980396251674858,-0.1604148598201767,0.8396411855098973,0.1345310293546687,0.11205902299391327,-0.38575174347229135,0.6787023823798306,-0.09322760846944347,-0.26074112055363974,-0.024947729861626466,-0.34026147827652853,-0.2469484386894806,0.14341842730052223,0.6653851540788173,-0.04611542283153123,0.45175934543539276,-0.032914440909998774,0.36252340605732686,-0.0015688344355572715,-0.1507048737986401,0.3753392139893115,0.30607930557133256,-0.35137546983663787,0.3285570414267141,0.16958236960938491,-0.1707402344599611,0.1883944039782016,-0.22963198675309393,0.44040724094224254,0.506318065890254,-0.2356170238850429,-0.1792167006492603,0.18667681403372977,0.40421041765369375,0.4645533205826636,0.057334084694421625,-0.17644192235806314,-0.02670321717437541,0.34065069034019135,0.2620548937568489,-0.35202536416758207,0.1722507872877681,-0.09258786841158448,0.05344183832425574,0.3178707894002735,-0.02108142461924934,0.3334973209636406,-0.13232578990474772,-0.26953468484437026,-0.33874929577940854,-0.6824709878651782,-0.33180714124855976,-0.6436347608996995,-0.2929541265930888,0.31414515220677697,-0.006885414194733128,-0.10652205133493349,0.7001794223369159,-0.2764873500848596,-0.11952935068819252,-0.2089898630849497,0.2500974731006018,-0.28789257980080607,0.17940852423182896,0.21817281240266825,0.18962833325795828,-0.09442219372406183,0.1853649318823246,0.1395106533602813,-0.17152550671702288,0.3383665933113078,0.13785619470043894,-0.33325379630751584,0.7560242176572916,0.4489634541441082,-0.332172199275611,-0.26979617963088337,0.07311632108879322,-0.17559766123972692,0.644682038075124,0.0877521067971623,-0.6704667391493794,-0.3184832378126542,-0.21382953176850616,-0.04698230570917894,-0.24170688735741838,0.4140408635038966,-0.29804063625835997,-0.20164554872031096,-0.31838535923286315,-0.1990101795123446,0.531659483070264,-0.24417091013762704,0.05361689161600848,0.4270851889189868,0.6292907642573967,0.13236420791083955,-0.4728855558167171,0.3379588916709153,-0.22670041266116028,0.21185040164655008,0.9762208561537624,-0.09125897197569359,-0.11199051836344989,0.02610275324395288,0.08005275191518312,-0.14890401981238952,0.1024211626106067,-0.40352085698302403,-0.21228882667771784,-0.5167114869660626,0.3901658860838829,-0.31949532406570397,0.3307339418375218,0.038126884768340766,-0.4756606814966038,0.3088416552061604,0.5412752971026304,-0.5415048957640967,0.36690844037313647,0.06642535210665855,0.5572920587787685,0.16251134383437724,-0.5054384017048197,-0.015797701573913855,0.1842235820026506,0.16202457842932785,-0.04336858531685759,0.15067194039703355,0.18321757063648267,-0.30893512627664627,-0.0499352311458284,-0.2292534555429686,0.6554322070713565,-0.22055460878200367,-0.4469873318740338,-0.2974401388998378,0.04414693753220777,-0.08273244370801588,0.04089838824983995,-0.4095024600014184,0.16482404617764268,0.2950075103704614,-0.26719625390059504,0.31758394433330916,-0.30705829483340735,0.1523153183089973,0.4165804572631586,0.42210454820204046,-0.19387517514263394,-0.3685271366320466,0.6846786709157807,-0.6754853648670038,-0.4334716660933204,-0.13243327461895996,-0.391269072491725,-0.36676844222740407,-0.4144616082784801,-0.26334457784291787,0.5859485874111335,-0.8853133311253923,0.35973951436128465,-0.2682955881388224,-0.3215109553880422,0.1574609173018652,-0.2923860576960361,0.11399466396032104,-0.12876266091326669,0.39921730961513135,-0.18657760563145134,-0.018290606821115094,0.1916246687135562,0.021856445723820242,-0.3792302693245371,-0.2359433423418696,0.3814752508259371,-0.42081542953672807,0.359962953341593,0.4353067119261765,-0.30278858348574605,-0.28702363465327146,-0.2718184307114589],\"y\":[-0.09473908054505915,-0.06082007419998915,-0.24258385757215384,-0.28420673827069143,0.07076787941052783,0.3951549430380065,0.3141020725811047,-0.12274799089163504,-0.11955193531236262,0.3431749038863376,-0.1291038296578684,-0.18523445970736696,0.254615132898204,-0.16432945296348467,0.17370039481023547,0.6546017394087829,-0.12233588290712585,-0.30366419714311954,-0.41263598048119,-0.2586971650787668,-0.4274051946433234,0.11236105301195334,-0.5620521284734261,0.6597607598730224,0.18835058194077514,0.12592559255430438,0.2718365825839619,0.35369556781065925,0.506262691980272,-0.11482663470450603,-0.3770264968912099,-0.30794849802697655,0.01468944905554162,0.33498539482890716,0.579577193174421,-0.46716096085448283,0.6533078942993944,0.3065688157047326,0.33559018541953684,-0.09958251283662231,0.3804093953128879,-0.5581548831803843,0.47961927862615317,0.3942813706415176,-0.22721869560542146,-1.0,0.19440032776866975,0.5310158325874729,0.29878873457655236,0.23778259648591643,0.3132392970495637,-0.50326941880636,-0.3055838737635453,-0.3179027491094956,-0.2607777315002243,-0.149092400476577,0.48813248042244944,-0.19694404797122134,-0.3494543011162388,-0.22219483872893078,-0.07423880547523237,-0.3354095993759736,0.5941155381969414,-0.39730755900042214,0.47959182577550863,0.8622150066205894,0.4493120540293686,-0.528428217263766,-0.08426985966657831,-0.5369972624122011,-0.5465194355149727,-0.11138942615615467,-0.3087611695043734,-0.15988802912569328,-0.35521836373331095,-0.03224720983741994,0.6861005384827137,-0.5207149603943795,-0.04111667540525816,0.518265287641295,0.09222520204066263,0.37451399347830866,0.21735276335940829,-0.263591124755981,0.052649666840792954,0.266023747764293,-0.2554792055879162,-0.5624230063059268,-0.35791354719913365,-0.18127835200762124,0.5663542233806376,-0.07010313110333175,-0.10536589546082997,-0.008583710418954723,-0.17235945649930776,0.11918769638471641,-0.050399974127980966,-0.12146979378652997,0.38761342108081936,-0.5541772559348288,0.8592776505394563,-0.20846962646702585,-0.13066228146212297,0.2843988726533322,-0.3349392610969492,-0.15401501457085723,-0.41904369022875587,-0.27996868292741217,0.8588965393840657,0.5733737388384338,-0.28222981032767774,0.5244992806762039,-0.23887250688075642,-0.18942984184687633,-0.443199296871594,-0.3361548289161914,-0.09200102558546301,0.00025612200150601744,0.08602485722027778,0.2951019654709796,-0.1439789199736273,0.06325564080489869,0.5603881322583837,-0.1127593069977425,0.5880525395555292,0.3057573173510207,-0.3615740065449596,-0.40024016499261583,0.26160188857133465,-0.14374444529798758,-0.3473607165809309,-0.18983859252456353,-0.3517239904887218,-0.08320674279248072,-0.7782674490851623,-0.24745149731883143,-0.09909122076099709,-0.09044651229440363,0.503358480185972,-0.3438704698833398,0.385318759069059,-0.21844932421191654,-0.36583053229955315,0.24265224161802026,0.16739073915131036,-0.01598159204432383,0.03501815974823906,0.01408884526820556,0.40512099193755413,-0.18003340801785414,-0.5264419572020417,-0.2905153570996774,-0.6197217261102363,-0.18619444411252264,0.26710866766898106,-0.31539317321703486,0.2789016591966121,0.06297236244816794,0.21394867572571552,-0.17569245652442902,0.29739027270763907,-0.12653258859693792,0.745872733958323,-0.21493607388453514,0.6723726360448772,-0.21191043533431878,0.6391017781762831,-0.20196141404205742,-0.3049620133078615,-0.2003527609540083,-0.015402010014341454,0.7982780026909637,-0.1490487361713442,0.308033277786985,-0.541320567250647,0.09175699714192388,-0.39634162035385134,-0.5171286673489337,0.10521408321066524,-0.23295063620578352,0.28990418153785036,-0.5024162905092944,0.3004791095447427,-0.5506125182696979,-0.2729992278294885,-0.1594006090732356,0.07690136371217947,-0.24831201015029286,-0.12195105897445904,0.24701501139127843,0.10292076687032965,0.79045574517452,-0.22926406759548967,0.1085929171523956,0.2452836542230963,-0.5383100084616208,-0.08778813364244716,0.1212639111459302,0.5557997537489542,0.7839455107548775,-0.06642716051546609,0.27415841801808505,0.005702495627258031,-0.32827514178451705,-0.12739979661233738,0.28085655268921983,0.2121067696568312,0.6376086499875915,0.5530246130869145,0.006149652292808575,0.5275680954238605,0.6655440508353427,-0.3408794941935949,0.39593031318845046,0.647746579709601,0.8387218736722715,-0.46428446534308687,0.2617417492897248,-0.1919979571571069,-0.5537058465216298,0.34410597769029616,0.3597555982236658,0.5389110475017207,0.44604187208914126,-0.3730988460220396,-0.5033085609723988,0.16073225673217204,0.23016917731583136,-0.29924112560559535,-0.21785857280854873,-0.14599749971722017,0.4014135031936428,-0.23945112261865942,0.08560768557096833,-0.18823801006385443,-0.1428074375587611,0.17474549952034865,0.24412493693926027,0.10877648537691828,0.2938682600004126,-0.1833406496335737,0.640287735166148,0.6415580880236341,0.4616782612452741,-0.08967916906965047,-0.11688158779776744,0.318219656918684,-0.4530630683994992,0.5667672134842375,-0.12278814593361007,-0.24582740485687266,-0.5352352554220844,0.3172362351302249,-0.2573360959957922,-0.29648274283588866,0.13349566988592435,0.20780692008662935,0.10540746857225657,0.30894671355136255,-0.23676427856980825,-0.5688892520783643,-0.4734834205993846,-0.11944601209031362,-0.16116816684656904,0.2145491517159694,-0.2131560247888121,0.1997562469747687,-0.3728646615254483,-0.32458218646632025,-0.09512268355096269,0.13998980537333214,-0.19268966830034723,0.425001856679912,0.2238684081605373,0.21540927585792963,-0.3865686063792784,-0.2509636185152038,0.6207718033590545,0.11606574454120086,-0.15363859377901015,0.0981341191941886,-0.11735359751284857,-0.19868126119735063,0.1886028543500137,-0.8582642246862597,0.07588138015509364,-0.35698018824925504,-0.26722440058438873,-0.07028601118818568,0.06180501724584363,0.21577549245314886,0.5221126583114559,0.49943140444975304,0.3309733858067373,0.6195368610878262,-0.15461997176419787,0.2514239849304563,0.124883029616991,0.13132351058790828,-0.19072842206004084,0.10419531045106283,-0.2619641311660235,-0.25040972511142157,0.2849445660598289,-0.029584526978025834,-0.10479584645712924,0.014441149754839167,-0.9829656234714816,-0.20055101774792666,-0.2907322999340731,-0.025648300008902995,-0.5653139543165341,-0.360236873171928,-0.5398906122194267,0.37843692938560886,-0.317820369700927,-0.08203170140299136,-0.17693417457559646,-0.32766176601098457,0.33162186363018303,0.15750947573299953,-0.2459278200545944,-0.14269091117895255,0.1980625658620864,0.575613665108678,-0.018776757671173976,-0.40021342415754274,-0.4755206106804721,0.28348276948518963,0.1602522709663446,0.17752001827538338,-0.07084393033765604,0.07069867077690756,-0.2073009583986664,-0.17893381660834062,-0.07291821986559399,-0.37030126941167885,0.4009207345818591,-0.06609687589514415,-0.27053688795599407,-0.3524958249757812,-0.2549894052920728,0.7409472282323009,-0.4977716600081553,-0.5535395370454727,-0.24248646465676577,-0.25338950384913905,-0.2563194451866121,-0.33115114853832545,-0.24573382045342956,0.20097900949511968,-0.14095356694899133,0.318922025205062,0.33406777440450847,-0.9940605143642297,-0.5424432011849858,-0.15887028102092338,-0.38984150531609296,0.37695327074156576,-0.26959520084182204,0.13491862699461749,-0.2754966802088814,0.6824995656981749,-0.2189904982597784,-0.27209373589842234,-0.45820637160626865,-0.09751548399638116,-0.2629302652147024,-0.025141016094927415,-0.43988845167183815,0.3060347413794392,0.09915528791016963,0.18824116219835,0.2939437129939946,0.22675600821213018,-0.08769625905626616,-0.17013989765121723,0.6609805333929283,0.15974965178049294,0.11669612500080291,0.035929639045516215,-0.45466075901141983,-0.7897003552318688,0.8406738436433908,0.6516218795677106,-0.4633522773668628,0.44845638310596414,0.039148380299295654,-0.1717537206387508,0.09662585050243096,-0.3285659796201382,0.29929500596893654,-0.309397667143193,0.07548617014643645,0.27954542884156214,-0.1993033227970437,-0.31641033915165484,-0.20981528519832734,-0.08410587306394082,-0.3144553106404237,0.14589954402870595,0.4836782572866589,-0.3847103358879526,0.8737295511080929,-0.05499295045622741,0.8796470292172429,-0.07996548652059092,-0.21804434281335933,0.20889158696368113,-0.28499666387946154,-0.05250722528903687,0.6004349393456646,-0.1912794137233783,0.1956563171328315,0.33570124808116836,-0.08580695485443018,-0.1605923937704407,-0.16797292313505285,-0.08774310123087792,0.0728193174884413,0.6704270004850482,0.7284613981228892,-0.7841655875793423,-0.1406472191013985,0.3498643543890353,-0.14882561163665764,0.723750950241385,0.047393329650638825,-0.21413781468010457,-0.5061353006305839],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"showgrid\":false,\"zeroline\":false,\"showticklabels\":false},\"yaxis\":{\"showgrid\":false,\"zeroline\":false,\"showticklabels\":false},\"title\":{\"text\":\"Interactive Visualization of Top-3 Communities\"},\"showlegend\":false,\"plot_bgcolor\":\"white\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('8252a310-19db-4718-ba40-54383b3f6899');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Оценка центральностей\n",
        "\n",
        "Вычисляем: degree, betweenness, closeness, eigenvector.\n",
        "Выводим топ-5 узлов по каждой метрике. Интерпретация:\n",
        "- degree → локальная важность,\n",
        "- betweenness → «мосты» между кластерами,\n",
        "- eigenvector → влияние через влиятельных соседей.\n"
      ],
      "metadata": {
        "id": "0epn3KEkKUNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# top-10 nodes by degree centrality\n",
        "[(node[0], round(node[1],2))\n",
        "for node in sorted(nx.degree_centrality(S).items(), key=itemgetter(1), reverse=True)][:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ku5QCls_KM14",
        "outputId": "bbf4e5c3-d1e7-4e3e-d67b-d2ef3bdecd4c"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Computer Vision', 0.22),\n",
              " ('Deep Learning', 0.14),\n",
              " ('Convolutional Neural Networks', 0.13),\n",
              " ('CNN', 0.11),\n",
              " ('Face Recognition', 0.07),\n",
              " ('SVM', 0.07),\n",
              " ('Convolutional Neural Network', 0.05),\n",
              " ('Semantic Segmentation', 0.05),\n",
              " ('CNNs', 0.04),\n",
              " ('Clustering', 0.04)]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# top nodes by degree centrality in top-3 communities\n",
        "\n",
        "for i, nodes in enumerate(top3_comm[\"nodes\"].tolist(), start=1):\n",
        "  s = G.subgraph(nodes)\n",
        "  top_node = max(nx.degree_centrality(s).items(), key=itemgetter(1))\n",
        "\n",
        "  top3_nodes = [(node[0], round(node[1],2))\n",
        "  for node in sorted(nx.degree_centrality(s).items(), key=itemgetter(1), reverse=True)][:3]\n",
        "\n",
        "  print(f\"# of cluster: {i}, Central node by degree: {(top_node[0], round(top_node[1], 2))}\")\n",
        "  print(f\"Top-3 nodes: {top3_nodes}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TrVgoLtKMzH",
        "outputId": "6efbca97-041f-4c52-95e7-eb19455ee0ca"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of cluster: 1, Central node by degree: ('Deep Learning', 0.33)\n",
            "Top-3 nodes: [('Deep Learning', 0.33), ('CNN', 0.2), ('Convolutional Neural Network', 0.13)]\n",
            "\n",
            "# of cluster: 2, Central node by degree: ('Convolutional Neural Networks', 0.34)\n",
            "Top-3 nodes: [('Convolutional Neural Networks', 0.34), ('SVM', 0.22), ('CNNs', 0.13)]\n",
            "\n",
            "# of cluster: 3, Central node by degree: ('Computer Vision', 0.65)\n",
            "Top-3 nodes: [('Computer Vision', 0.65), ('Semantic Segmentation', 0.17), ('Clustering', 0.14)]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = {\n",
        "    \"degree\": nx.degree_centrality(S),\n",
        "    \"betweenness\": nx.betweenness_centrality(S),\n",
        "    \"closeness\": nx.closeness_centrality(S),\n",
        "    \"eigenvector\": nx.eigenvector_centrality(S)\n",
        "}\n",
        "\n",
        "for name, values in metrics.items():\n",
        "    top5 = sorted(values.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "    print(f\"\\nTop-5 {name} centrality:\")\n",
        "    for node, val in top5:\n",
        "        print(f\"{node}: {val:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Y7GscJMTend",
        "outputId": "2ff30040-444d-4e10-a5a7-790f8a16ab69"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top-5 degree centrality:\n",
            "Computer Vision: 0.224\n",
            "Deep Learning: 0.142\n",
            "Convolutional Neural Networks: 0.128\n",
            "CNN: 0.107\n",
            "Face Recognition: 0.072\n",
            "\n",
            "Top-5 betweenness centrality:\n",
            "Computer Vision: 0.488\n",
            "Convolutional Neural Networks: 0.325\n",
            "Deep Learning: 0.290\n",
            "CNN: 0.288\n",
            "Convolutional Neural Network: 0.103\n",
            "\n",
            "Top-5 closeness centrality:\n",
            "Computer Vision: 0.462\n",
            "CNN: 0.455\n",
            "Convolutional Neural Networks: 0.447\n",
            "Deep Learning: 0.436\n",
            "Face Recognition: 0.379\n",
            "\n",
            "Top-5 eigenvector centrality:\n",
            "Computer Vision: 0.476\n",
            "Convolutional Neural Networks: 0.298\n",
            "Deep Learning: 0.268\n",
            "CNN: 0.242\n",
            "SVM: 0.155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Граф публикаций и поиск похожих статей\n",
        "\n",
        "Узлы = статьи. Ребро между статьями если у них есть общие ключевые слова.\n",
        "Вес = число общих ключевых слов.\n",
        "\n",
        "Реализована функция `find_similar_papers(paper_title, graph, top_n=5)` для поиска похожих работ.\n"
      ],
      "metadata": {
        "id": "1waeqb7IMTPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "G_papers = nx.Graph()\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    G_papers.add_node(row['title'], abstract=row['abstract'], authors=row['authors'])\n",
        "\n",
        "for i, row1 in df.iterrows():\n",
        "    keywords1 = set(kw.strip() for kw in row1['keywords'].split(','))\n",
        "    for j, row2 in df.iterrows():\n",
        "        if i < j:\n",
        "            keywords2 = set(kw.strip() for kw in row2['keywords'].split(','))\n",
        "            common_keywords = keywords1.intersection(keywords2)\n",
        "            weight = len(common_keywords)\n",
        "            if weight > 0:\n",
        "                G_papers.add_edge(row1['title'], row2['title'], weight=weight)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "pos = nx.spring_layout(G_papers, k=0.5, iterations=50)\n",
        "nx.draw(G_papers, pos, node_size=20, alpha=0.6, edge_color='gray', width=0.5)\n",
        "plt.title(\"Граф публикаций по компьютерному зрению\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 859
        },
        "id": "k_fxnBw2MV8l",
        "outputId": "9b3a6b61-8f05-49b9-8533-146fa2537a32"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABMQAAANKCAYAAABlLZLcAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQABAABJREFUeJzs3Xl4XGd5N/7vObOvkmZGu2XZlm15kW3JsmUnOHEgIYkJIQlp+xISEpZAyvKytAUCtJBAS5LSviX8UqAkFFriUAqEhBQcSEpCFiLLuyx5l2TJskbLjLbZ1/P7Q5nJjBZby2xn5vu5rlzg0WjOc86M5pxzP/d9P4IkSRKIiIiIiIiIiIgKhJjtARAREREREREREWUSA2JERERERERERFRQGBAjIiIiIiIiIqKCwoAYEREREREREREVFAbEiIiIiIiIiIiooDAgRkREREREREREBYUBMSIiIiIiIiIiKigMiBERERERERERUUFhQIyIiCgHuVwunD9/Hh6PJ9tDISIiIiLKOwyIERER5QBJkvCDH/wAO3fuhF6vh9lsxsqVK/Hkk09me2g5paOjA88880z830ePHsVvfvOb7A2IiIiIiGSJATEiIsqIH//4xxAEYc7/+vv7Mzoeo9GID37wgxnd5qW8//3vx1/+5V9i/fr1+MlPfoIXXngBL774It773vdme2g5xeVy4b777kNrayvOnj2Lz3zmMzh+/Hi2h0VEREREMqPM9gCIiKiwfP3rX8fKlStnPG6xWLIwmtzwn//5n/jZz36GJ598Eu9///uzPZycdsUVV8T/A4C1a9fiox/9aJZHRURERERyw4AYERFl1J49e7Bt27ZsDyOnfOtb38Idd9zBYNg8PfPMMzhx4gR8Ph82bdoEtVqd7SERERERkcywZJKIiHJKrLTylVdewX333Qer1Qqz2Yy7774bY2NjSc999tlncdNNN6GqqgoajQZ1dXX4xje+gUgkkvS8aDSKz3/+8ygqKsKKFSvw/PPPx3/2xS9+ESaTCWvWrMG+ffvmPb65/nvggQcAAC+99BIEQcCvfvWrGa/x1FNPQRAEvPHGG/B4POjo6EBNTQ1uuukmmM1mGAwGXHPNNXj11VdnHcM111wz67Z//OMfJz3nmmuuSfq9AwcOxJ+bKHHcABAOh/Gud70LFosFJ06ciD/+ox/9CO94xztQVlYGjUaDDRs24Hvf+96M8a1YsQLvfve7Zzz+qU99asa2H3jggRmPud1uVFRUQBAEvPzyy7Pu04YNG9Dc3Ixjx47Nuk+zmeu4xf47f/580vO/+93vYuPGjdBoNKiqqsInP/lJjI+PX3Y7s+3TSy+9BI1Gg7/8y79MevzIkSPYs2cPzGYzjEYjrr32WrS2tiY9J/aZU6vVGBkZSfrZG2+8ER//wYMHF7Wvc71f003/nAwODuLuu+9GaWkpNBoNGhoa8Pjjj8d/fv78+UuOQRCEpLLl8fFxfPazn0VNTQ00Gg1Wr16NRx55BNFodMZr/tM//RP+5V/+BbW1tdDpdNi9ezc6OjqSxvvBD34QRqNxxn784he/mPHZAoCf//znaG5uhk6ng81mw1133YWLFy/OeE1BENDY2DjjdR966CEIgpC0zd27d2PLli2zHs/6+nrccMMNs/4s5pZbbsGKFSug1WpRVlaG97znPTNKhAVBwKc+9Sns3bsX9fX10Gq1aG5uxiuvvDLj9S5evIgPf/jDKC8vh0ajwcaNG/Hv//7vSc95+eWXIQgCfvGLX8z4/eml5rHPZuLnKRqNYvPmzTO+k2b7u9i7dy8aGxuh1WphtVpxxx13oK+v75LHhIiIKFWYIUZERDnpU5/6FIqLi/HAAw/g9OnT+N73vofe3t74zRowdTNmNBrxV3/1VzAajfjDH/6Ar371q5icnMS3vvWt+Gs98sgj+Kd/+id84AMfQHNzMz73uc8hGAziN7/5DRobG/EP//APeOKJJ/De974XJ06cmLWkc7rppZ9utxsf//jH4/++5pprUFNTg7179+K2225L+t29e/eirq4OV1xxRfzm75FHHkFFRQU+//nPQ6vV4vHHH8d1112HF154AVdfffWM7a9btw5f+cpXAAAOhwOf+9znLjvmL37xi5d9DgDce++9ePnll/HCCy9gw4YN8ce/973vYePGjXjPe94DpVKJ5557Dp/4xCcQjUbxyU9+cl6vPR///M//jKGhoXk9d777FLNs2TI89NBDSY/99re/xU9/+tOkxx544AE8+OCDuO666/Dxj388/hk8cOAAXn/9dahUqnlv89ixY7j11lvxrne9C//6r/8af7yzsxNXXXUVzGYzvvCFL0ClUuHf/u3fcM011+CPf/wjduzYkfQ6CoUCTz75ZNJ7/aMf/QharRZ+v3/R+7oYwWAQ1113HU6dOoWPf/zjqK+vxzPPPIOPfexjcDqduP/++1FaWoqf/OQn8d95+umn8atf/Srpsbq6OgCA1+vF7t27cfHiRdx3331Yvnw5/vSnP+FLX/oS7HY7vv3tbydt/z//8z/hcrnwyU9+En6/H48++ije8Y534Pjx4ygvL1/w/vz4xz/Ghz70IWzfvh0PPfQQhoaG8Oijj+L111/HkSNHUFxcHH+uUqlEZ2cnjhw5gqampqTX0Gq1Sa/7gQ98AB/96EfR0dGBhoaG+OMHDhzAmTNn8Ld/+7eXHdvHPvYxVFRUYGBgAI899hiuu+469PT0QK/Xx5/zxz/+ET/72c/w6U9/GhqNBt/97ndx4403oq2tLb7doaEh7Ny5Mx5AKy0txb59+/CRj3wEk5OT+OxnP7vg4zabn/zkJ/Pq6/fUU0/hrrvuwpYtW/DQQw/B6XTiO9/5Dl577TUcOXIENpstJeMhIiKak0RERJQBP/rRjyQA0oEDB+b1vObmZikYDMYf/8d//EcJgPTss8/GH/N6vTN+/7777pP0er3k9/slSZIkv98vlZWVSXfccUf8OceOHZMUCoW0ZcsWKRAISJIkSQ6HQzKZTNJnPvOZRe3HyMiIBED62te+Fn/sS1/6kqTRaKTx8fH4Y8PDw5JSqYw/r6enRwIgqdVq6cyZM0mvZ7Vapebm5hljeNvb3ia9/e1vj/879ho/+tGP4o/t3r1b2r17d/zfv/3tbyUA0o033ihNP/0njvtLX/qSpFAopGeeeWbGdmc73jfccIO0atWqpMdqa2ulm266acZzP/nJT87Y9te+9rWkx4aHhyWTySTt2bNHAiC99NJLi9qn2ezevVvauHHjjMe/9a1vSQCknp6e+BjUarV0/fXXS5FIJP68xx57TAIg/fu///slt5O4T+fPn5cqKyulXbt2ST6fL+l5t956q6RWq6Wurq74YwMDA5LJZJKuvvrq+GOxz9wdd9whbdq0Kf64x+ORzGaz9P73v3/GZ3K++ypJc79f0yV+Tv6//+//kwBI3//+9+M/D4fD0rXXXitpNBrJ4XBc8rhM941vfEMyGAxJfwOSJEn333+/pFAopL6+PkmS3vqs63Q6qb+/P/68/fv3SwCkz33uc/HH7rnnHslgMMzY1s9//vOkz1YwGJTKysqkhoaGpPfof/7nfyQA0le/+tUZr3nzzTdLn/rUp+KPv/rqq5JOp5NuvfXWpG2Oj49LWq1W+uIXv5g0hk9/+tOSwWCQ3G73rMdjLv/93/8tAZAOHjwYfwzAjMd6e3slrVYr3XbbbfHHPvKRj0iVlZUz3pv3ve99UlFRUfzv+6WXXpIASD//+c9nbN9gMEj33HNP/N+xz2bs8+T3+6Xly5fH/34Tv5MS3/9wOCyVl5dLdXV1Scfg5ZdflgBIf/3Xf72g40JERLQYLJkkIqKc9LGPfSwpC+fjH/84lEolfvvb38Yf0+l08f/vcrngcDhw1VVXwev14tSpUwCA48ePY3h4OGm1xs2bN0Or1aKxsTHef8pqteLqq6/G//7v/6ZsH+6++24EAoGk0qOf/exnCIfDuOuuu5Kee8stt2DNmjXxf9tsNnzwgx/EoUOHZmRLBYNBaDSaeY9DkiR86Utfwu233z4j6yjRY489hoceegjf+c53cMstt8z4eeLxnpiYgMPhwO7du9Hd3Y2JiYl5j+dSvvGNb6CoqAif/vSnL/m8+e7TYrz44osIBoP47Gc/C1F861Lpox/9KMxmM37zm9/M63WcTiduuOEGmEwm/PrXv07KHopEIvj973+PW2+9FatWrYo/XllZife///147bXXMDk5mfR6H/jAB3Dq1Kl4aeQvf/lLFBUV4dprr13K7gIAQqEQHA4HnE4nwuHwnM/zer1wOBx47rnnUFRUhA9/+MPxnykUCnz2s59FIBDAiy++uKDt//znP8dVV12FkpISOByO+H/XXXcdIpHIjPK/W2+9FdXV1fF/t7S0YMeOHUnfDzGJr+dwOOByuZJ+fvDgQQwPD+MTn/hE0nt00003Yd26dbO+3x/+8Ifx1FNPIRAIAJjK1Hvve9+LoqKipOcVFRXhlltuwU9/+lNIkgRg6r3/2c9+hltvvRUGg+GyxyZ2zI8ePYrHH38c5eXlWLt2bdJzrrjiCjQ3N8f/vXz5ctxyyy343e9+h0gkAkmS8Mtf/hI333wzJElKOh433HADJiYmcPjw4aTXjH2nJv53Of/6r/8Kp9OJr33ta3M+x+Fw4OWXX8bQ0BDuu+++pGOwe/duNDc3z/tvjIiIaCkYECMiopyUGBwCpnrXVFZWJvWq6ezsxG233YaioiKYzWaUlpbGA02xAM2FCxcAIOnmeS7V1dXx56fCunXrsH37duzduzf+2N69e7Fz506sXr0aAOLln+vWrZvx++vXrweAGb2txsfHZ+2NNJe9e/eis7MT3/zmN+d8zr59+/CZz3wGADA6Ojrrc15//XVcd911MBgMKC4uRmlpKb785S8DQEoCYj09Pfi3f/s3PPjggzNKz6abzz4tVm9vL4CpHk+J1Go1Vq1aFf/55bz73e/G6dOnMT4+Hg+GxIyMjMDr9c7YBjD1vkej0RmfxdLSUtx0003xnk///u//jnvuuScpaLdYv//971FaWgqbzQatVoutW7fi97///Yznfetb30JpaSl+//vfY9WqVTNKR+f6zF7O2bNn8fzzz6O0tDTpv+uuuw4AMDw8nPT86d8PwNSKo9O36/F4ZrxmYhAPmPv9Bqb+Lmd7v2+66SYolUo8++yz8Hg8+O///m986EMfmnXf7r77bvT19cV7Ar744osYGhrCBz7wgTmORrKvf/3rKC0tRVNTE86fP4+XX34ZJpMp6TlzHQ+v14uRkRGMjIxgfHwcP/jBD2Ycj9i4px/jD3/4wzOe6/F45hznxMQEvvnNb+Kv/uqvLlm2mvi+zvX5X+jnh4iIaDHYQ4yIiGRpfHwcu3fvhtlsxte//nXU1dVBq9Xi8OHD+OIXvxhvxD1bb6VL8fl8KR3n3Xffjc985jPo7+9HIBBAa2srHnvssfjPE7Ou5mtwcPCyzbhjgsEg/u7v/g4f+chHZmSVJGpra8NHP/pRGAwG/P3f/z3+/M//POlmtaurC9deey3WrVuH//f//h9qamqgVqvx29/+Fv/yL/+S1Ph8sb7yla9gzZo1uOeee+ZcUGAh+5Rtp06dwr59+/AXf/EX+Ou//mv86Ec/WvJrfvjDH8bdd9+N//t//y9eeeUVPPHEE5c8VvO1Y8cO/P3f/z0AYGBgAI888ghuu+02dHZ2YsWKFfHnfeADH8Ddd9+d1C8vFaLRKN75znfiC1/4wqw/X+z7rNVq8dxzzyU99uqrr+LrX//6ol4vRqVS4a677sKPfvQjeL1eWK1WvOMd70jqjxZzww03oLy8HE8++SSuvvpqPPnkk6ioqIgHhS7n3nvvxbXXXov+/n78y7/8C26//Xb86U9/mpGNdimxv8+77roL99xzz6zP2bx5c9K/v/rVr+Kqq65Keuzmm2+ecxuPPPIIRFHE5z//eTidzjmf98ILL+CNN97AV7/61fkOn4iIKC0YECMiopx09uxZvP3tb4//2+12w263413veheAqZXQnE4nnn766aSm8z09PUmvU1lZCWDqJv9yLl68iKqqqlQMP+5973sf/uqv/go//elP4fP5oFKp8H/+z/+J/9xms8FoNOL06dMzfjdW9pkYkOjv74fL5Ypn4lzOd7/7XQwPDyetDjibd77znfje974Hv98fb46euIDBc889h0AggF//+tdYvnx5/PdeeumleY3jco4cOYL/+q//wjPPPAOFQnHJ5853nxartrYWAHD69OmkcsZgMIienp55BzJ+/etf46qrrsJDDz2ET33qU7jrrrvi5Y2lpaXQ6/Vzvu+iKKKmpmbGz/bs2QOtVov3ve992LVrF+rq6lISELPZbEn7tXr1arztbW/DK6+8kvT5W7VqFa677jqsXbsWr7/+OkKhUFKW2Gyf2fmoq6uD2+2e97E9e/bsjMfOnDkzY7sKhWLGa05fKTTx/X7HO96R9LPTp0/Hfz7dhz/8YWzZsgUXLlzAPffcM+dKpwqFAu9///vx4x//GI888gieeeYZfPSjH73s5zxm9erV8YzS6667DsuXL8dTTz2VFJSc63jo9XqUlpYCAEwmEyKRyLyP8aZNm2Y8d64xDwwM4NFHH8VDDz0Ek8l0yYDYddddh6KiInz1q1+d8/O/0M8PERHRYrBkkoiIctIPfvADhEKh+L+/973vIRwOY8+ePQDeujFLLEULBoP47ne/m/Q627dvh06nw69+9av4Y+3t7fD7/Th69CiCwSCAqTLBV155ZdYVHZfCZrNhz549ePLJJ7F3717ceOONSauniaKIG2+8Ec8++2xSMG90dBT/8R//gW3btiWVH/3Xf/0XAMy4cZ+Ny+XCP/zDP+Bzn/scKioqLvncK6+8EgqFAgaDAd///vfxyiuv4PHHH4//fLbjPTExkZKsJwC4//778ba3vQ3vec97Lvm8hezTYl133XVQq9X4zne+k7S/P/zhDzExMYGbbrppXq8Ty675xCc+gSuvvBL33XdfPANRoVDg+uuvx7PPPptUHjY0NISnnnoKu3btgtlsnvGaSqUSd999N9rb22eU/qVSLKNorgDIjTfeiImJCfz4xz9O+p1HH30UGo1m3kGXmL/4i7/AG2+8gd/97nczfjY+Pj6jr9kzzzyDixcvxv/d1taG/fv3x78fFmLbtm0oKyvD97///XhPMGCqjPjkyZNzvt8bN25Ec3MzTpw4gQ9+8IOX3MYHPvABjI2N4b777oPb7Z7RQ3C+Yn28EscJAG+88UZSD7ALFy7g2WefxfXXXw+FQgGFQoHbb78dv/zlL9HR0THjdUdGRhY1npgHH3wQ5eXl+Mu//Mt5Pb+xsRHl5eV4/PHH4fV644+/+uqrOHjwIN797ncvaTxERETzwQwxIiLKScFgENdeey3+4i/+AqdPn8Z3v/td7Nq1Kx4wufLKK1FSUoJ77rkHn/70pyEIAn7yk5/M6NVkMBjwmc98Bg8//DCUSiW2bt2K73//+xBFEXa7HTfddBPe85734IknnkAgEMDf/M3fpHxf7r77bvzZn/0ZgKmm8dN9/etfx/PPP49du3bhE5/4BDQaDR5//HFMTEzgn//5nwFMBUq+9rWv4YknnsD73ve+WXuOTXf48GHYbLY5y9DmcsMNN+Cuu+7CF77wBdx8882orKzE9ddfD7VajZtvvjl+U//444+jrKwMdrt9xmuMjIzg+eefT3qsr68PAPD888+jpaUFFosl/rPf//73eP3119O2TwtRWlqKL33pS3jwwQdx44034j3veU/8M7h9+/YFBzMEQcATTzyBxsZGfO1rX8M//uM/AgD+/u//Hi+88EL8fVcqlfi3f/s3BAKB+HNm841vfAOf//znUVJSsqT9TJT4ftntdjzyyCMoKipKytJMdO+99+L73/8+PvGJT+D48eOor6/Hs88+ixdeeAEPPfQQrFbrgrb/+c9/Hr/+9a/x7ne/Gx/84AfR3NwMj8eD48eP4xe/+AXOnz+fFEhevXo1du3ahY9//OMIBAL49re/DavVuqjPhUqlwiOPPIIPfehD2L17N+644w4MDQ3h0UcfxYoVK/C5z31uzt/9wx/+gEAgkPRZnk1TUxMaGhrw85//HOvXr8fWrVsvO67f/va3eOKJJ3DllVfCYrGgu7sbjz/+OAwGA2677bak5zY0NOCGG27Apz/9aWg0mvjEwIMPPhh/zsMPP4yXXnoJO3bswEc/+lFs2LABo6OjOHz4MF588cU5ewfOx+9//3vs3bs3vkjJ5cSO+Qc/+EG87W1vwz333IPR0VE8+uijqK6uxhe/+MVFj4WIiGi+GBAjIqKc9Nhjj2Hv3r346le/ilAohDvuuAPf+c534mVJVqsV//M//4O//uu/xt/+7d+ipKQkXpI2vb/WN77xDfj9fvzwhz/ESy+9hB/84Ae4/fbbsWfPHlRUVODLX/4yysrK8Itf/AKbNm1K+b7cfPPNKCkpQTQanTUDav369XjllVfwpS99CQ899BAkSUJLSwt++MMfYteuXQCmenj97//+L/7u7/4OX/rSl+a97a985SuzZhpdzre//W387ne/wyc/+Uk8/fTTqK+vxy9+8Qv87d/+Lf7mb/4GFRUV+PjHPz5rk3JgKmNnrmydPXv24KWXXsI111wTf+yWW27BlVdemdZ9WogHHngApaWleOyxx/C5z30OFosFH/vYx/DNb35zRiP5+Vi/fj2+8pWv4Bvf+AbuuOMONDU1YePGjXj11Vfj73s0GsWOHTvw5JNPXnLlTLVanRQcSoXE98tms2Hr1q34j//4jzlLiHU6HV566SXcf//9+OlPf4rJyUmsWbMGP/jBD/DRj350wdvX6/X44x//iG9+85v4+c9/jv/8z/+E2WzG2rVr8eCDD87ol3X33XdDFEV8+9vfxvDwMFpaWvDYY4/FS6QX6oMf/CD0ej0efvhhfPGLX4wHnR555BEUFxfP+XsGg2FeK0XGxvyFL3xh3s30a2tr4fF48PDDD8PlcqG8vBzveMc78OUvf3lGGefu3btxxRVX4MEHH0RfXx82bNiAH//4x0l9wcrLy9HW1oavf/3rePrpp/Hd734XVqsVGzduxCOPPDKvMc2lsbERd9xxx4J+55577oFWq8XDDz+M+++/H3q9Hnv27MEjjzyS8s83ERHRbARp+lQ6ERFRFv34xz/Ghz70IRw4cADbtm1L23aMRiP+7M/+LKnkK13C4TCqqqpw880344c//GHat5frBEGYERAjmo/z589j5cqV+Na3vpWWbM50evTRR/G5z30O58+fT+rDt1SCIOCTn/xk0mIdREREdHnsIUZERJRmzzzzDEZGRnD33XdneyhElAWSJOGHP/whdu/endJgGBERES0eSyaJiIjSZP/+/Whvb8c3vvENNDU1Yffu3dkeUk644YYbLttziSgfeDwe/PrXv8ZLL72E48eP49lnn832kIiIiOhNDIgRERGlyfe+9z08+eSTaGxszEhpplxMb7ZPlK9GRkbw/ve/H8XFxfjyl7982VVUiYiIKHPYQ4yIiIiIiIiIiAoKe4gREREREREREVFBYUCMiIiIiIiIiIgKCgNiRERERERERERUUBgQIyIiIiIiIiKigsKAGBERERERERERFRQGxIiIiIiIiIiIqKAwIEZERERERERERAWFATEiIiIiIiIiIiooDIgREREREREREVFBYUCMiIiIiIiIiIgKCgNiRERERERERERUUBgQIyIiIiIiIiKigsKAGBERERERERERFRQGxIiIiIiIiIiIqKAwIEZERERERERERAWFATEiIiIiIiIiIiooDIgREREREREREVFBYUCMiIiIiIiIiIgKCgNiRERERERERERUUBgQIyIiIiIiIiKigsKAGBERERERERERFRQGxIiIiIiIiIiIqKAwIEZERERERERERAWFATEiIiIiIiIiIiooDIgREREREREREVFBYUCMiIiIiIiIiIgKCgNiRERERERERERUUBgQIyIiIiIiIiKigsKAGBERERERERERFRQGxIiIiIiIiIiIqKAwIEZERERERERERAWFATEiIiIiIiIiIiooDIgREREREREREVFBYUCMiIiIiIiIiIgKCgNiRERERERERERUUJTZHgARERFRKo24Ahj1BGE1qmEzarI9HCIiIiLKQQyIERERUV7wBsN4qq0Pb3Q54Q1GoFcrcEWdFXe21EKnVmR7eERERESUQ1gySUREREs24grg9KALDncga2N4qq0P+zrsEEUBVcU6iKKAfR127G3rzdqYFiMXjiURERFRvmOGGBERES1armRljbgCeKPLCYtBEy+TjP1va5cTtzZW53z5ZK4cSyIiIqJCwAwxIiIiWrRcycoa9QThDUZg1qqSHjdrVfAGI3C6gxkdz2LkyrEkIiIiKgQMiBEREdGiTM/KUitF2IwaWAwatHY5M1ryZzGooVcrMOkPJT0+6Q9Br1bAalRnbCyLkUvHkoiIiKgQMCBGREREi5JLWVmlJg2uqLNi1BOAwx1AMByFwx3AqCeAnXXWnC+XzKVjSURERFQIGBAjIiKiRcm1rKw7W2qxp6ESUlSCfdwHKSphT0Ml7mypzeg4FiPXjmWu4UIDRERElGpsqk9ERESLEsvK2tdhBzCVzTTpD2HUE8CehsqMZ2Xp1Arcu2sVbm2shtMdhNWozvnMsJhcO5a5ggsNEBERUboIkiRJ2R4EERERyZMvGMHetl60JgQsdjJgsSg8ljM98Vo39nXYYTFoZgQJ7921KtvDIyIiIhljQIyIiKgAjLgCGPWkL2vK4Q7ILisrV/FYThlxBXD/0+0QRSHpODjcAUhRCQ/fvrmgjw8REREtDUsmiYiI8limSs5sRg2DEynCYzklttBAVbEu6XGzVgX7uA9Od5DHiYiIiBaNTfWJiIjy2FNtfdjXYYcoCqgq1kEUBezrsGNvW2+2h0Z0SVxogIiIiNKJATEiIqI8NeIK4I0uJyyGqYwjtVKEzaiBxaBBa5eTK/ZRTostNDDqCcDhDiAYjsLhDmDUE8DOOiuzw4iIiGhJGBAjIiLKU7GSM7NWlfS4WauCNxiB0x3M0shoLiOuAE4PuhisfNOdLbXY01AJKSrBPu6DFJWwp6ESd7bUZntoREREJHPsIUZERAUl3c3lc0liyVnivrLkLPdkqteb3OjUCty7axVubazmQgNERESUUgyIERFRQSjEgEOs5Gxfhx3AVGbYpD+EUU8AexoqGVjIIbFebxaDBlXFOkz6Q/H37d5dq7I8uuzjQgNERESUaiyZJCKiglCozeXzueQsX8oL2euNiIiIKPOYIUZERHlvesABQPx/W7ucuLWxOm+zT/Kx5Czfsv1ivd6qinVJj5u1KtjHfXC6g7J/z4iIiIhyDTPEiIgo72W7uXwuZDLZjBrUV5jyIrCSb9l+ib3eErHXGxEREVH6MEOMiIjyXraay+dbJlMuyMdsP/Z6IyIiIso8ZogREVHeiwUcRj0BONwBBMNRONwBjHoC2FlnTVvAId8ymXJBtrP90iWfe72lUi5kWxIREVF+YIYYEREVhFhgobXLCfu4D3q1Iq0Bh3zMZMoF2cr2S7d87PWWSsy2JCIiolRjQIyIiApCpgMObJSeHvleXmgzamS/D+kQy7a0GDSoKtZh0h+Kfwbu3bUqy6MjIiIiOWLJJBERFZRMNZdno/T0YXlhYZmebalWirAZNbAYNGjtcrJ8koiIiBaFGWJERERpkM+ZTCOuAEY92SvrY3lhYWG2Zepl+2+YiIgoFzAgRkRElCaZ7luWbrnWx4nlhYUhX/vGZUOu/Q0TERFlkyBJkpTtQRAREeUzhzuQF5lMT7zWHe/jND3jjX2cKJ342UsNHkciIqK3sIcYERFRmmWqb1k6sY9Tdo24Ajg96CrY48y+cUvHv2EiIqJkLJkkIiKiy8rnPk653E8p10rcsnWs2Ddu6fL5b5iIiGgxGBAjIiKiy8rHPk65FmyazVNtffESt6piHSb9ofhCDZksccuVY8W+cYuXj3/D85HLAW8iIsouBsSIiIjosvJx1cxcCTbNZXqJG4D4/7Z2OXFrY3XGjnuuHyu6vHz8G76UXAniEhFR7mIPMSIiojyTrn5T+dTHSQ79lGIlbmatKulxs1YFbzACpzuYkXHI4VjR/OTT3/DlxIK4oiigqlgHURSwr8OOvW292R4aERHlCGaIERER5Yl0Z0TkUx8nOfRTypUSNzkcq3TKp5K7fPobvpRcyq4kIqLcxYAYERFRnshUWVs+9HHKlWDTpeRKiZscjlU6ZLPkLt1BuHz4G76UQg/iEhHR/DAgRkSUR/IpkyGd8vE4MSNiYXIl2HQ5sVK21i4n7OM+6NWKjJe4yeVYpVo2+qax71VqFGoQl4iIFoYBMSKiPJDpmyi5BpTy+WaTGRELlwvBpsvJlRI3ORyrVMpWgJmLF6RGoQZxiYhoYRgQIyLKA5m6iZJ7QCmfbzaZEbFwuRJsmo9sl7jJ6VilQjYCzMzyTK1CC+ISEdHCMSBGRCRzmbyJknNAKd9vNpkRsXjZDjbJSaEcq2wEmJnlmVqFFsQlIqKFE7M9ACIiWprYTZRZq0p63KxVwRuMwOkOpmQ70wNKaqUIm1EDi0GD1i4nHO5ASraTLpk6Ttl0Z0st9jRUQopKsI/7IEUlZkQQLUIswDzqCcDhDiAYjsLhDmDUE8DOOmtaAiuJQbhEzPJcGptRg/oKE4NhREQ0AzPEiIhkLlOZDHLPXsh0xkc2+qwxIyL15Novj5Yu0yV3zPKcG/8OiYgoHRgQIyKSuUzdRMm9R1WmjlMu9FkrlLK2dMqF95HmJ13BkmwEmNn3Khn/DomIKJ0ESZKkbA+CiIiWxheMYG9bL1oTbhp2puGm4YnXuuM9xKYHlHK9hxiQmeMk92NEU/g+5r58DpY43AFmeYJ/h0RElF4MiBERpUG2yjvSfROVqcBbuqXrOI24Arj/6XaIopD0ug53AFJUwsO3by7om1u54PsoDwyW5De5/h2yvJOISD5YMklElELZzlhId6lcvvSoStdxknufNZpSyO+jXG7m833VWJLf32G2z/9ERLRwDIgREaXQU2198YyFqmIdJv2heM+qfMpYSHfgTS435dPJvc8aTSnE91FuN/NyC5bQwsnt77BQzv9ERPlEzPYAiIjyxfSMBbVShM2ogcWgQWuXEw53INtDzHneYBhPvNaN+59uxwPPdeKLv2zHE691wxeMZHto8xJr3D/qCcDhDiAYjsLhDmDUE8DOOmvO3aCPuAI4PejiZ3Maub2PqRC7mRdFAVXFOoiigH0dduxt68320GaVGCxJlKvBElo4Of0d8vxPRCRPzBAjIkoRZiwsXT7MsMthlTi5ZQNlgxzex1SRY/lhplaNpeySy98hz/9ERPLEgBgRUYrIrbwj18jxpnw2cuizlg+Bx3STw/uYKnK9mZdLsKSQpLrcXS5/hzz/ExHJEwNiREQpwoyFpZHrTflc0t1nbbHyJfCYKbn6PqaSXG/m5RIsKQTpzjrN9b9Dnv+JiOSJPcSIiFLozpZa7GmohBSVYB/3QYpKzFiYJ/YEyoxY4NGsVSU9btaq4A1G4HQHszQyyhY59Wqajc2oQX2FKefHmc/k1oMuHXj+JyKSH2aIERGlEDMWFo8z7Jkh12wgSi+WH+Yeuay2y6zTKTz/ExHJDwNiRERpkOvlHbmKN+Xpx8AjzYY387lDbote5Fu5+1Lx/E9EJB8MiBERUc7gTXlmMPA4k1yycdKNN/PZJ7dFL5h1SkREciVIkiRlexBERESUeQ53oOADj3LLxqH8NuIK4P6n2yGKQtLfpMMdgBSV8PDtm3Pyb/WJ17rjQbzpWae5GMSTKwbuiYhSixliREREKSSnGxZmA8kvG4fym1zLD5l1ml4M3BMRpQcDYkREOU5OAZZCxhsW+WEzcMo1ci0/ZLl7ejFwT0SUHgyIERHlKAZY5IU3LPIj12ycVGCgPTfJfdELZp2mHgP3RETpw4AYEVGOYoBFPvLthqVQgiVyzcZZCgbacx/LDylRIQfuiYjSjQExIioIcrvBz7cAS77LlxuWQguWyD0bZzEYaM99LD+kRIUYuCciyhQGxIgor8n1Bj/XAixyCyhmWr7csBRisKSQsnEYaJcXlh8SUJiBeyKiTGFAjIjymlxv8HMlwCLXgGKm5cMNS6EGSwopGyfXAu1END+FFLgnIsokBsSIKG/J+QY/VwIscg0oZoPcb1gKPVhSCNk4uRJoJ6KFkWvgntnlRJTrGBAjorwl9xv8bAdY5BxQzAa53rDEMFiS/3Il0E5EiyOXwD2zy4lILhgQI6K8Jfcb/GwHWOQeUMwWudywTMdgSWHIdqCdiPIfs8uJSC4YECOivJUvN/jZCrDIPaBIC8dgSf7LdqCdiPIbs8uJSE4YECOivMYb/MXLl4AizR+DJYVDrpmMRLmC/bFmx+xyIpITBsSIKK/xBn9pGFCcW6ZuhrJx08VgCRHR7Ngf69KYXU5EciJIkiRlexBERJTbHO4AA4pvytTNkJxuujIZtGNWBhFl0xOvdcf7Y03PnGZ/rCk8RkQkF8wQIyKiy2LG0Fsy1SxYDk2JMxm0k1OAkIjyU2J/LKMiCpXI/lizYXY5EckFA2JERETzlKlmwdO3Ew6FYNWrUr6dpcpk0E4OAUIiym+J/bGioQDGx8dRYrGwP9Y0bFdBRHLBgBgREdE8ZapZ8PTtBINBeL1eBMJRjIcEnOm1o2htNVQq1bxeLx1lhvYxN149PQi9GIUi6IE3LEIFQCNF8L/HL6CpJAyLXgVRFCEIAgRBSPr/0/99qZ+NesN4/ewISnQqWPUqCKLIrAwqOCwXzr7k/li6qe9lvx+usMD+WLNgdjkR5ToGxIiIiOYpU82Cp29HbzBAbzDA4fLDEgpDJ4Zx/PhxhEIhGAwGWK1WWCwWaDTJNx5LKTOUJAk+nw8ejwderzf+XyQSAQAMeIFxtx/lJhV8Xh9sNhsAQKGKwu4KIKTQwmjUQZIkRKNRSJI04//P92c9YwE4xl0o1SvgRggmsxkAVy2jwsBy4dwxffVlk6kIfYMjiKr0eNfmqrz/HmJQlojyDQNiRJQRvIiifDD9Zmh6s+BUfbbn3I43iD0NlWhaP1UiKEkSPB4PRkdHceLECQSDQeh0OlitVlitVjx1cOCSZYahUCge6IoFvvx+PwBAEATodDro9Xro9XpYLBbodDoolVOXDiOuAJ7rbcfYxDhWVJRB+Wa22mQogGK9FquXlafseJS5AvhVVztEUYCJq5ZRgWG5cG5J7I81OBmATm/A2iLkVH+sVF9zMShLRPmKATEiSiteRFG6ZTrYmqlmwfPZjiAIMBqNMBqNWL58OQDA6/VidHQU+4+dwG8POaEUBWghIugPQRkOQxEMYN+hLtRKw7Aa1DAYDNDr9SgpKUF1dTW0Wi0EQbjs+EpNGqwpkvDamALjgSjMQjQtwcHYtjIRiCTKNZnqW0jzN1t/LHvPGXgmx6B7M1M2W9J1zcWgLBHlKwbEiCiteBFF6ZKtYGummgUvdjuxjC6Psgia9iDKjSpI0TAEABq9Hjq9EYOTASxfsxH1FaZFj298fBy7qxSwWVegtTv9K4lx1TIqRJnqW5jrcjHLPLE/VvH69Thw4ACKi4sx5otkbazpuOZiUJaI8hkDYkSUNryISo9cvDHIhmwHWzPVLHix24n1IfOEJdiM+vjjDndg0WWGsc9esVZE98mTaG5uxhVqNW5tSv9KYly1jApRpvoW5iq5ZJkrlUosW7EKDz29H+d96qyMNV3XXAzKElE+Y0CMiNKGF1Gple0bg1wKxDHYenmpLDOc/tkL+z24am0ZGjH1ucvkSmJctYwKSaGXC2d74mMhftflwZ8u+FBWBFQVGzM+1nRdcxV6UJaI8puY7QEQUf5KvIhKxIuoxYndGIiigKpiHURRwL4OO/a29aZ1u95gGE+81o37n27HA8914ou/bMcTr3XDF4ykdbuXErvwN2tVSY+btSp4gxE43cEsjSy33NlSiz0NlZCiEuzjPkhRaVFlhomfvWJ1FEqFiD/2TKb9s0dEqfs7lpv4xIdOBTHgRsDrhs2ogcWgQWuXEw53INtDjIuNdZnNDFXED7VCyPhY03XNFQvKjnoCcLgDCIajcLgDGPUEsLPOmvdBWSLKb8wQI6K0yeeZ7UxnS2UzIyoXZ+g5Yz0/qSgznP7ZGx/3YUWFFQ5PkNl4RBmQz+XCc51LJUnCefsIhpxj0EZ8UCtFWCwWALmZZZ6YnaU2aYE3FybJ5FjTec3FHo5ElK8YECOitMq3i6hslS1mq/w0V0sT8znYmg5LKTOc/tkrLi4GkJ7PXi6V5RLlmnwqF57rXPrnjZVwDtsxODiI4Uk/VIIEk7UMipAXRqMRQG5OfOTKJE26rrnyOShLRIWNATEiSqt8u4jKVrZUti62c7kPXL4FW3NVJj572e6PR0SZFTuXGjRKGNQKeHx+/Lz1HC5cuID3NZYBALZvqofDEMBvjg9AFZZQFAUmvYGcnPjIlUmadF9z5VNQlogIYECMiDIkHy6ikrKlDGr4fL746n3pzpbK1sV2rsx6zybfgq25KtWfvdmywHKxLJeI0mPEFcBrZx1w+cO44HQjGIpArVTArFOhfTiIG7xhtLS0QKlU4s7SCMbGxnDMLuX8xEcuTdLkwzUXEVEmMCBGRDRPidlS0WgUXq8XOr0+Y9lS2bjYzpVZ70vhhX/6peKzN1cW2A0bKvBGlxNFWiW0CEElqnKiLJeI0mPUE0SPwwN3IAwhGoFWCUQkCcOTfgQMGlirV0CpnLpF0akV2F0Rwd27mzHui+T0xAcnaYiI5IcBMSKSvUz1HUrMljKrhfgFe6aypbJ1sZ1Ls96UHan47E3PAhvz+PHMwV50nO3D4GgA5SYNRL02/vxcKMslotSTJAmeYBiiIEApAIIgQJCiiChE+ELRWD96AIDf74dKpUJ5kR7lRdkb80JwkoaISD4YECMi2cp036HEbKmAX4JOJcaXHs9ktlSmL7Y5600xi/3sJZYbl2gVcI46oFIoUKJXwhFWwlKig0KlgN6QW2W5RJR6giDAoFHC5Q9BFBQQpCigUCIcDMOgEBCJRgFMfW8cP9ODihJblkdMRET5igExIpKtbPQdimVF/eH4BThDAooNUsFkS3HWmxYrsdxYoRRRVjbVNNsYjsI+7sOmCjMO9o0CyM2yXCJKHYtBjVVWAy5O+OAJhBGKSIAUQZFaQKlBgbMnOvDH06U40DuBQccoKmwWXDEQ4iIbRESUcgyIEZEsJTW4f/OGORN9h3RqBW7ZUg1TcAxl5eXYVFvKG3aiy7jc4gwfuKIWpWYNy3KJCkCpSYO3rbFhX4cdlUU6qBUCghEJbn8IVyzT4k99Tuy3j2GZrQhlBhVEUVjSZFem2ioQEZH8MCBGRLKUmHGSKJ19hxJLNIec4yizRHDlWJCz1kSXcbnFGZaV6FmWS1RAEntTegJTLQ/2bKrEDRsq8MBznai2+hCYcEBXXAyrQR1/7kImuzLdVoGIiOSHATEikqXLZZyko+9QYommVStAoRDTXqJJlC/mszgDy3KJCsNcvSlPD7qmJrssRVCXFsPj8WB4eBgavQFjQSRNdl0u8ysbbRWyiZlwREQLx4AYEcnS5TJOUn0xOL1E0+ETM1KiSZQvuDgDEU03PQg+fbLLYDRCr9ejb2gUwVAYaikAb1B32cyv2Dm7RKeCWQWolfl7zmYmHBHR4onZHgARycuIK4DTgy443IFsDwV3ttRiT0MlpKgE+7gPUjR9De5jJZpmrQoAYCstBTAViPMGI3C6gynfZi7Jpfed5M1m1KC+wpQ3N6NElDqxya5RTwAOdwDBcBRObwgBUY13bqnFxFA//vmZ/fhN+wBEUUBVsS7eY2xvWy/8fj8GBgaw/2gHBh2jCPvcCIZC8dfPx3N2LBNutuNBRESXxgwxIpqXXJyBzGTGSTZKNHNBLr7vRESUvy5VXu0OhHHu0CEowl6oI4AQFaBFCIqQD/sOdWG1cgx11WXYtHYVKrqjEEUBxjw+Z2drgaFsY3koEaUKA2JENC+53IsjE32HMl2imSty+X0nIqL8c6nJrr5RL4JREbUVpQiHAvB4PFCr1aiylWDYHULlirWoqTABQEGcs7OxwFA2pWOSjsE1osLGgBgRXVahzkBON5+m4PmE7zsREWXLbJNd8WztQBg2ox56vR4A4HAHZmR+FcI5u9Cy11M5SccMeCICGBAjonkotBnIuRRaU3C+70RElEsWkq1dCOfsQspeT/UkHTPgiQhgU30imofEGchE+ToDeTmF0hR8Me87m+8TEVE6LXRBnXw/Z2dygaFsmr64UcxiFkqYHlyLrUJqMWjQ2uXkNQxRAWGGGBFdViHNQOaibPW3WMj7ztIDIiLKhELI/FqIQjkeqSwPZQY8EcUwIEZE81IIvThyTS4Emeb7vrP0gIiIMikTC+rkgvlOiuX78Ujl5Gyh9V4jorkJkiRJ2R4EEcmHwx3I6xnIXPLEa93xINP0C79MB5ku9b6PuAK4/+l2iKIAi14Fh8MBs8kEd1iAJAEP376ZnxUiIqIFyIVJsVzjC0awt60XrQnHZOcij0kuXWMRUfYwQ4yIFiTfZyBzRa6t8Hip9z2x9EAUBEQjEfgDAQR8AYwFgLN9dljWLYcozmxbyeXOiYiIZmLm9UypLA9l5QMRAQyIERHlJDn1t5heemCxWOB2u6ExFcOmC0MVDeDAgQPQaDSoqqqCzWaDPxzlzDcREdEscm1SLNekYnI2Hb3XOMlHJD8MiBER5SA59beYra/HuD8Cv9+LmxuXYVvD1Ey2z+eD3W5HT08Pfnc+iINDEZQX6TnzTURElEBOk2Kpkq1gUiqCayxvJZIvBsSIiHKQ3Fb2nF56YDAYUa/24v3bl8efo9PpsGrVKphKq9Fz4ihMaj8Ck04oDZWc+SYiInqTnCbFliofgkksbyWSLwbEiGQm39Ox833/FkJO/S1mKz1wjQzAOWyHvqYm6bmjniD8YQkWgwYhfzTeWyyfZ76JiIjmS26TYksh92ASy1uJ5I0BMSKZyIcZtEvJ9/1bjHT0t0i3xNIDq2EFWltbUVFRAZVKFX+OxaCGWpQwNObGmmXlgCAAyM+ZbyIiosWQ06TYYuVDMKkQy1uJ8gkDYkQyIfcZtMvJ9/1bCrmu7CkIAtauXYvTp0+jvHZNPPPPpJKwXONDZ0AHhyeY1zPfREREiyHHSbGFyodgUiGVtxLlIwbEiGQgH2bQLiXf96+Q6UxF+OULJ9C9/zCCUQE6lYhlKg8+dn0jfndmPK9nvomIiJZKDpNii213kQ/BpEIqbyXKRwyIEclAPsygXUq+718he6qtD0edAhQRN2orSnFh0IFjggq/OzOe9zPfREREMfnYI3Wp7S7yJZhUCOWtRPmKATEiGciHGbRLyff9K1SxzD+bWQt1VMCoYxgVJSb4oErK/JPLBS8RERWGVAav8rlHairaXeRDMKkQyluJ8hUDYkQykC8zaHPJ9/0rVImZf2qFGmq1GlqtFqpwlJl/RESUc9IRvMrXHqmpaneRT8EkTvIRyY+Y7QEQ0fzc2VKLPQ2VkKIS7OM+SFFJdjNol5Lv+1eIEjP/IAjQarUAmPlHRES5KRa8EkUBVcU6iKKAfR127G3rXdTrJQaN9GIE4YAPNoMaFoMGrV1OONyBFO9B5sQmvcxaVdLjZq0K3mAETndwQa9nM2pQX2FiQImIMooZYkQykU8zaLPJ9/0rRMz8IyIiuUgMXpVoFVAohCUv8JOYKa0SVXC73RgeHoZKq8NESJR1pjTbXRBRPmCGGJHM5PsMWr7vX6Fh5h8REclBPONJo4TT6YQkSQAWn/EEJAeNBFGEyWxGaWkp3IEIAh4X/OMjiEajqd6VjIhNeo16AnC4AwiGo3C4Axj1BLCzzsrrOCKSBWaIEdG85eMKSZRezPwjIiI5iAWvhsYmUWIwQBCn8gaWkvE0V6a0T1Jgz9Y6lOiV2L9/PyorK7F8+XKI4lu5CnK45sqHhvhEVNgEKTb9QUQ0h3xeIYmIiIgIAH7wx3P4xf5zqK2wwqxTJ5X5L7YBvi8Ywd62XrQmXEPtTLiGkiQJFy9exIULF1BeXo7Symr87NBFWV1zOdwBTnoRkSwxIEZEl/XEa93xFZKm94GS8wpJRERERLFsLHtfD46NAseHArMGr5bickEjSZIwMDCA7790GodHJFRZTDDrNbzmKlByyBAkygcsmSSiS0rVstpEREREuSQxA97lDyLq9+LG5jo80LIanmAkpcEIm1FzydcSBAFqsw0DkQFYjAFEfZNQGmy85iowrMogyiw21SfKgBFXAKcHXbJcXjvVy2oTERER5YKn2vqwr8MOURRgkAIwmYzY12HH8ycGs7LAT+yaq6zYhLKyMoiKqQAIr7kKR+JnsqpYB1EUsK/Djr1tvdkeGlFeYoYYURrlwywPl9UmIiKifJOYAW/RKTEZVqG42AilO5C1bCxecxU2VmUQZR4zxIjSKB9mebisNhEREeWbxAx4UaFAcXExgOxmY/Gaq7CxKoMo8xgQI0qT6bM8aqUIm1EDi0GD1i6nrMon72ypxZ6GSkhRCfZxH6SoxGW1iYiISLYSs7ESZTsbi9dchStXP5NE+Ywlk0RpEpvlqSrWJT1u1qpgH/fB6Q7KZqZPp1bg3l2rcGtjdUEuq82VfoiIiPJLLBtrX4cdAGasop2t832hX3MVslz9TBLlMwbEiNIkH/tAXG6FpHyTDz3giIiIaHaxrKvWLifs4z7o1YqcycbKp2suTizOXy5/JonykSBJkpTtQRDlqyde68a+DjssBs2MWZ57d63K9vDoMvj+ERER5T+HO8BsrDTgxOLi8TNJlBkMiBGlkS8Ywd62XrQmXAjs5IWALIy4Arj/6XaIopB0IeJwByBFJTx8+2ZeoBARERHNgROLRJTrWDJJlEbsAyFf+dQDjoiIiCiTpi8uBSD+v61dTtzaWM3rKCLKOq4ySZQBNqMG9RUmnvhlhCv9EBERES1ObGLRrFUlPW7WquANRuB0B7M0MiKitzAgRkQ0i9hKP6OeABzuAILhKBzuAEY9AeysszK4SURERDQHTiwSkRwwIEZENIc7W2qxp6ESUlSCfdwHKSpxpR8iIiKiy+DEIhHJAZvqExFdBlf6ISIiIloYLi6VWSOuAEY9vF4lWggGxIiIiIiIiCgtOLGYXt5gGE+19eGNhMDjFQw8Es0LA2JEREREREREMvTEa93Y12GHxaCBWavCpD+EUU8Aexoqce+uVdkeHlFOYw8xylsjrgBOD7rgcAeyPRQiIiIiopzGa2f5GXEF8EaXExaDBjajBmqlCJtRA4tBg9YuJ99LostQZnsARKnGtGEiIiIiovnhtbN8jXqC8AYjqCrWJT1u1qpgH/fB6Q6yTJXoEpghRnnnqbY+7OuwQxQFVBXrIIoC9nXYsbetN9tDmxNn5IiIiIgoG+R47UxTLAY19GoFJv2hpMcn/SHo1QpYjeosjYxIHpghRnlletowgPj/tnY5cWtjdU7NknBGTj64cg8RERHlG7ldO1OyUpMGV9RZsa/DDgAzeojxvSO6NAbEKK/ILW04NiNnMWhQVazDpD8UP6GxCWZuYNCSiIiI8lXitXPA70cgEIDZbM7Za2ea6c6WWgBTAUz7uA96tQJ7GirjjxPR3BgQo7ySmDacePLOxbRhzsgtXDaytBi0JCIionyVdO1s0CAUDmNkZASSxphz1840O51agXt3rcKtjdVwulnNQLQQDIhRXklH2nC6gjByy2bLpmxlacWClsVaJcxqIb5yD8CgJREREcnfzGtnPdwh4OLwGN61iSV3cmIzavh+ES0QA2KUd1KVNpzuIIycstmyLVtZWr1DTgyNjsOmUwC6ovjjDFoSERFRvph57azEn+9cg61FXnR0dGD9+vVQKNgmgojyDwNilHdSlTac7iAMm2DOT7y0VK+GWYWUZmnNlv0nSRKGh4fR29uLgKCBrcgElUoJtfqtACWDlkRERJQvLnXtPDQ0hAMHDmDjxo0wmUwA8nOhoXzcJyK6PAbEKG8tJW04U/292ATz8mKlpUYhiCCUUGumjvtSsrRmy/7budKC3dUKOIftsNlsaGxshFqtxulAN4OWRERElPdmu3YuLy9HUVER2tvbYbaU4jW7hDe682ehIS6eRFTYGBAjmkWm+nuxCeblWQxqiNEQJsNh2Kwl8ceXkqWVmP1XYVJjaHQCP291YmJDGf7qXTsgimL8uQxaEhERUSHTarXYvn07Hnn2AF46N46a0uK8WWhIrosnMaONKDUYECOaRab7e7EJZrLEk7w66sdqUwQdExo43IElZ2klZv8ZFVG4JydRXmyEwWjEKWcEo97k95xBSyIiIip0DncQZycEVJWYIPkmIeqtsl9oSI4rvjOjjSi1GBAjmgX7e2XH9JO8VilgmcqDT97Ugv/pHE5JllZi9p9aIUCrm8oCFMPRS2b/MWhJREREheqt6yc9VKIegiAAkPdCQ3Jc8V2uGW1EuYoBMaI5sFQu85JO8kVa9A6O4Lioxf90DqcsS4urexIREREtTD5eP8ltn+SY0UaU6xgQI5pDoZfKZbo3wfST/NjoKJZZzfBEFUkn+aWOhdl/RERERAuTj9dPctsnOWa0EeU6BsSILiNXS+XSFbDKVm+C6Sf5ouJiiKIIxWVKGReD2X9EREREC5OP109y2ie5ZbQRyYEgSZKU7UEQ0fylO2D1xGvd8bLF6TNl6exNMOIK4P6n2yGKQtJJ3uEOQIpKePj2zSkPTDrcgYLM/iMiIiJarExcP2W6UkEu14TZuk4nylfMECOSmXQ208xmb4JspK3navYfERERUa5K5/VTtioV5HJNKKeMNiI5YECMSEYSA1YWvQqiKKY0YJXt3gQ8yRMREREVrnxbRTHVmW6F3uOYKNUYECOSEYfbjzGXFyYxBFdYg6LiYgCpC1hluzcBT/JEREREhWl6pULA74dFpwIw+8RvpssqFyLdmW5yyWgjynUMiBHJgNfrRW9vL3qHxqBVClDrzCgyv5XFlaqAVa6stsOTPBEREVFhmV6pEI1GMTo2hmAogsmIAhedk7AZS7NWVrkQ+ZbpRpSvGBAjylGSJGFoaAgXLlyASqVCbW0t1q1bh35lz9QJVRTTErBi2SIRERERZdr0SgWdXg+dXg/HpA/GYBDjgxewf7AbLw0IaL0YQGmRLieDTdnsyUtEC8OAGMlOLqdHz9el9sHv96Ovrw9OpxPl5eXYsmUL1Oq3Mr/SHbBi2SIRERERZdqclQq+EPY0VGH3jlUYnvTjsfZD0AohiIEI1EZrzgWbst2TV27y4d6O5IsBMZINOaRHX85c+/D+7cvhmRxDX18fBEHA8uXLsWbNGgiCMOM1MhWwYtkiEREREWXS5SZ+x7whhKFAdZkVaqUY/71cCjZluyevXOTDvR3JHwNiJBv5UIs/fR8mvAH86kAPes+fx907atDQ0ACNZn4ncQasiIiIiCifXG7iVw7BplzpyZvr8uHejuRPvPxTiLJvei2+WinCZtTAYtCgtcsJhzuQ7SFeVtI+GNRwjY9CCLhhNWowEDGhpHL5vINhRERERET5ymbUoL7CNCN4FAs2jXoCcLgDCIajcLgDGPUEsLPOmjPBpjtbarGnoRJSVIJ93AcpKsmmJ++IK4DTg6603l/lw70d5QdmiJEs5EMtftI+CAJKSkogKhQIhqOy2Qe5YU8CIiIiovwihwWg5NiTN5MljPlwb0f5gQExkgU5pEdfzvR9EBVTJxY57YNcsCcBERERUX6SU7BJTi1OMlnCmA/3dpQfWDJJsiCX9OhLyYd9kIvYCV0UBVQV6yCKAvZ12LG3rTfbQyMiIiKiFJirrJIWLtMljLwvolzBgBjJhpxr8WPyYR+y7XJ9DWIndJNKgCLogQJR9iQgIiIiIppDrITRrFUlPW7WquANRuB0B1O+Td4XUS4QJEmSsj0IooVwuAM5nx59OfmwD5l2uTLIcDiM4eFhtJ3px+OHxlBdrIfZqIdKpQIEId6r7Ws3b0R9hSnbu0NERERElBNGXAHc/3Q7RFFIujdxuAOQohIevn1z2u5ZeF9E2cQeYiQ7cqrFn0s+7EOmzdbX4DfHBjA2OoprKiUIgoDS0lJsa6jHc71nIIkCVOq3+g+wJwERERER0UyxEsZYzzCzVoVJfwijngD2NFSm9b6F90WUTQyIEVHOS+xrYNWrMDk5iWgwCFVUwrFB4O6rm1BebIg/P1sndCIiIiIiOZLD6p1EqcaAGBHlvMSlmQVBgFarRVFREYojUz0Hxv1RlCc8nyd0IiIiIqL5k9PqnUSpwoAYEeW86Usza7RaAMCkPzhrGSRP6EREREREC8cSRiokXGWSiHLeYpdm5nLcRERERJQPLrfSOhEtHFeZJCKMuAIY9eR2JpUvGMHetl60JqwyuTNhlUkiIiIionxzuZXWiWjxGBAjKmByPMFyaWYiIiIiKhRPvNYdX2l9+mJR9+5ale3hEckaSyaJCthTbX3Y12GHKAqoKtZBFAXs67Bjb1tvtoc2J5ZBEhEREVEhSFxp3WbUQK0UYTNqYDFo0NrlZPkk0RIxIEZLwlp2+eIJloiIiIgod8VWWjdrVUmPm7UqeIMRON3BLI2MKD9wlUlaFDmW2lGy2Am2qliX9LhZq4J93AenO8gsLCIiIiKiLJm+0nrMpD8060rruU4OfYupsDAgRosSK7WzGDSoKtZh0h/Cvg47AMimlr3Qv5Dz7QRLRERERJRPYiutx+6zpvcQk8s9DJMpKFcxIEYLNr3UDkD8f1u7nLi1sTqnv5z5hTwlX06wRERERET56s6WWgBT91n2cR/0agX2NFTGH5eDfEimoPzEgBgtmNxL7fiF/JZ8OMESEREREeUrnVqBe3etwq2N1bJcaV3uyRSU3xgQowWTc6kdv5CTyf0ES0RERERUCGxGjSyv0+WeTEH5jatM0oLFSu1GPQE43AEEw1E43AGMegLYWWfN6S80rtQyO5tRg/oKU06/d0REREREJC+JyRSJ5JBMQfmPATFalDtbarGnoRJSVIJ93AcpKsmi1I5fyOkz4grg9KALDncgo79LRERERES5Sc7JFJT/BEmSpGwPguTL4Q7IrtTuide64z3EpjeSL7QeYqmwlEUKuMABEREREVF+8wUj2NvWi9aEa/6dvOanHMCAGBUcfiGn1lICjAxOEhEREREVBjkmU1B+Y0CMClYufyGPuAIY9eTm2BKNuAK4/+l2iKIAm0ENSZIgSRJGXH5EolF85Z0rYVIB4XAYoVAI4XA4/v8d7gC+e3ACogCUmrQoKi4GMPW+SFEJD9++Oaf3nYiIiIiIiOSLq0xSwcrFlVrkVkKYuGpMJBLB2NgYRFGEEAUmfFFcGB7DmjIjVCoVtFotVCoVlEollEolVKN+qE+cRFWxHmrlW+0MueIMEREREVFuksvEfSI5jpkygwExohzyVFtfvISwqliHSX8I+zrsAJCTJYSJixTYjBrYSksBTGV52TQStqyrm/OkU2oSoFcr478bwwUOiIiIiIhyi9wm7gF5jpkyi6tMEuWIEVcAb3Q5YTFoYNGroBKnstgsBg1au5w5uQLjUlaN4YozRERERETyEJu4F0UBVcU6iKKAfR127G3rzfbQ5iTHMVNmMSBGlCNi5YdGlQjHyAgikQiAqRJCbzACpzuY5RHO7s6WWuxpqIQUlWAf90GKStjTUIk7W2rT+rtERERERJR+iRP3NqMGaqWY8xP3chwzZR5LJmlJ8rUeOxv7ZTGooVUAF4YcWFlpg1KlApD7JYQ6tQL37lqFWxurF7xIwVJ+l4iIiIiI0i+xb3CiXO79K8cxU+YxIEaLkq/12NncL40UwDK1Fx1KPcYDUZiFKCb9IYx6AtjTUJnzX9hLWaQgFxc4ICIiIiKimX2DY3J54l6OY6bMY8kkLUq+1mNna79GR0dx4sQJ/M2tV+CmLVUsISQiIiIiopwgx96/chwzZZ4gSZKU7UGQvIy4Arj/6XaIopD0ReJwByBFJTx8+2ZZfsFkcr8SSzKj3gn09vaiqakJSqUyvk2WEBIRERERUS7wBSPY29aL1oRKmp05XiEkxzFTZrFkkhYsX+uxM7Ff00syxWgIa4okfP7WK+LBMIAlhERERERElDvk2PtXjmOmzGLJJC1YYj12IrnXY2divxJLMk2KEMKhII6PKfDTgxeW/NpERERERETpZDNqUF9hklVgSY5jpsxgQIwWLF/rsdO9X4lL/+oQgkIAVlWVwmLk0r9EREREREREmcSSSVqUWJP31i4n7OM+6NWKvGj+ns79SizJVCvUgCAAkH+pKREREREREZHcsKk+LUm+Nn9Px37l62IERERERERERHLDDDFaknxt/p6O/YqVZO7rsAOYygyb9Icw6glgT0PloreXuGJlPr4XRERERERERKnGDDGiDErl0r/TV6zUqxW4gssIExHJHic6iIgoVXhOIZobA2JEWZCKkswnXuvGvg47LAbNjGyze3etSvGIiYgo3TjRQUREqcJzCtHlcZVJoixY6tK/iStW2owaqJUibEYNLAauWElEJFdPtfVhX4cdoiigqlgHURSwr8OOvW292R4aERHJDM8pRJfHgBiRDMVWrDRrVUmPm7UqeIMRON3BLI2MiIgWI2miw6CGWiFwooOIiBYl8Zxi0SkRCfphM6hlc04ZcQVwetCV8+Mk+WNTfSIZshjU0KsVmPSHkrLMJv0h6NUKWI3qLI6OiIgWKjbRUVWsw+TkJDQaDTRaLcxaFezjPjjdQfZ+ISKieUk8pwgCEA6HMTw8DFGphiuimPOcku1+YyzzpExjQIxIhtK1YiUREWVH4kSHVqFANBoFwIkOmlu2b1yJKHdNnzw3mc0wmUywj7kRDXrRf+4kioValJWVQRCEnAlExco8LQbN1ASRPxS/32GPZEoHBsQo63hBtzh3ttQCAFq7nLCP+6BXK7CnoTL+OBERyUfiRIdBIUGnisLjDnCig2bIlRtXIspdc02eu0IS9jStwq7t1ejv70d3dzesViv+MAC8cGokq4Go6T2SAcT/t7XLiVsbq3kupJRjQIyyhhd0S6NTK3DvrlW4tbF6yStWEhFR9sUmNF45NYihST9sRWpOdNAMzKAgovm41OS5Rq1AXV0dVq1ahdO9drzYfgIKAEazKt7DMva7mQpEJZZ5JmLrAEonBsQoa3hBlxo2o4YnByKiPBCb6HjnmmJ0nOvFji0b+P1OSeIZFHo1dAhBJaqYQUFEs5rP5LkgCBC0Jqh0RlSY1Aj6vVAoFFCp1RkPRLFHMmUDV5mkrJieEqtWilxNi4iICECVxYQqPRjYoBlGPUG4fAEE3BOIRqMQBAEAV5kmornZjBrUV5jmPKfEAlHuUBTmoiKo1FOBp0wHomJlnqOeABzuAILhKBxvtg7YWWflOZHSggExGcvkcrSp3lYsJdasVSU9zgs6IiIqdEqlEqFQKNvDoBwTiUQwNtiHaNAPld4Ek9kMvBkQYwYFES1WLgWi7mypxZ6GSkhRCfZxH6SolNLWAZm8fyZ5YMmkDGWy91a6tpWOlFg25ycionwQy/ohinE6nThz5gxWrFiBG5t02Ndph6gIcJVpIkqJXFmsK109ktm7mubCgJgMZbL3Vrq2NdfKJ4u5oOMXHBEREeWjYDCIkydPQhRFbNu2DSqVCndaI4CQ/RtXIsofubZYV6p7JLN3Nc2FATGZyeRytOneVqpmIvgFR0RERPlEkiQMDAygr68P69atQ0lJSfxnuXbjSkT5Ix8X68rk/TPJDwNiMpPJ5WjTva1UXNDxC46IiPKRQqFAOByGUslLtULj9XrR2dmJ4uJi7NixA6I4e8vffLxxJaKlYxuZZJm8fyb54VWWzGRyOdpMbWspF3T8giMionyk0WgQDAYZECsgkiShu7sbTqcTGzduhMFgyPaQiEhG2EZmdpm8fyb54SqTMpPJVUByacWRuSR+wSXiFxwRFQKulpS/1Go1gkGuuFwoxsfHsX//fqjVamzfvp3BMCJasFgbGVEUUFWsgygK2Ndhx9623mwPLavkcE9L2cNpRxnK5CogubLiyFxS2ZyfiCjXzFX2wFng/MeAWGEIh8M4ffo0QqEQmpqaoNHwuoWIFo5tZC4t3fe0LFOVL0GSJCnbg6DFcbgDGWummsltLZQvGMHetl60JtwY7uSNIRHJ2OUCXk+81h1fTGT6RAAXE8kPAwMDiEajWLZsWbaHQmkyNDSE7u5urF69GqWlpdkeDhHJ2OlBFx54rhNVxTqolW8VgQXDUdjHffjazRtRX2HK4ghzQ6rvaTlBKX/MEJOxTDZTzeXGrVxtiYjyzZyr50rA9fUW/PGkHTohAmXIC1FjTPssMGc+M0+j0WB8fDzbw6A0CAQC6OzshE6nQ0tLCxQK3jQR0dKwT9b8pPqeds7rNYATlDLBgBjljVwO2hERzdf0sgenwwEpGoUYjOL5w10wBJ1w+YOoKtJCq1JidHQUpaWlaVlMhDOf2cOSyfwjSRL6+vpgt9uxYcMGmM3mbA+JiPIE28hkHstU8wOb6hMREeWQ2Oq5Zq0KkCRIAPR6PZZXlkGtN2HFilWwmo0ICSpotFqYzWaMj4+nZRaYDXqzhwGx/OJyudDW1oZoNIodO3YwGEZEKXdnSy32NFRCikqwj/sgRaWc6v2cb5Ku1xKYtSp4gxE43TyHywEzxIiIiHLI9LIHm9UKh8OBiUAUerUaa8qN02aBNRgbc8Pn9eA9TTUpm43kzGd2MSCWH6LRKM6ePQuXy4XNmzdDp9Nle0hElKfYRiazWKaaH5ghRkRElENmLA8ekQCNEYNjbjRW6WEzambMAhsNRmwpieAvmqpSNo7pM59ulwuQJM58ZoggCOC6R/LmdDqxf/9+mM1mNDc3MxhGRBlhM2pQX2FiMCzNZlyvhaNwuAMY9QSws87K4y8TzBAjIiLKMbMtD357yyrUKxxwu90wGo0zZoHFoAc9505jy5YtKRnD9JlPQRDgcrkQEDWc+SS6hFAohJMnT0IQBGzbtg0qleryv0RERLIz2/Uay1TlRZA4/UhERJSTpi8P7vf7ceTIETQ1NUGr1c54/smTJ1FcXIzKysqUbP+J17rjqyeZNUr02ocRVenxrs1VXD0pA9ra2tDS0pLtYdA8SZIEu92O3t5e1NfXw2KxZHtIRESUAdOv10g+GBAjIiKSEY/Hg/b29lkzT6LRKNra2tDU1ASNZukXZL5gBHvbetH65iqTCoSxwarCZ9+9jatMZsDBgwexdetWiCI7XOQ6r9eLEydOoKioCHV1dXzPiIiIZIABMaIcN+IKYNTDGQciesvExAROnTqFbdu2YdQbTvqOmJycxNmzZ7F161YIgpCS7cVmPi0GFbpPHENjY2NKAm50ae3t7Vi7du2s2YD5Rq7nOkmS0NPTA4fDgQ0bNsBoNGZ7SERERDRPDIgR5ShvMIyn2vrwxpuZGXq1AlfUWXFnSy0zM4gIF+xDeOLlU+gP6md8R1zs64FGo0FNTU3Ktzs2NoaLFy+ioaEh5a9NyU6dOoWqqiqYzeZsDyVt5HyuGx8fx6lTp1BdXY1ly5alLABNREREmcF8bhkZcQVwetAFhzuQ7aFkRaHt/1NtfdjXYYcoCqgq1kEUBezrsGNvW2+2h0ZEOeB3XR4cHpHgdk2iqkib9B1RV1eHgYEBeL3elG+3pKQEgUAAHo8n5a9NydRqNQKB/D3njbgCePTFs/j10QFZnevC4TA6OzvR09ODpqYm1NTUMBhGREQkQ1xlUgbkPHuaCoW4/yOuAN7ocsJi0MRLR2L/29rlxK2N1bIqKSGi1Ip9R1SUGKGVgvC6J2ErKQHw1ndEQ0MDOjo6sH379pTfrK9duxZnzpxBU1NTSl+Xkmk0GgSDwWwPI+Vi5/WXT4+gc2ACClGAQhRQrFPl/LlueHgYXV1dqKurQ1lZWbaHQ0REs5BrGT5lHgNiMhDLFLIYNKgq1mHSH8K+DjsAFMQqX4W4/6OeILzBCKqKdZCiUfj8fuj1epi1KtjHfXC6g/xyJypgid8RaqUG9oEBiKIIs8EU/46orzChvLwcPT09WLUqtd+VJpMJCoUCExMTKCoqSulr01vUajXcbne2h5FysfO6WqGAUhQBKYwzA064XJNYWaKBqNZgPICcOtcFAgGcOHECWq0WLS0tUCjyc0KOiEjOCjGRgpaGJZM5bnqmkFopwmbUwGLQoLXLmfflg4W6/xaDGnq1ApP+EHw+H6LRKABg0h+CXq2A1ajO8giJKJsSvyMAoKi4GOMTExgenUz6jli+fDmcTidcLlfKxxDLEqP0yceSyaTzul6BcNCPcCgEnVoJr6SG3lQMbzCKaNCP86ePo7OzE8PDw4hEIlkZryRJ6Ovrw5EjR7Bq1SqsX7+ewTAiohzFljO0UAyI5bhYFoBZq0p63KxVwRuMwOnOv1KKRIW6/6UmDa6os2Jo0oeu4UlEoITDHcCoJ4CdddacmTEnouyIfUeMegJwuANQqrWQ1AZcHJ1EQ/lbpdaCIGDTpk3o7OyMB9ZTRavVwmg0wuFwpPR1c1mme1nmY8lk7LyuRhhDA/0wKaOQBCVC4TA8fj96hkYx5g3jmvWVeOdVO7F8+XK43W4cPnwYbW1t6OrqwuTkJDKxJpTb7caBAwcQiUSwY8cOZkMSEeWwQk2koKVhyWSOS8wCSAyCFEqmUKHuvzcYRigswReMwj4RxkXPGGxGDd67tRp3ttRme3hElANi3wWtXU7Yx33Q6Q3YXaLAWnEYY2NjKHmzp5hWq0VNTQ3Onj2L+vr6lI5h9erVOHToEKxWa143Fc9WCYZarc67gFiJXoVI0IvzTheqrcWwqTWweyT0DE9AUChhMujRXKXDleUSDh48CEmSoNfrUVFRAaPRiFAohP7+frhcLqjVapSWlqK0tBQaTeomiqLRKM6dO4fJyUls2rQJOp0uZa9NRFSIMtHTK7GdRCK2nKFLYUAsx8WyAGI9s8xaFSb9IYx6AtjTUJn3f9SFuv9PtfXhxVODqDIpUWUwIiSo4A5EoFKIrH8nIgCATq3AvbtW4dbGajjdUxeZFr0Kra2tOHbsGJqbm2EymQAA1dXVOHLkCMbHx1FcXJyyMahUKpSWlsJut6Oqqiplr5trstXLUhTFlGf2ZVM4HMbJI/tRo/Khy2iBJxJFsSiiWCuh1KjC2zdW42NX1SWd2yVJgs/nw/j4OAYHB+F2uxGNRqHX66HVauF2uzE8PIxQKASz2YyysjKUlJRAFC9fBDHbDZrT6cSZM2ewYsUKrFmzJmcCvWwQTURylMkJpUJNpKClEaRM5JzTkviCEext60VrwhfJzgJqDlho+z/iCuD+p9shigIUQQ9MJhOUKhUc7gCkqISHb9/Mi2EimlMwGMT+/fsBANu2bYtntwSDQRw6dCjlDcEjkQja2tqwY8eOeQUh5CbxO9lm1CDg90Oj1WbsO7mtrQ0tLS1pe/1MmZycxMsvvwyr1YotW7fjvw7148VjvYgqVFALEpqqDfjE9VvmdV6PBckmJiYwPj4Ot9uNSCQCQRAQiUQQDoeh0+lgs9lQWloKg8GQFNia7QZtx4piNJk80KoUWLduHVQq1SVGkDlsEE1EqZbJAPsTr3XHJ5SmJzakY0Ip09sj+WNATEYc7kA8CyBdX165PAOZif3PBacHXXjguU5UFesQDQWgjd3MhqOwj/vwtZs3or7ClOVRElEuc7lcOHr0KBQKBbZt2wa1empWdHh4GCMjI9i4cWNKt3fhwgVEIhGsWLEipa+bCxK/k9VKEQ6HA5aSEoQlISPfyfkQEOvt7cWBAwfQ2NiIurq6eHDq5dZDsFatgHd0EOtWLltSj67EIFlioCwYDEIQBFgsFixbtgxlZWX4j/0X3rph0igxPO7C0LgHNzcuw/+9viFVu50SvLkjolTJdIB9+oRSTDonlAotkYKWjiWTMmIzatIWCJLDDGQ69z+XJKf7vlUDz3RfIpovk8mE+vp6dHV14fDhw9i+fTsUCgXKysowODgIh8MBm82Wsu0tW7YMra2tWLZsGZTK1F9aZHOyZnoJhl6ng9fngx+qjHwnC4IASZJypnRvIaLRKA4ePIiBgQFce+218b52MQZFFA01Fhwa7oHRaFzStgRBgF6vh16vR2VlJYDkINnw8DA6OjowMOrCr/rU0Go0UGqiGHP7oVepUG0rwqF+F3qHRmHRz50dNn0eebZ55bnmmufz3MR/Oz1BvHLSDpNKgFEpxRtEA1O9A29trC6I6yIiSo1Ml/9no6fXbO0k+D1Jl8KAGAHIXn8UmqlQ+6YRUWqVlZXB4/FgdHQUR44cwdatWyGKIjZs2IADBw6gqKgoZWVhgiBg5cqV6O7uxtq1a1PymkBuTNZM/042qTWwDzkRUesy8p2sUqkQDAZT2jQ+E/x+P/7whz9Ao9HgpptumvWzFgv0RaPRlJbxxkwPkm3ZsgXH+5z41fBRSP5JDAyMoaS4BIIgQBEJYtgbRue5PqwoUc8agJwrKDmf514qoDnXc/tGA5j0BVFmUGJkeAxVVVVQKJVsEE1ECxZfgVGvhjoaQCQYgc2oB5C+AHs2e3oVSiIFLR0DYjRjiVoAnIHMsumrx+nVCuxpqOQKk0S0ICtWrIDL5YIgCGhvb8eWLVugVCpRX1+PEydOYMuWLSnbVnl5OXp7exEIBFIWvMmVyZrE7+TByQCikHDD+vKMfCdrNJqMBsRSkY03MjKCP/7xj6ivr0dDQ8Mlg0GRSCTtvefC4TAGBwcxODiIUW8YYiQEjakYy8tK4HA6UVJSAocniDK1hJYt63PmmsdcFoD1hAdurwdWiwWKN7MvmTFORAs1la0Vhi7qBzQqeL1ehMNhmPXGtAXYOclPcsCAGOXFErW53PtsMZjuS0SpIAgCGhoacPDgQRiNRpw8eRIbNmyAxWLB0NAQBgcHUVFRkbJtrVmzBufOnUtJj7LEyZoSrQIKhZC1yZrp38n+8RGUmrUZyVJTq9UIBoNp304qsvEkSUJnZydOnjyJq6++GuXl5Zd8LgC43e74aqipFIlEMDQ0BLvdjmg0ioqKCtTW1iJ07hzesbEaf+yZxKgvDIVSjX7nJLwRIedu0EpNGjRW6vA/7WMwGAxQh6O8mSSieUu8PyrSigh53VDqtCgvKgIkCRMTE7g47ITBaEpbgJ2T/JTrGBAjWS9RmwvlNOnEdF8iWipRFNHY2IhDhw7BbDajq6sLdXV1qK+vx/79+1FSUpKy7COLxYLu7m54vV7o9folvVbiZE0g4EMgEECJxZLVyZrYd3KgRI3Ozk5UVVWlfZuZCogtNRsvHA7jlVdegd/vx7vf/e746qZzCYVCUKvVcLlcKQuIRaNRjIyMYGBgAKFQCOXl5di0aRNUKhW6urowMjKC5uZmNEIBvWGq6bIrokTQ68K7mlfn3A1aMBjEJt0kNNtXoe38GG8miWhept8faZUCatReXLGmHH/qc8HhDsCsVSGk1MEdDqNe7UWxduq+KdVJBpzkp1zHgBjJOp01V8ppiIhymVqtxqZNm3D8+HFEIhH09/dj2bJl2LBhAzo6OrB169aUNW2vr6/HmTNn0NjYuKTXSZ6sMSAYCsHtdmesmf2laDQahMNhRCKRtPS+mr4tr9eb1m0stXXC5OQkXnzxRVRXV+Ptb3/7vD5LsdLayclJ1NTULHrskiTB6XTi4sWL8Pv9KC0txfr166HVagFM9TI7ePAgysrK0NTUFB9b4g2ad2wINqMqpybSJElCe3s7mjZvxDVFRXhvc2GstE1ES5d4f1RmUOLiyCjaFTrcsEyPPQ3GpGytW5trsWeNCa++0YZTYSsO9k2mJcmgECf5862CKV8xIEYA5JnOyt5nRETzZzQasWbNGvT29mJ4eBgqlQrl5eUwm83o7+9fUlAikclkgiAImJiYQFFR0aJfZ8ZkjcGEC4MOBAQVbm6qyfr3u81mg8PhuGRZYCqo1WqMj4+ndRvTWyc4RkZgKy2dVzZeT08P2trasHPnTtTWzv+aIRYQczqdMBgMCxqvJEkYGxvDwMAA3G43rFYr1qxZMyMrcWhoCN3d3WhoaJg1Cy12gxYtM2D//v2orq5Oez+z+erp6YHVao3/DRXizSQRLVzS/ZFBjf7+flj0OngiUbx+ehAPvmsNblxnhTskwGZ663vl2U4nnjnYg2prEaqK9UwyWIJ8r2DKNwyIEQB5prPmQ+8zIqJMstls8Hg88Hq96Ovrg0qlwurVq9HW1gar1brkMseYtWvXorOzE9u2bZv370yfSR1xBbC91gJvIIz2/gnYJ/wwmc1Yp/Lgtk2lKRnnUlRUVODs2bMZCYilu2RyeuuEWH+vS7VOkCQJ+/fvx+DgIN71rnctuOwxFhCTJGleQShJkjA5OYmLFy9icnISJSUlqK2tnXW7kUgEJ0+ehCAIaGlpuWwWnyiKqKmpQW9vL1auXLmg/UiH8fFxjI2NYevWrdkeChHJTNL9kSCguroa4XAYqmAI9gk/ugZGUKGNIhAIYFKS0A1gMijh5RMeWI0ahL3jCKok2N78bmWSwcKxgkleGBCjJHKagZRz7zMiomypra1FR0cHysrKcObMGWzcuBENDQ3o6OjA9u3bU1I6qdPpoNfr4XA4YLPZLvnc6TOpGqUIlUJAOCLBH45Cr1Zg87IiXLO2DNUlOhgUURw9ehTbt2+HUpm9yxi9Xg+fzwdJklJWbjobtVqNQCCQttcHZmbjhSISHO7AnK0T/H4/XnjhBZhMJtx8882LKhsNBALQ6/WXfQ9dLhcGBgYwNjYGs9mM6upqrF+/fs5j7nK50NHRgVWrVi0oWFldXY3W1lbU1NRk9XMVCoVw8uRJbNu2La2fKyLKT9PvjwRRhEqtxkRQgsVkwNYNa2Z8p58YGAc6O2HVKyFCG/9OZ5LBwrGCSX5yIy+caBFiF/CjngAc7gCC4Wj8An5nnZVfNkREc9i4cSOGh4excuVKdHR0QBRFlJWVoaenJ2XbWL16Nbq6uuLZRnOJzaSKooCqYh0GJnz4U7cTFyd8qCrWQRQFvHJ2BAd6R2EzaqDT6bBu3TocPXr0sq+dblarFaOjo2ndhkKhQCQSSes2gKnWCXsaKiFFJTj9UUhRadbWCUNDQ3j22Wexdu1aXHPNNYvuoRYIBBAKhWbN8PJ6vTh37hxaW1vR29uL0tJS7NixAxs2bEBRUdGsgSJJktDb24tTp06hqalpwZl7giBg5cqVKf0bWChJknD8+HGsW7cOKpUqa+MgIvlazP1RqUkHk1aNABQwGI3QvrkoCpMMFi6WoWfWJn+Hm7UqeIMRON3pXySHFoYZYjmGzfcWRo69z4iIsk0QBDQ2NuLgwYNYu3Ytjh49iq1bt6K9vR2lpaUpWfVPrVbDarVicHAQlZWVsz5neq8TXzgKdyAMvUoJTyCMqCTNOrNaUlKCiooKnDx5Ehs2bFjyWBeroqICvb29sFqtadtGprKEElsn/LH1MHbv3Jx0HSJJEjo6OnDq1Clcd911S95nv98PtVod/6z5/X4MDAxgZGQEWq0WVVVVqKurm9f+B4NBHD9+HMXFxUvKrCovL0dvby+CwSDU6szfAPb29qKoqAglJSUZ3zYR5Y+F3h/JeYG1XMMKJvlhQCxHsPne4six9xkRUS5QqVTYvHkzjh8/jvr6ehw9ehQbNmxAZ2cnWlpaUtJcfOXKlWhra0N5efmsr5fY68TtdmPMG0Q4IkGnVsAfimBoZBQ1FdZZyzaWLVuGU6dOoa+vD8uXL1/yWBfDaDTC7XanvWwyk2xGDZYXKZPOpeFwGC+99BIikQhuueWWlASLQqEQXC4XwuFwvJ9dVVUVVqxYsaDPntPpxJkzZ7B+/XoUFxcvaUyCIGD16tU4d+5cxgOtExMTcDgcaG5uzuh2iSj/LOb+iEkGqcHgovwwIJYj2HxvabLd+4yZfUQkRwaDAWvWrEFPTw/q6upw+vRpVFdX49y5c1i7du2SX1+hUKC6uhoXLlyYdQXCpJlUkwmB8ASiER98kgS1UoSlyAjHyAgEnXnWmdX6+nocPnwYBoMhrVlacxEEAUVFRZiYmFhyMOZSxv1RnLJPJq0IlikTExN44YUXUFdXh8bGxiUH/sLhMAYHB9HX1wdBELBz506sXr16waWX0WgUZ86cQSAQSGk/OavViu7ubvj9fmi12pS85uWEw2GcOHECzc3NeRNYJaLsW8j9EZMMUofBRXlhQCwHsPmefDGzj4jkzmq1wuPxYGRkBMuWLYPdbkc0GsX4+HhKgjw1NTVobW1FdXX1jKDFjJlUkwlmbQADE37UWAzQaDTwBA24ODSK97asmnEuFAQBW7ZswYEDB7Bly5aUrZK5EBUVFRgcHLzssVrMxEnsHPP8MTeUpzph0Cgzeo7p7u7GgQMHsGvXLlRXVy/6dSKRCIaGhuKfrYqKClRXV8cDpgvl9Xpx/PhxLFu2DOvWrVv0uOayZs0anDlzBps3b075a08XK0Vdu3ZtVso0iYgSZTvJIBPSncjA4KK8MCCWA5KWx03AlT1yHzP7iCgfLF++HCdOnEAoFILVasXY2BhOnDiBHTt2LLppekxis/I1a9bM+Pn0mdRamwnLLTqMT7rQP+qGSavGrc21WKdwwO9fPiNrR6lUorGxMWsrTxYXF+P06dNz/nwpEyexc4xKFFFuVMEbQUbOMZIk4fXXX8fIyAje8573QKfTXf6XpolGoxgZGcHAwABCoRDKy8uxadMmqNVqSJKE/v7+RQWALl68iP7+fmzatCltAdDi4mJ0d3fD4/HAYDCkZRsx/f39Kc1wZMY6EdHsMp3IUAjBxXzAgFgOYPM9eUrM7LPqVYhEo8zsIyLZWr9+PQ4dOoRVq1bFVwA8deoUNm7cuOTXvlSz8rlmUi+MjOONw8exbdMKrKoqhcfjwZEjR2bNBEtceTLTZWeCIMBgMMDtdsNoNM74+WInThLPMeqIBIUgwWacCgam8xzj9/vR3t6OdevW4ZZbblnQsZQkCU6nExcvXoTf70dpaSnWr18/I4gZDAYRiUQWtHhDOBxGR0cHtFotWlpa0v4er127FmfOnEFTU1PatuFyuWC327F9+/YlvxYz1omILo2JDDSbpXfMpSVbzPK4lH2xzD6TWgGHw4FwOAyAy+rKzYgrgNODLjjcgWwPhSirYitPnj59GlVVVdDpdBgeHobD4UjJa69evRpnz56d8zk2owb1Fab4Oa+mtBi3vn0HRi50Y3h4GAaDAVu2bMGxY8fg8Xhm/H7iypOZVllZCbvdPuPxGatouidhVERh0avR2uW85PdO4tLtIw4HXC4XgPSeY+x2O5555hlUVVVh165d8wo6SZKE0dFRdHR0YP/+/RgbG8OaNWuwY8cOrFq1atY+XIFAYEEBsfHxcRw4cAA1NTVYt25dRgKeRqMRoihiYmIiLa8fiUTQ0dGBLVu2pGR/Yjd6oiigqlgHURSwr8OOvW298efwfEdEhWp6iyK1UoTNqIHFoLns+ZjyGzPEcgSb76VXOkoILAY19CoR/UNOVNvM8Yt+ZvbJA2fTiWZSKpXxoNO2bdvg9/tx+PBhvOMd78CYL7Kk71Gr1Yqenh54vd55l7qp1Wps374dx44dg8/nQ21tLZqamnDkyBE0NDTMCKgsW7YMJ0+ezPjKkxaLBV1dXTMeT2yJ4PP5IEkSQqEQgh4fnH4JJ7r6sHN97aylg7Hs8VGXF+FQCKFQCEB6zjGSJOHYsWM4e/Ys9uzZc8kS0NjzJycncfHiRUxOTqKkpAS1tbXzDnAFAgGEw+HLPl+SJHR1dWFychLNzc0Z77G1du1adHZ2Ytu2bSl/7c7OTqxevRoazdKvSaZnrEuSlJSxfsOGCvzuxCDPd0RUsNiiiObCgFiOYPO99Ehn0MNqUGGZxocjUMIdESGGo1xWV0aYNk00O71ej3Xr1uHYsWNoamrCiy+9ggd++grsUdOSv0djZWiNjY3z/h1RFNHY2IhTp07h1KlTqK+vx9atW3H48GFs2LABRUVFSc9ft25dxleeFEURGo0GPp8vqd9WYksEwe9FcXExFEolAqIGZfoISnRKHD9+HJFIBFarFRUVFfGeVbHs8Z++dgpmjQkRCBiZ9GHMF0rpOSYcDuPFF1+EKIq47bbboFQq58xYcrlcGBgYwNjYGMxmM6qrq7F+/foFZzgFAgFEo9FLruLo9/tx/PhxlJWVoampKSurL+p0Ouh0OoyOjsJisaTsdWP900pLS1PyerEbvXKjCkPDw1AqlYhGoxAUKkyEBPyk9TwO9Y3xfEdEBYstimguLJnMMdNLRmhp5lNCsBCxcoPhSR8OHz6Mj1y9Fu/ZuhxSVIJ93AcpKjGzTwamp01HQwFYdEqmTRO9KVZ+eOrUKfSqavB6nwc+n3fJ36NmsxkAMDk5uaDfEwQB69evh06nw9GjR6FSqdDc3IyTJ09ibGxsxnO3bNmCM2fOwOv1LniMixVbbTJRvCWCO4BRXxgRiPGWCFeuLsWmtSvR3NyMbdu2wWQyobu7G62trTh58iRGR0dxy0Yr1hv8UKlVcEdV8Pp8KT3HjI+P4+mnn0ZVVRWuv/76WRck8Hq9OHfuHFpbW9Hb24vS0lLs2LEjHoxcTKDK7/dfMvA2NDSEo0ePYt26daitrc1KMCwmVuorSVJKXs/tduPixYuor69PyesBUzd6WqWA/mEnbFYrbDYbykpLERFUiITDaD3dD0XIBx1CLBMiooLEFkU0F2aIUd6aHvQAsOim94mZZp5gGGGvG7vqy3Hf5jLcW61gZp/MTE+bFhUKjIyMwGAqgjMYZdo0EabKDy+MjOPlkwNYWV0G18hFuBRRmE0m4M0eWItp7L527VqcOHFiUWVotbW10Gq1OHjwIJqamtDc3IzDhw9j9erVSdlgiaWfmVp5srS0FIcOHcLKlSuTHr+zpRZutxv7e8JztkQQRRFlZWUoKyuLlyMODg7iwIEDeFedBRGliOq69XAOnMc7U5TRc/bsWRw6dAhvf/vbUV5envSzUCiE7u5ujIyMQKvVoqqqCnV1dSkLTLlcrlkXIIhEIjh58iQEQcD27duXvMJpKmg0GhQXF2NkZARlZWVLeq1U9w2LKdIIWKby4LhCh/FAFGZhKmN9MhTFttXlON4/AQOC8Pv9MLx53FkmRESFhi2KaDYMiFHeSmWteLy8Tq+GNuxFVK/Dq+ddMLb14t5dq7isrsxMT5uOla70DDigVKthMaiyPUSinFBSsRyTXjtKDBoYKioxNDQEURQRCIQwFpBwsqcfO9cth0o1/78ZvV4PnU4Hp9O5qJLG8vJyaDQaHDx4EFu2bIkHxaLRaFIJml6vR319fcZWnlQoFFAoFDNW0tSpFXh7pYTbm7fAG1FcduJEEAQUFRVBo9Hg6NGjuOaaa/Dqq68i5OiF22HH2bNnUVNTc8lyw0uJRqN47bXXMD4+jttuuy3ewyoYDMJut2NoaAiDg4Ooq6vDihUrIIqpLyaYnJxEdXV10mMulwsdHR1YtWrVjABdttXV1eHgwYMoLS1d0ufo5MmTWLlyZVJZ7VKFQiEcPnwYn9rThOdOjs240XvnujL8zbn98KmUWF721t8by4SIKNelugc0WxTRbBgQKxDpaCqf61JVK56YaWZURCGZjdDp9XC4A4vOkKDsiqVNx3qomLUqTPrDiKh0aCpTYqD7NCwNDTmRnUCUTVajBmWWIgyNjmNlpQ0GvR7BYBAaYzFsuijMKgHt7e2IRCKwWCwoKyuDyWS6bNBgzZo1OHLkCCwWy6ICDMXFxfEMsPXr16O5uRlHjhxBNBpNCqZYLBZ4vV6cPHkSGzZsWPB2Fqq8vBxDQ0OoqamJPxaNRuH3+1FTWhx/bD7n5IMHD2LdunXQ6/UoKSnB9u3bMTAwgIGBAbjdbgSDQVgsFlRUVMBoNM55HBO3pRcjeP7551FVVYWbb74ZkUgE/f39GBwchCAIqKysjAcP0xWUGnEFcGbIjcqVU73SJElCX18fhoeH0dTUtOhAXzoplUqUlZVhYGBgRiBvvux2O0RRTOlxDYfDOHz4MNatW4fi4mLcu6sk6UbPrBZw5MgRvG21Da/2uuHwBN8837HfKRHlrnQvfMVEBkrEgFieK+SV9GYPeiz8IjAx00ytfGumnOUG8jZr2vSmqbRp17gTBw4cwObNm+e9Gh5RPio1aXDlaht+cyyIHrsDlbYSXBgcQcA3gdtbVmHLuqnyvWg0itHRUfT398PlckGn06GsrAw2m23WckW1Wg2LxYLBwUEojZZFTdjo9Xps27YNR44cQW1tLbZu3YqjR48iEomgqqoq/rzYypMXLlxIClSlQ3l5OY4dO5a0ncRMuPmek4PBIC5evIjbb789/lgsYNXf34+mpiZEo1GMjY2hr68PLpcLJpMJ5eXlsFgsEEVxxrbEaAjFgSF88sZGFBv18ay6iooKbNmyZUFZfouROJ7eixH8ydOHnSvd2KSbQLmtBNu2bctqr7DLqa2tRVtbGyorKxecNef1etHX14ft27enbDzhcBiHDh3C2rVrUVxcHH88dqPndrtx6NBxbNiwAZt1RhjbelkmRESywIWvKJMYEMtzhf6Fkopaca5Kkp8ulTatKyuD0WhEe3s7Vq5cmXPlO0SZFPu+fPX0IHqGxmDQ6rC5GLiq4q3ghSiKsNlssNlsAKYCAMPDwzh69CgkSYLVakV5eXl8BUUAKK+uwbee/hMGIotfvVKlUmHbtm04duwYfD4fmpqacPToUUSjUSxbtiz+vNjKk3q9Pq0rT6pUKkiShHA4HA8EDgwMYM2aNQDmf04+cuQI6urqZgQTRVGEVquF1+uN74vVaoUkSXC73RgcHERXVxfUajVeGgBe6/XAatJCHXJjaNyFEaURPzvYjw+/bSU2bdqUVNqZbon7XqSKQopG8Mu2LoSaavCpHXUZG8diKRQKVFdX48KFC6itnf81RDQaxfHjx7Fp06aUlZ9GIpF477ySkpIZP3c4HDh37lxSxh3LhIhIDlLZA5poPhgQy2P8QklNrXiqMs0oN82VNq3X67F9+3Z0dnZifHwca9euzensBaJ0SfwePdnTj6h3AqqIH+6JUTgcpngQLJFer8eKFSuwYsUKRCIROJ1OdHd3w+PxwGQyoaysDM+cnMQRB1Cs86PKYl70hI0oimhsbMTp06dx6tQpbNmyBR0dHYhEIvHARWzlyQMHDmDLli1pzfwsKyvD8PAwqqqqIEkSfD4f9Hr9vM/JoVAI58+fx2233Tbr61dVVWFgYACrV6+OPyYIAkwmEwwGA6qrq9E9MILXz51BOBjA8KgbEoBSsxl+KHDCGcH5QScmJiYgCEL8v9jrAMDg4CBOnTo168+n/zfb49MfG/WG8PKJARgUgF4IwxEOQiMFUVtuwdEBLxzugCzOpTU1NWhtbcWyZcvmXVJ/6tQpLF++PGWfuWg0iiNHjmDlypWzBnf7+vowMjKCbdu2zQioprtMqBDbcxBRaqWyBzTRfDAglsf4hfKWpV4EclWSwqRQKLB582ZcuHAh3sA7kxkVRLnEZtTgqk11OHPmDCYmIjCbzTh37hy0Wu2sKwbGKBSKpBUU3W43Tp0fwPNH+qATgajXA5XNvKQJG0EQsG7dOvT19eHYsWPYtGkTTp06he7ubqxaNRVcS/fKk7FggMlowVDfOVRVVSWVSyadk9/sm7V8+fIZ5+Tjx49j+fLls37XSJIEg8GA48ePxzPFvF4vgsFg/DjodDoM+kRERRWUUTcspaWw2WyQAATDUdgnfLBU1mJNmQGSJEGSpPhrS5KEaDQKp9MZD+hN/3nif7M9Pv21AMDhCsATCKPMqITL5YJeb4DFap0aj4yuRwRBQG1tLc6fP4+6ustntQ0NDSEajaKysjIl248Fw2pqapIWkACmjvmpU6cAAFu3bs3oBE4ht+cgotRiZQ5lGgNiaZILs2T8QkkdrkpS2GpqamA2m3Hw4EFs2LAhqV8LUaFZs2YNjh49ir6+Pqxcvxm/ee0Irm5pQqVl7qAYMHXD7vF4MDExgaFxN/zhKKxaEaJCDZfLBXNR0ZInbJYvXw6tVotDhw6hsbER3d3dOHv2bLxkMR0rT84WDKhWurF2fQgDAwPxwMmMc7IgwOfzwRNVxM/JoVAIZ86cwdVXX43+/n74fD4MjLrQfroPE4EoirUKaLVaRCIRBAIBlJWVQafTQa1WJ+2LfmAEfvcZlBSXoPTNlQUFAO5gCEaNCpUWI7Ta2Y+vJEnQ6XQwm81LPjYxmqIALMfGERUFBIMTsL0ZJJTj9UhlZSX279+P5csvvbqqz+dDT09PyvqGSZKEY8eOoaqqakYZfzgcxrFjx1BaWorly5enZHsLUejtOYgodViZQ5nGgFiK5dIsGb9QUo+rkhSuoqIibN++PemmgyWUVIgEQcDqdRvwD//1R5ztOAqFWoefnX4De5pW4a6dK+LnumAwiPHxcYyNjWFychLRaBQGgwElJSXYtHYVKrqjEEUBNoMawyMjMEYimPSHlxwgKSsrg0ajweHDh7F582ZcvHgRp06dQn19PQRBSPnKk7MFAw4NR/H4Syews8QX75tWatLgilVW/Ob4AMLBECIQ0D0wgqCowY5KFbpPHMPAwABEUYTH44Gg0uD5niAOXwxhZNSMo9Kb1xNbalFVVYXR0dFZg/ODg4M48Oofcf2WerzW54HDHVjQ+T+W5ZVKseuR37YPIOALo1ythcMdkOX1iCAIWLVqFbq6urBu3bpZnyNJEtrb27Fp06aUrFYce72ysrIZ2WZ+vx9Hjx7F6tWrZy1fTrfEUmCLTglRIRZcew4iSi1W5lAmMSCWYrk2S8YvFKLUUalUaG5uxtmzZ9He3o6GhoaU3OwQyc3PDl3Eaa8eQZ8TKy1mTPiBn7eexdjYKN65XIlgMAi1Wo3i4mKUl5djzZo1MxqKJ07Y6A0m9A6NIqTQpCRAUlRUhMbGRhw9ehTr16+H0+nEyZMnsX79egiCkLKVJ2f0BZMkWHRK+I0a/G9HP1Y3TpU3+v1+SJKEeoWEAStwwhnAZFCERinixnVl+Ng7NkKtAC5cuIA9e/bAZDLhide68XL3OEp0KpQalBBFIX68PvK2lUmZbzHnzp3DkSNH8M53vhM6YxFMi1hZUJKktAT772ypxcjwCPb7NbBP+GV9PVJaWorz588jEAhAo5n5WT19+jSWLVuWtIjEYkmShI6ODlgsFlRXVyf9bGJiAidOnMCmTZsuWbacTlOlwGEYhBDcERHmoiIAhdmeg4hSg5U5lEkMiKVQLjax5xcKUWoJgoC1a9dieHgYBw4cwObNm9PaoJso18TOdTazFobiCgwODsJgNMKoBNrOj+MDV11x2fJJIHnCxhmMIhyJ4tp1lpQFSHQ6HbZt24YjR45g+fLlEEURHR0daGhoiPccO3ToEAwGAywWy6K2kdgXLBIOw263Q6fTQSso4PGHYLRVoa5uGXQ6XTzIdBUAhzuA9jM9sOhVkHyT0GuUOHHiBEpKSmAymZKuJ0xKCa7AzOsJo9EIl8sFk8kESZJw9OhR9Pb2Ys+ePfHgSC6d/7UqEU36Mdx0Uz00Rbasj2cpBEHAmjVrcPbsWTQ0NCT9bGRkBMFgcEbwajEkScKJEydgMplmBG6HhobQ29uL5ubmrPa2LNGrEPZ74VMpUWotij8ux3LYxcqFNilE+YiVOZQJDIilUC43sc/FLxReQJCclZWVwWg0or29HStXrpzR04UoXyWe69RKDSorK+H2eLByWQXODjhw7uLQvAJi0ydsdGIYIxe6U9peQKVSYdu2bWhvb0dRURGKiorQ3t6OzZs3QxAENDY2LmnlycS+YIqgB2q1GlabDQ6XH3qNAtVW86yvazNqsKnGCpfLhXHv1EIDp0+fxjXXXAPgrWNcYVKjv78PGrUakKSk64nYapN1dXVobW2Fy+XCjTfeCK1WO2NbCznHpitDbHx8HJFIBOtWVufF4iQlJSXo6uqC1+uFJ6KYWlBBBfSeO4eWlpYlv36sSb5Op8OKFSuSHu/p6YHL5cK2bdtmZF5mUiQSQd+ZDlxZZ11Uea7c5VKbFCIiWhwGxFKITeznhxcQlC/0ej22b9+Ozs5OjI+PY+3atewrRnlv+rlOq9MhEong4sgoSkvMCIw7MD5unffiE4kBG9eIFqOjo4vO2JqNKIrYsmULzpw5g0gkAqvViqNHj2LLli1LXnkysVdn2O1GdZkFDncAwxNetNQWI+hyAqiY9XcNBgMGBwexatUqtLa2wmg0oqSkBMCbx1gl4mxfP5aXWqFUKuF2u+EX1PHriRKDEZ2dnbhw4QJEUcR11113ySbvC5GO77He3l6Yzea8CIbFLFtRh3967hB6/Vp4g2EEvW5c27AMjRFAt8TLmbNnz0KpVMZXSQWmVpns7OyEVquNB3WzJRgM4siRI1i9ejU2bi5eVHmu3OVamxQiIlq47E0r5aHYhfGoJwCHO4BgOBpvGruzzirLWbIRVwCnB11wuAMpe83YBYQoCqgq1sX7ouxt603ZNjIlHceH5EWhUMTLJg8ePIhgMJjtIRGl1WznOh9UcAUlrDEDu3duxcmTJ+Hz+Rb82rEytFQ3dhcEAfX19TAajRgeHkZZWRmOHDmCaDSatPLkYrZ7Z0st9mysRCgcxlgAkKIStleocN91DXC5XHO+pk6ng8/ng9lsxvnz57Fx48b4z2xGNSwRJ0KCBiGlDiqNDvYxN0bdb11PTE5Oor+/H4Ig4Oqrr05ZMCwdTfXD4TDGxsZQUTF7cFCu/ufUGF6/4EM0EoZJDEOv1eJ/z44u+Xqmq6sL0Wg0qUdcKBTCoUOHYLPZsGbNmjmDYZm4LvF6vTh06BDWr18Pq9Uaz/Z8+PbN+NrNG/Hw7Ztx765VeT3JGS9r1quhFyNQKwTYjBpYDBq0djl5XUhEJBPMEEuxfGlin64srlzss7YYzHKj6WpqamA2m+M3CfPNjiGSo9nOdbdtW4kmsweDg4PYsmULjh49uuCsK7VaDavVCrvdjqqqqpSPe/ny5dDpdOju7sayZctw6NAhbN26FRaLBR6PZ1ErT+rUCtzeUALdsIBtb2uAxaBCV+dRlFtL4BwyY3JyEkVFRTN+LxbQOH/+fHzlS2AqIPX666/jHcvVqF+7Eq3dTtgn/NBodWis1OLOlloMDg7i2LFjqK6uhtVqXXBm2+WkOvPo4sWLUKlUKCsrS+nrZlPseqbaakbUOwGo1VhWaoXDE1zS9UxPTw8CgQDWr18ff8zr9eLYsWOXPLdk6rpkcnISnZ2ds5YZ52J7jnQZ9QQx6fVDJwWgMbzVKiUX2qQQEdH8MSCWYvnSxD5daeCJvWe8Xi8UogiNViu7CwimydNsioqKsG3bNhw7dgylpaVYvnw5SygpL811rpMkCUeOHIFGo4lnXTU3Ny/o72DlypVoa2tDRUVFWvojlZaWQqPRoLOzE9XV1fGgWE1NDU6cOLGolSf7+vqwelk56itMGB0djZc+VlZWYnBwcNaAGDAVeOrs7ERLSwsuXLiAlStX4tixY5icnMT1118PpVKJW5umjrHFoMLZ44fR39uN3t5eVFZWoqGhAW1tbSnt+5WODLHBwUEoFAqYzeaUv/Z8pbpvaex6xqoVcGFsDCtXroQ/FAUkCeO+0KKuZ3p7e+HxeLBx48b4+zk6OorTp0+jsbEROp1uzt/NxHWJw+FAV1dX1hv5Z1sgEMBg71kgHITabIbJ/Nb7wjYpRETywpLJNLEZNaivMMkiuDPd9CwutVJMWRp4Yu8Zr8cD1ZsXVHK6gEjn8SH5U6lUaG5uRjAYRHt7OyKRSLaHRJS2Mqrp57pYo/rz589DEARUVFTgxIkTC3pNhUKBmpoanD9/PqVjTWQ2m9HY2IiLFy+itLQUhw4dQigUwvr16zE0NITR0dEFvZ7dbo8H0ex2OyorKwEAxcXFGBsbm/P3PB4PotEoqqurUVlZiTfeeAMXL17E29/+9njWV+wYWw1q+P1+nDhxAsuWLcOmTZsgiiKKi4sxMTGxyCMxU6qb6k9OTkKlUkGv12dlgsAbDOOJ17px/9PteOC5Tnzxl+144rVu+IJL+262GNRQCVH0DY6gqMSCjv5R7D/vxMHeMZx3evB8p31B2+jv78fExERSMOzixYvo7u7G9u3bLxkMG3EF8PqZYWgQhhahtFyXDAwM4Pz589i2bVvBBsOi0Si6urqmehDWr8INTSsx5gvlTZsUIqJCxIAYzRCb9TRrk/uRmLUqeIMRON2L75EU6z3jcPkx4Y8iHIXsLiCmH5/JyUlEwuGUHB/KD4IgYM2aNaisrMSBAwfipVBEmZauYMCliKKIpqYmnDp1CsXFxVAqlQsOblVXV2N4eBihUCg9g8RUD6/t27fD4XCgpKQkHhRrbGzE6dOn5/13GwwGEQgEYLFYIEkSXC4XTCYTgKnvAoPBAI/HM+P3otEohoeHUVNTA0EQEA6Hcfr0aVx55ZUzgh/BYBAHDhyAUqmEUqlMWsCjqqoKFy9eXOLReCtomupzWF9fH7RabdbKJdPVt1QMulGj9sEvKTHgV2LQHUUwFIUoCLAY1Hjl7Mi8tzEwMACHw4FNmzZBEARIkoQzZ85gbGwMzc3Nc5bEer1edHV14eXWg3BMuFFi0MBoMMR/nqrrkp6eHjgcDjQ3N0OhkEdbiFRPAgwPD2P//v3QaDRoaWlBcXHxVP/AhkpIUQn2cR+kqCTLNilERIWMJZM0Q7pXy7yzpRZjo6M4NghZ9lmbfnz0Oh2cTiegNckmy40yo6ysDEajEe3t7Vi5ciXKy8uzPSQqMNkq71apVGhqasLhw4fjwTGDwYDS0tJ5/b4gCFi9ejXOnj274J5eC6FUKtHc3Izjx4/DYDDEyycXsvLkyMgINBoNtFotxsbGUFxcnJQJVVFRAbvdjtWrVyf9Xn9/P0RRhMlkQn9/P86dO4e1a9fOeH23241jx44BAFatWoVoNIoLFy6gtnbqnGkymeLN+xeTgTW995RWKWCVIYy16yNL7j0ViUTg8XigUChm3bd0i2d061TQC2GoFeqU9C0dGRlBT08PblxtRLHNgJ8dsQMCEI0EUWM1Y22FGeO+0Ly2MTg4iMHBQTQ1NUEQBEQiEbS3t6OkpGTWY+b1emG32+FwOKDValFZWYmrd1Th+YEOBCEACZ+BpV63SZKEU6dOQRTFeLAu16W6l5rb7capU6dgNBpnfB/kS5sUIqJCxgwxmiGdq2WOuALoG/ViS3EI//x/tspyNaLpxycqKCBpjLgwPIZttUWwGTVcfZLi9Ho9WlpaMDQ0hFOnTqWlPw/RdJIk4cLIBF45aYdeiEIV9iHo86BYI2asvFur1WLz5s04duwYNmzYgO7ubrhcrnn/vs1mg9frTXuGpSiK2Lx5MzQaDVQqFQ4fPhxflXI+K0/a7fZ4z7DEcskYq9U6NWmSIBZoqK+vx9DQEE6fPo2amhq0tLSgp6cn/jyHw4H29vZ4gHDZsmVYtmwZBgYGEI1GAUwFD0tKSi5ZmnkpMzKoBOD1C96UrPxst9tRXl6OaDSa8sb/8zHgnMDI2CQ8E0709/djeGRkyRndQ0ND6O3txf/P3n8H2ZXm6ZnYc8653mdel94iLRLITJgECtXVXVvV3dU13TPTMz0cDjkMbqx2VlqFdrmhVYiklpK4+4ciyBUpKWJ3Ka04jKW4nFkOOZyedlPT011dVV0OLjOR3nt7TV7vj9MfWXkbWUigABSyYOo8EQi4POeec+5x3/u9v/fX0tKC3+fm14dbqHeZaDAXCelpOmqtmCTxoT4jGo2yvb3N0NAQgiBQLpe5ffs2jY2NtLW1VX+uWCyysrLCjRs3WFpawu12c/nyZQYHBwmFQoQ99if+3qZpGhMTE9jtdnp6ep4LMQyenCNQURRmZ2dZWFigv7+f3t7e+57Dz3NMioGBwReHMT58NjEcYgYn8qS7Zd49Y5ctVdArJd4Q95/brownHZ/vXmqjXY/x/35H5/Zmxug+aVDlaMC9tbXF7du3GRwc/NJmsBg8GXRdp1gsUiwWKRQK1d8rlcPBtyAIREoimWKFeq8Vm8WMpmlkMhnKZZmULLK0tU9tT/OpBNcf4XK56OnpYWJigvPnz3Pnzh0uXLiA1fpwA8eenh4WFhYYHh4+tW2Ew+PV3d3N1tYW29vbVWdbOBxmfn7+WMe/u9F1nVQqRWdnJ7quk8lk7vlZURSxWA7zv2w2G7Fsmfm1LZIFmfPBIFNTU3R1ddHf348gCJjNZnK5HIlEgv39/ao4V1tbW11fY2Mj29vbtLS0AIdlkxsbG9WfeVhO6vzsd1rI5aUn0vl5d3eX5uZmAoHAY6/jUdF1nVgsxubmJjlFwOu0kcvKBP1+vF4vBwcHqGYHDosZv8vySGH7e3t77O7uMjQ0xI0bNwiHwyzNTyHIRQQE2luaUBQF+Gx3VjweZ2NjgwsXLiCKItlslunpaQYGBnC73RSLRfb29ojFYlUnWHt7+32v1yf53qYoCnfu3Klm2z0v3H0+19gkpE+y1ODhHYG6rrO9vc329jZnzpx5aFergYHBs8+Tbq7ysHxRXYANHg9DEDM4kSdtA7+7bMcrqcgO+3PdlfF+x+ef/WKeP7+9RnOoxug+aXAPzc3NeDweRkdH6evrw+fzAU/vAW3w7KJpGqVSqeqQOhK8jjK1BEHAbrdjt9txOByEQiHsdjsWi6Xq5Ihly3x/dRJFFLDaDs8ru8NBPFuiVlYwKUVu376NKIoEg8HqOp40NTU1tLa2Mj8/z9mzZ7lz5w6XLl16qCwit9uNJEmkUqnq9XKaNDc3Y7fbmZubY3R0lOHhYbLZ7H07T6bT6apDK51O4/V6T3TS1NXVsba1y4cR+HjlgJ1IHKfVwmRmhh6TWHUIwWGXzffff5/GxkZUVeXs2bP3dGdsamrixo0bNDU1Vcsuc7ncI5dN3t35OZ1KkcvlsFgsmHWFXPnxOiUekcvlsNlsHBwcHHM7nRayLLO1tUUkEiEQCHDu3DksFgvvrr/PjYyJImbcgoRo97AdS/G1zhr+fHyHj1cfboCyvb1NLBajv7+fDz74gHK5THNzMy6TRpdHZ95UQ04WEIp5SphJFCq8OVB/4vFLJBKsrq5WM7lisRirq6v09fURi8WYnZ3FarXS0NBAW1vbQ4nWT+q9rVwuMz4+TldXF36//5GXf5ok8hUyhRJSKUNB0A8FY0F46E7myWSShYUFQqEQV65cOdXJAgMDgy+Opy1IPa34CoOHwxDEDB5IwGX93AP0YzPQTgvRfIr6kA9zvvJEZqCfJncfn1i2zOhWlqagD5NawmJyPJGsEoMXC6/Xy6VLl5iYmMDl8/PBnv7QAzKDFwdN0+5xdxWLxargJYoiNpsNh8OB3W7H6/Vit9sxm80PLXgclXcfvXR5bGYyJbk6UL987vAlTJZlYrEYCwsLlEolfD4foVCImpqaJ1YmFQ6HKZfLbG1t0dbWxuTk5DER6EF0d3czOTnJ5cuXv5CyrUAgwODgIHfu3KmKYouLizidznscWNFoFFEU8Xg8LCws3NdNEwqF+P+88wHTGRMOUcNrUpFVgZv7KkJ9sCoOKorC0tISqqqSTqe5cOECzrtC0o8QRbEapn8k1B2VZj6KG+soEzOSSGNWS9UmCIm9A0StwMbiNHqqhmAwSE1NzSMJBEci4sLCwon78KRIp9NsbGxUBaqrV69Wz5PNzU1+Z6gO2GezZKk6p/7alTPsRWL86Y0VmoPezxygbG5usr+/j9Vq5c6dO6iqyrVr17h58yaCIPAPf/91/vX1Nd6d36OimnCXyrw50HCiOyuVSrG0tFQVwxYXF1lfX8ftdrO+vv5IIthJfJ73tkKhwMTERNWl9jyRyWSYu3ObfDpHbY2P5lDNr/7vM9x6pVKpmpU2PDz80A5WAwOD54OnKUid5MQ2xofPFoYgZnDq3D0DDRwOKB5hxu554e79tJiOd3l6kfbT4PNjNpu5ePEi//jPb/HuSvqhBmQGzxeqqh4Tuo7+fFROJYriMYeXz+erCl5PkocpozKbzTQ0NNDQ0ICu66TTaaLRKEtLS5jNZkKhEMFg8HMPEltaWlhaWiKXy+Hz+VhaWnqooHWbzYbX6yUajX5hjSk8Hg8jIyPcvn27Wua8sLDA4OAgDoej+nPJZBKHw4EgCKTTaXp7e09cX6KgMBOXqfHaqWQOQFOoddtxSzaWMhU2o0nCXgcTExPU1dWRTCapqal5oJDU3NxcdYkJgkBDQwMrKyuPJIgF3VZ6awR+sVTBZzfjEk2kyhpF3cT3LnTwjZfbSafTxGIxVlZWEEURv99PMBjE6XTeV6DUNI10Ok1TUxMul+uJC5maprG3t8f29jZOp5OOjg5cLtexn8lms+zv73Pp0iXeTCfpPDtYdU6Vy2X+/vwebgvkExECnrb7DlCmp6dZW1ujqamJ9vZ2VlZWqKur4+bNm1itVq5evYrJZOJ/+1ovZ8wpGjt62F1b5Bsn3MPT6TTz8/P09/ezsbHB9PQ0FouFCxcuEAwGn6ojKZ1OMzs7y9DQ0Kk4RU+LbDbL1NQUkUiErq4ufsvRyl/O7hPPlX81CZAvn+jW0zSN1dVVDg4O6O3txev1PqW9MDAwOC2etiD16XHwEcb48NnBEMQMTp1Pd2U0fTLge1JdK58VTrs7p8GLRTxXYSkjEPY50IoZNLOHgOvwYWnMGJ0eT6o8VVGUEzO8VFUFQJKkY4JXTU0NDofjCw8Wf9QyKkEQ8Pl81fLEcrlMLBZjZmYGWZapqakhFArdtzTwszhz5gzT09PU1NSQz+fZ2dmhsbHxM5fr7Ozk1q1bhEKhLyzc22azcfXqVW7fvs3o6CgDAwO8d2Oc1u5+Ql4HbvPheVBbW0smk8Hj8dx32xL5CppoRi3mKBSLCMJhh0uvx8XqfpLFzT321Gw1MP+rX/0qY2NjqKp639LSI5fY9vY2zc3NOJ1OisUimqY9tLCytrbGa61Wamo6+MuxFeIFBZsk8HKrg98fab3nfFAUhYODA9bX18nn89hsNoLBIIFA4FguYiQSIRwOE41GCYVCj3bgH0CpVGJjY4NEIkF9fT0XL1488ZpSVZXp6WmGhoaIRqMoikJ8e41sNktK09gvimQKZSSlApKFkqxiM0vVAUo8W0bNpxgdHUWSJF5//XUcDgfxeJxEIkE+n8fpdHLp0qXq5wuCgM8m0dfgg7T7njLfaDTKzZs3qampYWlpiUwmw/DwcDUH7mlyVLJ58eLF5ybfMpPJsLCwQDQaJRAI8Oabb2Kz2eisqAii8MBJAF3XiUajrK6u0tLSwsjIyHPTNMDAwODReNqClDE+fPYxBDGDU+e+ZTv3mbF7Xvmy7KfBk+FXD2gXqFZ2d3dxOByYbXbSsmjMGD1hHjU/4kjw+nSG11FnP0mSquWMDocDv9+P3W5/Kp30HobHLaOyWq3Vzoa6rpNMJtnf32dhYQGr1Vp1jz2ss00QBAYGBhgfH6epqYmNjY2qYPggTCYTDQ0NbG5u0tr6eM1dHgeTycTIyAg3Rsf5pz8eZ0dxkhu/Tdjvo98vccVvpqamhr29PRoaGu67nlqnBZ/Txu7uDh6LgNfrRVEUIokMFlEnubdJ/8Xz7O/vV0Weo+D8B+3vp11igUCAeDz+UCLU6uoqxWKRC+cHuCgItOgRWrsHsAky+YP9E68Lk8lEOByuOvWKxSKxWIzp6WlkWcbr9RIMBtna2mJoaIg7d+587vwwXddJJBJsbBx2CWxtbaW7u/seAUNVVbLZLJlMhpmZGcxmM1NTU2SzWYLBIPX19XR3dyOKIst7SeIfXidbAZtFZGftgLDbRo3DjKDJrC9MsY9MS0sLZ8+eJZ6rsL6T5Ob773G2swVd17lw4cI917vH4yGbzdLe3s7CwgK9vb1VJ1ssFmNkZITa2lqmpqY4f/78M5HRtbOzU3XSPUyu39Mmk8mwtLREOp3GZDLx0ksvHStl/qxJgFwux/z8PC6Xi8uXLz+z92wDA4Mnw9MWpIzx4bOP8RQw+EJ40l0rn1W+LPtp8Pn59AO6vr6eTCZDrqxSLuVJ7G5Q8HQcK88yeHw+nR+RLpT50Z1tspks3+3zVEWvI8HLZDJVxS6n00kwGMRutz8XA8bTQhAEamtrq4PPYrFINBplYmICTdOora0lHA5/ZomcIAgMDg5y+/ZtOjs7mZ+fv6cU8SRaWlq4fv06jY2NX+ggVhRFZko1zOWSSEqWkNdFIZ/jr2IymQYLl4Z9rK+v09PTc+LyR67EDp/EwgaIokCzr5ZMSWZ/N86AVybgchKLxaodBwEaGxu5ceMGLS0t9z2eoihSX1/Pzs4OTU1N1NfXs7i4+JmC2MrKCpVKpdrZslKpEHTb6Kk7DOfPHzzcsbHb7bS0tNDS0lItt93Z2WF7e7vagbNYLD6wvPJ+rk1FUdje3mZ/f5+amhr6+vqqpXyVSoVMJkMmkyGdTlOpVJAkCbfbjSzLhMNhhoeHEQSB27dv09/fXz1nCoUC/7+3x1EFCQ0dSRTRNZ3laAabqPO7QyFaQg7MZjONre38iw/X+HApyk7kAKfVxZpe4P/43ZdOPAd9Ph+RSASTycTq6iqyLBMKhTCZTHz729+mVCoxOTnJ+fPnTzVX7WFZXV0ln89z4cKFZ94hlU6nWV5eplKpIMsyHR0dtLa23ne7Pz0JIMsyi4uLlEol+vv7jWergcGXhGdBkDLGh882gq7r+tPeiBcdo4Pcr4jnyk+ka+WzzpdlPw0+H3/4wWpVpPHYzEQSaRIFmd++1M5vn/WxsbGBqqq0tLQQDAaf+QHLs0osW+bv/9kkoihg0yvk83lMJhNZGURR4v/65hmaAl5sNtuXWvD6PGiaRiKRIBKJkM1mcTgchMNh/H7/fcUrWZa5ffs2XV1dLC0tPZRbIxKJkEql7is+nQbHzh9kYtEodoeDRL6CyWzhn/zuMIVEhLNnzx5b7rgrUSF1EEPXVKw2Gw63D6VU4HKLh6/UCcxMjvM3/sbfuCcHa2lpCY/H88DsNE3TuHHjRjVM/saNG1y+fPm+ZZPLy8soikJPT0/1nhKPx0mn03R2dpLNZtne3qavr++xjtfCwgKBQIBCoUA8HsdsNp9YXnk/1+ZvnvUT2dkin8/T2NiIz+erOr+y2SyqqmKxWPB4PHi9XjweT7XMr1QqcefOHUZGRhBFEU3TuH37NiMjI8Chu+jD0Sn+7bpEOpNFEQ+D9mX1sMzU77bzf7rmpTXgprOzkz/8YJUfjW8hKUXMuoJusqGa7bx5rv5YzmOxWGRvb4/d3V0SiQRXrlzBZDKxublJqVRiaGiIVCrF1tYWw8PDTzwr8FHRdZ25uTlMJhNdXV0P9Wx5Wu+yqVSKlZUVBEFAURTsdjs9PT0PXdqp6zpbW1vs7OzQ1dX1SBl7BgYGT4cnfb8pVlT+6OYG1+963lx9Ck2sjPHhs4nhEDtFnnaL12eRJ9G18nngy7KfBp+Pe2aMbHZeqpF4vdVadeIcdeZbWVkhFArR3Nz83GS8PCvcnR9RKcm4PR4cDgduRWMvVQSr+5lwazzPiKJIIBCoDjbz+TzRaJTNzU3gsHNjKBQ6dpzNZnO1rK6jo4M7d+5w8eLFBw7OQ6EQGxsblEolbDbb6e7UJ9x9/ogcZsPlsllEDYpILKzvcKX33hyou12JbkkhocgINg9tbpVX2nX6O7oRK3kEQaCvr4+ZmRlaW1upq6urrqO1tZU7d+48UBD7tEssGAwSi8VOXOaog+XdYhhQzUA74nHF96Pyxu7ubra3t+nt7a26uj5dXvn2ts6N3TIhr4MGr41oKsuffLjA8pLE9wYOO5zu7u6STCbxer2Ew2HOnDlzX9Fa13UmJycZGBioioF353glEgmWlpZoPtNHdm4Gr1Uin03QU+tCxQy6TjybYW27gEuosLIT44c3DlBVBVEvY/d4qa8PEf+kQ/abfQHK6TixWAyr9dDle+3aNW7dukU4HKZUKrG4uMgbb7zBzs4OhUKBS5cuPdXgfDgUUCcnJ6mpqXmo8uOn9S6bSqVYXl7GYrFgt9vJZrOPHHyfTCZZWFggHA4f6z5qYGDwbHJa95tHzVM9LYzx4bOJIYidIk+zxauBgcGzz0kP6FqHmVu3buH3ufF4PFitVs6cOUNnZ2e1PM1sNtPW1nYssNng/txdnuoxmykWi+BwGIGmp4jT6aS9vZ329nZUVSUej1fLs9xuN6FQqJq7NjAwwMzMDA0NDczMzDAwMHDf9QqCQHd3N4uLi5w/fx44fefKp8ubJUnC5/OxGUkiaCWUXBJZ6mFhP1vdhmNdrZwW5hdWOdPUQCRdYCFW4u+8OUw+EcHtdtPe3k42m612skwkEvT19SEIAhaLBYfDQTqdfqAQ0NzczM2bN2lsbKShoYG5ubl7BLHFxUWAEzthZjKZanODz1M4EIvFCAaDwKEAdne3wrvLK6OZEgvTYzhNOpVsgrWdzGEWn8vOdsVMuPUMzUHfIwkYy8vL1NXVHXPZxeNxAoEAkUiEzc1NLl68SLKoolWKHMgVmgIBvD4f6DqruzFCNV5+840r1DrM/MWH42C24TaXUMoKdXV1qKqKIBfZTRa4PbXA1b4W2trajolcJpOJfD7P5OQkFy9e5Pbt23jDzbiDbSQK8lMdDCmKwvj4OM3NzceE1wfxMO+yj3MN3m+ZIyHMZrMRDoerTSOOromHoVQqMT8/jyRJXLhwwZhEMjB4TjjtsbMhSBmchCGInRJfRItXoxTTwODF4NMP6KGhIUZHR7l06RLpsl69zo/CrPP5PBsbGywsLNDQ0EBDQ4NR6vcAjuVH2M3IpQpyrmwEmn5BSJJUPXd1XSeXyxGNRllbW0MURYLBIC0tLezs7OB2u1lbW6O9vf2+6/P5DjO7IgcpfjSXOHXnyqfzR8x2BzuRBNjdNIkpfjS1z5/tTFKUteo2XG6trbrKFEXBarFSKpXQSnksDg/X78zwtaFumpqaAHA4HJRKJQYGBtjZ2eHWrVucP38em81Ge3s7i4uLDA8PP/AY19XVsbu7S2NjI7IsVztU6rrO4uJiVUw8iXK5jNV6eB3ouv7YTpqtrS0GBgbIZDIPFPCSBZmKJhL0OMlnFaxWK4FAAMliYy9VpKiZHmkbkskkuVyOM2fOHPv3VCqFzWbj4OCAixcvIooiQjlFq63MZElClmxUZJWtSJyCJvHdoSbcZg5dXj4fKBFS5TLt9UHi8TiiKFLCTF2glpcvnT/x3uF0Orl+/TrDw8NMzS3y0w2ZxE6GQiX5VCsFyuUy4+PjdHd3HwuhfxB3v8u6TTqFbBqPwwEOC9dXDnijP8xPZyOPdA3ezwHya90edjfXsdlsdHR0sLa2Ri6Xe6Tge03TWF1d5eDg4JHdZAYGBk+XL2LsbGBwEoYgdkqcZotXoxTTwODFxmq10t7Vy//tTz9iW3bec507nU76+/tRVZXd3V1u376Ny+Wira3NKP27D3eXp8YKCvUO3Qg0fQoIgoDb7cbtPsxokmWZeDxONBollUoRjUbxfFLS+qAywe7ubv7xD24xnTZ9IS7su8+fRElHUVWuNDsQTI28sxDHXdmjt6OVTFnhrek9cmWl6ipzCAoulwtFUSjrIpTzuEyuqhgGhyLKkXuusbERj8dTFS/8fj+apt3juPo0LS0t3Lx5k4aGBkKhENFolLq6OhYWFpAkia6urhOX+zwC2N2Uy2Xg8P61sbHxwGD/WqcFmwS78SSdjSHi8TjZbBbBLjyya1OWZebn57l06dKx/VBVlYODA+x2O0NDQwiCgK7rfPTRR3yry02b7GcyUmZhO4bPZeeb/Q18p9fH2NgYLpeLhYUp6kWFZdFNXhUJ1vjJlBXy+TJvdvpPfIeTZZmdnR1Us5Of35phXfEymy1Q6yjRUON5apUCR461gYEB3G73Qy939C4bcEhVl2KhUKBcLJNWRP7FL5eY2stT63r4a/DTDpCDTJ5/9/ES+/tu/vdvDrK9vc3q6iq9vb33ZOrdD13XiUQirK2t0draSmdnp1EeaWDwnHGaY+cvE4Zh5tExBLFT4jRbvBqlmAYGLz4/WUgxFgOnKUtjyH/idS5JEs3NzTQ3N1fLTCqVCs3NzYTDYWNAcBd3l6e+f3Ocaxf6CfsM8fBpYzabqa+vp76+Hl3XmZ2dZXd3l/fee4+mpqZqQ4kj99IReVViPqHhsgkEXFZ0TTvVmeS7z5/ZlU1yByrlcoX/eSFPT2s9mfguSwtzh6KT08r0dprBJi/vLcaIFtLUOG2oJgdFXeGN3hDlzBpzc3N0dnZisVhwOp2k0+nq57ndbi5fvszU1BTJZJL29nbW1tbo7++/7zYeOfF2d3epr69nenqaVCqFxWKhs7PzvsvlcrljwsPjCmRbW1s0NzcDh46t+wlwAB4LNFkKTJscJIoKqi4gWBzsRpP8tatdD/3d6brO9PQ0vb29x4LqdV1ndHS0WpJ7tD9TU1OYzWb8Xjf/2fBZ3v14FPulLrpb68nGdhm98fFhSTWHTsTfC9lZEeq5vnrAXrr0wM5giqLw0c1RbmXd/HJuD5vXz2YyQo3djKSWsUjCU3E7pFIp5ubmGB4efuTcvSPhcu8T4VKUJKw2G4qphKNYZmLjAAkVk6IhIX3m/t3tAPFaBNLJA2ySRFPQx3xC5v1bdxjq7aS7u/uhz8FsNsv8/Dwej4eRkRHDLW1g8JxymmPnLwOGYebxebrJni8wRyUWiXyZeK5MRdGIf1Kic/U+M4sPw6ftpBaTSMBlpdZp5frKAfFc+QnviYGBwRfN0XVeV+uixmGmXMgRcFoeeJ37fD4GBwcZHBwkn89z/fp1lpaWqq4Ng0MCLisDzX7MmnFcnjUEQeDs2bO0tLQwODiIoijIsszs7Cw3btxgcXGRVCp1GNyeryCYbeilPKqicHBwALqOx2amUFE5yFVOZRsDLisuNcu5rjb2k1kURDwOKx3tHZTKZaamp9lZWyKWyvJSm5cBn4KuQ1Gwkkyl+Pa5ev7Or11gcHCQnCLyg/ducX18utqJ8W5MJhNDQ0OYTCbW1tZIpVIoivLA7WttbWVzcxOz2czOzg4mk+mBYhjcG6gPjx6qr+t6NT+sXC5jNpvvuw5ZlhkbG+PvfPsi3x5sQNd0khXQgNe7/bxc9/AZZltbW7hcLmpqao5ty9TUFMVikeHh4ep2xGIxFhcX8fv9nDlzhrGxMXraGqiza3z0i58yMTGByWQi3NqF5G/G6g1w/mwvf/BKB//oe+f5h79+ln/0vfP8wVc67hlcqKrK2NgY7+9q/GIxgdlixopCuSKzm8iyFM1Wf/a0z9G7iUajLC4ucunSpcdqQnEkXKqfCJfVd9lChfPNfmwOJy11QUrFIpXK4f48aP8OHSAKQqVIOp2mpqbm0D1ZyJIrVWjpPktdXd1DnX+yLDMzM8PS0hJnz56lp6fHEMMMDJ5jTmvs/GXhyDAjisJhEyBR4K3pPf7o5sbT3rRnHsMhdorc00HuATOLD4thJzUwePG5+zq3SBYy2SzRaBRdkMgoEtFM8b7X+ZEbpKOjg1gsxtTUFJIk0draSk1NjeEaAzweD5lM5qFzdAy+WLq7u5mamqK+vp5oNFrtzJdMJtnf32dhYYGibkLSFRTBRKlUwvxJs4S8Jp3qTLKu68iyjKIo2EUVt91anc0O+P3EDw7IllWE2D5/8ad3uHKulz6bDjYVV6eHb78+TFFW+dmWyi8X9pEsdkzradpnYwy58vT19R1zOgmCQFtbG16vl5s3bzI3N8e5c+fuu32SJBEKhfjwww8JhUI4HI7P3Ke7A/WP9vFRSSQS1NbWIggCsVjsvuWSiqIwNjZGf38/Xq+XP/iKt+q6q3WYOXumldu3b5PNhj6ztC+bzbK/v8/ly5er/6aqKhMTE4TDYYrFYlXoy2azTE5O0tLSgiAIfPTRR9jtdvb29qr7nyvJ3DwwcePjGILZiq5k+VbZw++P+O7JeVRVlVwuRy6XI5PJMD09TaKg8OGuA6fVikktoRQ1JHQkk4RudVBSNGxm6QtzO2xvbxONRrl48eJjCUVHAfz/+a9d4IeziXveZd/or+Mf/nCKzUic5oAP2yflvA/aP4teoZzPYrLbafDXkk6nUTUNk8NNQBAJee5fEnyErutsbW2xs7NDV1dXtbOtgYHB889pjJ2/DBj5a58PQxA7RU6jxeuLaic16p0NDH7Fp69zj8eDx+Mhki5g0UrsrixQ2JUIBoOEQqETc4UEQSAUChEKhSgUCmxubrK4uEh9fT2NjY0PHVL8IuLxeIhEIk97MwzugyAInDt3jrGxMWpqapicnGRoaIja2tqqiFkqlbgdm+FnC3FS6QwNwRr2IkkEm4tfO99was+RZDKJz+cjmUzS6Pdw1mxiNHboNjRZ7Vg9flLpAu1iErcZpqen0TSNvr4+ygWFWCzGD+azvLOURFAUmoN2MmWFmQyUyxUaRkfx+/20t7cfu0Zramp49dVX+cEPfoDH46G19eTBga7r5PN5MpkMX//613n/1h1ykueBz9ZPl0we5CusJcvU5MoPfRw3Nzfp6ekBDl1JZ8+evednNE1jfHycM2fOHAs7D7isnG8NEolEqt/9nTt3GBkZOda98W5UVWV6evqYA+xIbGtra6Ompob9/X0EQaBSqTA+Pk4+nycSiWA2mxkcHCQUCjEzM4PT6cThcPDefomfL8ZoCfoo5zOY3W5+MrlLLpvljTYLqVSKTCZTbVggiiIWi4WDgwN0Xcflr0c6ULFQRjKZcXvctNlEtpIlSrJKriSTKyun3sxD13VWV1fvccg9Ckff1WEAv48/+IrvnnfZUqlEoynPgWglp4qIikamJJ+4f7qus7KyQiqV4huDrfxkYpfiTpRwrRcVidRDHpNEIsHi4iJ1dXVcvXrVmOAxMHjBOI2x85cBwzDz+fjyjoi+QJ5ki9dPd7vy2Mz3fQF5HjDqnQ0M7uV+13m6pPDmuWa+/pUOFOVwcL2wsEC5XMbr9RIOh/H5fPcMEhwOB729vWiaxt7eHqOjozidTlpbWx8pYPlFwWazVXOCDJ5NBEFgaGio2jBicXGxKrjA4Xf4v3tjCF/NBu/M7LCfKWERBVrFJIMOF/v7+wQCgScu/B4F1U9OThIOh3nZnicYrOPGWoJUBRBEBms1mktJTCYnuq5XO2dKksSHo1O8taBiNkm4rCJoSvW5vZIs0N57Dso5bt++TSAQoK2trboPVquVS5cusbu7SyqVYmBg4Jjz5yhPy+v1Ilrs/A8/n+Wj5Qym2WmcVtN9n62apiGKYvV5/Mv5fTKFMv65Ii91+nmjv45CRb3vwOTIMedwOKoOuk9nvum6zp07d2htbcXv99+zDpfLxerqKnD43ba1tTE/P3/fzLS5uTk6OjqqZYBHHRR7enqqYpjb7WZlZYXR0VFqamowmUy43W5eeeUVSqUSk5OTiKJ46KbNlvhoZYeWoA+TWuIgk8JczFFRRH4xk6HX7qUlVENbWxt2ux273Y7ZbOb27duIosiZM2eYX99BVNPoVhuN9X5MJhMeVaMgayTyFZIFGZ/dfKpuh6McPovFwtmzZx9LMNJ1ncnJSZqbm4+5aO9+l81ms0xPT/NffOcSfz4df6CbI5vNMjs7S2NjI52dnRRmF8i2u1hIQbKk4rB8doOTUqnE3NwcJpOJCxcuYLE8n5O/BgYGD8eTHDt/GXhRDTNfFIL+ON54g6dKsaLyRzc3uH6XiHT1ORWR/vCD1WqDgE+Le0aDAIMvM49yneu6TiqVIhKJkE6nsVqthMNhgsHgfQWBdDrNxsYGpVKpGsJ/PzfGi8jNmze5fPmy4TB4xqlUKoyOjmK1WgmFQsc6Mx4Ry5Z49/ooL184z+7aAq2treTzeWKxGAB+v59wOPxEOrDeuHGD8+fP88tf/pKLFy+Sy+Uwm81YPH7+8p0PsVLhfHc7o6OjrK2t4ff7sdvtSJKEKIqsxIv8Il1Ls99FOnFAqVympbkZb42fue04/8krnbzS14DfaSESibC+vk4wGKStrQ1JklAUhdHRUdra2lhbW+PcuXM4nc5qbpbX66W1tZX/73vL/OmNZcJeJ06riCpZT3y2VioVZmZmGB4erj6P3WYBi6CiSFYW93PYLYdh6SdNWMWyZaYWVwn7nJztbOHg4ICDgwO6u7urn3EksAQCgWOlmScd2ytXrlT/Pjk5SUNDA4FA4JiLXM2niMViDAwMAFAoFJiYmGBgYACz2cze3h4TExOEQiFkWaarq4vl5WW2t7d58803icVi7O3tYbVaGRwcJJ1O8/HsOv9mScOuFXHarbjdbgRRpKJo7KWK/MNfP0tPnbu6HbVOM8vT4xwcHFBbW1vtwDhW8PHOcvKed5qvdgX51tn6z+V2+CwnvaZpTExMEAgEqs0NHoe5uTkcDsd9XYjJZJKFhQWGh4erwmc8V77HzXG3K6yrq4uNjQ10Xae3txer1XriMp9GVVVWV1dJJpP09vbek3X3RWJUMhgYGDzLGGPqx8cQxJ5jHuZl4lkmli3z9/9sElEUjm1/PFdG13T+0ffOP5f7ZWDwJHmc67xYLBKNRonFYui6TiAQIBwOn5gnJMsy29vb7O/v4/f7aWlpeazw5eeN6elp2tvbn4hIYnC6FItFxsfHEUXxkxKue7PfEokEu7u7tLe3Mzc3x6VLl4DDAfXBwQGRSIR8Po/L5SIcDlNbW/vIuUrlcpnZ2Vnq6uqYmJjgjTfeQFVVbt++XXUdtrS0MDc3R6lUolwus7CwQHd3dzW43umv43+4laJcLmGSC6iaBgikJC9lwUKr30HQ46wKTzazyP7+Puvr64TDYVpbW1lcXCQcDpNXJa7fmaa/oxkll6SmpoaWlpbqs7VYLOB3HIb1h+vqTny2xuNx0uk0Fk+A/+rPp9FUBa2Qxmw2k5U8rMZzmCWRK+1+KqpWfbn+myMtVXf3fjxBXaCWlzr9XPQWaW9uxOfzAb9yLLlcrvsKLEd8WqRWFIX3P77Johrk5kaKQkXFZhJoMuf5e9/7Ci67hWw2y/j4OKFQiEwmU+1aura2Rm1tLVarFVEUee+997hy5QqZTIZsNks4HKa3t5doNMrOzg7uYAN//8+m8Lrd1NX+qnz06Jj9w18/y09n9z9xsysUc2kapRyvtVip9boYGBggGAyeyoTlwzjpj8pFW1tbCYfDj/U5AKurq8iyfMyNeTeRSITNzU2Gh4cf6L7M5XLMzMxQX19fdTP39PRUz4vPQtd1IpEIa2trtLa2Ul9f/9QmL4xKBgODJ4MhKp8uL5Jh5ovGEMQMnhoL+1n+6x/NHAaHm37lTPn0jKyBgcHjo3zSgS8SiVAoFKqllZ8O2Nd1nYODAzY3N4HDbnVHIdkvIked+Orr65/2phg8BEfB5bquMzw8fKK4OzY2Rk9PD1tbW9TU1NwjDOi6Ti6XIxqNcnBwgCiKD8zh+zRbW1uIokgkEkFRFEZGRiiXy7z11lucP38em83G1tYW+/v7fPOb32RsbIzJyUkKhQK1tbW0tLRQKBT4IGZhIiFSzsSR1ApxzUEOKz6TSpNDwxuqJyfrx2Z1dV1nb2+PjY0NHG4ffzYZZVt2kC8r5NIJBsNW/sFffxWH1VR9ttZ7rBzEoyiyTFNTE4oGW8k8/8XLDYRtKtlslr29PWw2GwWzlz+8k8EtypQLeUSLjeU0mM1mVB2GmzzUuu1Vgehck5dfLsXwmEUkrYJgc5LIlznrrvDf/M1Xq/eNpaUlBEHgzJkzDzy2sWyZ6+NTDPd30RT4Vb7Yf/+zGX40sUNLqAaPzczmfgxZtPGtgTDXag/L6Nrb22lpaSEYDCJJEuVyudpUoL29nR/84AcEg0EsFguKonD27GEnw/39/UMxzO0mn89zK+Pip3PRE2fXgcOZd4eFZHSPdLGCZrLz5rl6/g/fvjen60lOWN5v1v+VriBvnq3HZdbZWpqtlos+Ljs7OxwcHHDu3LkT7/ubm5scHBwwODh4XzfxUX5ZMpmkvr6ezc1NGhsbaW5ufuhnSTabZX5+Ho/Hw5kzZ55650jDdWFg8PkwROUvlufdMPM0MDLEDJ4aRr2zgcHpYzKZCIfDhMNhdF0nk8kQiURYWlrCYrFUSyvNZjOBQIBAIECpVGJjY6MaXtzc3PzChfAfBesbgtjzgcfjoaenh4WFBe7cucPly5ePdWMEqv8/ODjIzZs3CQQCxwbTgiDgdrtxu910dnZWnSuLi4sUi0W8Xi+hUIiampoTB/yxWIy6ti5Gl+/wyshQNUdpZGSEWCxGPB6nVCoxODhIMpmkqamJRCJBPB4nFotVRZtXzBny+SzzJhN5XaKsm3FToZY8paIAkW1Kop2/nNL4aleQ7rAbQRBoaGjA5Krl//XWBO+vpGnya9hFHbPHw2RS4R//+w/4r773Ej6HCYuoE8vk0RSFilxhd3eXEmYEScJp0qpuMkEQ6OvrI13W+Z/G3iddqtDX3s76XgxB0ilXKljMZirFPLjteGxmNg7yfPRJNyu9kMbhcGA2Q1nSmY7JrO3FCbisbG1tUalU6OrqIp/PIwhCVRQ5+nNR1viT29vcWE+QyORxz0/x1d5DB1q+ojK+W8DvtGAXFAq5Il6rSLpc4i/HVgn1Snz3u9+9J69sdXUVVVVpa2vjww8/xGo9DIC32Wy89NJLOBwO9vf32dg4bEVvtVrp6uqiV9YQJYnrKwfspgpYRXilzc2AV+X/8cstlFKJnf0Euq7httkpCwrvzOzQUN7BZ5eQpMNfJpOp+itnNrMtSdX9FQQBURSP/f1B/5csqrwzs4NDELAjYxLN+OyH38G/vr7Be/MRBLnI1881M+h8/HLCWCzG/v4+Fy5cuEe40nWd5eVlKpUKQ0ND9xW2jlxhRxMpqVSKS5cu3XOd3g9ZlllcXKRcLnP27NmH6pB62tzduc3vMCOIotG5zcDgEfnjm5tVUbnBZydTkqsZuYao/OQx8tcenRdrhGPwXPGiNQgwMHjWEQQBr9db7fBWKpWIRqNMTEygaVq1tNLpdNLT04Omaezv7zM2NlYNun6aGS5PErfbzfLy8tPeDINH4Kj74traGuPj4/dkwDmdTiwWC+l0mra2NlZWVo5lWX0ak8lEfX099fX1VbE4Go2yvLyMyWSqdmm1Wq3kyzJ/PpdmZ2qe9W2VX6bWaHfK/Je/cQWvy87Nmzdpb2/H4/HQ0NDA1NQUra2tSJLE4OAgN27cYGZmhqGhITpamji7N4ZbrhDJyywTxqXLKBUdRdPZzUNBV5EzCf7X/+PbfKXdzWsdHj7eUxiPlFmKl5AEK9vxNA0uCY/LicssMBkp86//3Q/obArR7bXy8baMQ7IimaGEGc3i4M2BeoZ6j2eISZLE8vRN2h0yq2YfiaKCIICOgIqEQ5QRORQIMyUZkyigaBpqMUcqkcBqtaKqKlqlQFnVWdrcY0/Nks1maW5urgpPuq5Xfx39/QfzGT7cOhS6nIJCJlPkT2/m2d7Z5nzQwn48g9uksrq2i2S2UhsM43ToxLMl+oZH7hHDCoUCs7OzXLt2jcnJSVKpFIVCgYsXL3L27FlE8bAEdW5uDl3XaW1tRdM05ubmKBQKnLdotHZo5GSBsNdJfa2Lhf0MsUQGm1bA5XTQ1nrY6KAsq+ykCnQPttPms1CpVJBlmUqlcs+vu/dZEISqcGY2m6t/vltQO8qc280XKSk6PrNGOp2nUCiwndOIFTR0XUdUivh8Xt5eSmCxbjzW4DKdTrO6usqlS5dOFMNmZmaw2Wz09/efKIbpus7a2hrxeByXy0UymaSvr++hG7bous7m5ia7u7t0d3ef2HThaXHUua3eYyUWixEMBhFE0ejcZmDwkNwtKh9dKy+KqGyUgL44GILYl5Rn5SI+6ir0oA5FBgYGp4PNZqOlpYWWlpZq1tLq6ir5fB63201dXR11dXU0NDSQzWZZX1+nUCjQ1NREfX39cx3CL0kSqqo+7c0weETq6+spl8vs7u4yPT3NwMDAsUF6V1dX1UG2vb1NoVB4KKfJp8XiSqVCLBZjdnaWSqXCX6yWuLEnE/KU8Fk0yuUS06qdfze+xyV3hra2Nmw2G+VyGYvFgqZpJJNJzp49y9TUFFeuXGFiYoKxyRl2bK0sZQKkcgVUWaYkgqCBWRBJaHYKuhlRAKsIaBV+sZRkOlZBNzswSyKiAKpSIa4ISBI47SqCqoLJytVXv04ltsF3612EwxJ/NbFOARuiUuFb54PHnq2apqEoCh999BGVSoV/8Htf499PRA7zRxQRr02krGqE3TZkVWZ1J4os2Xils5YP57ZJqSperxfPJ8cslilR5/fQHKqhlFK4du3aA8vkYtkyG3cmaQnbCbisKIpCJpNBMztYz8tcajRjNYmUNJ00TgoVM9t7eVRNI+hx4LYddx7JsszExAQej4elpSUSiQT7+/tVoWd0dJRUKsX29jY1NTV0dXVhMpmw2+0Eg8Fq84Mj8vk87733HrOrW1jEEOGGNiSliOkTx1O2ouK2WWir8z/Se5SmaVXhTJbl6q9KpUK5XCaXy1GpVFAUhVxJQy0X2c+WqatxIWsC8byCIiuIaNTV1uFxORCl8mMNLvP5PLOzs1y8ePGe0kRVVavNCU5qZnG0/PT0NFbr4fdXW1t7X+HsJBKJRNWJfPXq1WeuRP+okmE3nqS+xoMg/koYNioZDAw+myNRucF3PJbgeRaVjRLQFw9DEPuS8axdxHaLxB98pYPvDjUa9c4GBk8RSZKqjhhd18lms0QiEZaXlzGbzYRCIXp6ehBFkZ2dHW7evInP56O1tfWh8peeRcxmM7IsP3RJj8GzQVtbG6VSiVgsxtraGh0dv3LFWK1WamtriUQi9Pf3Mzc3x8WLFx/5MywWC42NjTQ2NhLNlJi5/j5ei0gxGcVht9PUGCKeLfHT8VW+8ht99LQ2cOPGjWppcSgUYnV1lWvXrjExMUFdXR3FYpF/dXObyUieoMdBrdPKfqJCGYmM4kBAp4yIAAi6jkUt47JBRoH5eAW/lKHea0fQJETA47SRVVRUQUJBwCrC1vIcZztbkGWZYUeS5nM2mjp7mbtzi4ut5mPP+Ugkwt7eHn6/n2vXruFwOKrP46mldbx2K2NxjZ9PbFCSbFjNAp2WAmdNMmJPmB/e2cZmt1FRNDKFMumyylfq7eQP9rl48eJnihvVgZLXRi6bRZIkcrkckqlCWhFpaurkahL+zdg+imDFJGjoqoIumiiUFb5/a43vDdRQKBQoFAosLi5itVrZ2dnB4XCQSqXo7++nu7sbu91ONBplc3OT119/nZaWlvtulyzLXL9+nfn5eVwuF9cunMMjh/jLmX1Mio5P0T6Xm10URaxW6z3utpPI5XK8v3mTmYwL0W7FpOlUtDIIIrU2Ebvl8Hx7nMFluVxmcnKSoaEhLJbjwo4sy4yPj9PW1kYoFLpn2SNX2O7uLqIoYrfbOXfu3H3zvj49CVssFpmfn8dsNnPhwoV7Pv9ZIei2cj5s5a14CrcqIn7O797A4MvGixiPY5SAvngYgtiXjGf1IjbqnQ0Mnh0EQcDj8VTLI8vlMrFYjOnpaWRZxu/309/fT6VSYX5+Hk3TaGlpIRAIPHMz/A/C7XaTzWZP7Fr4vPCsuH2/aHp6eiiVSmxtbeF0Oo8F6Hd0dHDz5k2uXLmC0+kkEol8rs57yYJMUdHoagiwt5vD4XCgaRrlfAYsNkS7F1EUqa2tZWdnB4C6ujpu3bqF2WwmHA6zsbFBW+85dj9KYZfy7Kfy5HUTOlYqio6CiBkVAQEd0BDIYmWpIKDqoCJwoJipJApIgkjZ5EQUJXRBJyuLaIKZYT8E3TZyuRypVIqamhqWlpZ4aXgAU1cbY7PLVOwB/C4LejHD+Pg4NpuNS5cuHXPRBVxWhtpCxGIx/uAr3XSIB4RbOhEqeQ521hEEgcvOColWJ6s5E3upIhIqL7c4GXLluXDh8kO5R2udFuxmgbXtfRySit1uxyRJWNw+gjpQztJhyeJyWMkUFXQEdE0hYBOpdeh8uBzj1TYH9bU+UqkUAwMDzM3NVTuJ1tTU8Oqrr2IymZidnWVmZoY33njjvmXfuq4zMTHB6OgoHo+Hrq4uWltbaWtrY1jWKJWKfLyqfmFu9lKpxNTUFP/lb17h309EDjt6JrMIgMcqMtQRwvyJkPSog0tFURgfH2dgYOCeCY1SqcT4+Dh9fX0ndoXM5/NMTk4iyzIOh4P+/v77ujDvmYQ1i3R54eWwzuBA3zNfgi/LMoPOHPZL7dxcTxqVDAYGj8iLFo/zIpeAfpkxBLEvEcZFbGBg8DhYrVaamppoampC0zQSiQRbW1tks1lcLhc1NTUkEgmWl5cJh8M0Nzc/F64rj8dDJpN5LgWxZ83t+0UjCAKDg4PcunWLmZkZ7HZ7dXAtSVI1u+rMmTPcunXrnoD9R8Fp0nBYTOwdpHC73JTLZeLxOJLNiU8yVUWI1tZWpqenAapOsYODA2pqavB6vYwvriFabJQLChlNR0JFQEdDQgBsyBQBFQkJHR2Rig46IHCY6ZXDhosKHkkmUwJRMiHLZb5zoYO/fa0DtVJkeXm52lFR13Xe//gGMyUf784VsW3cQVUUenxwyVnh5auXcblc9+yz2+1mdXUVAJdJx5SPUalUePnll9E0jR/+8Ie8VOPgD75+lYIqsb08h0kt8tLIlc88zrquk06n2VhexpHdYUdzUeuvxeOwsrUXI5sr81KTDTWfQjdZ8JhlGpwmdB3sFhuSruKt8bOfKWPxBEino4iiyPLyMrFYjNdff535+XnOnz+PKIp8+OGHJBIJvvvd7554XzrqjPj+++9jMpl4+eWXSSQS9Pb2Vu8NdovEr3XaebOvB83iPHUBWpZl7ty5w7lz53C5nPzHL7fTa8uSKvp4dznFcsFCTtYRpUd3LGmaxvj4ON3d3ffkfOVyOaampj753OPnxZErbGVlBYvFQl9f34nusbupTsI6LPisOtFEkg8zVgKBFl45QQx71gT+mZkZBgf6eLWmht++aHRuMzB4HJ52PM6TvK+8iCWgBoYg9qXCuIgNDAw+L6IoVrtR6rpOPp8nEomQTqcxm81kMhlu376Nw+Ggra2tmsl0xLM04DnqNPk88qy6fb9IBEHgwoUL3Lhxg9HRUV566SVsNhsAjY2N3Lhxg6ampocK2H8QWiHNhUYX761mEASJUiZHbV0jmbLKmwO/yo86+uxSqUSpVCIUCrG+vk5bW9thJ9flDRSlQh4LFqlCRRWRkdAQAZ0ilqr4BToaIiIawidymNlkQUSnoOp4UHGLAh3OIn9zuI6wJ3sohFpcDA0NVbOhHA4HY2kHP1+MUJR1KsU0miCxHNdZ8ln5zndOds6ZTCYURUFRFDY3N7l8+TI9PT0A7O7ucvHiRebn59lbW6Srq4vx7dX7Ck5HlMtltre3icVi1b//N3/7G3x/KlYdKCmCSLtbZyggUC6XsAsKZkFHNFkJe+zIsozFYmEnlsDt9qAWUkR2dtjd3cVqtdLT01MVIwOBAG+//TaSJPHtb3/7RNdaJBLhF7/4BaVSiStXruDxeNjZ2eHixYv3lDSm02kGB9tOveOupmm88/EoNXVNlDBj/yTLK+h2IskJ/sHvXK1mvT3q4FLXdSYnJ2lubr5nIiCZTDI/P8/w8HD1XD6iUChw+/Ztcrkc3d3ddHR0fKYL8GgS1qrLlNMZzC4XXc11HBRkrq8e8N3hX03CPosC/97eHjabjZqaGsCoZDAweFyeVjzOadxXXsQSUANDEPtSYVzEBgYGTxJBEHC5XLhcLjo7O6tB5NFolHQ6ze3btzGZTHR0dOALhPiT0Z1nasBjs9koFotP5bM/D3e7ff0OM4IofmndviaTicuXL/PRRx9x69Ytrl27hiRJCIJAZ2cny8vL9Pb2PlLA/t3EsmVGl3fpd8vYB8L8fHITRbQjiBJvDoTuESFqa2tZX1/HbrfT2trK2NgYQ0NDAJzvbid8e59VRQJMVDhq6nDYgVBBAnREdNRPujoKgAkFEYFau5lMWUHBgtVhYsij0KnH2F5OsqXrWCwWBgYGgMOOmx0dHZQFK5Mfx1BFibyqI6hlHDYbqtnEYkrlDz9Y5e+8frJQqCgKH3/8McFgkNbWw/3UdZ2trS0uX75MJBKhvb2dH//4x7S3t98josChuBOJRNjZ2UEURRoaGlBVlWKxyOXLl5EkiT/4ips3+sP8zzc2uD6XYXa/wkaqTKu1zDc67Hytr57317PEs2UsZpFETiFZ0hlu1NlcnGFnZ4fOzk6GhobY2dlhbGyMrq4uPv74Y1wuF1euXLlHvMlms7z99tvEYjEGBwcZHh5maWmJdDrN5csnl3wqinLqYli+LPNPvn+d5ayIsrCN3bxDs6XI715sJJk84OLFi1gslsceXM7NzVFTU0NdXd2xf49Go9yZX6Wxo4ecInD0Teq6zvLyMjMzM9TX1zMyMnLi9/xpVFXlxsQM69u7hN0WQuFflXeeNAn7rAn8pVKJjY0NRkZGvvDPNjB4UfmiReXTuK+8aCWgBocYgtiXCOMiNjAwOE3uDiLXdZ1EIsHe3h7T09P85VqFuZyVxoCXBp/jqQ944FDQEwQBXde/kOyzJ+WOu9vtm8/nqcgyNT7fl9bta7FYGBkZ4cMPP2R0dJTLly8jCALBYJCNjQ1KpRJ9fX2PFLB/98zy9n4Ct91Cv/+Av/taK3vxFNcudtEc9N2zXE1NDfF4HLfbTVdXF7IsV8WVg4MDvtPnZfZmgVhBA/hE9tKrLjHgLpeYhhUFER1EEVs5ga6K6JKZV91F/tPf+00UReHmzZusra3x1ltvsby8zOuvv47b7aZQKIDNhWrKUJDLHOrOApVKCYvVhiQIfLRywN+8Ur7nfIlEIhwcHHDu3LnD9XxCNBolGAxSqVSwWCwsLy8zODhIIpFgfX2d1tZWBEEgnU6ztbVFPp8nFApx/vx5BEFgcnKSQCBAV1fXsWvup7MRPlzYx2ES8FgEEtkcE3krnZ31/G9e68b60Rr/5sYa6ZKGIArUOszMLy5id2b55uuv0tfXV+2QWygUyGaz+Hw+Lly4cEzckmWZ9957j5WVFc6cOcO3vvUtBEFgfHy8eu86iaPuoaeJruv8P394i1v7CvV+N06TwHb0gNu6Cf32Nv+X3/3KMQfeow4uV1dXMZlMVXHziKW1Df7oxgZbZTuFhcXqZMV3BwLcvv4Rqqryyiuv4Pf7P/MzyuUy09PTrKys4KgN09IQxmQyVcUwuHcS9lmL89B1nenpac6ePftcd1I2MPgyc5r3laddAmrw5DEEsS8Zz/NF/CyVWhkYGDwYQRDw+/34/X7CrV38yd4YNVqBUipGvGjF6/FQazc/dUeT0+mkUCjgdDpP7TOetG3/mNvX7aZYKBCLxdCtri+t29fhcHDlyhU+/PBD5ubm6O/vB6C7u5uFhQWGhoZwOBxEo9HPzD2CX80su0zgFCoImJlOm3HtlPhWuxeLVj5xOavVitlsZmdnh66uLgKBANFolLq6OlKpFI1eG+dqcvyyBJKuI2gqJUyfROkfISChAToKEoIAXqGCJpixuFz02AuQ3uOf//N/js1mo6WlhW9+85tMTEywvLzM+vo6g4ODh8HydhfFfB4NCTQZyWxF1HWKZRkEgbKsHBNQdV1naWmJUqnExYsXOTg4qOZM6brO+vo6Fy5cIBqNsre3x7Vr11hYWOArX/kKS0tL/OxnP8PlcuH1emlpaanmumWzWaanp+np6cHlcpHL5ahUKlQqFfZTBX5yaxmTqmA1QyKWwOFw4HXbuLWZ4XcrCmaTiFnU6alz4XdaWdvcYrFspy4Uore3l1i2zPX5TXZXF3jt2mU0TTsmhqmqyu3bt6sdP3//938fl8tVLRMcGBi4J0/rbtLp9IkB80+S29MLTMcq1PvdeC0CO7s7uCxWKoUCi2kXc6vbBD02zGbzoch01++fJdzs7OyQz+erDkI4/D5XVlb4k/EoU0mJWpdw6KIoVvjTGyvcGb/Df/7Ns3R3d3/mhEE2m2Vqaoq9vT0aGxv5jd/4DWw2G/sfrH7mJOyRwF9rg0ymXD1nnpbAv7m5SU1NzQPPBwMDg2eb04wJeloloAanhyGIfcl4Hi/iZzFbwsDA4OFJ5CuUVYGmcACLKYSmaRQLBSr5DAclnbm1bV7qaz31cqSTcLvdZDKZUxXEnrRt/163rw1ssB1L8a3+0DN/Tz8tPB4Ply9f5uOPP8btdtPc3IzH46k6lrq6uqoB+w8SEO6eWc7FdrFbTLQ0hYnnK4zv5nmt87AZw0mdKy0WC06nk9nZWVKpFK2trezt7SFJEk6nE03TeLVRZCImU1CEI/8XFlREVHREnMgUMaEgYRNVJHREXUVWNYa9Cl+ps5BO+HE6nZjNZvb29lhcXKx+vs/nY2xsDFmW6ezsZKStjT8ei6LpImZEVE1FxIxbqJBPRklHt6GuD0VRmJiYIBAI0N3dTS6XY3l5uSogJhIJPB4PkiRx584d+vr6sNlspFIprl+/jqqquFwuEokETqeTjY0NyuUyyWSSVCpFQ0MDKysrmM1mLBYLVqsVi8XC+l6MiiZQ73Wxv7uNx+ultqaGdC7P1l6Wf/4nP+TDAztWTaXGZGV3Yxu7KNLY0cpavsh//ac3WToosxNJ4LI1kF3I8nd/6xqiKKLrOgsLC3zwwQe4XC5+67d+i2AwWBX3Dg4OuHTp0mc2AUmlUgSDwcc/OT+Dra0topkigtmKFZVEIoNJMiGKIi11QfazZTKyjk+WKRaLyLKMoijV3zVNu2edoihiMpkoFoskk0l6enrY2trCbDYjSRLr6+sUdTMrOYlah0DAaaFSqZCL72EXzBRcjfgb2+4rhum6TjweZ3Z2lmQySXNzM9/5zneOZa89zCSsFRm5kCUhm2gN/yrX7GnEeRQKBfb3941SSQOD55wvIibIyBV8cTAEscfkeXcrPU8X8bOWLWFgYPBofPrFRBRFnC4XRcyEHCpuM4yPjyNJEvX19YRCocfuCPioeDwe9vf3qa+vP5X1H7PtOy0gCE/Etn/SQPOvXTnDRV+R6elp+vv7v5TlPoFAgAsXLnD79m3sdjuBQICenh4mJye5fPkyra2trKys0NXVdd91HM0suyWZbC6H75PGEB6bmXhKo4yFbDZ54rIWi4V8Po/L5WJ3d5fe3l4ODg7Y2tpC0zTcbjdBh8Tl2jzjSROqXiKrWdEBHQm3UKJGLFHEislm55s1aRySzn4qT8Bt5fWX+qmtreXWrVvk8/lDJ5XXi8PhQJZlMpkMW1tbwGHm1dzcHPN6GoupnpIsoumHBZmqrmNyuOhyZlicHOVgZx23201vby9Op5N0Ok25XGZ3d5fa2lpSqRQLCwsEg0G+//3vk81mKZVK/PKXv8Rut+Pz+bDZbEiShMvlYnNzk1AoRKFQwGQyMTg4iK7rqKqKqqpUKhVKpRKRSIRcrowui6xuJ3CaBJwOx6F7TJfwOu3U1dVQiWWQynnW1pOYJBO+mhq0Up69rMZWPEqd24xLrGC3uZhOmfhfbm/xZruVt99+G03T+PrXv14t51RVlampKVwuFxcuXHioculMJkNnZ+cjn48PQyQSIR6Pc/lcH//LzG0iyRx1NS5i0ShNTU3E8xV8Tju9bY92r9A0jXg8zsLCAlevXkXTNBRFoVKpMDU1hdPpJI+FeCpLrRWWo9sICNQ3NGCy2NhLFVmK5O6ZPNU0jZ2dHZaWligWizQ1NXH16tV7mhDAgydhZVmuruP1cy38fDHOQUF+anEeuq5XO2x+ESX0BgYGp8fzEhP0vOsJLwqGIPaIGG6lL5ZnLVvCwMDg0fmsF5Oh3g6gk3K5zN7eHqOjo5jN5qo4dprCjtvtZnl5+dTW/+m8r3L5sCTo89r2HzTQjEQi3Lx5k/Pnzz9yiPyLQGNjI8VikY8++ojXX38dp9OJ1+slFotRX1/P7du3HxiwX+u0IGoyOwcJ6sNhypUKiUQCzeLEZhIJemwk9+QTl7VarUQiETo6OqqZZbW1tSwvL1NbW8vW1ha6rvO3rraT/dkkW5rjsPujYMKmlfEIFSqChYomMuTTCdk0isUiAXMFt9nK7Owsv/d7v0c8HkcURTo7O3G73ciyTDqdJh6Pc/v2bdLpNOl0mpwisFuxUasnKQgWZNGBrusoqopW1uhvkNnb22N7exuTycTS0lJV2BJFkWg0ysrKCnAoCkWjUeBQzAiHw8iyTDAYRJZlVFWtNjQIBALMzMzgcrno7e1FVVVEUcRsNmO1Hori8+u7RCsmLJJKsyXHbMWG5LBgttnJlzVySplhv45DyWIWVIqaiMfuoKmxEVmW2YhniWc1QuYyhWQSl9NJV3Md+8k8f/b+FLnpOF//yhXOnj1bFTjy+TxTU1N0dnY+tOPrSMg7DZE+mUyysbHBpUuX2Nraoturc7MgEU0XsNqdxPOVxx7AFYtFVlZWuHr1atUBpygK4+Pj1ZLaaKaE487HxNJpmgI1hzlhgkAkUySWK/PP3l1G0XQcFomRVh/XwhqxvR1kWaaxsZGurq4ThbBPc/ck7FFjhrvLivsrKmaL+anGeaysrFBfX/+lvGcaGLyIPMsxQYae8GxhCGKPiOFW+mI5zRpwAwODL46HKp2xWmlra6OtrY1SqcTe3h63bt3CarXS0NDwmaVuj4MkSaiq+tk/+Jgcd8e5sFgspNNpUiUVu8P5uW37J7l9w+EwbrebqakpWlpaTs399ixz5swZ8vk87733Ht/4xjfo7Ozk1q1bBINBent7mZ+f58KFCycuK2cPOOPReDcmgM2NVSySKilkM2le7fRS6zCTkSQ0TbvnfDz6fs+cOcPU1BSqqqJpGplMhtraWgqFAqFQiOjeNl9vlljZ2SIqWtiQ3aQFK0XBgk0S6bLkaJH3KYuH+VA+n49sNovX62VsbIza2lo0TWN3d5eenh5MJhNOp5PFxUVaW1uZm5ujsbER1RXmxnwZoZDCLhQQJRWLzUGxWKAiWClWNAKBAPl8HpPJRC6Xq4rRFouFZDKJx+NhY2MDQRBobm6mu7ub5eVl+vr6mJmZobv7eJfKbDbLxsYGL7/8MoVCgb29Pbq6uqqh9EVZ5V9+tMr4boGCrGJGpcVh4kqjnfWcyGY8hwmVi2Eb3+n1kohFaLIUmcyZ8NpcJDNZUvkyqRI4zRIuM2gVAbvdzvLKMoVyBWw1fPM3vkd/g6+6XZFIhLW1NQYHB7Hbj79TPIhSqfRIP/+w5HI5FhYWuHjxIsvLyyiKwu8M1yMIEdaLFiLxJHVO/bEGcOVymcnJSYaGhqpiWLlcZnx8nJ6eHmpqaohGo3zwwQe02Z0sCgF0m42KqpMpVVjcz4GgY7NIOCTYT6T5dzfirIVE/tblhxfCPs3BwQFLS0vU1dVx9erVqlD5tOM8MpkMqVTqoZtuGBgYPPs87fvKg/ii9ATDgfZwGILYI2C4lb54nkQNuHEzMDB4+jzqi4nNZqO9vZ329naKxSJ7e3usra1hs9mq4tiTKmsxm83IsvyZOUKPw0nuON3mplIpct6jszY3idDRQW1t7RMt03E4HFy+fJm5uTkSiQR9fX1fuhLK8+fPV0Wx1157jYaGBra2tmhpacFut58YsL+3t0ckEuHNMy5URWFfE8hqZlRN5iutTkZqZXRdrwbDHwWAH2GxWMjlcqiqSlNTE9vb28TjcRRFYW9vD4vFUs3Y2o8nWFFq2KhY0UQLoqQTNJc5b08xeKaZjY0MiUSCnp4ednd3cbvd5HI5pqamuHDhAslkkoODA1KpFJqmsbe3h81mI5PJEAgEyGQyXDrXw19FNojLZbwOEy6XC1VVMVtt5AsFrg6f5eWL59F1nbfffhtd19F1nVgsRjAYRNM0KpUKVquV+vp6AoEAe3t7lMtl1tbWqFQq7O7uIggCZrOZdDpNPp+np6cHu91ObW0toVCI5eVlent7CQaD/Lc/uM2Hm0VcZgi7zOTKIksFMz16hv/oXBhFsuM262Riu+xupcjlclzxi5hNZtI2N4lMDpfbw0CjwOROFkQbHpuVdDqNw+HA6vThcDgIum3AoSNpcXGRcrnMyMjII18HqVQK7ydls0+KUqnE1NQUw8PDzM3N4XK5EEURWZb5P/+1axzkK7x7fZRXr55/5PeWIxfYwMBAVcjL5/NMTk4yMDCAyWTiZz/7GdlslldeeYU3vbX80c2N6mSFJArYLRJhlwmKGbKqikPUEVxWojipa3t0MaxQKDA/P4/FYuHChQv37dj5NOI8NE1jZmbmoctnDQwMni+etZigL0JPMBxoj4YhiD0Chlvpi+fz1IAbNwMDg2ePx3kxsdvtdHR00NHRQaFQYHd3l9XVVex2Ow0NDfj9/s81kPF4DkPS/X7/Y6/jQZzkjvvOYCO/P9KKqCusra2xvLxMa2sr4XD4iQ3KRFHk7Nmz7O/vfylLKAVB4OrVq7zzzjvcuHGDK1eucOPGDRoaGk4M2N/b22Nvb4+zZ8/y1ltv8Xd/8zUqopWDXIXdtQVGzvfx8ccfE4vF8Hq9ZDKZewQxs9mMoiikUqlq2WSlUqk6x4LBIKqqEo/Hmcy7WSpr2AQZm15CNVnJ2UKsVlSaPunuWCqVWFhYwOfzUalUqufp8vIywWCQtrY2LBYLCwsLmEwmdF3n9ddfR1EUVlZWcJvBV4myhQmPxUltIMTs8ho5ReBKvRUbMoIgIAgCX/va1/jpT39KPB4nHA6zv79PbW0t+Xyerq4uLl26hCAIbG9v09bWRqFQ4OzZs/j9fhRF4c6dO9TU1HDu3DlkWa5mmsmyjMfj4aOPPiKSLvLzTQsmScSiC+TSJQRBwG1xsSM7aGpswi4q7O/vc+3aNaampvD7/WiaxkCvk1i2REXwYkXGhkwuWeHOASgWgZbGJnSznYNcmYGwmdXZCfT2djY3NwmHw/T09DzWeZROp5+oy1KWZcbHx+nv72dmZoZwOEwul0MURXp7ew9LTl1WWr1m/M5Hc5Bqmsb4+Djd3d3VLompVIq5uTkGBweZn59ncXGR4eHhY90jjyYrYtkSK5u7/LMPUggVAVUUsNlsuN1uFF145Hfdo/Mwk8nQ29v7THZuXFhYoK2t7bEcbwYGBgaPyhehJxgVbY+GIYg9Al9ExwqDe3ncGnDjZmBg8OLhcDg4c+ZMtSRud3eX5eVlnE4nDQ0Nj+W0Om1B7MHuOIne3t7DTKSNDdbW1mhubqaxsfGJCWN1dXV4PB4mJydpa2ujrq7uiaz3eUAURb72ta/x1ltvMTs7S3t7O6urq3R3dx8L2N/f32dvb4+hoSFmZ2cPc94+EbsCLith2xnW19dp/0RgcTqd5PP5ez5PlmVEUSSZTNLa2loVsUqlEqqqkslkDn+v6KzkbAScAmqhiKbruE06uqCwoziJZiLYBYWWlhY2NjbI5XKEw2EkSaJSqRCJRCiVSgSDQaLRKJ2dnfj9frq6uhAEgdXVVdra2nj//fcZsGZxNjSyVTaxEc8gShIDzjLf6Q2Rz+dZXV0lnU6jqioXL15ke3ubvb09rl69yk9/+lPy+Tzf+ta3qudjJpOhubmZ7e1tOjs7KRQKTE5O0tHRcY/j7m5MJhNLH02gChJ1XgeVYgGX200wECBXKJGSBdb3D+gKObl27RrvvfceJpOJpqYmlpaWSCQS1NXVEQjUsLy8zMzqKgM2O10vjTC6lSGWV/B7dH7t3OH7QS6d4P3336euru5zXdvZbPaestDHRVVVxsfHOXPmDPPz83R0dBCNRrHZbPeE9h85V+/npvo0uq4zMTFBc3MztbWH3RpjsRirq6uEQiF+/OMf09DQwO/8zu/c09G3UqmQ3Nskvr+PKINVBN1kIxTwInwiGGdy5Yd+19V1nd3dXTY3N+no6Dgmvj1LJBIJyuXyl7Ks3MDA4Olw2nqCUdH26BiC2CPwvHSseNF4nBpw42ZgYPDi43Q66erqoquri2w2y+7uLktLS7jdbhoaGvD5fA81CPN4POzt7Z369j7IHWc2mzlz5gzt7e1sbW1x/fp16uvraW5ufiJh3g6Hg5GREebm5jg4OPhSlVCaTCa+8Y1v8JOf/ASXy0UqlaoOgm/dusXGxgbxeJzh4WFkWWZra4uhoaFj6/D5fCwuLmKxWOjr62N9fR1ZvjdYP5PJYLPZqiHsxWKReDyO2+0mn89XSxJNnjrcNR7KyX1Es7maZRf2uVmXNfwNbaQ35+jv76dSqbC/v8/u7i4+nw9VVbHb7WQyGZqampAkifb29mNCZzabZSuWwt3UzeLme/xnv9vPzMommtmBTTAz2NPBD3/4QywWC8VikVdffRWb7bDEsKGhgenpad555x0cDgc2m4333nuPgYEBmpubyefzmM1mzGYzBwcHrKysfKb78Pbt26yvr3OmuQ5XpkwiV8JlAqvFgt1uZ20vjmQyc66rl/aGIL/4xS+q3Sl/9KMfYTKZ6OnpQRAE3n77bQRBoK+vj69//esArO5EWdqOcPFsNwGXla2tLfb39/nOd75DuVxmdna2er/4tBj0IHRdPzEr7nE4Eqzq6uqqGWybm5t4vV5aW++d4LNYLFV34cMwNzeH3++vngfb29tsbGyQSCSIxWJ861vfusfReCSIJpNJBOHQDdbT08quOcdbM3uP1fXxqCNpIBDgypUrz+x9RlEUFhYWuHTp0tPeFAMDgy8Rp60nGBVtj44hiD0ip92xwsi7uj+PUmpl3AwMDL5cuN1uenp60HW9Ko4tLCzg8XhoaGjA6/XeVxyzWq2USqUveItPRpIk2traaG1tZWdnh5s3bxIMBmltbf3cGWdHJZRHzQrOnTv3pSmhtNlsvPHGG/zkJz9hZGSEhYUFzp8/TzAYZHR0lO9+97uIosjKygpms/lEF11LSwsrKyv4fD4uXLjA97///Xu6VWYyGUwmEw6Hg0gkQj6fp1gsVv+/WCxy7tw57DVh/uIXm6iiBZOkEgwG2Y/sE0lmENQKQY8X1eXiZz/7GXV1dbS0tLC5uUk2m6WmpgZVVclms8RisWpe1xHZYpk/fH+VtC3MfnwftdxM6qfTfG8wTFN9gJs3bxLxOXnttdf4xS9+gd1uR5blqiAGUF9fX81ZGxoaoqGhgZmZGfb391EUhVgsRrlcZmdnh8uXL99XtNV1nV/+8pckEgm8Xi9Wq5lvDtbxg7EtDnIZ3N5aFtZ3ySsif32kk7b6AL/4xS+QJInm5mb+7b/9twwMDGCxWLh161Y1Bwyo/g5QV+OikIxS6zAzNTWFxWKplnge/TkWi3Hr1i0aGhpoaWl5KLG8UCjgdDo/8+c+C13XmZmZwel0srOzw/nz51lcXCQYDNLU1HTiMmazmUql8lDrX1lZwWQy0dLSAsDi4iKTk5NomsbIyAhtbW3HtiWRSLC+vk4ulwOgtraW9vZ2XC4XAL/v84PwaO+65XKZ+fl5BEFgaGjomS9BnJ2dpaur61SyIw0MDAwexGnqCUZF26NjCGKPyGl1rDDyrp4sxs3AwODLiSAI1XI3XdfJZDLs7u4yPz+Pz+ejoaEBt9t9bDB8lKGk6/ozU9YjCAJNTU00NjYSjUYZGxvD6/XS3t7+uQea9fX1eL1eJicnaW9vJxwOP6GtfrZxuVy8/vrr/PznP6etrY21tTUODg7o7u7m4OAAj8dDJBKhtbX1xPMgHA4zOjqKrutYLBba2toYHR3lypUrVRfPUUmkz+djdnaWVCoFHDpx7HY7fr//UHDzOfi1C5388QdziIqIrEEJC5WyRjNp8gdFmpqa2NjYAKC5uRmA9fV14vE4NTU1CIJAJBLhtddeY29vD6/Xi67r/NMf3GA2Z6XDa0EqpTHbnNzcl7Hb0/wndUG6uro4f/48cCjQffzxxzidzqpTJpVKsby8TDgcprm5mZmZGTo7O3nppZe4desWkUiEjY0NLl68iK+uheVY4cR3IVmW+dnPfoYsy1itVvx+P+fPn6d+c4edHZGJfRsbsQwWUeO3L3fxN0daePfdd6tB/mNjYwwMDJBKpZiYmODcuXN84xvfYGxsjOHh4WPOo6NmBrdu3aKtre3EczoYDBIIBNjc3OT69et0dnY+sMTz6Fj4fL6HOLsezPLyMrIsk8/nGR4eZmZmhoaGhgeW6lkslhNdiJ9me3ubQqHAwMAAmqbx9ttvs7m5yeDgIBcuXKgep6PGCxsbG1QqFURRpLm5mebm5ntEoUd519U0jdXVVQ4ODujt7X3iDQhOg2g0iiRJBAKBp70pBgYGX0JOswOmUdH26BiC2GPypDtWGHlXTxbjZmBg8OzwtJyvgiDg9XqrQkE6nWZra6vqsjkSx4BqJtSRQ+JZQRAEwuEwoVCIRCLB5ORkNW/o87i7jkooZ2dnqwPZZ7W06UkSCAR46aWXeOedd9ja2uKr3/w2B7kyozOzNPo9SJJUddl8GkEQqK2trXanPBK3xsfHuXjxIiaTCVVVOcjL7OR1xueW0QqHDhxJkujo6KBYLDI8PIzNZuN7vjxTU1NM7KlsJ3Loms6rXV5qE7t0dPTi8/lIJpMkEglCoRChUOhYZ8n29vZqWVy5XEbTNN6/dYeltECtw4yaS6JrCs1BH6ZEjtW8ibJgOSbwDAwMsLi4yMzMTDV0fn5+nuHhYX74wx/ym7/5m7g+caq9+eabh40C1nZIyypTtyJE5BQFWatO4r3RX0ehomLRy0zd+ghFUbBYLJw7d47m5mZkWWZ9ZZEednnt5V4a2rtZmByl3pHi/XffJp1OIwgCAwMDTE1NMTo6itPp5KWXXuKrX/0q4+Pj9PT0HHOzASSTSTY2NvjOd77zQEeXIAi0trbS2NjI8vIyGxsbDwx7T6VSJ5YzPgqbm5vs7+9jt9sZGhpicnKS1tbWzxTjjkomH0QsFiMajTI8PMz+/j5vvfUWHo+H3/u936seB1mW2dzcZGtrC03TsNvt9Pb2PlQDjwe96+q6TiQSYW1tjdbWVjo7O5+ZCYUHUalUWFlZOeYwNDAwMHganFYHzNOuaHvREHRd15/2RnzZiWXL/P0/m0QUhWMXRTxXRtd0/tH3Hr3ttgEUK2q1lfiR6+6q4bozMPjCeFadr7quk0wm2d3dJZfLUVtbi67r1eyxZ51UKsXq6iqiKNLZ2fm5O7ft7e2xubnJ+fPnj5XevajE43Heef8jfr5WoOBuRkECpUydkOGvX2zg6uWL9112ZWWF9fV1Xn/9daLRKPl8Ho/Hw9raGh3dffx3fzHGzY0UumQml0pQL2Z5OazTEA4wODhIoVCgUCggiiJWq5V4PM7NyTn6hi6zuThLW70fl8tFsVikoaEBq9XK7u4uY2Nj1NbWUqlUkCSJg4MDXC5XVXB69dVXKZVKZCU3//1HEcRSGrlcwGy20NXVxc5ehERJ4z8dCfBSX+uxkPlIJMI777wDHJaFXrx4kWg0yszMDN/61rdIpVKsrKwwNjnNRM5NVPCxkSyRrejU2ATON/sp6wKL+znsFgm3GSqFDK22Mt864+I/+OpXcLlcVfdSNBqlubmZkZER4vE4lUqF0dFRdnZ2aG9vp6Wlhffffx+z2cy5c+e4evUqY2Nj1NTUYDab7yn/W1lZIZfLUS6XuXLlyiOdC8VikYWFBURRpKen5x735c2bN7l8+fJjCz2RSISJiQnC4TD9/f2Mj4/T0dHxUM6keDxOOp2+J2z/iKOsrv7+fj7++GM2NjZ46aWXGBgYAA7LPVdXV4lEIsChGNze3n5PjtjjkM1mmZ+fx+v10tnZ+URyDr8IdF2vfgdPwvlnYGBg8CwTz5WfuAPtRcRwiD0DGHlXp8Np2lENDAw+m2fV+Xrk9DkSwhKJBMvLyywsLNDX10djY+Mzna11lGGVy+VYXV2lUqnQ2dlJTU3NY62vvr4ej8fDxMTEZ3YKfN74tDvx4OCA1dVVUv6zrC4uIUUjnO3qYDtWYCyp0xwVufqA9UmShMPhIJlM4vF42N/fp729nUqlwj/98+uMH4BZFCmmY+i6wBYBop4arra4uHPnDn19fWiaxtWrVxEEgfn5eaampgjbNPa0EoIgEAgEKBaLZLNZFEVhZGSE1dVV4vE4wWAQURRpaGggHo+jaRqqqjIxMUEgEMDs1kGtIFocqIUc/lo36DpFRSdY4yG+vYZ7ZODYPoVCIRoaGhgfH6elpQWLxcL29nb1fHK5XOzu7rJjb2d8I47PWQTJjM2kkSyUmd+OY7VayZZVcqUyHpeGUqmwJLgZcp2pNjK4c+cO+XyekZERDg4OsFqtzM7Osru7C8C5c+eYnZ2tfvaVK1fo6uoinqswv5+hVTfx1ZHh6nYrisLExAR+v5/BwUFu3br1yOfHkWvraPtqa2vp6OhAkiSO5osfVwxLJBLcvHmTrq4uOjo6GBsbo7u7+6Gv0weVTObzeWZnZzGbzfzkJz/BarXy27/92/j9fhKJBCsrK6RSqarjsbW19aHD+R9EpVJhcXERWZYZGBh47gT0nZ0dXC7XMyOGGbnBBgYGp8lpOdBeNAxB7BnAyLs6XYybgYHBF8/dnV59NgldU/A7DnNqnqVOr4Ig4Pf78fl8jI2N4fP5WF5eplgsEggEqK+vf2bFMZfLxfnz5ykWi6yurrK0tER7ezuBQIB4rvJIAy2n08nly5erJZQ9PT3PdQnlSe7EwTobw+4CXf0D/I8/mKWtIUB0Y5npyTvU1dXhd1j4aOWAv50rP/CYNTU1sba2xvDwcLUZg8lVy1xCwaprCEoBdBWP2YTV62StYKapsw+bzUZbWxszMzPVvLqjvKVCoUCpVMLpdOJwOFBVFVEUkSSJH/3oRwwMDDA/P4+u61QqFcrlMk1NTezu7lIqlVhbW0MQBF4eGqFne4kPFiM4RCuqLhBJF8jJAm90hQhlEuzv7x8rC1UUhWw2y7Vr15iamqrmfWUyGcrlMhMTE5QFC1tlO3U+J5l0inzFhknQkASRgxLoxRKCrqKLEoViia6WBkqChetrB5x1FrCLCqIoYnLVkBac5JUkH3zwAaurq1gsFgRBYGVlhVdffZU7d+7g9/sJNzbzLz5c4/2FCNv7BdoSHpbkNX5/pBWlXGB6epqenh5qa2ur+/K4OYA+n4+RkRH29/e5efMmLS0tuFyuxy6hTqfTvPPOOwwPD9PY2MjY2Bh9fX2PlK91v5LJUqnEu+++Sz6fp7a2loaGBoaGhsjlcrz//vsUi0UcDgdnz56lrq7uiXXIXF9fJxKJ0N3dfeyYPy5ftBhULBbZ3t5+Jkoln1X3tIGBgcGXEUMQewYw8q4MDAxeNO52vmqqTKFQQFEUKorGQVHjg9sT9Df4cDgcVRHgaGD8NJAkiURBwaPaaOjoodZhJh6Ps7i4SLlcJhgM0tDQcE920bOA3W7n7NmzVCoV5hZX+O/+aprVvAlVMOGwmB56oCVJEufOnWN3d5dbt2491yWUn3YnHmTy/HgygW2kk1BJq56bWYeDYqlEJBLBZLWhWD0sbe0T6Lt/zobVakUQhGr3SF3XSeQrVDQRm1oiXywicNj5VNNkDtI5PhydIGzTeO+993C73UxOTtLY2Fh1IR0cHGA2mykWi4iiiNPpZGNjg1KpRGtrK06nk9/5nd/hX/7Lf0kwGCSZTLK3t0djYyNLS0uUVfirDZm/+sk88VSeYkVFszuIFVT8ZpXXunz8jUvNzE4dsLu7Szgcxmq1oigKY2NjXLlyhZWVFTo7O7l58ybf/OY30TSNsbExzp49y3Z2lnxZRivlETQFXdMoS2ZUXUCRdQRABCxqhc7uFlxOO1quwHo8hWAP4XaI/PHoHvuah+joHexmkTApWpUcHoeNK1eu0NnZWc0I6+7u5p98/2MmkxJauUitx4Giw1vTe2TSab4SlLlw4cKxEkez2YyiKI/dNVAQBOrr6wmHw6ytrTE1NXWsPPNhyeVy/PSnP+Xq1auEQqFqc4BHLW02m81EM0UW9rNV0Sgej/PDH/6QmpoaRkZG2N7epra2lp+9f4NUSaG9zs/IyLkn6oCKxWIsLy/T2NjIlStXPvc9+mmIQbquMz09zcDAwDMh9D+r7mkDAwODLyOGIPaMYITfGRgYvEh82vlq+WTgGs+VCTt1rpzvwy4q5PN5YrEYhUKh6oY4Kku7Wyyz2+2nJpYdDdD+ciKPaWEap/VXItJQKISmacRiMebm5qhUKtUSs8/b7fFJY7FYuJWyMp02YxdVzGqOsmbjralHG2g1NDTg9Xqf2xLKu92JAZeVSrmMpBRpDdVwcz3J13rCOCwSqXwJs8mEJImoqoYmWVFKed5960d4tP+A/v7++55z7e3trK+vUxasTG7EEEUBrVwgJZepcbtQVQ23x4NqsiOVyvjsJhx2E9lslubmZjY3N/F6vWQyGSqVCru7u+i6zt7eHpIkkUqlKBQKXLlyhdbW1mrnx29/+9v84Ac/wOl0Ui6XicfjdHR08BcrRZZydhqsKaxqBb9Fw+Ry0OlS+Q+vBGkJ1aCUC7jdburr65mdnWVwcJDx8XHOnDmD3+8nEolgt9sJhUK8++67BINB3nzzTcxmM0GPnWRs9dCZ5veRSAskSzroGgI6OqAg4rWZcNosJA4OSJc16vw1dDQE+b//u/cYP4BaR5EaG8STGXZlAXdbP3/vf/UGOzs7rK6uYrPZGBoaIl3W2Sg7iCSiyJigqGItZDHrCh/JZf7WKy/dc/1ZLBbK5fJjC2JHHGXzZTIZ8vk8Y2Nj9Pb2PpRTNJfL8eMf/5hXXnmFmpoaxsfHOX/+/AOD/k/i6J701mgGy9wMNpNAg5SjobDKtSuXCAQC3L59G12y8IP5DJtlO5icuHICq3qC3x9xf25xKZ/PMz8/j8Ph4PLly5hMT2bI8DTEoPX1dQKBwDPRNOXT7mlJpDrx/Sy5pw0MDAy+LBiC2DPC85p3ZeQfGBgYnMRnOV/raw8HJic5GVRVpVAokM/nyWQy7O3tUSqVquVQdrv9mFjmcDg+V6jz0QDNajZRaxcpIxwboImiSDgcJhwOo6pqNXBcURTC4TB1dXXPhDh2NNDyu214LQL5QgFRECil87w9tcWvD4QJ+x5uYH5UQjkzM0MikaCnp+fUBMkn/Ry5252oaxpr6+s4HQ4kDZIV2Nnd4VzYyl9ORzBrKnaHm0S2SCYv841ePxeCbj744AM+/vhjhoeHGRwcrIoBR8fA4nDxb8YjLGVE8qUYEiqKIlPGijdUT3R3k2RBpqBp/Pbldr7xlQ5kWWZ6epqVlRUSiQR1dXXU1dUhSRLBYJB8Po/X6yWZTFIsFnnppZdwOBxsb29TKpVIpVIsLS1hsViIx+OIooiqqlg8fopeCVs5QiEZxWG347VZ8NY42UkkKRQKOByNxOPx6jUjCAK/+MUv6O7uxm63k8vlCAQCvPvuu9VOhJlMhmg0SjqdZvzj9wjpLjZMfipmF4qawyToyDpI6EiiiC6IlBSNhbVNLFYHOQXOhVV+/JOfML4LHrebQiKCrCjYrRZ8tUGSFh8fjU0R8tjx+/2IoojFYiGRyLIczVDQTQiqjNWkUyoq5AUJwSSQLCiEPpUNb7VaP7Mr46NQqVQYGRmp5nU5HA66urruK7hlMhneeustXnnllaqgPDQ09Fguy6N7koCOkzJ7kRRrisA3+waIxWLcuXOHlpYW7hRrWJWL1Hqs1Xvs5xWXFEVhaWmJfD5PX1/fI4t5D+KYWO20gCCcuhiUy+WIx+NcunTpia73cVAUhcWNXWKJND6LTqZsxuv1IomikRtsYGBg8JQwBLFnjOcl78rIPzAwMPgsHtf5KkkSbrf7xBIjXdcpFovk83kKhQIHBwcUCgU0TQMOB8V3i2VOp/OBjpG7B2guk4VSsUjgE5HupAGaJEnU19dTX1+PoihEo1Gmp6fRNK0qjj2J8OrH4W4hKJmI43I6KRQKuK0mEmWVX94cZ7AtRFtb20NtoyRJnD9/np2dHW7evMng4OATLRk9refI3e5Ep6geCkReL7vJHA6rjTqfk3q3mZXlHItpyKo6iCLtUgrfwT72lkFcLhcDAwPMzMxUHUIXLlwADs/BP765yVgMhEoGt81EuihTFqw01DoBgYrkRCuXudTo4vWWw/PHbDYzODjIzZs3aW1tRdM0lpaWUBSFXC6HLMtEIhF8Ph99fX309/dX90mWZZaWljCZTNTW1jIzM0OlUqFUKrGfyiNrHprCfuLRCIVi8TCD7CBCMiczPpvCqleIRCIEAgFisRhbW1tkMhmCwSDZbBZBEEin00SjUXp7e0kmk0iSxI9//GMKhQJer5dvBp2MpgRWyypFWcWkK7hFFZ9Fo625iZVImmhBoCw58TpsDNdKdOp7LBxk0cRasgdRREFHEKChvgHJYmUjlkCwd9HW1sj4+Hi1S2QikSBfVhHQsYgCoghOs5lMSSGdL6Opyj3f+5FD7EmgaRqiKCIIAi6Xi0uXLhGPxxkdHaWuro6WlpZjpXfJZJKf//znXLt2DbfbzdTU1D0lnQ/L0T3JKemUlQLlooLfaUEoyvxyMUJjh8Zv/MZvoJqd/PH3p6h1WvBaBMzS5xOXdF1na2uLnZ0dzpw5Q19f3yNv+2eRyFfIFMq4hApZzYr7k46XpyUG6brOzMwM58+ffyrl+Lquk0qliMVi1WvK7vBR63VhNknUun91PzVygw0MXgwMs8jzhyGIGTwWRv6BgYHBZ3EazldBEKqusE9zFDh+JJZFIhEKhUK1U5skSceEMqfTyUHuVyKSRRKoVCrEYjFcHh/RnPLAAZrJZKKhoYGGhgYURSESiTA5OQlAXV0d4XD4c5dvPQpHQlA8k8dhNuNwOnE4nURSeaxqAa9NolKpMD4+jtvtpqOj46EErsbGRrxeb7W8LhgMPpHtPa3nyN3uxIxaxma2UhatYNE5HzaTje1SU1PDd7ocWL0BNqNJ1FwSs2YnEjl0RXV1dVWD0AuFAnNzc4yNjeF0Ogk2d/LTRQ2TWsZpESgV8vjsdpwOK2aTmb/7rV4mp6awIVPrMLO9sYrf58bj8SCKIj6fD6fTSTQaRVEUAoEAsiyTyWRwOp3V7oLlchmLxcLm5iZ7e3ucOXOGtrY2rl+/ztDQELdv30aWZeRsAqvkJV/WMEkSiqpSrlTIJ7OYLRZcJp1USWM7p9HSFUbJJxkcHMThcBCPx+nv7ycejzM7O8s3v/lN4FDgURSFdDpNU1MTr3/7u/zRn/6Al+olLDMTxKUGLJJIT3OIg3gcrVygLVxDTb7Ir3daEDJ7+M0WkskMlWwCpWRFF81Y9Artbe0Ui0VyuTJBn4fetkYWFhbo7e1FEAQKhQIrKyuYdYWCqqOj4bHZ0AQTJpOA3QTTs7OQOewIeSQ6Wa3WapODz0smk8HjOW5BCwQC+P1+tre3uXHjRrWcOBqN8vHHH3P58mXcbjezs7NcvHjxsYXx3USGnWgclyDjdbnIZrPouo7LYqWgO7jy1RH8fg9TmwccpHN4JIVYRiUQDGKxWB5LXEokEiwuLhIOh6sdUJ8kuq4Ti8XYXFpDUCsILiduz6/u4aclBi0tLdHY2PiFZiEWCgVisRjxeBxFUfD5fASDQc6cOVMVUb+SEA/vdYJg5AYbGLwgGGaR5xdDEDN4ZD6dzwJG/oGBgcH9+aKcr4IgYLVasVqtJ3ZBUxSlKpYlk0l2dnbYTxWoFLJsFXPUOs2YTCbsdjub+zFsdju1zocTtEwmE42NjTQ2NladPhMTEwDVoO4nlcFzP4JuKy91+Pm315doCdVQUTQyJZl0WeXNwTZeu9rC1tYW2WyWUqnEnTt3cDgcdHZ2fmZZlMvlYmRkhJmZmWoXys8zaD7mzJM0KsU8/k+24Uk8R45ciD++uUDFbMOv6fza+QZ+f6QVUVf4yU9+gq7riKLIS31tRCI2tre3CYfDbG1tcebMGYaHh6vloq+99hqxWIy33nqLmxMzRIsNNPhsZDNZVFXF6/ViRqCIiK5DV/BwX+rq6lhaWmJiYoKRkZFDB2NtHTdnF6GUxWeTsFqtRKNRHA4H7e3t+Hw+amtruXHjBnt7ezQ1NXHu3DksFgvj4+NcvXoVVVUpFouMjo6Sje0SDNYxlRGRsOIwKciCibwMTcUY721YKSTyZEsWfhZZpsuj8Z1eBatJIBqNEovFkCSJtrY2uru7+fnPf87W1hYWi4Vf/63f4V+8O8+f/k8fsB8TMK2l6a2p42KNg6nEoeNH0QXSZY1MJk23PUdAtXLxqy/xox/9iP39faySxPmwjTsHAmhQUTSSRRWT08tXesJIcqF6ji0vL/Pee+9REa2EHD5ki5tYIkNR0bFadPwuC41eO69ePY9YyTM1NYXFYqGzsxOLxUImk3nsc+ZuUqnUieXcgiDQ3NxMfX09KysrTExMUCwWOXv2LG63m4WFBS5evPhYQrimaczOznJ7YhZJs1MWBPL5PHa7HYvVSlGTCNrsUM4yOrpEqqTitluwWl2IlTzSJ2LLSeLS/RwLxWKR+fl5zGYzFy5ceOLuVk3T2NnZYWdnB7/fz9euDLMhbPPW9B6SVD5VMSiVSpHL5ejq6npi6zwJRVGIx+PEYjHy+TwOh4NgMMj58+fvex4YucEGBi8ehlnk+cUQxAwemaOynDq3hVw2i+uTsiYj/8DAwOBZxmQy4fV68Xq91X87D2yKq7w1tUdFPAw4ThUqFDUTHVKOd//yRzQ0NOB2u+8pxbyfwGU2m2lqaqKpqYlKpcL+/j7j4+OIokh9fT2hUOieZT+vxf5o+QthifiZGpYzwj0DLZNJor29nba2NmKxGBsbGxSLRSYmJrDZbHR2dh47Np/mqIRye3u72oXycUso7y7vRJVJJJPk8nkQJLKqiVi29LmeI3aLxH/8cjvu+BwtPR10NddV11cqyYiiyMsvv8xPfvIT7HY7Ho8Hs9lMOBzGZrPx7rvv8lu/9VsUCgVSqRSJRIJQKEQgEKCxs5exX+6xF0/htYk47S4SiQSyZCNc58LvspDMOjCbzSSTh26ssbExPrxxmxXquL6WIHKQAqVMlxeaS2ksFguSJCGKIuVymVKpRDAY5MqVK2SzWdbX15mdnaWzs5N8Po/f76e/v5+5xRWuH5iIRQqUBAcFxYrdZKPOIXE2qBA9MDOft+OsxLGJGgpu5gseurQg/+FwMx9//DF7e3u4XC5kWeb73/8+y8vLmM1mzp07xy93VMbjOnIuikNQsTh9rMhm+h0qZ6xZ4oqZAhbM5QrXmt1891wzMxPj/Kt/9a+oVCq43W4uXbrEbiROLpdhR3aynSrQGPTzcneIv3Gpmesf/hK73c6PfvQjcrkcIyMjZLNZpI4Qfzmzj2DTCPhtmK12cmWZl7sCBN02wIbf7yeTybC4uEipVHpiHQRTqRT19fX3/X9JkpAkCUEQMJvN7O/vE4vFGBkZeSzhe29vjw8//JBcLofH6WSw7lBAVE06iAIFVSRZlLnoVDApRQYGBrBarazpq4fiUkXB5RFI5srHxKX7ORa+0RtkfmUdqVLgylD/I3fA/CxkWWZzc5NoNEpDQwOXL1+u5jx+EWKQqqrMzc1x8eLFU3G7fboMMhAI0HOVYrsAAQAASURBVNHRUc3n+yye19xgAwODkzHMIs83gn7U89vA4CGJZcv8vX8/QSadoq3Oj/mTGcV4royu6fyj7503LnoDA4PnhmJF5Y9ubnD9rkHj1U9s7mqlyPT0NH6/n5qammP5ZYpymGNkNpvvEcssFss9A6Nyucz+/j6RSASTyUR9fT0uXy1/Mrrz2Bb74wNehXI+y7eGO/i1cw3kK+pnDrSOxJZ0Oo2u69hsNjo6OvD7/Q/83Gw2y/T09GOXUMayZf7+n00iioe5R6qicHBwgGKyU5EV/qOzFsJeBw0NDQQCgcca1BYKBd577z1eeeWVY93lJiYmyGQydHV1sbCwQNf/n70/D5YjT/D7sM8vz7qvd1947+FuoAH0fXfP7MzscGfI2eUul7RM2SIZXIVNiwrZZlimpbCXsiyLDFp2OCRStkyLNLk7tpbce3ZnlzPTMz19NxpoNG7gAXj3XfeZ989/ZFXiPdzdg+7po74RFa9eHVmZWZlZ+fvm9zhwgOvXr7O5uUksFsN1XaSUbG1t8Qu/8AsIcVNJdfDgQcrlMv/DmW0uNk3SOvhWE8sXdAKVF/ck+DtfO4yu60gpWV1d5dlnn6VSqfCP/uAkH5YVpoZzeO0G66UqdUfyi4cGSa+dRFVV8vk8qVSKb33rW5Fqz/d9Tp06xYEDB9A0jbW1NS5evIiu65xzhvj9UwuYuAxmEpRbLmqqwBPDCo+n2/zLyy4EEmk3AEgmEgR6EqGq/PX9kq+98DT1ep2LFy/i+z5LS0s88sgjrKysoKcH+O/ONEBCSg/wPI94LE5HaqQzWf7XL47guA7XVjYxpcPEQIYPPvgA13XxfZ9CocBLL73EwsICQgguXLjA/kcfR5opnnvsKO3yJufPn8dxHGZnZ6N9p9FoMDk5SSKd41+8eY0/fvcyRiLDYDYV7Zd32jfq9TqvvfYao6OjzMzM3HG7eVDi+d13343yzG6FlJKLFy/SarWi/bhHIhYKBfbt2/fAJR/NZpNXX32VlZUVcrkchw4d4vDhw9TbFv/kz85wdtPGlYJUTOflgyP8xlcfIWHeJNw6js9/9/p1/vzMPIlkmlxc37WO/tkbNyLFQiamU2nbXFqtokif0VyCXCrxUC09nU6H+fl5Go0G09PTjIyM3HXfLTbtT4wMunDhAoODg4yMjDyU6d3NBpnL5R4aCdtHH318fnFlo8E/+OMLYfyGdvOY4HgB69UOv/mdoxwafbgXHvp4ePjSK8T6wXcfHQNJnSmjwxnFpOZIMkrQzz/oo48+Pre459V6I8Wzzz7LjRs3WFhY4OjRo0xNTe16v+u6u0L+l5eXo3BvRVF2kWVDQ0Ps2bMHx3FYW1vjv/29t3h33Q3Jn2yKuu19JIn9Tol+zpDUfZM/v7SJoioP9P50Os2xY8ciRcfa2touG9rw8PAdB7TpdJpnnnmG8+fPUy6XOXjw4Ecire7UQiriGdY3y3znsUm++cqjtNtt1tbWuHHjBvF4nImJCQqFwgN/TqVSQdf1XSo2KSVXr17l29/+NhcvXqRQKJBOp3nppZf4yU9+wtWrV9m/fz/NZpN4PM5bb71FLBbj5Zdf5tixY/zu7/4umqbxnSNTHBXjnFqus1k2EM0aR5I2e4Mqp0+3o+86k8lQr9fx9SSrXhKTGprXoVgpktI1VFVjrg777YCE6mGaJk888UREhkkpOXPmDLOzs+TzeSqVCuVymZdffhlHMfkXv/UuQ+kYbrODa3VIKZJcxuBauc2UodDouCSxUQgVkpZtITyfZqDTkVmuXLlCpVLhypUrpFIpfv3Xfx1d17l06RIr6zV8ZZy8KbHboa3R9Vz2zkyx3fS4trrJbN7kkbEsP/3pT6lupAiCAFVVKRQKfOUrX+Hy5cuMjIzwwQcfkEwmGcunqNVqXD17Ctd1UVWVX/mVX6HVarG8vEw8HseyrIiQ/XefHCO+eZ6JvcM8fmT/Pc8v0uk0IyMjHD9+nIWFBa5fv86ePXsYGxuj4/oPnO3ied5dCS3f9/nwww8xTRMhBMPDw2xsbPDiiy8ihGBzc5P33nuPqakpJiYm7rqt2rbNa6+9xqVLl8hms3z9619n3759aJrG1tYWb73+OkdUm1/66lFyI1OM5pO3LXuPDD+7UsMPQFUExyaz0TLtVCwMJHRs22Z5s0LTDjANjdF8GscPHoqlp16vc+PGDYIgYHZ2dlchxN3wSVnpi8Uivu//TGTYx7FB9tFHH19e7Czz2Xlc65dlfD7wpSXE+sF3Hw9SSs6dO8fffGk/P1qy+/kHffTRxxcGdxugCSHYt28fzWaTs2fPMjY2xtTUVDTY1XWdXC53x8wh3/dpt9u0220ajQabm5t0Op3QdmMFXK4EFBI6pnQoF7fQdZ2UZvD2A0jsd0n0kwZbW1X2DA9TbDkfWaKv6zr79u1j7969bGxssLCwwNmzZzEMg/379zM+Pn7b4F5VVU6cOMHy8jInT57kxIkTH6lV707WqV9/dj+P6CW2trYYHh5m//79EUG1trbG3NwcqVSKiYkJcrncPcmxcrmMaZq7LGwLCwtks1mklJimiaIoUZtgJpPh0UcfxXVdKpUKhmGwsbHBzMwMS0tLuK5LJpPBsiwGcxm++cRBSi2HH79/kYXFFt989gkqG0tcu3aNRqPBysoKExMTOI5DfHQfvtAYKWRYX19DURRSySRevUmx2mQ80MiYAaZpMjw8DIS/tx9++CHj4+MMDAxw5coV2u02Tz31FLqu88b5G1QbbUYyJjXHwHEcVEUhZao4MkW1towqJY5QeWR2mmq1iuM4lNsumhrQLK5zccumWCxGuWVXrlzh9OnTAIzlc+jrAY2OjwFhRpWEa4ur5PIF9k+M8O5PfxgVXLiuy+TkJI1Gg+PHj3PhwgX27t3LuXPnsCwLXddZXl5mbGyM6elpKpUKMzMzSCm5du0ax48f58MPP9ylzOqRU0Hg33d76m0Lpmly6NAhPM9jaWmJd955hze2Dd5etRhI3T/bpVar3dE67LouH3zwAUNDQ2xubjIyMkKpVOKxxx6LPnt0dJTh4WEWFxd59913OXDgwC61pWVZvP3225w9e5ZkMsl3vvMdZmdnCYIg2u+Wl5fJ5/P80i/90j2z/XaS4aNJFSOu89O5bZKmxt9+cZalrTKlWpO06rK4beFKQbmjoisSz3Xxg+BnbqUsFossLCwQi8U4cODAfbMIP2n02liffvrpj/S+h2GD7KOPPr68uNNFvr5Y5PODLy0h1g+++3i4cuUK2WyWmT0T/MYe+vkHffTRx5cGvWD5Gzdu8P777/Poo4/et71MVVXS6fQdM3our9cRl85RSGgoBLiOg+M4WFabUsfnj/68xHOP7GFycpJUKnWbNWdnDpfn+3i+T7PZJKmb923IvBuEEIyNjTE2NkatVuPGjRucO3eOixcvcuDAAaampm5Tz0xNTZHL5Th9+jQHDhxgcHAQuL8C+27KPN+f5cyZM7iuy8TERLTuDx48CIR2zdXVVa5cuUImk2F8fJxsNnvboLXdbt8WEn7mzBm++tWvsrCwwPT0NMvLy9H7BgYGcF2X06dPMz4+ztraGrOzs9y4cYPNzc2QuDCS5Mf2YWbyvP72e1x08rx6vkKxCpcp8fy+Pfzyrz3Gu2++zvXr13EchzfffJPJ/RV0YVJudMhkMnSsDp7n0bA9YnGfGC6+H5I5yWQysuX1LJTvvfcee/bs4cCBA9y4cYPLly8zt7xJKjaO1OPk83k2NzcJgoBirUWz1ULom0waKVbFMJu1Dkog8Y0kKpIZtUJjey0i+XK5HMvLy8zPzxOLxdB1nWajxHRM40o7jgSSKjhCp+UpDFfmOfPuFWKxWJTddejQIfbu3ctPf/pTFhYWSKVSkTKs1/A6PDzMyy+/TLVapVarkU6nOXnyJMeOHePixYscPXo02s7bjse/fHeZH85rKGvbDF2xP9KFS03T2Lt3L8nBMf6b/+/7KE4HzQswNPOeRFCtVruN3LYsiw8++IDZ2Vnm5+cpFAq0Wi2OHz9+23anKAqzs7NMTEwwNzfHwsIC4+PjXLhwgStXrmAYBi987S+QGZqg2mryJ29+gBHY7JsYpt1uMzY2xvPPP3/PgPuIDI/rJBQfNIWBhI5lWfzbD+aZ8jeIxxNo+LRdyWihQLXjEjQ7qEJBUxViXVvPR81/DYKAtbU1VlZWKBQKnDhx4qGH8X9cXLx4kUOHDj1QjtuDtEH20UcffTwo+mUZn198KQmxfvDdx8PCwgIAMzMz0WOfVnvcp4m+jbaPPvq4G3pqsZGRkTuqxT4KBlImqZiBg2AwlYgeLzZtUn7AM8eHqW6vcfXqVQAymQyZTCYi2AyxW6I/NjpKx7JYK1bwA0l1c5l6YpJ0Ov2x5i+bzfL4449j2zYLCwtcvnw5zIHavz+yd/WQTqd5+umnOX/+PKubRU6WDd6+cWcF9q3H2Ft/R1RV5YknnuDcuXO4rrvrN6f3WYcPH0ZKSb1eZ21tjcuXL5PL5aICBN/3UVWVnTGpW1tbEUHZarXIZrMsLi5Gg9+BgQHefvttLMti//79pFIpNjc3yWazrG0Veb9RpGIMI5br5FIegR+wvH2DtA45AxRF3Lyw9p3v8Od//udUKhVisRiN7VWU7YCKGMTVJGoQNjP6WpzjIzHiqx627eN5HpZlsbKygmEY+L7PmTNnGBgY4Pz587z++usMDw+TzWb5d558kvT1Dt8/v470FSQKHalQ3a5yJONwdN8elLnrPP/4c/zxe1dAM9kzPsRRqmS2t1BQ0DQNx3G4ceNGpPRyHCdSZj0Wa6Jpoa2zLmMEnsceo8FRs0qrFa7b9NA4flplYHyat99+m0ajEVmEs9ksGxsbjIyMMDo6yqFDh4Dw4trjjz/OuXPn2LdvH6VSiVwuRyaTib6v7763xI+ullGFYCihIXau33tcuJRS7treq20PD5WEJqjtaI+8GxFUrVZ32aJ7eXmPPPIIly5dIpUKraFHjx69537VU46ePHmSN954AyEETzzzPBesHP+X19dYKN3ADgSZuMHeoRQj1+b4xmycF1988Z6ETqvV4uyVRTaKFfIx8JVwHkqlEgndwNZNAj1JeW2e2YTB2apKzQ5IJROomofrB+wtpIibofXvQS09nuexuLh4x6D8zwLW19cxDOOODcPQt0H20Ucfnyz6ZRmfX3wpCbFd7VY70G9J3I2dgxa3UaZWq3H8+PGf92x9YujbaPvoo48HRUfqZPc8wlZpnc0HVIvdivtJ7B89MAMHZgAiy2CxWKRareL7oYVs0mjzzopFzVTJJAzsQGALg7/42Dj7J/OsrKxQr9dJpVKMjIwwMDDwkdUPPQvagQMHWFtb49KlS1y+fJmZmRkOHz4c2SQ1TePEiRP8V987zQ8uLzI1nN+lwHa8AENTHugYK4Tg2LFjXLp0ibm5Ofbv338b+SCEiFpDe5an5eXlsKGw2wK4c6B76tQpTpw4werqKpOTk0CodlEUJSI9AF566aVIDd1sNpmamuLdaoILNZ1crMnEkIHtuHy4WqcQU7Brm0gpCVpV0nqCt68Vwwtrg4O88MIL/OAHP2Bra4uXxhK8vVVmsW2imkliquBQosbL4wneWtFpSYOhyb385Cc/oVwuI6Ukk8mQSCRYXV3lwIEDJAojzC1voNgttFSBp6c7tG2PPztZoiUNVDz26nWeysLGRp1UzOCpdIO/+Lde5rf+zR/x3MwA185fxcimsW2bTqcTqcTa7TZSSmzbJpfLMTg4yI21IsNqk00Uiq6CqhtUlAyXXMksWyyKEcrbOewAvvdHF5nUBE9mk0xMTNBsNjl69CjVahVFCcm34eFhlpaWGBsbY21tjVQqRTKZ5MaNGzzzzDPRd7XdsHn7WpG44qOqPqoChQe4cKnrOp7nRd+7lJJOdQu308SVAfu6ikO4OxHkum70/nK5zNWrVzlx4kRkIe7ZA+8G3/eZm5vj0qVLOF3V57Fjx5iYmOCf/ugS56pbSM3ERUPXFJqOz9WVIquGwv79k7eRYb7vUy6X2dzcpFarEQQBliMxVfDQGMqmEEJg2zbrlSaW7eA2XF5++WW+mcry3ZNLvHO9RMP2yScMOq5HPmngeA+W/2pZFvPz89Trdfbs2cNzzz33mbMQ9kj7nXbbL4INsn+Bto8+Pn/4IopFvuj4UhJi/eC7e+NWYkgXAbNJl//4V1/4XJxAfFz0bbR99NHH/XAn4vzJyRSd0x8yMzX+kdViDyqx71kGDx48SKPRYG1tjUqlwi8fKTA0qPDBaot6x0FXJM+OGRw1K1y7VkJVVbLZLKqqsrq6yrVr1zBNk5GREYaHhz+SMkJRFCYnJ5mcnKRcLnPhwgW+973vMT4+zokTJ0gkEhSbDue3XXJxjaW5i3hSkM7mMVMZfv+DVeK6ykg2RiFhUGnb/NGZNeDOx1ghBEeOHOHatWtcvHiRI0eO3HXdCiHI5/Pk83mklJw6dYpKpUKn0yEWi5FIJLBtm6mpKd57772IfAmCIGrFe/zxx7l69SqxWIyVlRVmZmZIp9NsVFtsqwVSep2gU2d1uYGnGPhekooHmgwwNEGtVsP2y7QCgz/5UY2CYlEul4nFYgRBwMG9M5w4Gufkucs0PZuhlEllu8jvn5NcsSZwpML1V5fYE7N5ZTxJ3FAjC93sgcP8wflt3vzxWTbKNTw1Bj99h2xMA7dD0i6yXy/z6N4JSqtFGlUR5WDF43Emh3IM6g6n33qN48ePs729HamyxsbGuHz5Mr7vR4SS7Un+4JrNqjdK1VFw0UgqLkNY2B1Yig2wEaRo+Qpxp8nYUJ6lhsN1mWXIGObX9u+n0+kwNzfHxMQE5XKZTCaD53msr6+zf/9+lpaWeOyxxyLLpBCCVqvF1tYWp69vsFlqkIuB0DSCIADuf+HSMMIsNV3XabVaUdvgC/sG+PG1KlU7ICPuTgTtJMM2NjZYXl7mySef5MMPP0RKST6fZ+/eO58PWJbF2bNnWVxcJJPJoKoqhmFw8OBBbNum2HSomiMMZC3mSxZx08BQBdVGB1tROTBU4N35Mr/6xCRJNWBra4utrS06nU603cfjcQYGBkgkElxoLvGjuTK+XyMbN6i2bVq+wl9+5hC//LVD0XztVCwkDZU/u7jxQJaeXlC+7/vMzs7yyCOP3HG5f97oZdsePXoUy7K+EDbI/gXaPvroo49PD0Lu9BN8iXBrFfXOk6PPA/nxSV412rlu4opkrVgh0BN8+/j452LdfBxsN2z+/u+dRVEEhbiG0rUBFJs2MpD8w79yvM/299FHH3f87disWTy3t8A396jITv1jqcWKTfsjS+yllBE5trhZJtCTHJoZZ3ZsMBpA+75Pq9Wi0WjQbDZpNptYlkWr1YqIg/Hxcaanp3fZ1R4UlmVx8eJFrly5gq7rdIw8v3NdMpQyOL+wQSvQCBAIAY5UGTFcNF2nJXUURSMA8gmDf/43nmaykLjr5ywtLVEulzl+/PgDDWxPnjzJ6OgoQghisRg/+tGPMAyDmZkZVFXl+PHjtFotvv/97/P000+zZ88ehBB8+OGHrK+vI4SgXq/zxBNP8Ft//Cqvt0fI6ZJ6rYIiBKlcgUtlHzcQjCgNhlJmaHHUE3Qsm29mt7Fq26iqyujoKIqiEAQBuq5HpFOpVOL1LY1rTgZDOhj4mKksHanx1b1Z/lffPkE6naZUKvFPfniJt5bbxBWfjpJgtWaBlAzFJHrgUmo5HE7b/N2v7uPHP/4xiqJQKBRIJpM4jhM1oSqKgud5DA4OcvDgQc6fP0+xWGRwcDDMhio3aGHydmOAbZlEAC4CgURDktc8BnWHpq9RJsV4zEN3m9iOg6oqjOzZT6PZ4lf3qhw6dJB3f/ojnnvsKIuLi3zta19jeXmZoaEhFhYWePrpp7lx4wadTgdVVcP8u27OGLE0/8kfXKDdaoYZWcDwyMh9f5Pn5ubI5/NUq1Wq1SpHjhxBURTeff8D5uQI7+yw8T53B4Jhe3s7UhhWKhWOHTvG+fPnqVQqzM7OMj19O3FUqVQ4ffo0pVKJ2dlZfN/n2rVrFAoFhoaGmJycZGhoiA8Xt/nP/+QybrvBqqUS0wSObeP5PrFUjsPDcWoti//xQZ2xRGj71DSNbDYbkYmVSgXXdUmn06Syef7sWpOfXFqj1rIYyKZ45fDYA5EmdzveSCkplUrMz89jmiZ79+4llUrdd3/7ecHzvGgbTqVSkQ1ycHDwc22D/LyPUfroo48+Pk/40hJiHcfnt99b5J3r9z45+qzhk75qtJMYGkjoUfZHueN9oYmhKxsN/sEfX2A0bVCvVRgeGgIhcLyA9WqH3/zOUQ6N3h6K3UcffXx5sPP4OJg02NwustYM2Gx5BBKOjKZ4fibLiWST2T0THztb7ONASkmtVmNtbY16vb4rT+tO8xAEAa1WK3rP+vo6lmWRSCSiprxMJkMqlbrrwLLRaLC+vk65XCYej+M4DheuL/FvlmOUOpJ2oODZHVQkNhoOKgkVAhkgAh8BSASu0PjaTIL/4688yuDg4F1zidbX11lbW+Oxxx67Z3aRlJL33nuPgYEB8vk8AO+++y6vvPIK77//Pslkku3tbYQQJBIJXnnlFQAuX76M4zgRmbK5uUkqlULEs/zzCzaKpmG3GojAJ0jkWW2BJ8EgIKe5xBUPNZbm6wcHOKJuIKUMScJOh0ajQRCELZLFYpGhoSFKLZffW4khA5+Y8ABIp1KoyRzJVJp//FcfYzBlRttdtVaj1HJZbfihRZPw9C2hSrwAFAEvjwQckqtIzyadTtPpdLBtOyIRU6kUmqYRj8epVquR3XWrXGU9sY/rLZWlqkcLHRUJBHhdM4FAouOzL+GAqjPfUhlVmhjSQdc0hKKQKQxyZduiEFMJAg/hu+xPBzyRbXNo3yy1Wg3Xdcnn87RaLarVKo8++iijo6PkcjkMw4i+23/2xg3+zbvXyRgCQ/jEswP3JQUuXLjA6uoqR48eZXx8HIDTp09z8OBB0un0fYnnq1evUq/XSSQSPPLII1y5coWlpSWOHDkS2Wx729jy8jIffvghnufx6KOPIoTgjTfeQNM0Dh06RC6Xw7Is2u02Qgg6UuO/en2DZCrNQsXGti0Cx0KqOqqmMZlSURX4D57Ks2ckH5GEvf1ycHCQgYGBKMC+WCwyNzdHPD9MLDvEYPrj23SCIGB9fT1quJyZmflIjbGfFnbaIKvVKp7nUavVePnll0kmk18IF8NmvcN//K/PEPhe6GjpZvv1L9D20cfHR99+3Me98KW0TMLnN/juk7b17cxXC4IA27GxbJtMLPaFzlfr2WhXt8tMDeWge1LVt9H20UcfPezKnxSCsmew3W5jaAquL2l3bP70/AalMZ1n6+d59913mZycJJPJEI/HicVixGIx4vE4pmk+VAuPEIJcLkcul7stTyufz0fkWA+KokTh/JOTk2w3bEpNG9Vt0yytc+XKFYAoM0lRFBKJBKqq0mq1sG2bbDbL2NgYBw4ciAaiTz31FIu/e5Lvnt7AUCS6quAGEl8qSKDlCwQKilABQSAlUkpenW+w8k//jOeyTWaGMkxOTjIxMcH4+Hg00B0bG0PTNE6dOsXjjz9+T6Iuk8nQ6XQYGxvj7Nmz5PN5NE0jFoshpeTQoUMkEgneeuutKAh+enoax3E4f/48Bw8epNPpsHfvXh597En+5aWfsNhSkX4KT4LbBIGCIMATKuVAI6t4PK7X0Bbn2EiETY22bZNKpfB9n1KphJQyUmtVgjhOME4C/+a8N5vQ6rBZrvPd313i6GSB5brHwopNx/WpuBq+lKiABCQKtu8TFx52oPLBtmRg/yGGKxeoVqtYloWu65GldO/evaysrDA/P8/IyEj0/V7wR3hvqU1ME0glBgH4CCQqAknvyqmLymYHEkoHIRNIoZJJpelYHaQMmK+4tD3B/mwKt1lBz+a5UG4wPDzEULmMbdsMDQ0xPj7OwsICL7wQRjHUajW2t7dxXTfKxzukSA7GWsy3dUqOx5Bo8MxEiq+Mq2xubqLrOoZhoOs6iqJw9epVisUi+/bti9pJ19bWdrW83ivbRUrJpUuXOHLkCAcOHGB+fp5r167x5JNPMjY2BoSKpMuXL3Pp0iXS6TRPPfUUlUqFH//4x9i2zezsLENDQ1E2mpkdQvcVUrpkee4iX31kjD+7sIlv2bQdUISGkAppLSSGn56IkzbC4Pyeiu9WtWm73ebSpUvE43Geeuqpn0kJ5XkeS0tLbG5uMjY29qkG5T/oAPVubZD79u3j1KlTvPLKKySTyU9lnh8m7qTe9X2flUZApdlmPBtD35En18857qOPj46+/biPB8GXlhDr4fMUfPdptGPuyldLGiQSSdrtNnbbJWGYX1hiaChtcmI0xveLNeouZNQHC5vto48+Pj/4Wa8Q7jw+pgyNraZNzAh/RlVFMjOSo+34rLiC//CV54nhcu7cOXRdJ50OQ8xLpRKWZWFZVtSAqChKRJTtJM16RNRHxa15WuVymYWFBVqtFgMDAxHJBPc4WfzqcQLXYmNjg9XVVer1OlJKstksqVSKeDxOq9Xi+vXrrK6uYpomqqpSbruoVpm4KpFCwXbBJ1wGjQCvqzvyJRDRLAIXjUtOgY1Klse8Jo+UznH27FlM0ySZTFIoFBgdHWVycpK9e/dGpNidVCyVSoV8Ps/y8jLNZhPXddm3bx/vv/8+juPw7LPPRss/ODhIs9nEtm3eeOONiLBxHIeXXnqJSqXCd99dwNMS5AyPWkfiEpJ5Ao+U8AmEiiMVXMdlqL2Ir0PNtdE0DcuyaDabUZ7U8PAwnU6HZrNJq95BFxJXqqh40fx3AgFOm+LKGmfLK7hqDOmP0JQmuuLhBoKgu95AIhF4UqARkFA9zm5aPOl7KE4HRVFQFIWhoSGGh4c5f/581LDZaDQYHR3l2soWH25CIWXQaTbxAx0FlVDDR5cQE8ju/ZbUUSUMiSaeiNNwQASChicoBR5J4VDfrGDoBglTRw9szqx3ePxomuOHDkVtjY899hijo6P33JZN7T2URJZzVxf46vPHyBgCx3GwLItGo4HjOJRKJdbX1xkcHERRFC5fvkypVAJgdXWVRx55hPn5+Yg8Mwwjuq9pGkIIfN/nJ++couzHyI/tYXV1lTNnzvDCCy8wMjJCu93m9OnTLCwsROrLUqnEH/7hH+J5HidOnOCJJ56Iwtp7+9Vbc8uUG22cVp2jwwZHky0OxASXOgG2ouIJjaShktUlX92f42++fIDBXOaOSifP87h27RrNZpPDhw//TFbGn2dQ/v0GqA/aBnn9+nVGRkY+82SY7/s0m81dxFevzCOVSpFKpRgdHSWVSqGqKrMNmz9ZOYuvCHTj5jl3/wJtH318dHxZ86H7iriPhi89IfZ5wqfRjnlr65nnS4x4is2tCt86mv7C7lRBEHA80SD21AzvLVTuGzbbRx99fH7wsK4Q7jw+tiwXxwtCBZQfMJrS6DTruF5AsR0OrqezOg1XcOHDOVS3zcHpcWKxGKqqEo/H0TQNVVURQoSKXNum3W7jum50E0KgKEr0np1kWe/vvRQdQggGBgYYGBiI8oGuX79Ou91mcHCQHyz5/PBqcffJ4rnQPvmNSYVqtUqhUGB2dpZ6vc7Kygrb29vRPPXm2/IC3ilqXG+qNCwfJzCJCYdR02PT1hAEBEi8Lpl0JwRAzROcrRm0jTjPZlt4nke5XKZYLLKwsBAts6ZpzM3N8dhjjzE9Pb3LGloulzly5AgfXpln4cM5TOmzvLxMtVrl29/+Noqi4Loup06d4uzZs0xPT5NMJtm7dy/PPfccr776amR1m1ve4AcbLfKpJAmvTkNC79RJomDJgJh00RBYaJAeZWY0xuTkJOvr69RqNfbv30+z2WRpaYmFhQV83ycWizGQNBip1VgMCkjAwMdBxZY602oZ1etQr3fQtDZpP8ayU0DHR0XgoCIR3fUako5ZxUb3HYo1n01VEBMx8jEdPZNhbrvF3PJZkmpAIpHANE1s2+bSpUt4yWEsL0Bx66iAisQj6C5nSIWJLiGmdmmxcbXJk8kK8wyz7qewgjhS8TADlxxtfF+ixEJVYFIXtGyXjYrDnuEOr732GpVKhYmJCZaWlm7bDnrblqZpbGxsMDjokfLrCLtJ21Mjy6dpmty4cYNEIsG3vvWtaJkWFxc5evQop0+f5rnnniMej+O6Lo7j0G63qVarUa6a53k0LZffPbPBkh2j7fj84eKPGfLL/DtPT3Hx4kW+973v0el0GBoaYmJiAtd1qdfr1Ot1jh49yrPPPhvZGKWU1Ot1/umPLvPqXJm44iM8G8OI8d66C6MJXhioMOGVMdIDjI2O8vgTj7M+P8fXX3pq13roDWgKSR2rus3Kygr79u3j8OHDt62zBx38NBoNbty4geu6zM7Ocvjw4U/dZnjbALXj8MdnVtje2uZrE+xqg7wb2dVoNKhUKjz55JOf6rzfC57nRcRXo9Gg1WohpURV1Yj46l2MuNcx+37tw1/U8/A++njY+DSEJJ819BVxHw99QuxzhE+rHXNn61nJkowkJL/+zD6OGGW2trYYHh7+wjHP169f5+C+Gb46Ps6vPfnRw6376KOPzy4e5hXC3vHxtSvb+EGYsTSVT7B/OIWmKhSbNqMJybNPHOHPL27y9nyJtjOAqRaYX7D468+MMbtnAt/38X0fz/Oiv737t/7fu1+v1ymVSnieFw3wPc+L1AY9FdJOpVkikSAej2MYRvSaXoD8jbUiPzy3hAx8FM1lca2E67o0PcHvba5Qz2wRF+H0pQxDvoUQyK7NsXdTVZWz9gCXLQND+CSFiyUFjcDAlxAgUAi6VE6oNrobPFQ6gcqyk2C6Gn4+hMSerus0Gg2klGiahmmavPrqq+TzeWKxGKlUiuHhYTaKZd6tLfL9U1VsH1Tp8fLBJL967Ci1Wo1z585x7do1EokEBw4cYHTmAJfmVzg0Pc4PfvADGo0G1Wo1JA83m9RsDewWJUfFJkZP2Sa78+sAmgiXs91psbAQthNCqA4pl8tRQHoymURKSSwWo1gs8ojWQRGCNT9FCwMdn3Glxoho4CoxYiIkbR7Riiy6aRw0lB3RrxKlS1R5pLHo+CoWGuf8cXwEVlWFGph4GCIbElkxi+bWVphDFgR02quocgZX6MSEQxIXLzDxu1bJkA6DOC5pLDRFcEzfJq4oHA7WGMakGCgkDckZOYQUOroakMrkqTRa+GqclK7xa99+hYGUycmTJ/nOd76Dpt1+CtrbpnrbfrVaZXh4mK2tLRKJBL7v47ouKysrkc0vHo+zsLCA53k4jhM912q1IvtlD0KIiGzr7RN/dGqDOStJQg3IaD61WpuSkuC331nghYGwmVRRFFqtFp1Oh83NTaSU7N27F9M0OX36NO12O7ISNz3B64sGpgDddzDjMRzHYiST4Xoz4IChcHDPGLOzs2QyGQ5M5Fm+5nNlo8FAKjzP6w1o6m0L3+7w3Gye/+AvPEUyttse+SCDn55KtKeS27dv388tKH+z1ub1K5sklACnVqTcUFBUlaSicKns8xu/+ARD6dg9pxEEARcuXODxxx//uWSGua67i/hqt9vRMSmVSpFOp5mamiKZTH5sW/yDtg/30Ucfd8enIST5rOHLqoj7WdEnxB4yPkmi6NO6arQzX+3k2cscnp1gdmyQIJjlrfdO8d3TG5zbtL8wzHMvWHr//v3A58tG20cffdwbvSuEuZhGTDq4lkscSIiAH59f4alBST6u3Uby9G7AbY+9UJAceSzBvzEczm1YGNKmVHJouQE1K+CFqRj//b89zVsrHbKGQkIXtDuSd0s+rR9f4rn8KcbGxiJVSW9Qt7PjZudAr/d4z+516+M7SYSeHa9YLGLbdkScBUEQ3XrT33Y0NqsDpBWX1brEcd3uhAVtDLZqbfKKdcf12lOuCSGoOApnOymsQOnmaqnEcUgi6QR6lz5SUAlwuN/vhMRGpRaYWFKLCDEpJY7jRK/qqX0AqtUqHanha3Fy8WtcttLccJvovkXKVElkBvj+hQ1W11Z5ccCOQsqblsMfv7fC6psVPKGhv7PBdMziF/cmyGazDA8P4+vbnF3VcSV0hIomPTx0ApSuciq0UPpSkhQuKb8FWjh/iqLg+z5CCEzTjO738sx83yeTyTBp2yxurlBzVVaCDBWRZtNPo/k+Y0qT48k6WA4TSpWNIIODiikUXBngI1AJ8FEoBXE8oaGpGrmUSbHp0A50kKDiYwiY9/IElTKParVoXcaFx7jaYN7PI9FICxvMGCU7JP10JAkssnpAJ9CYVMrogUXT0rnsFtiQGVypoPkBioAOJjVHsl506dgCFMFze3NkEjEuXAjz2e5EhvW2KyEEhmFEpQTZbBZN08jlcrRaLRYWFkilUrzyyisRQdsjbX3fjwoMnn766bs+X6vVKBaLXF/b5nJ1EFNzUVyLTrtDQlVRhcd6kMYiYGNjg3g8Tq1WizL5YrEYCwsLWJaF7/vRPiiEYNvVqbcHSAkHKSStdgvTjBF4Vao2rMo6mqczPz9PIpPjt0+t8/5SHfXMSVIxHVVRKLcdYsIjbagouSzvrjsU3l++bUBzr8HP335xlvX1dZaWlsjlchw7duxTCcqXUkYEYbPZjIhEgJVGwFa5hum1SMQMTNNkeHg4KjAqt9z7EmJXr15lenr6E18Wx3Eim2OP+ILwONwjvnrq0odNzH1ec4776OOzhE9LSPJZwZdREfew0CfEHhI+LYnip3nVaDBlMlZIsVqqk06HdslLToHvn7vGaD7FeC71uWeepZRcvHiRI0eOfCHaifroo49wv242m5TLZT6Y32SjWGE4qdPwwyv7Q8PDpOIaWw2HlqcwFotFA/HeDbjtsZ3P7RGCg/sC/vXpdd5brNJyfJJJlV94NM83Hxnm//T9K+wZMhlImWGbny7RDZ1tRefJF55me2We0dHR25ooe8RWj8jqWSfvdL+H3nzF43EymUyUj9S7GYYR5hq129TrdSqVCp7nYWFw8WwbVRFkjNC22el0WK80iQWSoUwcU6qRCm2nSmzn/F6wCjQDHQM/ImeaxEhikRE+GWGxFmS62Vv3QqgkU5AP8NoQrlS44g+yEaRxXRWlI2kHOknRxlA8HAsca4VAapzpQHJ7kYTik0qluBSMcsNLMJZPYaoBtbbDgh/nJ8tNDostVlZW0HWdvJNmzi3goGIg0Qhwuiq3UO0m0PB5blRhysyRSqXY3NwEQNM0NE2L7K9BEETT7RE+7XabjCG46qRYC3KYwiUf02hYcMPPs15PEigaDhq2DK2ZQZc7VboWRh/oYJBUYdBw6dSbNIM0Oh4gsDDIYYOAjSDNPlkmofjouo7jOBxUthFCYc1P0pYGcb/NCbOD5UlqMh4SgEIwo5R5PNPGtzXO2gMsBHnSBsQ8h04ALU/HxaeNgWK7qARkTI2VYo1/8Fs/4MVBB9u2uX79+n2/WyEEKysr+L7P9vY2P/3pT2k0GkxOTkbP9bbHHjmrKApra2ucOHEitPJaFq1WaxcpoygKnU6HUqmEiKVpOz56s4YiJAOFAolEAtvzafo6L/zCc6T8OqdPn2ZqaoqhoSEajUak5Ox9pmEYZDIZCoUCVcvnzB9dwnN08ikD0zDodDps1Tso0icmXYIgLKd4v5Hh1HabGJJY0KFa73C9GpBWfQ4PmeiqRmA3UZyA75+6zrTcIhdT0XWdpqfww7Nl4qpCQniIIGy/JQh49dwyk946h6bHeeqpp+5KQH5cSCl3rdtmsxkppXrtrclkknQ6zejoKPF4nO3tba698R6qhNzwGGOFNJVyGce2qbs80AC1UqlgWdYdbaMfdzluJb5620kv+zGdTjM4OBjlw32a6F+g7aOPj48vm/34y6iIe1joE2IPCZ+WRPHTumrUI/h+emmDesdhIFvi+GSWD5aqjOZTYDUwCunPPfO8vr5OLpf7zIey9tFHH3dHEARUq1XK5TLVahXfDwmPQqHA08cO88eLV1AUwWDKxOp0WF1bAzNFLpPl0Mz4xz5upYC/87UMf7W522Z9ZaNB2w0YS5t4nkcikcD1PEzfYqNS59W3TjKgOVy5cgXP8xgbGyOdTkdteT07105SK5FI7CK4es2Bd4Nt25TLZcrlchTq3mubPHz4cGTlWVZv8P3z68iYSTamIyyXvGnz2ACMNtpImWB4eBhVValWq7TbbRzHiQgBC4O6m8GUAulLhAjD85HQwiRPB0N6uKh4KNwtP6yHXkaVviNk/l644g+y6BcwhUsSh7bUaWGgyoDkjmkY+LQwQtWZ9Niqd7jsBmhYNMtNmt3XBWhctVWm0gr5VIrJyUn8GwuItsk5q4ATaBgKGIFLAEjVREifR9QS+/wWrVa4XvL5PK7rRsH609PTGIbBwsJCNE89tZJt27R8hY0gjSlc4sLDsz3iAuqBQZksuu93I/QFXledFsfrFhUoOChAAJ6HH3TwUQgQ6OFchio2lNvWg+M4IVnieRzTtzmk1Gm6gpQWoLht0KEjNSypERMehaTOyNAI2w2Lza0spnRQHA/TNIkJQWD5NPw4edrEREiQ7h/dg6vGmKs3+Hu//osMZ3afrN8NPdXVzMwMFy9eZP/+/UxPT99xu5dS0mg0uHbtGrZts76+zvz8PL7vR6rIng3Tsiwcx2FwcJDAAeE52KjE8ch0SyPqjkRYFqffeg3FbTEyMoKiKGxubkb76MDAAMPDwwwMDJBIJIBQrXj58kkm9RbzIkcslyOhwnq5iauYHB/VmRobZiyf4tr1a1wu+6QNhYRQcD0XHRVD11DNOIMjg8T0kBjOdhVUew4c5eBICtd1ubhawRc1Cl2Fa8/C5wXgaEmmDz3G7Fj2gdb13da/4zgR6dUjvnrrs2dTTiaT0Tq4k0WwXC7zzjvvUK/XeeGJx2gPBXz/wjp60yaRSLG8XcHX43cdoPacF7mYwo3Ll3nqqadue82DLItt29E6ajabWFaofjVNM1J8DQ8PE4/H+xdI++jjC4Ivk/34y6aIe5joE2IPAT8PieInfdWoR/DlYhqFwEER8L0zK9TbFsdGkwyMjkL3hOHzyjy7rsvi4iLPPvvsz3tW+ujjS42PajV3HIdKpUK5XKZer6MoCrlcjnw+z8zMzG1qiN1XCE2yQ+PcWC8yoW8QF0eAO39mz2J1P6WW67oEQUANuAFUrQCv3WTNblNI6FHTn6voDOYMvvbiQcbyKRRFYXt7m3PnzuF5HrquEwRBlEWTzWZJp9P3DGDuzWen06FcLkctloZhUCgU2LNnD6lU6q4DvDueLB4LTxZ15VkuXLjAhQsXUBSF/fv3MzMzQ6fTYXt7G9u2WWkEmGebZHEp+SpIHxWJROKjIiVskoVuLHsobAqpnVvJMdFVX5l4JIVNTNybFOtIbReJBJDARZUBLQyy0kIT4Sc6qOj4xISHoigESgIZGMQDC1VRQvIFSUxI2qi0fAWn1uFGaY644jMr16grkjWZIikdYoqHJTWswOdAos3jegmr6ZDJZBgcHCSVSrG9vU0ymWRkZIR0Os3c3BzNZhOl+3k9pZKUobXUkhqKDHAwiSseKgFtDCQCBYlOgIsCaN3csJCYCBAEKPiogEIQCHKijYLE6yrYBBJNAUeq6DJcD739RFVVkskktm2TMFSG0no3v6uNEIKkEhAPLIQQdDoeCwsLVElg+xkyaqjr0zQNx3EQgSRAYIgAU4T5XQuLiyBULDXO7/zR93np0b3s3bt3VxHCneA4DsViESEEhUKByclJOp0O7XabUqlEqVSiWCxSr9cj22Kr1YqaCNPpdETQ9CyX7XabWq0Wqfi2rl1iiDxLYoB0Mo1mxFjZrrJebrJXr5KY1klmBsL1kEwyPj4efb+9ee8VS2xvb/Pmm2/iOA7//tee5IfzHd5fbrBQa6BIiebbXN72ubDtkI47DOgZrEAQc5pstRrETJOBkTEMy6JlOaxtbpOJaZiGQctTiHcHND1L6cRgaEO1fB/dtwiCgJHRUSptFyyLpasXUOphq2w6nb7reu6Fwe8kvnoKVMMwSCaTdw2E7x27VXRSt5Bh1WqVixcvUi6XmZqa4pVXXkHTNP7dcR9EeMzZbnpI4PmZHE9PFyg27eg34FbnhW+3eHH/EI9Jhd1JajfRU67tVHzZtg3cJPDS6TTj4+OYptknvvro4wuOL5P9+MumiHuYEHJnaEkfHwtXNhr8gz++wHgujqHdPCHoZSL85neOcmj07icjnzVsN2z+/u+dDRUVSYO1tTU0TaPuq8xXPI5OZHfJMYtNGxlI/uFfOf652tnOnz/P2NgYAwMDP+9Z6aOPLyUeNBC6R/aUy2U6nU5oY8vnKRQKZDKZ+w5qWpbLv3pnnndulGjZHjFN8NhYgsczbW5cvczMzAwDAwO7bIg97FRp7VRn3Xr/VmXEP3vjRqQavvWk5FbVsJSS+fl5SqUSR48eBaBWq7G4WWat1CClSwZTJul0OiLJdpKCrusSj8cZGBigUCgQjz+YAmcnis27l4lYlsXZs2dZXl5GCMHQ0BCHDh1iYGCAaytb/B++dxnHc2k5sN10cDwPUDCER1zaSCnYkgl6BkPvFjukQoBOqIDK0kEKlWm1zKPaVpQB1fsLN/PVKkGMt90pkjio4uapTMmPUSdOQbRICXdXc+Oj2hYQkmlvuiEZmNYh2VX41B1Jx3YZ0zusOwZSi9EJ1DBUXoOG5YXEGR66CDO+HtGLZJJxbNvG8zzi8fiugoT7oR4YfOiOcV0WkCj0Qvs1PDw0FCRxHDQBnlRooQOCGA4SgdvV1alI4rh00Eni4KLQwehOyyeJgy4kM2qZx+OVqBDB931UVY2IXSEESjJHpeUyPpAmhhs1oPZe72lxXm2OgQyICQ9VVRAIaq5CmSTjcZ+sFtBstaL1resG/8vnhyiVS6yVG6RUycRghtnZWaampsjlcriuS7vdZmNjg6tXr+J5HiMjI5w9e5aBgYGI+DIMI1KBDg4OYhgG169fJxaLsbm5yeTkJAMDA+TzebLZLI7j8NZbb9FsNslkMly5cuWmPU7V2Ugf4GpV4is6uDazSZe/sDfJ6FAhUm/2yi1uhaIoNJtNLl26RCaT4cknnySZTKJpGu+fu0zLV3ljscGlqiCueEwMFWh0XDabNvWWTTJokTUVDh06RMeyOLNSo9wJODqaYDSbpNZx2G5YPD2q8e29YVFGNhsqv/7fb8zz7prDWCFFIZ3YdZz52y/OUqlUWFtbo16vR8SWlJLVUp3thkXWVBhIhqRX7/lkMhmRinfDvY7dTqfJlStXqNfrxONxjh8/fkdCrti0Wa20+bOzy5yaL6KaiV3T+e33FqNjqIFHqd7GVc1o2Tqdzi7FVy9jMB6PR8RXOp2OLON99NFHHz9PfBqFdB3H57ffW+SdHcfm5z7nWd+fBvqE2EPALgJpxwb+eSWKbiX4vK7tw/ElpxbLJAyViXzivoO8zzKq1SoLCws89thjP+9Z6aOPLy3uSBo1bX7hQJ6/uC9OpVLBdV0SiQS5XC4K1+61yd1NtdWz9PSgKAq6rtPyFJq+YDgTZzgTR9dDJcw777xDIpHglVdeIRa7d6Dzg+LjnJQ0m00uXLhAdmCYN9Ylb9+4+d7jIybPDrjUykXa7TaKopBMJhkaGoqIwZ2KmE8CvYbGer2OlJJkMsns7Cx/vuDy55c2ycU0As9htVSn1rLYnwko+kmk22G+FZr3BBIfERE/KgEJXGx0TFxyisWo0uCwWiRhhk2SjuPsCi3v5UU1XMFrnQmQARo+fje435UqDUwSOARCQcePpqmJm9vGeW84slsa+BFxZuJgY5BQfZq+RgMTEGQVh6Ti0nBVRtQ6x7RNUtrNwPb7Yaf1MC68KP9szh+kJmP3bN9UCaKGTr+r+lIIiUdBSCqqSIZFk7o0aWJGGjyxY8ojosFXjQWyqTjT09PE43E6nQ6WZbFV77BcarHkJtkOErioGEIyZbQ5rJdQAo9YLEa73SadTnM2mOBCNWymTOkBTSugE6jEhIuFTkqTmCKg7Uk6gcYetYauCcraAGYqiyEkE1qLA2ITz25HTZxBEESNqUDYHLqxwfPPP8/4+Hg0z9VqlUqlwnbDptJ20PwwW+ri9WX2TQ4zmDIpl8usra2xvr6OpmlRrli73cbzvKjwwDRNyi0HPT3AMyce4ZHZSUzTjAhvTdPQdR1VVW8jV27cuMGPfvQjjh8/zjPPPBM9f/nyZaSUXFlc519d8dE0FTOwcT0X27Jp+wrbtoKpCg5ODTOQSVK3XIoNm4GkjmU71FodDAWens7yt79yiIFsmsXFRebm5sLGTDPOayselysSD4WkqfHEZJpv7k3g250oD6tHKFfqLX687HCjrYNqko4bHyvv9k7H7u1ah8cGJb84FZKm+/btY3x8/J5kVG86itNmcmSQpuNTbtm8fGCIcys1FAEpTVIsFkml05SaNn4g+Z8/kWYsH5JeqVSKVCr1qRQG9NFHH318VHxaOeM7ca+LnH3cjj4h9pDwUdQAn3Xci+Dz/IDHpnKcW6l97pjnHjNfSOpcv3CGxx9/vH8C1UcfPyds1S3+/u9+iJQBGVOhXCrheh5ND4RQ+dvHYoxkE7vsOb1B6b2UWr1B60eBlJKzZ89y9epVXnjhBSYmJh7acn7UkxIpJf/oD07ywyslBlIGMUXS9iQtX+GXjgzzv/j6I5HVrWcRq9fr1Ot1Go1GFNSeyWSi28M8zkkp2dra4sqVK5GiKFA0TlVjzNUVbE+GBN5oDLN4hd9f1PB8n01bww0EnoRQMxW2I6pIBpQ2I0qLCVEhpbgkFD8iv55++mmee+45fN/n+vXr1GphO6KiKOTzeX7vcpM/vlghEKENMZASQxEcVLY4qJewpE4hqSPs5m0KQE8qXO4F8nctlQOiRVGmEEgMfDaCnrIlpJdGlEYU+P+ivhhZNe8FVypc9YdYD9J4KGhdgi6QgoUgT1Oa2A+cYHHzlE0lIOjaKSUKCgEmPgYeTQwyWGSV0C7mo+BKBVVIvhJbJSZccrkcsVgMLZbg9bWARStG2dNpupDAIS/aeOKmuu6xWDkkp4plrjPKspuk5Go4gSBpqCSDFrMJh31qiTknx6IVw/JlREj6UrAS5EmoPqYiEXoMR41xLOvxXL5Nu92O8u56Yfm97Lv19XWGhoZQVRVFUYjH46hmnLc2BRdLPpVmB1cxEYogLnwSusK+lM+LI5J6pUg8HqdUKkU2Zd/3I6VpJpOh0Wiwf/9+vvGNbzzg9xDi/fff5/Tp03zjG98gPTQRXf23qttUKhXq9ToLFZv/z/kOadXFd8Lvw3EcYokUFVfhsakcDRK3nVO1HI9S0yGf0AjaNS5dusTm5iaDg4Ps2bMHwzAii+NmtcV2wyap+mTMcP0MDQ0xPDwctZpC91z13DopQ6B6NnXbo+Mr/KUTk/ydrx16oGW+1UVg2zb1ep2q5eMHkv/o+UGeevTQruNUtPXuuF9sOvzm9y4DMiSIu3mL27U2tY6LF0gGYhLpOlGzp1Q0Nuv258550UcffXx58UXiCL6o6GeIPSR8kUL77udB/o2X9n6umOdbmXk8mycmUpwQ/c2/jz4+KfTaEtvtcKDby/6xrDDrZqnusVVtMZY2kYFOLBYjBhiuT8kKcNXYbTaeu4U2/6wQQnDixAkmJyd57bXXGBsb4+mnn34ozWwPkve4MwB/pVjn/aUmg2kTUzqkkmnG02mKLYezGxZVy2cwpUXz3VNHjI+PR9NzHCciyVZWVnAcB0VRSKVSEUl2r1yxe0EIwcjICENDQywtLbG0tISqqrw46HAiJ3DVGMf272Hf5Ait1l7mf+cNTm76pJDUbB9dStQAYjjoBJHSKi68SMUiZUh4BUHAuXPnyGaz1B3wVJM90wfAbtBqtSiXyxS3awhFR1N1HMcO89pUFT2V55VnjjKSTfD6668jzAy/8Au/wPnz5yP10cjICM9sbVFs2ixvVdH8DjU7YKOZIYmzI5g+pPBcFHwUFBnQxKQWmMTV+xNiYfB/HlO4JHBwUJn3BnC6IffBfYoGbkVPDRbmiSnduZMIBC4KNiYSQUo4UYaaho9GmK3W9EBXfEqlEqqqct4dZsHPo+Fg+woqAgudJiaDmoPie2wGGRpumWBjgwveCPN+nLhwGFMdmggsz2RANHg83qZWa/BkVmFGFGm6goTi4/kebzh7QjWe9JA+SN/FlR3eb6pQKTKR1pgZG8PTErR9gV0v4dtNisUitm1HQfi5XI5MJsO/XfI4V20R1wVmIkWxHqpDc6qLFkhOtVWq1TonYvVQYZRKRcs8ODiI53lIKdm3bx/rlRYyM7orw+pe8H2fH/7wh6yurvKXfuXX+NO5Om+/fjZsrBQBe8wOzw8HVItbVDoews/Q8iWjuRzNZhOJpN5xyWYy/G9/+Uk0XY/OqQaSBq7rojgt9HaVcxfmKZfLZLNZZmZmAFhbW8N1Q1Jzz549PPXU8K5jo+d51Go1qtUqy8vLuK5LR2q8eq5JxjAYzSdBpBmRkrVyg1cvrLBPKzM7Nsjg4CBCCBzHueNtoeqwUWwyEFdYKlk3V4oblmxs1dqcOXNml214562HtTYsrwtSiksVn1QqTTwWYyCTxBIuqm3hC5idHkN0l63YtPvh0H300cfnBj+PnPE+Pjr6CrGHjM8TUXQvfJE8yDuZ+aQmWNkuI/UE3zrWZ+b76ONnQS/vp0d2dTqhRadnWey1I8bjcRKJBIlEglgsFqo07mM1/y9/7RhpnSjsudls0m63I+VQIpGIiLKeXeZh5MR4nsfbb79NuVzmpZdeeugZg3fKROsF4BcKBVabkv/sexcZz8WRnsP29jaO62DEEtQ9lb/31T08NjtyW7j1/RAEAc1mMyLKWq0WQRAQi8V2qcnulx10K1zXZW5ujlqthq7rOI4TlQNMTU0RT2f5v/7hu5zdsllvBrTdMC9sOA4DbpF9cn2XjfG26UuF9cQswdB+WraPKj32pXyOmGU8VH7rqkQGPoHTwZUCR0vSxsT1fGYzKmOiwolkE/QYNjp+q4ritjh06FAUzP7oo49SKpVYW1uj0vF405shbpoETod1L9H74ggQxHCjoPsBpc2EUrvNirkTO7PKemqyQMJmkKRFDA0fD4WAByd6Y7jdtknwo+uasmuNFN2/oX0ypdxUxXVk+Nqdyrad86ci2QpSGIrECyQIhQm9je95tDB4Kb6O6nV2LY+iKCQTCUotBwn8YnYbxWmRSCSQUhKLxTBNk7U2/Pl2moS0EdIP91VFoezHqHoaGcUhoQZICU4gQAg0AgbUDk+km2h2mEelKAq+79NB5yfNMYQi0AKPVTeGlCBlgBAKE0YHywuX4a/vl8yMDnD27FmEEKTTacrlMnv27GH2wCEu2Hn+6J3LaLEk+XTivlaWVqvFn/7pn+J5Hr/6q7/Kb72/FqmuFNdis1LHQudQos0394QW7zOdPG8stUnpkI5pVJsWFjpH0w7/4TePoijKrjD7IAiiwpD9+/czMTFxx32z1WqxsbFBqVRC0zRGR0cZHh5G08LmSdd1sSyLer3O+9fW+K/f3sbABafDyGABTdNwfUmpE/DvPRqnoIZNjL7vk8lkyOfzuz5XSkmx6fBPTlZwbJtcTMHzfWKmiYWOUFT+3sujkSX9TjfHcVhbW2NueZN/drZNMhFnLJ+Kipo2a21qtTqP78nzwabdV1X00Ucfn1t80XLGv6joS2QeMj7p9sdPC1+UVo5bmflSscjMSIGaI/vMfB993Aeu60Zk190Irx7ZlclkGBkZiQat98P9lKhD6TDLKxaLMTg4uOu9QRDQbrdptVrUajXW1tawrFCpoKrqLqLsQQKid0LTNF5++WUWFhZ47bXX2Lt3L8ePH4+W6fJ6ncVSm9mhJAdH7n8SI6Wk2WxSKpV2BeAXCgUOHDhwWwD+APaO2uwYk1NTeK7L/HqRmKaQj2usra3RbDajMPR0Oh1liCWTyTsSg4qiRKTXznnr2Z0qlQqLi4u4rrtrmplMhkQicVeyUdd1jhw5Qrvd5sqVK8RiMVRVpdPpUCwWaS0u8j99ZpKlzQoNV1Iul9D0GBsLcwwkDba3NXzfj3K4dobnQ1ddVdUZZotDM3so1lu8u9aG8QIFd4tmB+LYqEJSlzEajoKKjURQqjXZkgZXmgPomoLrB+gMssdMwvV5hsYmqdLh5LnLxAi39bSucyAecL7uk9YNcppKyVHDAPrAo0m4XabpoBKw6BcAorD+W2FJDRc1VJxJgY9CQ5rY3Z48BYlC8ICEmETpWiZVgltslr2ksDCXzcDHkgaqlLvy0abV8i4ybMtPYkuNtLCjqbhBSI65EiwvQKKi4WNIh45i4gudhAz3tyAIaDSbqFJgq3ECPcme4TyVSoWhoaEokN5aLzKUz9Jo1MkagmarRcnRqEsdVYFcMsZqU9KRKqamoKkKnudT9NJsdYbZr2zxiFdCuh08z6MSxGh6HkkcOii4gRkVM7hSYLuhbdKPpanbNT744IPIhlksFpmensYwDP7kco331rdRXIuUIajXHP7122VWVlb4tSM5NE3bdatWq5w6dYp0Os3Xv/51zs0t8P1TyygCkrEYNbvDUDrGRsNhrhXjMVtydHKI0qUrPJrJsuImWSk1GMimORizeXlM48aNG0xMTHDkyBFarRbz8/MYhsHRo0d3hdEHQbBLqdVTzfWKHOr1OidPnqRarUaqUMMwQnJMNXizFGOrA36gookEblMylYG2J4lpgoGkyWA6w/j4OKqqRsfXIAgYGRlhfHycRCLBwsICx0favLsmCAydycE8Dcen1T12P3P8drLKdV3W19fZ2NhA13XGx8f5xZefZVld4Pvn19FbDpmYzlalzmatza8+OcPffGl/dGH28+686KOPPr6cKCSNHed0N8ebdcvtq10/Q+grxPr4QiNk5s+T1QMqxW00TWPP9HSfme+jDz4a4bXz78OyLX4SSlTP8yJVWU9Z1muGM01zF1GWTCbvuSytVos333wTKSUHjz/J//kH83ywXMXxAgxN4fGpHP/o105Q2HFCEwQBtVqNcrlMpVLpNvIlwEyxd3yIsULqvstwt7KBZ8cNfmEcHn300YhI6y1vo9GIlF+9Fr5ey1o6nSYejz+wgs73/Wh69XqddrsNhO1tPZIsnU7f0VJaLpeZm5sjk8ngeR6dTodsNhsFoOfzeba2tkin09y4cYPNzc2ICHNdFyllVBhQbDm8bk2Gny08DMNAEYJ2oBJIOCEWed8ZQxEKmnTZCNIIRSCDAIkgawpKroYXCGJKQBKbuHCxhYEZWCiqRqBoKIHHkKzwlXGVgVyaUrVBffg457ZsSvUm5ZaDoho03AB8j5SwyQkLIUJSyUfhkLKJKQJyirUrV6wjNV53Z2gFOi4afrcVUuxSdAURQXY/CCQ6HgECbxchFp7KKd3pDogO40qNkkxG+Wi9YgFJSDRuBGksqdGQJnHhMxH32LIV6r5Bj3zLiQ4eWtTQ2VOUKUKQUHtE5p3VZ7321SAI0HWds/YA150sWtBBkQHr3ew2rUsIelHDpuyWAYiuSRVSOOzTSndsCb1T1tuY2sRDI5CSF/SFaJ40TUNVVQYGBmh6Cn9WHcYLAhI4FLIhodxwACH4T785y1A6VLS6rsvFixcj0rdQKKCqKpUgxneveGR1n0a1QseT1EnS8gUBKgOGw1TM5cVhn68/9zjff/tDZGKQ6YE4f+G54/i+z/vvv4+UkvX1dcbHx6M2zY1qi2rHJ6kFJNUgagD1fT8qEfE8jyAIUBQFTdOIx+ORvdz3fZrNJkEQ8FY5xqmtgFrbptrx0TUVFCVsr41p91Rd+b7P1tYW169fZ2VlhXw+jxFPccktcHqleddjd+99a2trSCkZHR1ldHR013Gj9xvw9vUi2+U6cUPlG8f28D95biaazhfFedFHH318OdHPEPvso0+I9fGFhWVZfHDpGv/FD+ZRhcL4YIZGvc7U1BTFlvO5bADto4+PgjtleO0kvDRNi6yMnwTh9aD4NAY8Ukocx6HZbEaEWbvdjgaZiURiV2ZZLBZDCEEQBJw/f57/5PuLXG+qpGI6cUOj4/i0XY/nZwv8l9+epVwuR4Hv2WyWgYEBjESK3zm99pGbhe5FFPpOhwsXLjA6OsqePXvuSnI5jkOj0YhunU4nsrDtJMoe1Gras3ruDPD3PA9N03ZZLnstnevr6ywuLjI+Ph6t75GREa5du0atVguzvhSFzc1NisUiUspIKdZr/stOP8J3LzsYXitSRgH4UtDC4DltibUgy4KfR8WnIuNdtZWKio+H2rUjQgwPiUJG2LgotKTOkNJCkz4WGr7U2KdXeCpdj5rxGi7UbYnTKFF0NN5qD5MSNmrQtbVJKAdx6sQRXY1XXLjsU0oc0bYjG+UP7b2sywxatymyg0bYDhlmgYVE0IORlaKbOCZ3KMJuvjd8FAT7RJGnjDWAXc2WcHu7ZilI0MQgLRwGVJsqCRqeioZHSjiMKXVOGNso0r/j+y2p0UFnj1LlcX39rvO+s8SgLXWqsqeOlF3aa+c6uPl9C0DHJ4HLN4w50l0baG8+YsKlFeg0CffZPYU4MWmzXbeYUSscFmsoikIikcDzPPbu3cvqZpF3mnnONBJhUlwQkFQ8CpqDEApNqfPNgToTKYHnedTrdbxu6Pvo6CipVEhsN1z4Fxcd/EDS8RW2LIErby6HIQKkUMnqASnZRo/FsRyfmCZ4pKByItXA7bTwfZ/R0VGazSaF4VHeKxlc2HboeAGGAodyglcmFBKGRjKZJJPJkMvl7kl29yyTS1sV/tM/vEijXscULjWZwFJMOo6Pqgj+veem+fdf3nfXY1IQBFy9epWtrS2klORyuYiIM9IDqMkco/kkgykztFQWi6yurmLbNiMjI4yNjd2z2KNer/PW6XNkRyY5MDXaPyfro48+vlD4IsUQfVHRJ8T6+EJgZ4MkVoPFxUXq9TpBEHDWHuDNpTYx4aFLD1sxqdkBf+HoKP/R1w/+vGe9jy8wetvlJ0X09AivW1VetxJet2Z4fdQWxi86pJSRBbNHmO20YG45Gv+7f7uG73nEtXC9+oGkYUsQ8J99fYyvH58hl8vtIhN/1quCdyMKpZTMz89H+Ve32i7vtZy2be8iynrLmUgkdhFlhvFgMv4eWdC79abXIxdrtRrtdpu9e/dSLpdpNBokEgnm5+ep1+vUajUMw2BrK1T+9MK/bXQy6TRvNAdxHHuX6kqYaSzH5qvxVdAMTtWSFGWSiowjkCRwsNCRgI2GQtia6Ee6rLB1MYmDE9FUIWnWI1w0TUNRFFQ1tEv6epIf1IYIfA+TkJCpBDHKMoFEEBM+QgZdNZbHEW0rUlS97kzTkGHG0u68sACdoNtaeT9CLDxVi3Wj9B1U/IgmvPW9kgJtkoobqcJcFKyuiuuUF7ao7sw0KwZJLDQywsYQPoEEXypIITCFF01HE0FEbK0HGWqBGSnQMsJiXG3cM1Ott96WvTRngsmuyfHeyy4IiOHioXFI2eZpY5WO1GgGOitBjpJMYHW3GU1RSKg+imczrrU4pG6Rz6TI5XJsbm5GRNbpVpZFhqg6CoHn4Hkuih5jNK1h+jZtq8OL+iK6b+H7YeZZT4XVI5+CIFRtnXOGOGcPdL+TcFvqFSWYQmIQYMmw7GE4qTJqOChmknLL5RcPD/IbL+9ldXWVpaUlBgYG+O1T61ysGwxl4mSTJrYvqNsB3z4+Hh03giDAtm06nQ6WZe269RSxAIZhsN4W/N/fWGMoqaLIgKGhIXyh0rA8qm2H/+IvH7urUr5arXL27Fl832d4eJhDh262R3qex+bmJhsbG5EyVdO0XfbKe0FKycLCAqVSiWPHjvVbv/voo48vNPpq188u+oRYH58r3Eow9Bok35zbptJoowQuj43FeWlMsGd8lNnZWRwffvu9Rb7/wTyb7YBmxyObirF3IMmLBwb7DH0fDx23Nps+qDLoVnxUwisejxOPx/uE10OE53n80elF/vffu4ohwlY6ywdPqnjdX8+BuOCX9ib4pb1xTC3MKWoHKv/0/RqaGtqSemHZDReEovKff+cwY/kUuq5/7DKAVqvF+fPn76sWux966q+dRJnjOAghIjVKOp0mlUo9UPNmj2DsEV7VapWNjQ2EEMzMzGBZVhTc3W63uXHjBkIIKo02V/1B1vxURLKoikLLV4l11Ui9LKwppULM0Ji3EtioaF31koeGJjxqXXLMRkMjIC68MLAdjV7PnULYvhiqtATuDsLlTjjvDXPDL2B0Gxu3ZBIHDQ0fs0uFBCj4CApKh1f0eSyp8Y63BzdQaGACEiciwG4qu+5NCgXdPsnQxpgWNoGEGvEu8XK7sipPh5TiYAU6MeGACK2aUkqa0iCPhaH4UQulLwV1aXJM3aAs46wFOUzhosggUtDN7rAsAnzgjrLkF4gJh7jwduWU3SlTzZVKZNUsBXHa9AYE91t+SQIXH0FGOAyLOnXiuKio0g8JTqEhu5bLAk0e1TYjJdmt2Gm3DC2jMTQCpAgJ07TqcTzr8HzBotPpRNbxeDyOlBJd12k0GiiKghCCK6tF/qQxRdNTcKSCQHbNsKGGz8DFQkfHR0NyIAfZZIJWEG4H/5uvjDM1lKNarVK1fP5fH7Zo1OskFJ9cLofv+xRbDr7n8z97Ik3WDD83FotFt3g8Ht3fuY9KKXnt3dP8k5NVkAF7J262UfYKTO6klA+CgMuXL7O8vEw6nebYsWO78swgLDxZXV2lXC5HWXGNRgPgjvbInXAch3PnzpHP55mdnX0ohSh99NFHH3308XHQD9Xv43OBOxIMeweoNZr824sbZA2F0UyccsPnrRWL4ZFpvrZ/PwBxFX7jpb3M3ViiLXUG9ID9k3nqthcFevc93H08THz3vaVIGTSei1O33Dtua72MpVttjTsJrx7ZlUwmGRoa6hNenzI8z8OrF/GlpNXNrvKC3Xauji94bdVndmaI33hmL1JKzi+Xkcolcsmw7c0PAlzHwXNcSh3Jn776OiNmaBHcGdptGEak5usp+gzD2HVTVTUiq5555hnm5+d5//33P5JabCd6ltFEIsHIyEj0uJSSVqtFo9Fgc3OT69ev4/s+iqKQSqUiNVkqldqljOvNWzKZZGxsLHq8XC5z9uxZANLpNGtra6yvr0eD+HerCea9PKZwu+otlZYXkmEALQwUJMOigRWoXOkU8HYQKSqSnGjvsBNCHBcfFU8qyG4+lewqtDS8iBASUqASUCZBR2q7FGkQkjm+FPhSoYyJhCjzKkChg4IA1C7d5kg1sioSBNRIROq0O5M/PVKot23tfI3o6sHCedYJ0BUfAkGNOHQ1bn53HjR8HDR0OjQwqckMQzSJ41CSCdqEof5m4JMQDllh4aASEx4ZxWbOG0THxZIabWlECro5b5BppUJaCVV8JZkkqdxU78UJ/24EafbJ8m3r8Io/yKJfQMdDInaUCdybDAOJhRZ+PzJORZqkcMmLNiWZoIWJLkO9nI9ClRgt1+CrxgKaCKKihl77qeXeLDqICY9AKHQw0A0TKeGF2QJPxTZx2jaDg4Mkk0kmJyej4PqNjQ0mJibY2trCMAxm9h0ie8kirzmsO+G2YQcgJARA0CVADSRoGqqhY5gmpqKxVutweWGVjYU5HMdhbqvFYiXHSNrANGORfXjMjLPZdNhz4MFzT4Mg4MyZMyTVgOMjMc6UoNx2b1Or3kqGVatV3n//fXzf5+jRo0xMTESElWVZrK6usr29TTKZZGJigoMHD+4itFzXZWNjgzNnziCEYHR0lJGRkYgcKxaLzM3NceTIEbLZ7AMtSx999NFHH318UugTYn18LrCTYBhNG2xVG/z2G0VcBHtyMeLCo1oukemGZL96bplH4k0yRhhAW+n4nF2tkUzE0bAh8KKTwH7bZB8PE7uaTZMGruuSVHw6wucHHy4yS5F0142mqmpERPQJr88WgiDgxo0blMtltkljalWatkdwi6Y6pgnSukKjY/Mnp+f52t40s2ODjOZTpBMmDoLBlEmy+/pi0yYZSP7qd0JVhu/7OI6DZVmRBapXCNBrpfQ8D9/3o/D5nnWrd+vlnf3oRz9iaGiIycnJXURaL9z8o0IIQSqVIpVK7SK2fN+n1WpRr9dZXV2Ngrs1Tdtlu7y18bJQKPDVr36V7e1trl+/zvShY8yceIEf/9kfUW87bMiQDNtJsPhC4KBwXFlnTWapBHG2SFGWoTrK6JoQJQoOKg1p8qS2ynpgsy2TJHBpS40WJn6XBDGFS0kmQ4JKhiSKj4KJgydDa+GdyJyVIE9ascnKDh2ps02yS7wFURy8i4ZCgCH8KLfL7wbG3zRs9lRdvcbIcCo7TKG9rbB77ya5lxQ2Q0qbtSCLITwUGRYI9GyYBh5GNz/NkSpON8BfFwFNaeKgoxJ05ymgLmO4UsUQAdNqGQAXFVcqtDBR8VEJ8FBoYnDZG+ZpY3VXe+ZOGPi0MG5bhx2psRGkMYWLigQp0AhwojW3c7l7uLmuJJIAQQAYSDroODKF3e2W9NHR8TC7ls4NmeGGMcMrA22q1SpDQ0PUajWazSbZmEo2MBFKAqnHoeUhXA9FUcnEVJ4tWAwnB8jl9pHJZNi3bx/lcplz585Fis7NzU2CIGBubo66A649hSIgJaDmG9Cd197860IiFR1dESiBT61Wo+FKkokUjx3ax77JYZLJJNsNm9P//HViqRSDOwo4ik2bpKE9cCOZ7/t88MEHWJbFyMgIf+9XZvnuyaV7Njb28hLn5+eZmZnh6NGjaJqG67qsra2xubmJrutMTEwwOzt712OKrutMTU0xNTWF4zhsbGxELZ+O4xCPx3n66acfSG3aRx999NFHH580+r9GfXzmsZNgsKvbbNZqGLqOInUqHZXRWBtVC4NvXddFFwplR1Js2KQLJrquY1sKHir5ZIxOy41O5DIxnfVqh1LT6RNifTwUlFsObcdnPBfH6xIHmqaRTRhst3zG9h7kyHju5z2bfdwDvUa16elpMsOT/D9//xyP78nx4VKZiuXveKXECHxcOwwM3yjZ/Mvf+X0KioVhGGjtHJeacVZ1SGhgS4WWp/DUiMKFU++g6zq6rkfklWmaGIZBMplEVdUow6oXQB8EQXSccxwH27axLCtqB02lUqysrHDlyhUGBwd35YCpqoqu61HLnq7ru1Rnvc/u3XZmJd0KVVWjEP2d8DwvslwuLi7SarWAcIDsaXE8NcbkUI5CJsd5d5hXf7pCsdYAd4CcYmEHCjEZWu+CQNIgFtr1UHjbmwEEWdFGkX5XnQUW+g7jocQlxilvggQOCeEghYIuJAU6FGhzWNsCBD9092NJLfys7hQsDGwpmfMKzKpVUkpoBdxJ5sSFBwJU6VAOEniIrpExgC7dJREMiwYaAR+4o10VV6+3MbzXw82A/LA90kND6VoyRaSiCqerIaOMLt33WQ2yqDJMrDLwcNBQu8RR75N8FFSC0MYqDVR8dCQWOgqhgslCY0IUGRFNABQZhAH1XSIspMRCLAU5DgdbvRWOhUZS3LQmOl2ra6y73mx0zK7abCeB1mvD9PHxufUCgOzelOj/nh0VJCZhEUIHI9LcgcRFxZdqZFk824gzZi+TVlxWVlYwTZPp6ekwSuF6hx8sujh2WGygCvB8n2bL5WI9wSN7ClFO1+///u9TLBYJggDXdUmlUriuS7VapVAoMJVMUtxQOF83SAkfhEfRDbdbDUlWc5GqQc1TyMdU8vkkHV/QabscLgQUknpEHg9nYrx4ZIo/eH8BT8JwNnFPNded0GuubLfbHDx4kImJMDPuN17ay19+bOKOGTblcpk333yTeDzO1772NRKJBJubm6ythcUMY2NjPPHEEx+ZxDIMgz179jA4OMgHH3yAaYb28bNnzzI2Nsbw8HD/AlAfffTRRx8/V/QJsT4+89hJMCgjIySTSRzHIa2ZlNc7GKk0k4MpfM+jVCoR6EnGUhpfee5mLsZQuUHugwqeEp54anpYcV+3XBKG+sBXXfvo434oJA0ShkrdchlMmeTyeSC8wp+JqwxnPrqlrY9PB+12m0uXLpFMJiMFw5WNBm3HZzip8cREkpPLDdouKAIURSWdTOBLifACpocS/PVvP0fWVCgWi0yvbZBesLje1LBQiClwYkzlpTGixssewdVTgfWaFiG0LPZuPfRIKiFEpBbbSZ6pqoppmqysrES5cj3LmJQymvbO6amquitEvveYruvRtDVNwzTN28i7W0m0fD5PvrvNQ2h3/1dvzfPm3BYNy0EXAYHvU257GNLGkC5N2+O6p9FCD+kOqUSB8WFuVqgkkii0pdG1Sd4k6+Qu1RW0MMNsLymZFUX2aZVdLYsAB9QiN/wCjlS7wfsyslnOBUMsBXlyisWo0mBENG9TQ4VkRxDRNn73vRo+MXwmlTpX/EGW/EJkZbypg+p9lqC3pKHObLd+LLTbKd01IRhR60wrVRrSYJ9aZp9a5kN3lMUgj9slDp2uEi1HJ5qegQdCEEiB3lWHGfgMKS08KajKOEWZpOSnUKVPXYbrD3ame0k0AtrovO7OYIiAltSx0Wn5NgXRwRNhhtikUuG6X2AjSOOiYgjJkNImrmt4XoCJS0I4UXZXEFlcd/dnBjseDY2VAT4Cu9se2iMfd24LATJS3HXQuewN83xiE0VRcF2XhYUFFhYWcH0NzzmAg9adviQlbWLC47XLdfS1D8nHNXzfJ5vNkkqlmJmZwbZtFhYWGBgYYHx8nGKxSKPR4LGUiuMYrPtpYkKyN650zzMkqppAV2BK+gQobDQckobGC1MJvrZH5+zZs7z33nvkBkd4Y11ybtvFkYIPl8pkk232DqX41tHdaq67wfM83nvvPTqdDo8//jiFQmHX87emBvfIs+XlZZ588kkSiQTXrl3DdV1GRkY4fvz4Axds3A1ra2ssLy/z+OOPR0H7tm2zvr7OqVOn0HU9Isc+7YbjPvroo48HwSddktXHzxd9QqyPzzzuRDDIIGClWCeh+pTqLRKGSjZhIuIZVrcq/MVju6+kJhSfJyZS/ORGjYnBLI4XfOSrrn308SAYSps8v28gygy7X15LHz9/+L7PtWvXaDQaPPLIIySTocGx2WxS21rBbtXZsFXGC2n2jahc3WxgeyEF4gQS2/NJxzS+8eg4B/eE1sKRkRGOHj3KL3oeb12Y5+z8BlM5k+ePzDA0NPSRQ6SllGGe2Y6b67qRWsxxnOj/3mNbW1u0223y+TyqqkZkWxAE0ef3Huu10wVBEBFzvu9Hn90j5XYScjvn7VbSrkeuvVdLcbFhkNIhqSs0XMGNekBCOAwrTRwJBuCjd8Pm4SZlJ7q6KwWXkPaoErtD4tTuR3wEeje8fjnIc1Rs32aBPKwWcaXKZX+om/0VKq081CiLKpCCRb+Aqyjo3UD/Xk5WTzOlEzCqNKO2xECCIiSD+QznyhkM4YI0UbpLcpP8CddXDou0sHFQKcsEQZd4MvAjq6WOT0o4IOENd4aAm82Psa5GDUILIoQkkYNKUriM0MDCwO2GvbtduigtLBK6YN0xcVBJSUJbpQgz1NhBEMrutA0CbFQqMskgTQZFi4pM0MbAlwo5YTGtlnECjYUg27V4OjhSZdlLY2JhYxAISRobF4VG9/tUuvSXjt+1RoakXbwbSC+6z3to2GjctJzeGsgvovlVkNS1DErcQg/C7VvX9XDbljoxxSMrOyBCS2MqEcMLFJqBjoilUdWwWMKyLOr1OktLSxiGQTqdZnFxMSKRe/vT33huP8vbVTYqbV555hCK2yY9NI6IZRhIGQwkDZa3q1xf3UJ26iTVgHQixmyXfP+//ekZPigKhjJxnsglWK+02Kx1GNVVjptFzp0pAmCaZhSi3wv7j8VibDcsXn3zPRJqwDdfeT46jsGdc1iPDRsMN+YYLuQ4cOAAm5ubDA0Ncfjw4Y+VRXgrPM/j4sWLGIbB008/vYvsMk2TmZmZqGRjfX2dkydPYpom4+PjDA4O9smxPvro4+eOh1WS1cdnG/2WyT4+F/hnb9yIMsR2EgzfODyCbbV548omTiAoZJK8eGCYo2aF6cmxyCpwbm6Bha0ab83X2Pbj0UHtuf5BrY9PAB3H57ffW+SdHT+g/W3t54s7Xd2TUrKxscHCwgJ79+5leHiYRqPB+vo6lUqFZDLJ+Pg4v3+xxvcvhMefpK5yfq3OSqWNEBDTVQZTJr/2xAR/8/nZXd/vrSdSMU1wOK/w3KDHQC7N5OQk2Wz2E21Yu1MTpZQS3/dvI9judesRbXd6rpdv1lOX9RRmTU/wW1clQoDudag3GjhSZT1IoxIwqjTQhMSTgo0gjbNLL3XTCCmiIPy7ZU3dDtEllgSSr+vXGFVbt72mEsR4053GxEMloChT9PoBfcQOogsGRJjbZe5ou6wECSSSrLC6psmQsJlWy0wodd719qBKn02Zxt9hYwxQopD8nGiHBKIMaGN0bYh6N7tLdi2dCjEcrK5FUO1aJFURtkOmFTsi0FQCLBmqxV6Or5FWHDbTBzi3ZVPxwwyxhHDIizaBFmfdjaFEgfRhE2XPhuhH6rSQdFJ3KOJGlQamCAnTltTxpOAZfZVlP8NlfzgipRLCIScsLDQkgkHRpCSTuF3isRkYJHCIKx5NadCWBu4OIlDrrgO3a5+8abXs7We32itDlZ6GJKO6JE2NX5l00VpbHD16FCklN27coGYH/Ml2lsD3yRiCsbExWq0WVcvH8wP+ypRFLhaqIzc3N0mn0wwPD9NsNqlWq+i6HjX+ptNppqenuXbtGtVqlYmJCarVKolEAtMMjzWKokT26J03CNWia+UG//05CykD8nEttBJqGitbVRzP45/8jRcYTJlIKXEcJ7JJW5ZFpd7iD85v8+5CBVcq5FIJHh9P8pePDZJPJ4nH4/zOh0V+eKVIIWWSMlRuLK9Rbrm8MBXjb70ww8TExG0tkj8LarUaFy9e5MCBAwwODj7w+zqdDuvr62xvbxOLxRgfH2dgYKBPjvXRRx8/F9xt/PmtR8f6hWxfIPQJsT4+NfwsctP7EQzFps3SZoVmaZ248JienmZjYwPVTPD2tsIPzy5Rb1sMF3I8MV3gKweHmcjH+2qdPj5RFJv2HfNa+vj0cLere798pMDi9Tmy2SyDg4NsbW1RrVZJp9OMjY2Rz+cjoupOx5/jk1lOTOWIaSr7hlN3/H7vdSL1144PsrKyQr1eZ2BggImJichO9DDQs0f2lF7z8/OUSiX279+PaZoEQcBW3aLcdsgagmzspoLs1r+33r/TZ+18vkeSLVYd/odrkpweYHdaWLaNLwXrQRofJSJVbKmyEaS74egKStfWF+JuQev3hugaKTUk39DnGFObt72mIzXedKd7C8GGzODvsC0WFIussGhJnafUZTZlOrIB6vgUaFEmxXaQQGgGugJ7Ej7fSG+xsbXJm+40fteSGEb/93RikBIOaWHzlLaCJcMygDl/iCQ2Tcyo3TFUdYU5ZTuVYz4Kejc3bFTUMZVuM62qohgmnp7ibx5LIerrzM/PMzZ7kFOX59lSB/Ey46xvl/EDSSWIIRDddswwrF7SU8z16MgQKWzsbih/j8yEkJRrYURh/81Aw8Aj6M5nWlhkhE0Lg+e0JWLCw5IatlT5wJ8giYO6Y1qOVNmQKQKULrkV0CIMqlcIMPGxu3MogBgObcxou1CQYW5cXGUwrvDNzAZxxScWi1EsFlFVlXg8zptFg8vtBNmYSi5h0HYlnUDl2XGTXz2S5fr16wwODkbnE47j0Gg0KJVK+L5PPB5H13VyuRxra2vYts03v/lNVlZWOHToEMPDw7v2D8uyaDabUatwr1nYsiwWqy7/vzmPmLTAC3PKhoeH8REsF5v83RdG+PpTR+6Y7ff/+MlVfuetOfIJjemxYRq2R7Fh87UDOX7taJ61cpN//NoKjm2j+Ra+55FIJtFTeVRN5x/9leMP7fdJSsnCwgKlUulntlu2223W19cpFovE4/GIHLvTBYS+namPPr58+KT3++2Gzd//vbMoitg1/WLTRgaSf/gQj50PG/1j4kdDnxD7BNHfGEM8TLnpgxAMtm2zuLhIsVjkT651OL0tSWoBSV3BSOf6zH4ffXyJcBsp1bZZKdZ5YkjwV48PYFkW2WyWsbGx+6q1PgrBGZ1IIUlqknq9jpSSmh0QSPhbRwzSRmhZbLfbNBoNPM/blfsFu3PE7mZd7Km+7pcv5nkexWIRM5nhg3qCi2Ufx5fEdIXjwzG+dTBN0tSjLLLerff+nkVs52fcetv5XKnl8pt/chlF3DyZbDQanFwoUXMEw0orCq3fDBJRelQvY8tnpyqku9y77t0PoUrsLxmXySvWHV9x3htm0S/QkYIGPZuY6CaWSZLYZBSHl4xFYrh0pIYlNWLC44Y/wGKQR5MeiggzwFypMq2WeVTbiqbtSAVLGCDD6P0YHjHhM6lUUIVkI0hjo1EPTGJ4DCotgu7y24FKmZAojeHR2zxdqXTJO8GgYjEY6xJKvk9gJNF0g//RrMd4IWz8fOedd8jn86yurjKx7zBVy2dpo8gflULLqCF8AgkdjIi0y2CFqrXuusgJm5JMkMBheIfiriM1fClACJCSmoyH615IPBlqzJLCRheSV/T5yL5aCWK84+5BJSCphMH8gYStIEkDM9IJiqgoIEAjYEhpUw4SkaJQJ8DtEo6CgJQIs8ZcKZjUW3wrtUwmk6Fer1MoFNizZw+lUonB0XH+9EqdC0UHVJOEqXF8xOSgus1wIcfExETUJmmaoTprfX2dsbGxqNlVURTW1tao1+t85Stf4fz58+RyOVRVjVSTyWSSdDodNbaqqopt25TLZYrFIltbW6yVm/zrBY1UMsHEYDYk0YKAYsPC8wN+eaTB5FAW0zSpWgF1JyBnhqTgP35tlbhpMj4YFl0oikKxaeP7Pn/3mQJnr9zgX120GEpqDORzZNJpEALHC1ivdvjN7xzl0OjPrg6zbZtz584xMDDAzMzMQ1W+tlot1tbWKJVKkXK3UCjQcf2+namPPr5k+LRsjFc2GvyDP77AeC6Ood08H3nYx86Hib7F8+OhnyH2CaC/Me7Gd99bigak47k4dcuN8pU+Kik1mDLvOxA1TZODBw+SHZ1i4dxJjKCB32oxtG8fevdq5TvXS/zlxya+1ERlH19c9Mn4EFFDbVwnH1OpVsvUazV0qXOlaiLNFLNjY2EDX5eUupdCqvd/PQiYhzuG3fceW6p7bJaaDCY02qpAN4xQdWQIii2P5MAos4PxXWH4UspokKyqKmNjYwwODkZEVG8A3vucu5Fl93ouCAL+6Y8u8/Zqg7F8itG4TsP2ObnhkUoF/LUTmdum0bNMPshn3PqavUmfN5fbNJsKCU3QcgPyqTiTWkCnE9BxJSYeQ67Ftp/o2uFu2iV3El9m11DZecBTF4WAOO6ux3YSiEZmkIm6hSNaXHAHdnxWAEg8FBqYzIoKiqJSlyYDaYN0p07LV9jwM8SEh7mjZbGDxjY5bFHlsBpmPq35GQJUHBRMfLKqw6TewnFh0S9gCpec4uIEKk0MCGBAaRMgcISGLj0kIQGmytBC6HU1XSpQlzFMzyam+LQ9Bct1eCRVZyQ7zsTEBK1WK7IEAmwszIXbtKViikEsqeBJ0U0XC2PpFSRJxUMG0MZEw0UTPiPUsaRBR2qRddSWOkNKg5JMYuJh4NHGQHbbQF0UHKmSkg7X/QL7lBLXgwE2gvRtwfxlmaCFiYIkgYuPwEMFJAY+EgUVSUI4uDKOQUCWNtXuN60REAiBIn2yGmRyeVSzQSIRI5PJEI/HQ8Xc2BhD+SwTzZMcnhonNZDHrhUZySUxMtNcXi1ztbrM4bE8KV1GhNiRI0dotVp4nkelUiEej9Nut/nOd77DpUuXeOaZZ3YF2XueR6vVolwuc+HCBba3t7FtOyLLDMNgcnKSp556Cvtcie+fX6fa8UgaKrW2R83yeWVvhicPjvHBuYtcsLJcKHpYnkRXAsxOBU9JkUno2JaFY5pYtk2jVqfY9jl3tcaJw/t5vVZCURUyO34PHmapULFYZG5ujqNHj97WPvswkEwmOXDgAAcOHKDZbLK2tsbc3Bw/XA44ueEynE38zOeXffTRx+cDD3NceS/cmmHdw2e5kO3TWjdfNPQJsU8A/Y3xJqIBafImkdX7+0mTUtW2R6AY7J+eoNmohYG6hkEmprNe7VBqOl9qsqCPLx4+y2T8zpbDnaHtdyKg7vX8rY/1cCshJYRgqeaxWWowmNCouQqtVotMJkNa0Si2PUpNh3y8s0sB1VNE3aqu2vm3R07dC7MNm++vhlL7gVuk9oO65LFH9t3x+DM+Pg6AZVmsra1x+fJlUqkUExMT5HK5n1l1sd2wWbJjTI/oCKcFvmAwaWIaBheLHrHc0EM9Lu476DO2w26aiat88/Fwm2w5HtdXtqhvr7KxXeK/OedTt7sNi5KudfImORbaDne3Ct7dQikxCMgIi1hXkdQjw2wfrvhDbGyn8VBxg1BpZeDio0ah7r1g+aav8VNrIrRK2gGjisaIaOIFCnHs6OMVIYgR0Ao0Aj1JSjZ51N1in1pGTWTRNJVOxyKGSyDhNWeChOpjSA8kDCkthAwVVw1pYgqPPUqV7SBBS5pY6LiA11XOCQQpxSUQCo3AQKqSWFww6ZcY9cr85IMGg9dCZdvw8DCdTgcI1TYdqWErMVKKiy50rEDBDcKSgJ4Z0VZMUjjMiE2m1BpJ4aATcNkfDMksDHT8MDNN1Fh1s1SIdWk18NAj22cam4RwWPQLbPgpbAxM4e4K5g9Vb2GbZ89iqosAIX1cVFy0qMwghkcbD10R2IGORJCnTUbYSBQSMR3HtilVPXInptCdKqZpUqlUGBkZIR6P8+qrr+J5HkZgk/TqDAwP8v3rHU4WSzTcUJeWv17l15+a4q/tG2HpxrVw3XU6lEolHn30US5evMg3vvENLl26xKFDh8jlckColiqVSpRKJZrNJqqqRuqwSqWCpmnEYjE8z2N9PbS1xlyfcU9nflPDQyMV03h+MsOvHRsmm4qzkdzL29fWmB4tEFfh+vIandgw0pfYAYjAZWt7G13T0OIZ9uRM/uI3TjCYMnm+JD6RopcgCLj6/2fvT4MkufP0TOz5++1xR2RE5FmZWfeNKlyFo9HXTPdMz9HD4dAoyjjUjoziLk27MskkrclIM5lxv4lrWq2+rUxrlMgVrblc7txXdw/Z7JkG0ACqANRdqCOrsvLOjPsOv//6EFmBKtxoXFWAP2bViY6MjPD0cPd0f/39ve/Nm3ieN27m/axJpVIcOnSIatfl/3X1TZJqRDRoI/UsxdQoly2+6RkT8+Xk87yufNRKsr7Ia+5HnVgQ+5SJN8YHafQ9Bl7ITO7BxqLPQ5QaK/tuwEQuR7Vaxbbth1rZj4n5JHxUMf6eOPVOgenDBKgPeu57ZUu9k/cTmd7rv3Vd/8Dv3x/g/n4sdl3+cleUKqRMChMTwIeLUp8Gn/REyrIs9u3bx759++h2u6yvr3P9+nUmJiaYm5v7hfPG7j8mG2oCz/NwXZdg6FLt+7x47gKnFkrkcjmy2Syq+smEVNtQ+Ucv7OO3T8++a9zUNlSKR/bAkT2sra3x01t/zU2RJZswkL5Dq+/QCkch6+E43v1+9xi8nxgG4KNSUgYklBBN0ymXyziOwysNi/Uwi6n4WNKntSvcSNRx4yG77xWhsE6eEn2SeHio4+ZJleCB5slISlypjrxbbhd/V4jLGAJb93GFQtvzMHVBz5eEQiNjSgg1fH80DllSHDqRwTFlm7LaJ6GEXPZL3A1Gg4G9+7KybDxKoovUEkSKypN2HSsasCpynPUXCYcaSt9nzhhwcrhG5Dn03YDrQZntaCQGDqVKICErHExTYegGdLBQkcgwIkJiaXBkOkdtZxOAE1qFw0qHXgCWCLBFwEV/kj46wX2h/NHugGuBIRPqEE+qDCNBizRFMRqXRUCZPr1Ix5UqQoRkhUNHmnSlBfLtz0MjwhQBLhqqiCgqLl4EDtpIRBUqmiIxdYUo9AkUHUORVFfvYOKNx4pbrRau6xJF0dhxWa1WuVE1eLWm0XMDbENB13Q6fsj//OYWtZ1tfnXRoFqtMhwOeeaZZ3j11Vd58sknef311ymXy1y7do1ms8lgMBiPSiYSCVzXpd/vo2kaU1NTnDhxAsuyME0TwzAwDGN8HPtdKVmvtVmvttAjFz10qG5vcMuJeOnWgEJSp1/fpu46lNIZup5P142oBoKZfIJyLk3Pj+j2Xb51oDje1373zCgr79XbdbZaQxKGyq+dmB4//oswGAy4fPkye/bsGQv5nyeNvocXCWZKBQz17eNAfNMzJubLy+d9XflZHDs/K77Ia+5Hna+8IPZpjxbFG+ODfJF203dekOqWzWqliYP2UCr7MTGfhHeK8dVKBQDFi/jhG7dZkBWy5tsZCO8lML3XY/fG9T7suQ9jC9gXfXfv0zqRSqfTHD16FCkl9XqdW7du4TgOU1NTzMzMoOv6R36tdx6TDXP0z1Vcpm3Js6eOooUjB8ydO3fG4eHZbJZcLkcmk/mFXGofNO4upWR5eZmvTcHj0/v40fk7BEIjk7CYdBvc8UYZWxKBB6i7ofsjr1hIOD6VGTmbFN4eZS2JHlJKfN+n1+vhKSabYZqkKjFFRBBEpBWPRhQRou52Ot4Lkx8JOhLQCVGFxCYYFwMUxYCqHOWH3D8+uKA2xjlZAG4IF5s2OzKLE+XRg5CS6qAJydBn11M1WvphJDBEyLThkDE0DCPBaa9HmhQ3eibDYOTgSuJRUIYjaSxwcDCQoseKkue2nxm5nvQIVwpuDpN4rssJvcutaJKVKIuJTymp0ey7tEQCX0tA6DIUJkIK0mKIJUbOrDt+FmWny3FNG4vfCTVEj0ajokOpcSecQCDQd0P/RxLW6LNICo+NMM1wV3gEQUMmsKI292JZbBHg765/D5WcGGW+DaSBh4aC5LBa4YDaIEDhbphjM8phCZ+EGCIjQVcayBDmrAgPwTCExahJxhTkcpN0Op3xCKOqquOm1e3tbRx0Xu2n6UQWpiowFAVVRBhC0uk7vLLik25skbc1FhYW+MlPfsLs7Cznzp0jn8/T7XYplUocOXKEbDZLu91mfX2d4XA4Fow+SsC8EII9pRx7SrkHHr+8Wse7cJGkcEEIMukMqqpSzFootsITi3nuVPrsdL33PM58kDD9i7CxscH6+jonT578VMtAPg6P4jhTTEzMJ+Pz3u8/7WPnZ0l8TPzF+coKYp/VaNEXsTE+zHlBD9MFadtX8FyP7556OJX9mJhPwjvF+NJuw1l2N/xz/uDDF/75efBF3t37tE+khBAUi0WKxSJhGLK9vc3FixcRQjA7O0u5XP5QYfLDjsnThRSQolQqASOxynEcWq0WW1tb3LhxAyklqVSKXC5HLpcjkUj8wqOcUkrOnTvHYDDg2KEDPPnkIY7afXbafTrVTe5stNgObCaEi49CPRyNRHm7spdBxHBXAFN3PWQmPhKIUNFENM5ecxyHRgReBAULTD1Bp9NBEyOBqYM9FnJG/yt3c7UYtV/KkLa06EuDAAWJwMQjkoK+eHt88F522D2ueXlWwjym8EkyEpl2lDx66BJoSSLFQPEdhgF4qByy+uybLtHr9XjqqaeoVqt8d2aGvzp7hb/YSqArkFQ8ogikBEeO3GqKItgM0ySUEBG4BO4oY8yQGltRhgNWSHWQI6EEGDLAGQRkDA3FHwA635wT/HQ9REiHlCoJQ4lKgATWPIt5CbYYrSHffzs3rRWZDNHRCNCFRMrR+hruipjbMoWHDvcJlQ462zLNHF0UZVQeYBEwIUZNlQjICBdTiRhEGvNKiyNaDWmmCBxnlFcm/PE4bEnto0gFR+oMFA1bjyhG2xy3WoRhgm63i6qqOI5DuVymXC6zsbExDn/f7Ev++qqH5oAiAhzHwbZsLF3D9QMGAVjZIvNTGZaXl1lYGB0/vve9743bJAeDAevr69y4cYN8Ps++fftIpVK/0H4Bo3HEnZ0dNjc32WkPUKWPKxXKuRzJRALdMKj1XHKa5D/7+n6ADz3OfJQc1g8iCAKuXr2KaZqcOXPmUw3O/7h80eeXMTExnz9f1H7/SY+dnwfxMfEX5ysriH1WOV+f58b4MOcF3c/DdEGqBQMGzcpDtX5iYj4N4jtD783DcHfvsziRUlWV2dlZZmdncV2XjY0Nzp49SzKZZG5u7gPzxj7OMVkIgW3b2LbN9PQ0MBKx+v0+rVaL5eVl+v0+iqKQTqfHIpllWR/p97hy5Qo7Ozv86q/+Kjdv3uTu3bsU0yYH56c4e7bOljJqFoxQKNgKKgbbvQCJJBpnio245+QSQIBGAo+s4o5H5RYXF4k2axguoNvousQ0DXzPZzJt4vYFQo5ENUVG2CKgJ3UEoCswVLP0vFEHpkaELiQeBnNqlz2iPh4fvJ+h1NiO0pjCH39PJcCLPKSisqD3WOlBIDSySZMpp8IJu8/6eodiscibb75JNpvFcRz2T9jsbw5YctNoGOgiIFR1XB8WlAae59EPfDJqOJaeTMMgYyep9HwcPU2k6ORTGoEHrushhELaUqkPI+qNNopWQvMcgvDt4VSDkD4GjtTGv4NlWTjOveZOcd/X3bZLxNgNNhLD7mfk5Bti0A01VGHgRLBXrXNEraGH4dsZZTJkr9JAAi/7i8hQJ5SSoTAoRB0QEEiBmUgx4Xu0PYVnsg7J4Q6u4dEJdfr1DpPZBJqm8dhjj3H69Gk2Nze5ffs2i4uLHDhwgHrf5z/Uz9NcrRNJBUMbfc7tbpcABVuXzJVydLttnnvuOfr9/nj0cXV1la2tLSzLYm5ujoMHD34igbjZbLK+vk673UZRFKSUzJfzfPtYgb++3SLQTKSiUeu57zq3/CyPbe12m2vXrnHo0CEmdsfPv2gepXGmmJiYT4d4v39/4nXziyHk/TVZXxGqXZd/8oejXJn7Tx5qPRcZSf7533nsE51UDL2QH9wXIpwwVJ79DISqf/HSnbGo907h7WEM76/13IfCbnrhwgX27dv3mTQhxcR8kTxqx4SYT59ut8vGxgbNZpOJiQlmZ2dJJpPv+dxP85gcRRHdbpdWq0Wr1cJxHFRVHQtk2Wz2XaOdS0tLXL58me9973vYts3q6iq3bt0imUwyPT3Nj3/8Y9rtNhedAktuBkMEeIpFMzDwI4lE7o44jgL3lV2hCkBDckitcErfAUbClDRTFGyNq/3E6PWkSyFlo9lpdjoDhNujF6pYqkCXHoGEpkygCIWZlMK2I/D8iEhK0sKhoDoMpYaiavxaoQFOF8dxEEJgmiZTU1OsdUP+qpZlwhakEzb1eh0/GI1d9jF4Tl/DVkLURI5iyqRT3QDAtm0cx6FUKvHNb36T6elpfv7zn1NptPgPy0M2/CSBULE0hcM5wWTvFgMv5CVvAYHEFgG6oaMqKm03QgrBC6k6L/eKqIogZ6kEQYDn+6iJLGEkedrY5KXuBEJK1GAw+pAEdMORzPiEtokpQmwlxBbBuMBiKDV+5B1iIA00Rs2Swa5fbzRyes+1KFF2/wW7aW0aISoSW/jsV+oc06poIhqF/qNj4nM7LLASFTDxMQgZSo2aTGLjYYqROBYhkBJMAn41vc6Kl2Tds/FRMUTEjNrnP/+Vx0haOr7vk0wmeeONN/i93/s9hBAMBgP+6z85x5/f6NH3QixNIYwC/EjBUuHXD9h8reihJvO0nZBDC9PIYYcoipienmZqauoT5e31+302NjbY2dkZr9dMJsPs7CzFYhEhxLvOLVVFcGw6w//quQXm8p/d2OK9keZms8nJkyc/0ujn583Dcn4ZExPz+RHv9+9PvG4+Hl9JQezGdpf/6s+ujkKFtbfHS7zd0aJ/9v1PZ7Tos9wYP2tR78vMcDjkypUrPPXUU1+o3T8m5tPm8xLjYx5+pJQ0Gg3W19dxHIfJycmPnGP0aREEAe12m1arRbvdxvd9DMMgl8sxHA65evUqv/ZrvzYeK6vVavzsZz9D13V0XUdVVa5fv0536HLVyXHdn6AXapi6ghk56Lvjh5O0ySUM1hyDni8xCZlV2xxRa0jgxm4rYih0ErrCfMJnOBxSJYMbgqFIJoIGB7U6d8UkW1GGAAUReswaLr4fsCGz1AIDUwFbeGTlAFURGIkknprkVyZadNdvAiNX3dzcHN/61reQZor/4n/4OcmEzXw5z+raKgD1rkcoI76mr2CLgFQqheM4aJpGGIZomobjOCQSifH6KRaLVCoVWq0WkZEk0GymckmMyKVUKnHr1i0uOgVuDpOY+NgqOFLgSJ3HcgEn9CovVQ1WmcAWAbYGA1/iCYMTuYCD4RqvD4ssBzn0YICBTyNKMEDf9XpJdEIywmFG7XJEraHtjlCe96a4Fk3hI3YFsJHwlcClx9tuwXspYv7uyGmOATnhEAoFV+rMKC0W1dbYbTeUGi/7i+iaihaOHGkC2AmTdDDRBWi7rkBPKmhE5MSAAA1bCckmdHpuyCBUOZ5x+V8/t4Bpmty+fZtWq8XBgwfHAfg7tQav1k3+/e0uQ6kRRZKUFnHM7nLc6rBhznKzJYkUg1zK5ptHp/m95/fTc4NfKLbC9302NzfZ3Nwcu+0ymQwzMzOUy+X3FdjWGn3+9WsrvLXZJYjkZzod4Louly9fplgssrCwEJ8zxcTExMR86fhKjkx+XqNFn+W8cRze/4tzLyB6Z2eHqampL3pxYmI+NR6G8cCYhwMhBBMTE0xMTBCGITs7O1y6dAkhBDMzM0xOTn7mRQiapo2X4R6u67K8vMwrr7zCgQMHuHr1KrZtk8vlWF9fZ3t7m+985zu0Wi2q1Sq6rqMNhxy2uqwFaQppnZTiE3oeQRjiYeDrOY7oaxzWJI3BaDTRwkdKyZWgzEpYwBQ+CekQRSZXWiqP5SyOi02agwBDuth6gKIonEn3GYQOsweOsLF0HeH1cCOX2ajBa8oChiZI4hGEYNk2PTfCsHxSOviWhZSSKIoolUqjFr7XXmNa8dmJUmy3+jR7LpFuExgpnpiISDYiogh6vR5CCMIwxLIswjAkkUgwNTVFvV5nfn6eZDLJ7du3AVC8PnbooIcqnW6XwWBAMpnkaNRAUVXWXJt+BLoScTzhclCp0+32eL6cx+r0WBmatH0NJfKZERUmBx2uqtNUZYKhH9IVSYIoQqKgECERKICHRl+arIQjt98Jbbe8Q0gMQkDBR4xlMWPXwXevqiDa7aAcvV7EhDJEFZJIRnSlyfWwzGaUxRQBU0qXSdHFR8EI3PFkpgQyDOligoyIYFcSGznPtmWajOJT1ENC3yWlqShCsjwwWFrbZt9smcFgwP79+zl8+DCDwYArV65gmiZ7+nf535+YphkIXN9nygYLg7+4neR6zWUql2LP5ATDSPCjaxXeWG0TST5ybEUURVQqFdbW1uh0OgghSCaTHDhwgOnpaTTtw0/Lf3xthzdWmg84gT+NyI93Uq1WWVpa4vjx47GjPiYmJibmS8tXUhD7MoTOxXlBn4z9+/dz9uzZjxRCHRPzqPEohH/GfH6oqsrMzAwzMzO4rsvm5ibnzp3Dtm3m5ubI5/Ofm/PDcRwuXrzIb/3Wb1EqlZBS0mq1uHDhAuvr6wDcuHGDTCaD53n0ej0SiQSNboRQdfKGJPADwmgkgSQMQdMNGRoaWdlnQvWwbRshTGp97135XUroYKJxsyWYSwfklSFhGOIKCzM9gZ42+f6Zxzl37hxqMGDougBMaB4LdFhXSnhSIOUQJZEliFxOT6gY0eh5QRAwNTWFaZqsrKyQTqf5+8+k+eNrLV7ZDBlEKUQkSGkRA8dBsxLYujoeORVC4Louqqriui7dbhfTNFldXUVRFBKJBKZp4rouiqJw9OhR5ubm+NGPfkSn02Fhdha7UmHGGyJSGfTQwYp8nJ6PrutEvsNJo8+CjBB2BjUYkjUVXm0WWB7a2IrPlBbQ8jVqJBFIQtTdAccIddeVl2C0bvfLBgA10uSVAYqMqMj0boKYwEUny5AWid2ig5FwJZBkGYlhAG1pMdzNGjMZfVYrYYFAVTEVCaoFwXD0GQpBoBhoUURZ9OhhMJQGKhES8FHpRypVV2HSHI3WGiKgHagkCtMMpcrWUPCtY6fYMzfJysoK5XKZWq3Gk08+Sbfbxblzh7DXo6VpuMKgwjTz5QQJJcQZ9EBKekNYrvU5vSf/gVm097bxtbU1KpUKUkoSiQR79+5lenoa0zTHY5JhGHL/4MY7/7vWc/n5rSp5WydvqaiaMj7Wv3q7zm+fnv1Uxp9v3LiB7/ucOXPmE42CxsTExMTEPOx8JQUxePRD574Mot4XiaqqzM/Ps7y8TKY899C2dMbExMR8mpimyd69e9m7dy+9Xm/cilcoFJidnf1ErXgfxmAw4Mc//jFf+9rXxmLY2toam5ubPPbYYwBUOkO6agJbsfA8bxzgnzJshONRabtkDEEURSiKQt+LsHWd00f2U0yZXL16FU3TaLVahNooQyqJB0AoBSEKugBHaLSGATldZcVaYHmg49cEWi3kb+6+zCGlgq6MxAhd1wmCgKN6jWwyy80WDDEo6gaHE01+99mTnHtlg0wmQ61Wo1gscvPmTWzb5tixYyNnXlSiVMjSb9YpZiz6Q5+3Ojq+OcE3JyL6/f7buWNra+MWx8FgQCqVGpcb9Pt9SqUSq6uruK7LzZs3GQwGuK6Lbds0m032799Pcnubfr/PgaOHx6Oz934PAEN6MKihqiqDKMNmmMRSAvRdYc/H3JWt3o7M91DQkGiAKiQBKo4cnUZ6UiGJhxRgSZ8hBqqQhBJMQpL4QLT78yGBUEkx+h0DKRhIY9zqaYgQVUgUodAUGSZkl3Uvgym03QwxFU8q2MInQuBJDZUITUT4UqDsvs4QA6n4GJrC0JNoIuIPzi6RmDvKUiXFzR/d5un5Krn6VR4/eRwhBMvLy+NGykOHDtHr9XCtCZTbEuE7VLttctksaAbVtosfSfqtOro3ugmpeBE/fOM2C7KCLYJxpt69Mdh8Pk82m8UwDBqNBo1GY7x/3BOl732VUuL7Pp7njf+ttH1WtwR5AxxdZc/8PPDpTQf0+30uX77MwsLCuEQjJiYmJibmy8xXVhD7MowWPeqi3hdNrljmv/79l1jzajjBZ5vDERMTE/OwkUqlOHLkyLjZ7s6dOwyHQ8rlMrOzs59q3pjv+/zwhz/kySefZG5ujn6/z9WrVykWizzzzDPcWl7lh3ccbrYVIiWAoM6srnDEThH5DsJ3mKLDcpRDOj4GgGYRCINfOTHN3/rVU9RqNVqtFrVajXQ6TbvWRifElSqu1BhIYzyyZ4kAQ5PcikqsOClKWQslcKl3BywHOdDgcauJEALHcTh8+DCrq6ssDG9T0lR6wLzoceTILC/+9Cc4joNhGExPT3Pr1i2CIMB1Xa5duwZWmjtNDY0e5YTANlSUUCB8yUaQ4tjjR7jw2ksUi0W2trYwDANVVen3+/R6PRRFYXZ2lmazSa/X46233kIIgaIoWJaFZVnk83l6vR4TExNUq1USiQTHjx/n6tWrBEHAN7/5Tba2tqjX6ziOQzabZXZ2ltu3b3O3NcSNCiPhUIyEQxeNUc8iY2FMIvB2HV5S0dCiAOtec6YMqcsEPioRgggI5KjoQArBgtJgr9JAExJLBKOg/LBwr5SSYDd4PyHcsWtMx6cXGsyoLVQ1YDtKM8BAJ2KvWkeisBwVCKSKQUAgBREqFj4hKr5UcKMAx40YhAomEXedBBPbWxRshXaryb/bqPDtA2W8i9cozCxQ73skdR1FUVhdXSWTyZBQQwh8hopgZnqaiYkJ2kMfpVEnoylkUzqZVALTskh7ASuVNtXOkLziYFnWeJtPpx/MppVS4roug8HggX+eNxJwFUUhlUqRSCTG/56IVF7/k2vvyo/9NKYD1tfX2djY4NSpU9i2/eE/EBMTExMT8yXgKyuI3eNRHi36Moh6XyT/47k1LjZVLHrMTRY/sxyOmJiYmIcZIQSFQoFCoUAYhlQqFS5fvoyUcpw39knGpsIw5C//8i85cuQI+/bt4/bt29TrdU6cOEEiMWrH+9c/v82FhjJqMJQubdflQkfQVnWeSPbRdZ0Tdgs91NnwEkgzjaHAAdvlV/YnuHjxIo1GYzx22O12SaoRU0qXa0EZH203fD0iQiMSOs3sITYaHmo0IOw7DDyXhKKSTqfZ6Ub0gzZ6NGrLbDabHDp0iEuXLqFLn4RqEOlJIiOJ7/sIMXKt6bqO53kIIQiCgEqlQk9N0xlo5C0YDIcjd5uqooQuanKCv3ntTfYWChw5coQLFy7gOA7NZnOcJ9XtdllaWiKfz6MoCrZt4/s+QRCws7NDr9djcXGR48eP8/LLL4/Fs3uusGazycWLF1lYWGD//v2sra2RSqXY2NjA8zxM6WGIiAANTfjoZgLFUVGjCJ/7P/eRXyxA0goMjmpNUpokDEMUJD0MNCJ0RtljIQoGPqqQ1GWSTmgxpYzC+I+oNQC2ozSOHIlvBiG28AmlIGWb9EMFK4pICY+SqHCAJsJKkzEUpNNB6CZpkef1toYfgCEkCemQFS51mWQoVfqBwFZhXu+x4RqYwiccDFF1HVtTsFX4D3eHFMw0w7vbSF9nb8LnicyQyWKRyclJ5ubmuOVX+ZvbHaSZwgslfTckiiQZW6c8kWc4HFLZ2aHWdZHAbHGakwdPkUwmGQ6H9Ho9KpUKg8EAx3HGo5CWZWHbNolEglKpRCKRwDCM9x1hzsCnPh0QBAFXrlzBtm3OnDkTB+fHxMTExHyl+Eq2TMbE3N/SaYTOKDBWiLilMyYmJmYXz/PY3NxkZ2cHy7KYm5ujUCh8rAtmKSU/+tGPKJfLHDx4kGvXrjEzM8OePXvGr3N5tcb/4V//HEtTyRoh7VYbRVEINBsJfDuxhRYOSZdmub7ZxEfjiROH0YYNLPxxQ2Or1aLRaBBFEblcjjAMqbvw+ztFHLTdHCxJQnhYIsKToy7E9K4ryTAM0ukU3YFDYxjxjWSFLH2iKCIMQ6anp9mpN7nYT1MlB7qJqQpmtB4HxQ5p2ySVSlGtVikWi7RarZELa6PKT/tTmIZBOGghhIIAXAwMy+I3y20KCZ1+v0+xWERRFBzHYXt7eyyAtdttPM8bj1IqioKqjrLH7gmWc3NzNBoN8vk8q6urTExMMBwOsSyLZrNJEARMTEywsrKC7/uEYTj+nG6KWe74WYTXR0GyFSQJdpPAYOQOixiNTxqMGiC/oy+RVjyGUuMlf5FepOOjETEaWwyBCIWi6GOLAA8VV+osqI1xGP9QavQigzeDaWoyPWqyFGDpAqKQBdHglLY5HhkVQoxHCRVF4fjx4/y7qx1uOUlMXGwZoCcz1PseB1M+T05qqP6AWtflP7YLJBUPf7fRMZlI0CDJdi/iYNEk6jcI0ekGcCof8U9/5xn27NnD3bt3aXR6nO8kefXOqMHXNlTCwGej0cWULpaQ+EJjGKk8N2fzGwdGDitd1x9weCUSCSzL+kSi06fZJtxqtXjrrbc4dOjQA+UXMTExMTEfj2rXjSN4HlG+8g6xmK8m97d0GtrbB624pTMmJiZmhGEYLC4usri4SL/fZ319nZs3b5LP55mbm3vfvLF7J4WFpM6V118hmUySTCZZWlri9OnTWNYoH2xlY4sfvLbCz5e7bA4ElgbDUCVr2eiqgjASbLQdGtjUKHD1boqWMxJFXjnb4LH0kINsYusqqqpimiaTk5Mj15Npsr29TWuokNQiCrJPKOVu1pQklAIHExVJqBoUUiaO4xBFEU4IpXyWI7M6vdoWvu/T6XRGTZ1ukZUwj6X4pHDxfJVrQxNHy/LtnCQIAkql0niEslqt8vTJw1z5+Qq3O2ksoZLQBC4q0kxyMBPwv/zt3+DFF18ERs2cTz31FOVymX/1r/4Vuq7T6/UoFouoqkq1WqXb7RKG4TgPbL3WJlWcZo+doVCA69evj8dgM5kMpVKJbDbLlStXuHPnDmEYkkql8FWbes8ln9DY39lmGDhsk8ZBxVYjhuFIENMId8cgFRLCI4uDI3QUw8LSFJqD0chjTjiEQkECGpJKlEQCuohQhcTeDcvfjtIcoElCCbGjgNuygC91Eni4u7lkA09gCNgRNpdlieNGA0VRSKfTLCwsMDs7y6uvvsqtW7c4pkqklmczSNINNYz+gJNZ2OOtsjd/lFRqAk+xePXlHfxAw7IskBInlDQDMFRB2K1D6KNrEcVUFjdXIlGYZHl5mXa7zfzcHKX8kEOmxs2VCk67hiYDrlij9w01g4xl8Kv7Cvwnz+8nZX92xUafxnSAlJI7d+7QarV48sknP9Xx6JiYmJivEgMv4N+cXeWV+25SxBE8jxaxIBbzlSRu6YyJiYn56CSTSQ4fPvxA3thgMBjnjZmm+a6TQrffYcF2+a2jBTKZDLOzs6yvr1Ov19F1nZ9uwqWGgqkrJAxBEEqqQ8kQMAyN9iDC9RV+WMntOrwiNEKQCp1A4Ww7QUMt8GSqTU4dtePV63W63S66rpPNZtmTtnh9YySA2UpIJCWmYdANBHYUMiEGbEZZmsMAfJ+O18fOlfn+M/v5zqLBlStXxiNlLSdka7e10iIg8AJUwJQaVZFDT4HbqZPJZGg2m0gpiaKInZ0dTibazM7NcqXi0h16GIpkv97mH37zcbrdLqurq8zOzvKtb32LZDIJwJ49e+h2u6TTaVqtFkIITNPEMAyGwyHN7oAbQZFqmMPrCv5qZ4PjEzoHDYt6ZRtd16nX6wwGAxKJBOl0GiEEnYHDbW2Ba42ASNERTY+JQPDNWVjfWaEfCDQibjPBW2GZABWNiJRwyAkHBw2dkIQaoaoquhLiSI2WtFAQCBlhiIAQBZUIlWi8HRmEDDCIjCSR18JVTKrksBSfvAiohTYRCoYycohJYCUqYCkWz9sOJ06cQNM0hBD0ej08z8O2LM6YPeYO7OHijTuo/oCFiQKbmyHLy8tMTk5y9OhR/vbXTvDDK1u4nToJTdDqeTheSDkB7DrOEokEiaRFpd7ij3/0E8pGwPz8PEtLS1QqFTzPYzGf5/Czv8T09DSaplHruV9IbMUvGvnhui6XL1+mVCrxxBNPxCOSMTExMZ+Af3N2lR9e2aKQND+wcTjm4SUWxGK+ksQtnTExMTEfn/vzxqIoolKpcOXKFaSU/GxH5aXVARMpEzPo0+x3ueImyW9F2MYqyWSSXC7H/v372W71ef21u+hRQOR10QOBExpIGdHAQISgKBFKFNJDRyIQSCI0QBAi8CO4EhWpdhIsDnscNepojNon74ljiqIwp09xJ8iiiQDhO7TcCBedRaXNqVSXS/2ITS+JvxvYnm4vsfqTV/n/CjnOBgNwpIWPMm6thFGqlikiQlVns9lkIZtibW2NKIqYmJhgMBjQarXYNz9HobVKOSUJsjYJJcQSPrXtTf7jpUvk83l+67d+a5wbBlAoFKjVavzWb/0Wf/zHfzxukZRy5ETrlE6ysuFgRj4pFaJA5edrAVua5JSlIaVE07RxUHsikSCZTPJGP8v19SGFpEExORIel50s0UaT0zajHK8w5LSyhaoq3PGzJBWfhBIxCLXx2KNwe4TCYlNMEEiVEAVkiAD60iREYOOjibeTOTxUdCFRvP4oZ021cCNIEBJIgYOOQoQSyVEjqCIxlIiqyONQ4/r16zz22GPcuXOHIAjI5/NIKbEsi5lCmlZWx/NsyuUy/X6ffD5PFEW89dZblOwkM6HLdTckVFPYJiR9HzEcItXRyKxlmgxDsA2FmUKapBqxvLxMIpHgxIkTzM3NYZoPnh88Slm01WqVpaUlTpw48a6Q/5iYmJiYj0e16/LK7TqF5Nt/B+59ffV2nd8+PfvI/H34KhMLYjFfWeKWzpiYmJhfHEVRmJqaYmpqis1Gl9dfP49wh9SaG/QHAwTg4fCTKw3C2xsk1AghBEIIGqHFWn+SpPAgCsgpCpKInjAIpMAgQg09XBTuhblLBAHquP2Q3VyrbqiyFKXxA59T5kgEy2az5PN5dF1nTxDx80rEjaZggIlGwKLS5LjZIHB8Thou+8I6w0jFEqN8LAAhRvldQRAQRRGWF6AT4gsVXYzcZgCuVGDYJzRaVNyRCOW6LqlUahyifi/zSw976KGDoqocPnmS8+fPMxwOmZqa4ic/+Qn5fH68fjc3N3Fdl7Nnz7Jnzx5u376Nr1psN3soislbjRBbjbBkiIxGIpMhNRr6BK7oovqDsehxTxzreLAyNEiqEjns0Bh2CKMQQ+psRSmO0EUjQFFGrY/P5YboLUmFLJ6iQzhgQW2woDRpRhYM4K5voRAiUPB2TysVJBohChFDqWEQvp0hpoyy36QExeujEeJIlYE0cHZ/XsDueCsUsil8I02g9snlEqyvr3P37l1s2yYMQ2ZnZwmCgFwuh2VZ4xFXRVGYm5vj2rVrCCHQ+n2ezih8/7GDhFqCreWbvLg25FrXQkVhIp3BV00qzR6LWoutZY9yuczx48fZs2fPI928GEUR169fJwxDzpw584lKMmJiYmJiRtwfwXM/cQTPo0UsiMV8ZYlbOmNiYmI+GVJKWq0Wl2+u0e4PKSd1Ij3L0HGQMkKXAf3IoBcoY0FM0zRSCuiKZBhCSh1lgJVUHy1UqAcqk4ZLwzfQo4gA5cH3RDASySQCSYiKLnx2ZIZAG5BQRvla3W4Xy7JQFIXH7SHHbJ1az8Xt1LFFQMJMIISB53mcOX6U6elpzp07RxiGJJNJarUaqqpy/PhxBoMBN2/eZCrsclcW8UMfkwCpqHjCYK/ewpQerjsK9M/n89xa28bFQAt9VldXEXaGmquihw7HD8zQbrcRQnDkyBF++7d/m7W1NTY3N5mbm2Nubo6rV6/i+z6NRoOjJ0/z57cGvFnv0XfTgGCAoBB5RIpEiFH+mC2h3XdpmSHzmVHDYSaTYWJiAl3Xee3GGqHIklB9Ql8ShiP3myYD+tKgNQzIK6OwfU3T6LUaHAEWqeJKA82MWJN5XvPmRw2UEhrSAgQqEfp9eWMaEQUxYIDJAA2NgAW1MW6YBLBFwJTS5a1gEne30VKMP1kFR0viodJtVHCTNe50A0zTxLZtPM9jfn6eRqPB3r17OX/+PNPT0wBks1mEELzwwgs899xz/Pmf/zm6rrO+vo4QChsbG2iKYMFfQ80tsjwwuFtpYyiSBcvlHzy7j+fPPDUaiazVuHnzJsPhkGw2S7lcHjd+Pgr0+30uX77MwsLCeP3ExMTExHxy4gieLwexIBbzledRGneIiYmJ+aJxXZdqtUq1WsXzvNEY5NwkU0UXRREUUyYzMzO4jsNqpUleUfnVU4v069t0u10URaHjSYpuyN2+QcLUIRjiKxZIjbQWEUaCUI5aCuG9Mo5GI5Q6ISMxRuJLlUCzMfVRE2MYhjSbTQDy+TyGlOw7vIcLF+oEAaiqSjqd5vnnn2dra4tcLsff+3t/jxdffJG1tbVR02UQsLy8PBJYdJNQCvwQOiQAyKhwxOxwkDq+H4wKAyLBqxuSO/0iaCYiCjA0hbAr8MIIVYZsr0c8lqhQKBTodru8+OKL5PN5yuUyzWaTu3fvIoRgbm6O7e1t/vDiDhcbCkHgkVYj+oHAQaOlpCiJLgB+4BPpSfJpg1QYjfK1bBvTNLl79y7D4RCkjqFIPKmSTaXQNZ1Op0PbG61LSwTjJsd7of0ACSUkKRwue0WWwwyW8ElKj640xq4wlVF7pkZIgMBHpyUTGAQUlB5H1SqaiOhKA4u3nXgLSotbokggRyLYSEwL0YioOQI/GPLCniSFQKff9wiCgEQiwW/8xm9Qq9UoFAqoqkq5XB7ngG01e3S9UXN0MWXy7W9/m//hB/+WG0GRf3+xT3co6Lc9ZrUyL6QHPD9jsFHvUspY/N3v/wbLy8v8xV/8BZqmsbi4yMzMDPl8nsFgQKVSYWlpCU3TKJfLlEqlUVD/Q8j6+jqbm5ucOnXqkXa4xcTExDyMxBE8Xw5iQSwmJiYmJibmfbnnAqtUKrRaLQzDoFQqcezYsQfylJ7b33/gpLAbCKSR5JlZi4zh88TXv46RSPHf/NErvFHt0nY9IkWh5oCtWOj47FM7BIrkjp9FSvBR7huRhPvFMZ0AbdcvFgmBLkOE08OTo9wv13UxDANN02i322SzWa5evUqxWGRnZwff94miCE3TyOfzhGHI+fPn6fV6qKpKKpXCsiz6/T4bGxss6wusR5BWXLJyiK8YoFggQ+Znpzhy5Ai1Wo0/fqvDLcfEUFyMaEglsOgFBmnhUVRcQs3krb5FFEV8fyZFIpGg2WwyPz+PrusIIQjDkOvXr3Pt2jWqXZf/2OuhKQqFhI7v+agEOKFPN9IxMTBUEIqG6wv2UeW5p0+ysrJCo9Gg1Wqh6zoHDhygWq0yUa2zJopogYLsdxkGgkA1OWT1SAQhtm1jWRZSStrtNlEUIaVkiM5mmMJkVCqAYOQSG38uo8/J2c150xjlpAkBlTBNJzJBCHxUdEKmlC5H1BoBCpYIyDEkQtCXBg46IQpSKEzpfWaGmwx8lyAI0HWdo0ePsr6+zsrKCvPz85imSRiGZAol/t3FGrc6CgEKF/7gEs/tm+CIVsOZfYI331ghodSR3hAhVHr5g5jH53k61yd9/TqtVou/+qu/4rvf/S5PPfUU7Xabt956izfffBPLsrAsi0wmw+LiIqlUilarxVtvvYXneQ+4xz4oqP5eC+tn6Uq/VwSRSCR4+umn4+D8mJiYmM+ILzKC5/P4e/JVQEgp5Yc/LeYXJd5QY2JiYmIeNd7LBVYul8nlcu97cT30Qn5wdoVX76sef3a3elwTowyjf/HiHW45SVI6BIMugWJQ6TqUZJPHjBoJJSRE4WZU5oo7QTvUUHadYCH3BJhRxL4pIpCShBJgKnDA6nJYbqCqKlEUjVxhhoHvjxxjjUZjHLgfBMG4tRHg4MGDtNtt9u3bx+XLlzEMg0wmQyKRYDAY8NbyOuf1Y3RaLRTp7wa+Q6QaaJrOf3oqwcJkge1Wn//u9TaB75HSJO2hT0WmCaMIBUlBDDCVkAANO5HgPzms8SvffI5ms8nm5ibpdJpms0mtVqPZbJLNZlnrhvzZhkXeAg1Jr98nRCCsDBsDjUiGIEaNjpPakK8bKyQ0KJfL7N27l6tXr+I4DsPhECklyWyeN1o2G0ECH5WErnB6OsHXpwUrd5bGQmIQBEgp6ff7ALRI8Jq3h5TqoyDxQ8lWlMZBI3rAxTcSx2x8ppUumpBUwiQDDEqihyWCcZ7YXq3FXqXGy/7owsEWAZGEZmTTxUQgWEjDHq3LUaMBgUexWKRQKDAcDqlWq0xNTY3dcFvZo/zbl26QUAJsXVCcmWet0mTRHHKnFYCMwO0BcOjgQVzFpN/r81+cyfP8Eye5cuUKS0tLeJ7HwsICtm0zNTVFOp1mbW2NRqMx3q6azSZBEJBKpcjn82iaRqvVotlsouv62D12bxt7ZwtrwlB5bnf/sI1PL9Prnkh3+PBhCoXCp/a6MTExMTHvz+fZOPx5/T35qhA7xD4j4g01JiYmJuZR4aO6wD6ID8pl7Pcd1msdamoePeqih5J2t40EdKnRwiaVTDFdSGHbNkeE4IlhwL+5qzP0JZqqo2sKSHD8CC+UKAgsHfIazKhdniuGDHs2iqKMg9WllOi6ThRF2LaNbdtEUUSn08HzPBzHAeDy5csIIVhfXyebzdJqtTh06BCtVotCoYDVcKlt9fAilaG0iBCoUpISUE5nGeyOZ168sU53mCatR9RCi3pk4EgFdnPQajKFwUj4CZyAy7fWGbYq6Lo+zhTLZrMUi0U0TcP3/d3geZ1hoGKJiDZJepGK21eJgCQBOcUjkUzRHRqsqtMcFdtUq9Wxy63T6YzFQafX4bjW4bBlkynPEvYblIwBK3fapNPpsQsriqJxw6aiKCSFRCXAi1RsJSAU6q4/D0YjrCNZLNr9/wCakARS4O2WIegiQhUSmwBVUdgMU+xT68xqA+74WRQh6EUaXUYjiCkcvGHIdTWB63o8kWxx+vRp9u3bh4PB//iHf4aRnqC9fodKx+HPLl3CVBS00CWIBM2tVfyBz9Wuih9EpIWLAGampxkOhwg1oOcGTMwsIoTgxIkTOI5Do9Gg1+tx4sQJqtUqKysrqKo6DvCvVEbjrvPz8wRBQL1eZ2dnZ9zmmU6n8X2fa9eujQXlf78a8NOlFoWUyUzOpuP4Y0flP3ph3yfZfYHRPnznzh3a7TZPPfUUuq5/4teMiYmJiflofJ4RPP/m7Co/vLJFIfnZ/D35qhELYp8R8YYaExMTE/Mw43kelUrlXS6wQ4cOfaIRq3eeFFarVW7fvs3MvkMMr13DVkcX7xJQhMBWJI5qY2YnyOVskskkyWSSRU2jRpuX1lwIHLKaQiQU2kKwN+Fx1OoSBCFqMEAPHRpVj1KphOM4BFoCaSYRbo/A6xOGIYqijJ1fURTR7XYRQpBOp8dilKZp9Ho9fN/nZz/7GfdM9L5qM4z205cahgjRZUgkBe3IQLb61PUdGm6PUr5MumfQcKEfKqgiAslYOAoRhFHEwEhhEmAYowyvIAjQNG0sQjUaDQaDAVJKVH/AoWyWy21BR7HpRJJIRoS7QfY+Gm4UkvN76EJlR5vidNEibKxz69YtdF0nm83S7XYxDAMpJVEUofoD/OoKtm3j+6MbdY7jjJdBUZSxO0xVVYzAYVrpcTccNWEKQiLYlcHkbhD+ve1mtN5CKQhRCBk52NRduUwRI4dfGxU7V+Zot4KqKqx6KXqYqESkhEtOOBiaRtcfsCXSeGLIy6+9zh9da7Hm2WzsJEm2A755+Gm+fniGP/3BS+B28eSoGCAMQ5KWTRSZKMLH8wMsETI9PU2r3Wat0qQ8OUkxPdpehRA8/vjjnD17lsFgwNmzZ/nmN7/J3r178TyPnZ0dms0miqIwGAw4f/48iUSCffv2sbCwgJSSwWBAvV6nXq/jeR6WZVHve/zsRgURRSiuT6QmKCZHot+rt+v89unZT3Qh5bouly5dolwu8/jjj8cjkjExMTFfUqpdl1du1ykk3z7Xuvf10/h78lUkFsQ+A+INNSYmJibmYeNeJlSlUnlgrOvjuMA+7vstLy/TarVYXFzkwvVbOJ0WqeIEsxMZ5ufn2dnZoR8o2Mkk33quSDRo02q12N7eJgxDjlvglGxutg0CoZFJWDxbMvj6tMA2VNrtNisrKwyHo/fcrNS5FZXZjFKEioEqcsyoXU5ZdXzPYXNzk0qlgm3bpFIpACzLwjRNfN8fZ2dZloXrukgp2djYIAwDpIx2f6/d3w9BFEYIXeHQwYNU15fx3B7zqSSrQx2FEBGFwNuucB+FQCpI12eI5K87BZ6dz/LClMR3BtRqNSqVCoZhIIQYL8OxRJOukeTiIAeM/GYaEbYICCX0pA6+iqvYVDsB/17Jkx/02C+3CEOHIAhQFAXXdceZaYPBgMFggOM4dLtdUqkU/X4f3/dpuxJX6CiBIGPo5HI5Op0Op60eSl9hM0ziyVFW2Gh57sliEO26wSIUPDRCOXrcFCGqGElmmqbR8UejsCYeoQJPJbsUun1e82bIqj4Z26DbgzAISeo63RBcYXCrbXFlrUIxZVJK21Qabf7dqx3+5udn8UObQAgsRSNSNKykRYiOFfjk9T5LroFpp6i32nSGPmoyxx7DYSL5dhOYpmmcPn2a8+fPMxwO+dnPfsY3vvENDMNgz5497NmzhzAMqVarbG9v02q1eOWVV7Btm0OHDlEul0kmk8zPzwMwGAx4/dYGQ1+SN0f5cJ1OZ5TtZtp0A5V6z/uFzwsrlQq3b9/mxIkTpNPpX+g1YmJiYmIeDRp9j4EXMpN7sCglY+lstYaf6O/JV5VYEPsMiDfUmJiYmJiHAc/zqFarVCqVB4K/Dx48+Jm6SKIo4s0332QwGKDrIxdUWpf85plD/ORWg1rfQwkcWk6INGx+5fAkTxx7t3s6iiJ+s9vl+t0NXj1/lZkJgeI12N7wabfb6LpOPp+nWCziOA5/erPHkpMmEAqhquOFEVvk2HJ1/lapSXkiR7/fp9Vq4TjOKCx+OOTgwYNUKhWSySTb29uYpkmxWERPF+ipGcLeEGsrwNIUBpFKEI5aLtPCxUDj7KXrHJ3JIoTguekMr9e6BAgCVYeRWWm3HODBigCJwosrPWq1IU+mOiMn1q4YJqVE0zSmpqYolUoczUzx3/z1Oob06XU61GRyNLopJEOp0goUVDxUBVr1FlVS+FqRY+rOOEcNwPd9arUaiUSCRCJBt9sliiKGwyFuKLkVTbLuj/LFROAxK/uc6LRImib9fp/HEyEn1CGbnsnZnkogTd4WxkAjIEIgAGEmUN0+s9qQXqAwlBqWIun64ESjDDG3XSOdTo9GDbsNTOniRwqe56HsbqMDX6KrIb1uh1tuElN6+L0Bld6oRVRDx0mW+NVjC/zP51aohYAvaao2igz55YU09to1Qm2CllJiq+1QLuT4zv4Jvj2nc/XqVU6cODFeR4lEgqNHj7K0tES32+Wll17ihRdeQFVH4qaqqkxNTTE1NYWUkkajwdraGmfPniUMQw4cOMCRI0fQNG30WvvmKV9ooSiCvKXiui6O47DT6uIHIVfffBX7xGHm5uZQFOUj72NvvfUWUkrOnDkzXraYmJiYmA/mUc74LiQNEoZKx/EfWPaO45MwVCZSxgf8dMx7EQtinwHxhhoTExMT80XwXi6wj5sF9knff2tri1deeYVCocDx48cplUpcvXqVffv28cREGcNc4ZXbddZrXZKmzvdOz75vG5OiKGSzWZ45lWV2IoPneRiGwZWlVfY9Pk/OVpHDDltbW7xx9SbbUZZAURlGGmrgYQgIUNkMklwNTSaFSyKRGAWwb20xHA5xHIdr166xf/9+DMNgYWGBervLixWNy1dqKEaGQV/gSMhKj5wuUWwTd9DDkRq+HyHE6Pe2bZvh+m32ZOcIJQyGQ1YGOqNhSUEA6AKiXdeU4g/RhcLK0ORvP3GctD4SW3Z2dmg0GliWhWEYLC0tEWgbhMMEroCUMWpRbHiCIJKECAwiJAJbOiQVn6GUbIYp9ip1bBE8sE7vfZ2dnR03biaTSX66Jbjjp7BFQMFWaYewJoqonsJjoj4qKyhN8h9XPFajHF0slN331XZbPyNGTZJPFGGqf5uEFSJCj7vJOVaGJr1IwVAiFtQGzxYjOk1JLpdjOByiqhplM2LNM5GehyEUfFRcoTMf1fGDAE8Kkrsqo65pGKZBKZVhszXkjYuXUdQcBBEoEIQhSctkc3ODo6rgmVyfPQcX0FIZMrrk2H6byclJbt0asLKywsLC29thoVBgZmaGRqNBpVLh5Zdf5mtf+9q7hCchBBMTE0xMTHDq1KlxqP0f/MEfkM1mOX78OLOzszy3f2I3OsMkY9lEUkNN6nzvSInHF3SuX7/OK6+8Qi6XY//+/UxOTpJKpd5TuO71ely5coXFxUWmpqY+yS4bExMT85Xhy5DxXUqb9/09GRluOo5Po+/yayemHzmB72Egbpn8jPgXL90ZZ4i9c0ONM8RiYmJiYj4t3s8Fls/nP7csId/3WV9fZ3l5mW63y/PPP0+pVAJgfX2dXq/HkSNHxs+/srTCG1du8M1nn2RxauIjvUe/3+eP/vQvWDMXWOoq45PZ09M20/07iHSJ/981l7XmABmF2LqG4zpEElypUtQ9fjNfIaVJFEWh0+kghMDzPGDkuLnnytrOHuONSoglAuanStzd3KEe2qiqSsnwsFTBjqvS8iBtKORVj1mtx3OlEBEFXHQmuNRSsQ2Nu52RYDVyhglUJIqqokiYN/vkUwk2WgO+kdyhbIYkEgmEEFiWxdzcHJqmcf36dYrFIj+843CpqWEJH0OBJglaniBEYBJgEZAVQyxN4IeSAQYv2Ft8+4kjbG5u0mq1sG0bwzBotVrk83lUVaVer9N2JS/7CyiALkdZYgKBmsyhGybfzVbo17c5P8xzx8uQMBQq7mgkMkBBQaLu/pYGIc8Za0yoDhY+UkqeeOIJXrtwbTfbrY8hXYBRoUCgcc2boCmSCFWn60mklCRUiY5PmTZH1Bo+Ci/7C+iaRtZUsGwL3w/oBwKpGiiKwBv0kN4Qqeg8dvwIO402G5ub/Gq+TlqHf/yP//F4RHZ9fZ2dnR0ymQztdpuDBw9SLBYf2O6uX7+OrussLy9jmibPP//8R3JjSSnZ3NzkypUr9Pt9shMlLvZSXK36DP3ogRbWexdi90Z0b9y4geM42LZNJpMZC26pVIqNjQ22trY4efIktm2/630fZedDTExMzGfJl+X6/INavR8VYe9hIhbEPiPiDTUmJiYm5rPg/Vxg5XL5c3GBvXM5VlZW8DwPXdfxfZ/Tp0+PG+7a7TY3b97kqaeeGotzUkpeeuklLMvi6aef/kjvs7y8TK1W42+2FX56u81sMUvG0tmut1ivtfnesUmemVL4b19rs9LyyCUMDF2n02kTSEEkYTpj8n/8+jTTtuTKlSsAbG9v43keUkrCMEQIQdeHl/2RUyghAgzTQCDw1AQ1J6KcMtjuhfQDSOBRUAboiTQuOvv1DqesOgMvYN1a5GLFZ9szQIxytXw5Gi5UibBUwZTSxUNF13V+baLJwmSBdrtNEAQMBgM8z6NQKKCqKmfOnOHq9Vv8+HaPu0ODoRehEpCWLqsyS4BGMmGhqyqa30MJHHRN5VeyVaTTRdd10uk01WoV0zTRdZ3hcMjMzAyKorDRi/hRNc1kysAZ9hkOBwgEoRT0MfhOvkVCjfiLahaQJJSARmjRlRbsDoOmcOljYhBiKQE6IVNKl1PJLglDo1gs0ul0xuOq3aHHzbDEdS9PHwONiKTwsJUAT1hMKR0e06uYjMLp+/0+t5Q5bg5T2EpAQhc4kWAQqkypfZoiQ1K4aAJS2TzZiRK3l27RDwS/PjVgX8Hi7//9v/+u7avVarGyssKNGzd48skn2bt371j0klJy/vx5pqenuXLlColEgmefffZjjSgOBgNu3rzJ9vY2Q6kR6Un2z5U5unfufffZwWDA6uoq9Xod27YRQnDnzh1s2+bw4cMUi0Uymcx4v/oyOB9iYmJiPiuqXZd/8oeXUBTxwM2CWs9FRpJ//ncee+RuItR67rtavWM+PrEg9hnzRWyo8d3BmJiYmC8X91xg1WoV13W/EBfYPYIgGLtUMplROP7q6ipCCI4cOTJeHs/zeOONN3jyyScxjLejAqrVKm+++SbPPvss2Wz2A9+r2+1y7do1pqensXJl/ukfXabdbrF/pkS9XqfX6xEZSSIp+X/8vSf51z+/w7+9UMU2dFKWRm8wRAqNXEIjqwb8l1+fRjodTp48STKZ5E//9E85cOAAFy5cYGJiAtd1ubHT5S+2bMxggLivFVGoOn1p8OyEw/l+ntAfonk9hAAhFMz0BIqm8oKxitep4/s+Djob6SNsBkn8fotuoNINNQSQVX1mC0lqXZcnywrfmh691+bmJsViEdu26Xa7nDlzhhdffJFOp0Ov1yMIAoZSw5EaSU2yY81zrp1kGIKlCYQALxLYmsI3JkO+VnRpNBo4jjNunFxbWxsLYoZhoCgKoZ7gR40JNE1lppCm3+sxHA7phwqKqvG3Z/qsVVu85MyQEh5EIZGErpKkF6j4KBgEgEJWDLBEiIeKK3X2mx2ezQ2YnZ3l1q1bo3wwReG6nOaGk6EvDcTuWGmIQkbxSCgBKAq/WWpj4Y9C6HWdoRdw1SuwHaWRqkHKNigpQ45PJfmLWwNs20TqNk1H4ocRnudjKxH/+UnBYjHNd77znffd3nq9Hj/72c/I5XJkMhkWFhZIp9OEYci5c+c4ePAgb775JslkkmeeeeZj53YFQcDq6ipbW1vj9R4EAcVikampKZLJ5Lt+Jooibt26xYULF5ibm+PQoUPjJtJOZ5Q7l8/n+eEdh58utSikHm3nQ0xMTMxnwY3tLv/Vn11lJmdjaG9nNXpBxFZryD/7/nEOT8XFJF9F4gyxz5h31s9/lsR3B2NiYmK+HNxzX1WrVRqNBpqmUS6XOXLkCJZlfSHL1Ol0WFlZYTAYCRtPP/00URRx4cIFZmZmmJ2dfWD5L126xNGjRx8QwwBu3bpFLpf7QDHsngjQ6/U4deoUlmVxY7vLwAuZymdotdu02m10XaOQtun4Kq1hwDfKASv7i5xdbtAejEb+LFVi6RrfPTlLt3YXXdcZDAak02kymQy6rhOGIaVSiYsXL/Lrv/TLvPJn13GdkTDU7fXotNsMAgl4DFp1+p5GRg2JAFUZDQq6/RaumkBOpTCMLmEYMp1NMSm2mcru46JvEIkIISIQYCoKnW6PY+mQA3TIZA5Qq9X4B//gH4xFw83NTX7/939/vE40TSOZTJIKw9ENN9/gVqBR0j06SAahjlBVNCEwRMR3j5Yxgx7lcpkrV66gaRqDwYBMJkOz2SSbzTIYDLAsi16vw75khqtdhebAJ/R8hlLDExp7lRbDVgMtDNFkyDASpDQFwoic7KMpOp5UQUo0BUxGjZI2o+yydS9BpVPHU2pUXBVbMVCEypprY2mCQSDRpESIUbjaINIopTR8YeIJh6DbJJPJYFkWe1IprNVVPMWh5QmqSpmWkuVHKy6ulqDaF0TSI6ELAt8nEiqRovHGjssLp+c+cBtPpVI8//zz3Lp1iz179rCyskK/32d6epoTJ05w+fJlzpw5w9mzZ3nttdc+tiimaRr79u1j7969bG9vs7a2RiKRwDAM7ty5Q7/fJ5fLMTU1Nd4/7j3+O7/zO3iex+rqKu12m6mpKfLT8zT6HrVul5durqKEAXLgoJj5uN08JiYm5j7ijO+Y9yMWxL5E/Juzq+O56JmcTcfxx4F78d3BmJiYmIcb3/fHWWD3XGClUon9+/d/5Oa5T5swDNna2mJjY4NkMsni4iLp9OgOarfb5cqVKxw9epRcLvfAz928eZNyufyux1utFq1Wi+eff/5937PRaHDjxg0WFxc5fPjw+PF7J7Ndz8dpNSmXSpiWxUatTRRJlq5e4Fe+8RxPP53gv/vrW/zJxU3aA5+OE2CbIaur6/z2rz9FKZ9laWmJtbU1JiYm2NjYYP/+/ayurmKaJtGgzeEcvLwGlc6Q6eIEPR+cvsdBs8tCzuRKFfqBJCFGjYSDwYBBICAc0q1tkjEEqVSKVCqFlJJ85TxnpILIZCmlLUrFItutASlNooUu7bbDuXPnyGazvPTSSwRBgOM4wEgIi6KIVCrFt7/9bf76pVe40DNYDWwcdDqhQV4PyYkuaTFENxKYpkk/gFZvwKQ1EiGPHTuGZVmcPXsWz/PQNA3P85idnWV9fR2AQ2oVJTvJ8kDghRqGBqcyksOKhzuQJJSQSdFmlQkGYYAm/V0XmIYmQ5okUCJJB5OE8MgpDglN0PIVLrhFOtsWPll0QnKKSyB0LOmioBIi0JGoQhJIhWGooOFiKwFPfe1rBEHA7du3KRaLNJtNzpw5w3/7Fxe4M1SxlTbZhMFUocD5tQ4yCnH9AFVAXvWYSJrc6algffjd/1wux+zsLGtra5w4cWK8D1y7dg0hBBcuXODZZ5/l1Vdf5dy5czz99NMf2ykmhGB6eprp6WlarRbLy8tEUcSBAwdQVZXt7W0uX75MvV5n3759nDp1ClUdjdYePXqUnuPx3//kKi8v3cKLBKZpUh3A6fkitqaMXZpxu3lMTEzMiDiMPub9iEcmvyR8GeeiY2JiYr7MSCnpdDpUKpWxC+xeFtgX5QK7R6/XY2VlhW63y8zMDDMzM2ja2/fQdnZ2uHv3LqdPn35XBtLOzg47Ozs89thj73rd1157jTAM31MQC4KA69evE0URR48eHeeQ3aPf7/N//7M3eG3TYzKbQAlc1ESaRt/liaLCd+dVoigik8nw11uCH71VJ2WqRE4XP5QEms33T8+NbxANBgNef/111tfX+eVf/mX+8i//kqmpqVEemp3kDy9WWPdtshNlus0aBzNQ7t7E1lXe7Oe46STJmIKMqdMeenQ9WFSbPGbWyGazHDx4kPX1daamprhz5w6ZTIZKpYIQgkKhQC6XGwX4b2+PBLXBgDActScahoGvWgSqxbeff5qt5ZsMh8NRg2E0xaY2Ra+2jW1prA1NQgQzKY2S4aFrOpXOEM8P+JVclVJ6tC1pmoZpmhiGMR43VVV1LL4pikKpVMLITHBtvUG32+HU3imypkK9XsfzPDqdDpqV4GZYYsW1cAKJTkgkBV1p4KKh7uaJhaikhYOtBHQiE01KbMXDICQQGv1IJ0ChaEUMQo1WoGJqCmEY4YcRSeFz2Orwf/27X+PgwYOoqsoPfvADms0mjz32GJduLvNXrRLddhuDkcAnNYsVx0DIiAJ9cgkDGfoYpk11EPB3FiXH5/LMzs4yNzdHOp1+35HjGzduYJomi4uLD+wXFy9eZGdnhyNHjrC6ukoul/uFRLF34jgOd+/epdVqkUgk6Pf77N27l16vR71exzAMpqamEHaGf/nzVV65U2cya5FQYa3W5kbVoZw2eWZ/CbEroMfngDExMTFvE2d8x7wXsUPsS0Kj7zHwQmZyDzYOxXcHY2JiYh4e3ukCy2QylMvlL9QFdo8oitjZ2WFtbQ3TNFlYWHiXw0tKydLSEsPhkKeffvpdy9zv91leXubMmTPvev1+v0+1WuWFF1541/cqlQq3b99+z5Y/3/e5efMmjuPwv/veaeau1Hj1dp2dTp8Jzedb+3I8nXd4+qknALizWeWnP7mMKSOKVoqOJymkEvia/cD4WCKR4IUXXuDP/uzPeP3114GRSFYoFLh79y7fnNJIlzJoSYudFZeULunoObrdLgfYIjIm2QpTuFoCy1YpRpsc0WoIIRgMBnS7XcrlMkEQkEwmmZ2dRVEUer3eeF3Yto2UkuFwOBJ0pCRA5aIzQTcxTa/n8cbPtihFPnujCkdOnOIvL/QhqmHioUVQMA3a0qQTqcynU/Qdl9DU2Jt2OHLoCH63wfr6+rhN0zRNstksx48f5+rVq4RhiJSSgRfycs1gqyLoDtPYeobO6qg5s9p1aLsRCcXksUMHmG42WatuUu0H3A0LLMmJXRkMPFQMAhRCutLEDxVCFBLCwRajEUpbh1wiyWbXZyghawnUQKEvdbwgwFZ89qtNnpmQvP7665RKJaSZZqDn6AdtarUa1Y5Duz8kl9DxfIGPQuR5aMLAC0HTgMhHVRQGgSRtGfzy1x7DkKM8vlu3bhGGIel0momJifEY770x2kOHDnH+/HmSySSlUmk3n1Vy+NRTFDfu0ul0SCaT3L59m8FgwDe+8Y0HROOPi2VZHDp0iKtXr1Kv19F1nW63y8LCAgcOHKDR6fP/+evrvLh0meV2gK4qKEJyaDLD4T1lemGDrfaAt1Z3mEgaSM2k40Wx8yEmJiZmF9tQ+Ucv7OO3T8/GYfTEueP3iAWxLwnxXHRMTEzMw8f7ucC+yCywdzIcDllZWaHVajE5Ocnjjz/+LncWjBxcly5dolAocODAgXc5a+59//Tp0+8p7t24cYNsNvuAyOa6LteuXcOyLM6cOfOAy0ZKycrKCtvb2xw4cGAslP2jFzL89ulZNmptqmvLJLUOT55+u8UyUG00K8l0xiTwHISi0G63KZQs6k74wA0iRVGYnJwcL8vW1tZ4JO9edpmmBTSlS7XaZnZ2dtwAeSbbo+N1idQOhnAp7bGoVhVyuRxCCNrtNk8//TS3bt2i3+/T6/VoNpsMBgNSqRSapuE4DlEUoes6mqaRy+U4206y6tuY/R5JHZxBwE1SqMk9TDsRmpUkKVwGvdFIZU5x8LyQjqfz1noXPwJNU7njRvzrGxFHCwVmEm1On94/bjusVqvU63UURcHzvFFmWzTJ3Z6FJQYkZEAQGtzwUqysuiBsAlR0EXHrzR0Oq1U0EbEjy2zKDBKBQUCIwGMkTim7LjEPiY9CKG38SCUrRsst/QFJTWfKcKh5KrrQyYuAPVqbf/jCflTXolKpUG/3+Kf/6q9oW1NUmz5qVOatG10WdY+kbbLZ8/ExkCgYukbguERAKFRCQE/kGA48npuyyNka/b6LbdtMT0+Pig8ch/X1ddbX10mlUti2jaIoqKpKIpHglXNvcEdMc7nivZ3Pum+Ck7bK4cOHOXDgAD/72c/4gz/4A5544gnm5+d/obbXXq/HlStX2Lt3LydPnkRKSbVa5eLFi5imyUtVnVc3HZLpDOawi0LEamOAUBSOTGU4Np3BCyPspEUvimAw5Hhe8M0ZlTAMP7GDLSYmJuaL4LMQbT7PjO+HkTh3/EFiQexLQjwXHRMTE/Nw8DC7wO4hpaRSqbC2toaqqiwsLHD48OH3HR8bDAZcunTpAWHqna93+fJlDh48iG3b7/q+67qsr6/z9a9/ffz8jY0N1tfXOXr06LsC9u85xmZnZ3nmmWfetVyjk9kyf37lLPuPH39AwHs7ayykmEqRTKXY3tqi0uriBwHDVgV/whr/zMTEBJubm0xNTdHv97l06RKqqlKtVjl69ChBELC8VUNPFZBmikajweTkJI7jYDHE7e6QzGQA2LNnD4lEgkwmw/r6Oi+//DK6ro8zuxKJBEEQ4Ps+QRAQBAGGYZDJZEa/o5Vhq5kgY4XkrSSdbhdLhCQTSdZ9wZF2C7ev4kQRedsik8kQBAFapHEglWEmY3BuuULOALwhg16PnzYk+3SFwL1MGIYEQYCqqvi+D4ycgUOpsRmmMPAxCUCAEbm0ogRt0hTpkxQerlRZoYAmNPapDTa9LCohKhERCoYiEVGARCCIiBBkcOhgIRF0pYWiqOjhkKGMsAx4oehSrVSIjBSK12NyIokYNmm222iaxrJS4FovxOxV0QIXX2jcVfKUJycJN/u0A4OUZSBCD9cP8KVCSnhoiqDtgU+Px/Iqv/vsAnv27HnXdut5Hv1+n06nw9bWFjs7O3ieh23bpFIpXq2qvLRyl4mUSdpU6bkKf/RGl8HREqG3xIkTJ/jud7/L+fPnWVlZodlsomkae/bsoVgsvuc+df8F3kTSYH19na2tLU6fPj0WyoUQlMtlyuUyy1t1fvo3F1BkRDabwtAUBCo6UOk4LE4k6fshB0op/i/fO4KUMJEyyFkqm5ubvP7666RSKebn58cZgDExMTEPM4+6aPMwu6/i3PEHiQWxLxG/e2YBGDUKbbWGJAyVXzsxPX48JiYmJubT5/1cYIcPH35PceiLxHVdVldXqdVqlMtlTp48+aFulnq9zs2bNzl16hSJROI9n3P37l0ymcy7xLJ7J4SVtduk02ny+TyDwYBr166Ry+U4c+bMAyJhr9fj+vXrpFIpnn766Q8cQdvc3GRhYYFqtcri4uJYeHivG0SuYjL0JN9/fD/ljM2FCxfQdZ35+XkKhQI7Ozv0ej3m5+epVqvjr33X57//yVU2gnl8R/DjnTp7E/PMJTQajQau65LP50mn0+i6Ti6X48aNG8zOzo6Xx3EcTNNkc3OTXq8HQLlcRlVVVldXSSaTY6HiJ6+/RbuvkNEC+qGCbVkYpkmEoDGQ9ESSXFSlqhbpBQF+q80wgF4gOOjVuVxLkhACPQwJZMhEysaOVJqeztyBPfTr25imie/79Ho9Go0GAJ5i4qOSxBuv31AKPDQEoIsIRUgsIQmiiGU3SVvXqUWJXSeYIEJBRAEKES4qESopPDKqRxgpdKUFSHqRhi51vAD2Rjt0qw30KETxfIrFIr7v02w2aTab+KrF+R0NDdB2ly1nqQxlwFIvRSg8slqIH3gIVQPfJaWETFgKz5jbqKpGxhSo/oB2vUS7PQXwgEglhEDXdSYmJpiYmODEiRMANJtN3lre4EqlQc4eiXiGlkGLIjwv4D9cXqO0L+LmzT9kcXGRZDJJtVrFsiyOHTvG1tYWS0tLFItF9uzZg2VZ77rAs3XBguXxO6fKPP300w8s173PqN/vc3O9wdCPyJuSbqtBOZVlvTVEVxWCMGKnPcQNR+ORhyYfFLzm5+eZn5+n3W6zvLyM4zjMzs4yPT390Aj0MTExMe/kURVtHnYhr9p1eeV2nULybZfcV72VOBbEvkTEc9ExMTExnw++71Or1ahUKjiO81C6wO4hpaRer7O6uoqUkvn5+fcceXwvVlZWqNfrHyhONRoNms0mjz/++Pix+08I+25At1njV08vMnXrNt1mnePHj5NMJsfP9zyPmzdv4vs+x44de1/h7R7D4ZDV1VXOnDnDzZs3qdVqlEql8fffeYPINEyOFkL+wbOL2IbKzMwMg8GAtbU16vU6Ozs76Lo+HpULgoCTJ0/yL392k6v9BAkNVOnjhHB9mMJdqvN4YhRCH4YhlUqFY8eO0e/3mZiYGDcU3rlzh/au00kIgWEYhGFIGIa4rsuRI0fY3t5mdXWVKIqYzGVI9lQcL0SGHp7nYQQhdZmkE5n8+7UhmsgwXUghhYJiWJR0lVm9j95aY70lKRgS1/NGeWX9Pn4Q0Y00riztkJV9crkcpmmSz+dpt9vouo6q6OhuhCt0bF0l8hxCRtlfKhFCRjSlxUAahAh8VFqehUAiiNARuAg8VAQqAklCiSjujkfmdr/2pYEvVUJFoyw6LKhNfN9HVVUKhQIHDx6k2WySSqXwPI+abyBVAxsXQzVwPY8gDDFUwWatOdqe0xqOO0BoBn40QFUV+uEoJmLfhEWtVsOwbZaXl1leXubEiRMPCNXv7Ja6//+Hmg26RcEEGY6EboQg74dsNAfsPz5Plj7Xr19HCIHv+1y5coW33nqLqakpstks9Xqdra0tLMvi53WTF+/2KKRMirbCZq3JOQwymSGWtkS/38d13bFIl0wmSSaTHJyfpnzdQVUViimTIIwQQrDWHBBGEk1V+KWjkx94AzSbzfLYY48RBAEbGxucPXuWTCbDwsLCA/tiTExMzBfN/aJNRgddeXREm4ddyItzx99NLIh9Cfmqz0XHxMTEfNpIKel2u2MXmKqqFItFDh069NC5wO7heR7r6+vs7OwwMTHBsWPHPnJuWRRFXLlyBcuyePzxx99XPHMchxs3brzL3XL/CWFa8RkoCn9+aRPXLfJffv/t50ZRxN27d6lUKhw6dIhCofChyyal5NKlS5w8eRJFUdi/fz9vvPHGA+Np73WDaOnKeSz9bbEykUhw+PBhpJT80R/9EY1Gg6WNCu3IwvYkO/2QN9sWg1ClFUKEjoJEDSUDkaOgucwMA/TQQdd1bt26xfz8PI7j4Ps+Fy5cIJ1Oo6oqiqJgGAbFYhEHAy2ZZa6YI21At9ulWCySzWbpdDqcUXV+vj7EdTpooc/OQKEvI7JaSEo6BIrOeq3NgYTLt2cmKGdtbAHXPQWzI+l5EbaAKAwxDINA1SiaJguTCiltJILt7OzQ7/eRUpJMJvE8F01V2ApTCDdCSB2NCInAFCF9TLrSQiVEABIIUdAJCFFRCTEJCRAkVElRdpCJHJpMIN0uAsgLBy0M6WEihKQhk7zimUwpXV4ojNyB586dI5lMks/n0TSNvaUi+YEAaaF4fTRVRQCZiUn61RphEKEl0pRSCRr1Oqam4Btp5NDFMAyef/55Xn/9dba2tvjOd76Dbdu88sorLCws8NRTT31oCH6p6/IHS+9u8O64ASlLZ99siWJqjnw+z+bmJt/4xjfY3Nzk5s2bo58vlahUKlSrVdZrHf5kK4mMQhw1IplMkkunGUTw+lqX75+c5uj8PIZhvOf+9vyB7gOux2LaJJQRz+6b4D/7+v6PfN6naRoLCwtj19jS0hKu67Jnzx4mJycfOkE/Jibmq0ej79EdeiRw6esq2VxudOx/yEWbR8F9FeeOv5tYEIuJiYmJiXkP7neBDYfDsQts3759D+1Fo5SSZrPJ6uoqQRAwNzfHs88++5HcYPdwXZcLFy6wsLDA1NTU+z4viiIuXrzIyZMnHxAWHjghTBrc3LxLIZVGTea40Yyo9z0mkgaVSoU7d+6wZ8+e98wJez+WlpaYmZkZu1p0XadUKrG1tcXMzMwDz73/BlE9n6fZbL5LdBNCoJo2b/QEy0ODzsBBbfn4t3boBjaSkAgVkAQIIsCVJn8zmKXgDDmzkOVvHS9y4Y2zbG5usmfPHlRVpdlsMjExgaqqpNNphn7In93sURF5ArrMTVk8u7fA808/w/lzryGlpNfrMed0OGSlWfJ1+ugMPBWbIblwgKIKMqbAQme177O9cguZHWWWTSQNvnV0lhdXeqjSxxAhrlRptfscok2n0sQslZiamuLAgQOcP3+ebDaLpmn8dBN8NUEi9HCkIEQhQCUpfFQp6QkLhQh2RyNVJBqjrLAELi46IYIAlSD06AgTtx8QSIW8omOJEEcqdLGRSNTIwyDEFyorYQGt1uHbMxlM02Q4HI7HOhWvz3P7FnjxbheUEClC+pGK0x7y9GyKSmWH5fYQLRyiCYVaYDLwFQw0/qaVI3pji6dyBSYmJnj11Vc5evQov/M7v8Mbb7zBn/zJn/DYY499oKuzlDZ5bt8Ef3l5kzAMSerKKJ+15/GNvWla22tUvJGTb3V1lbfeegvTNGk2m3S7XS5cuEAqlcIwDByp4ktBSpWk0ikmChN4nocpItpeQMeT7yuGwXvHYvzWqdlfeAxHCEEulyOXy+H7Puvr65w9e5ZcLsf8/PyHujRjYmJiPgtc12Vn9TaRN0RJp8jn3nawPuyizaPgvopzx99NLIjF/MI8zGGBMTExMR+XR9EFdo8gCFhfX2d7e5tcLsehQ4d+oQvadrvNtWvXOHHixIeGb1+7do35+XlSqdQDj99/QhhFEaqiomkapippOAGrO02WG2tkMpkPzQl7J61Wi263y4EDBx54fHFxkbNnzzI1NfW+4sb09DTr6+vv6UL72UbEmw2FhC5JqREdP6ISJQiRSBTEWBIbjREqSEIJEYKXVwc0m0s8kzNptVpsbm6yf/9+dF2nWq2yZ88estksP3h9k2s9HZsupXyKTrvF759tsLOQ5rBpjsfXVlZWcG/eZNoMqYssr4VpUnKIQBJKlWEgSKcN3Ehg5QST5RSpVIp6vU6mcolp32bFsXACSTGb4ulJlefLaWYm92KaJhsbG1y5coXp6elR/p0nCSYWmGi1yMouQz9EaDoBGqahM2FqnK+DEApCSPJaxDDSCAJBhCCteORwqEZJQhQywsUSAQYRTWnTlSahCEBGaCIijYMtAgBUAqSAzTDJ8tZdMoZA0zRs2+bZZ58lDEOeLxSpVF9jKdRoDFRMVbDPaPON2QLKnln++EqNmy1JNTBwUckaIVOmpOMp/Oh6k/5imv/0+TmSySTNZpMf//jHPPXUU0xMTHDhwgUuXrzI7Ows6XQa3/fHZQP3OKxK1gsR16o9Kn6ELiKOZCIWgzavv34LKSVCCCzLQlEU8vk8TzzxBIPBgJ2dHSqVCt1ul5SuMztZIpNOUUy/7dSsdYakPJ9hq8LZsytjJ2GxWHzguPNZxmLous7evXtZXFyk2Wxy8+bNsaA+OTn5sQT1mJiYmA/jva4hgyDg9u3btNttHjt0iO95GX54ZQu15z4yos2j4r6Kc8cfRMh3hifExHwID3tYYExMTMxHJQiCcSPk/S6wQqHw0LrA7qfdbrOysoLjOMzNzX2gIPRhbG5usr6+zunTpzGMDz5pW19fp9vtcvTo0Xd9r9p1+T/9uwuEUYSNRyaVwNB1VnYadHs9/jcnbb779Wc/dttdEAScO3eOJ5988j2X715G2sLCe5/QSSl57bXX3uVGu7XV5Hf/3y/SCyREEUJKDBHRkQYRCqOTJIlAIhGAQCfAUGBad1BVHRTBr0+0SGoRvV6PdDqNbdusrq6iKArFub38wZpFv9fFwseyLErlMtXOED8I+PVii7Df4tChQ+Tz+bGQ9jevnef31wykBFdqDIUBQiWSEluE/N3JOgVzJMhEUYSu65w+fZpKZ8iNuxsMGhWK6VFe2MzMDGEYoqoqN27cwPd9BoMBhb0n+JeXupSTOt1OE8u0Rg6tSNJwJC/ku/y8mcYLAmzpMjtV5lZ1QM1VUUREWbZxIkFNJkngUVYH43XrYoCq8fWJAUEQ8lIrjRkOUcXbp54Rgp40eFZbZULz0HV95NpTVRx09h19DD1ymZud48L1JbZXlrBFgJ4uML14kOXrl7FzJf5oVcdQBarXw7EKtNwIL5SoQuE7CzrfmRMkTB3HcajVapw+fZp8Ps/a2hrNZpNEIsG+ffuwbZtGo0Gr1aLdbjMYDAjDkJ4PgZZgupBiYbJAoTBynt0/hhxFEefOnSOXy3H79m2Wl5fRNI19+/bxS7/0S/zLV1bGo8TvvMC7ly3jui61Wo1arYbjOCQSibFAdn+b6mfN/SPXhUKB+fn5h/7GQExMzMPNe11DPru3wNenBc3aDvv27aNcLiOEYOiF/ODsCq/e/9xH4HrzX7x050OP8w8LtZ4b544TC2IPHY+C6+pR2tFjYmK+WnzYMfSdLjBFGQWjl0qlR2ZEKAxDNjc32dzcJJVKsbCw8C6X1sdBSsmNGzcIw5Bjx459qBuk0+lw48YNnnrqqXc9997J7v90bo2t9hCViPlimqQS0ui7/OZjM/ydEwXW19dJp9MsLi5+5PV+z8nzzibL+3+PV199lTNnzqCq732yfO3aNebm5shkMgRBwObmJv/sD9/k51UNU4lQCAlCiSN1At5/PeiEWIpkVh8gAFdL8L99psRsUnD79m1arRa6rtPv9wFIzBzgf1qKsMMhqgK+56GqKqpp4gib3zuRJGquc/jwYSzLQtM0yuUyP/zhD3m5ZvByzcCTKhohiiKIFAODkOeKLr95IMG3vvUtms0mf/VXf0UqlWLv3r0YhsHly5epVCpkMhkOHDiApmlcunQJIQSDwYAnn3ySm6vb/OG6hee5GKFDPp9HVVVqXYf8RJH/298+wQ9eusmb1YhOdZOUoaImMiw3XKTvkjHB83y6oUpJ9NGU0WllKAWeVHHR+JqxiiUCXvIW0DQFIxwFx0spGciRQ/Br+srYOYZm0ps6xRvrXbwATE2wN+HxeHpAtVbjLS9PVcnjhgJVBuQVl7pIM5XSWW25DJQEhipGeV0hzOYT/C+eXuSYus2xY8e4e/cu58+fJ5/PY5om3W6XZrOJpmnj9tHp6WkKhcK4hODDji2DwYClpSVu3LjB1tYWBw4c4LnnnqNardLpdJBScvTEKf7tG+sf+QJPSslgMKBWq1Gv1/F9n2w2S7FYHH9OnzVSShqNBqurq4RhyPz8PKVSKXaNxcTEfGweuIY0NSrtHjvNHr9ytMT/+TefeM/jyscVbb7oa+lHVcj7KhMLYg8Jj4rrqtp1+Sd/+O6A2VrPRUaSf/53HntohbyYmJgvLx90DNUV+Ui7wO7R7XZZWVmh3+8zMzPDzMzMJ74g9n2fixcvMjk5yZ49ez7S819//fX3dWndO9nN2gbr1RaVfoAbRExlTH732X387jNv/01rNpvcvXuXKIpYXFykUCi870X21tYWrVbrPR1p73xev99/10jlPer1+ti143keenqCf/LHV9lxdYQMkIGHRNCNdKKxICbhAXFsNESZ1kL2Wg6+amHbCX73kOD0kf1MTU3x0ksvIYTg2p11al2HdDrFT2opZBSStzUiKYnCEF+xQFH4vaMacthhY2MDKSWqqtLtdkkmk/Sx+MNamX4wEic0BbIGZC0N0zL55dQ2Qa/J1772Ncrl8thZlMlkuHLlCq7r8thjj9FsNtna2qJer+O6LqZpYlkWnufxWivBLSdNUovIJUx8odHzBX/7qUX+4dcWub50h/PdJH/40mXMZIZ+u8H+VMQBs8cwiGi3WrzqTAESk4AOCfqRRoCCQHJErXBSq3AjLLES5THwsUSE1CwGkcKC2uQImyiKMip0CMqshAUSaogpIlypMIxUDtl9PM/jblTAxMNWwJUKvVAjFCpJXFrSRghQiUaDroqgoLokLItfLzaZzqfIZDIYhsHa2hqTk5M8//zzGIbB9evX8X1/vB0ePnwYNON9jy2mJtja2mJpaYlKpUIYhszNzXHgwAHu3r3LU089haqqXL9+HcdxCIKAJ554guYw+IXuyksp6XQ61Go1Go0GAPl8flzK8FmLVK7rsra2RrVapVgssmfPno9c1BETE/PV5p3XkN3dGwWeMJCIT3wN+bBdS8fuq0eHOEPsIeFhr2i9x6MQFhgTE/PV44FjaNai0RvyR+eWWVtd5fuHUpRKJQ4ePPjIuMDuEUURW1tbbGxsYFkWCwsLZLPZT+W1+/0+ly5d4siRI+Tz+Q99vpSSixcvcvTo0fcUw+4P059I6HR2ukzkbAbSQtcUHi8E7GyuoSgKiqKgqiozMzP4vs+dO3e4ePEi09PTTE9Po+v6+Dmu67K8vMwzzzzzocs4NTXFa6+9xsLCwgPjZcPhkPX1darVKrVaje985zuYpskf/Ief44cRKcWnj4Gh6/hBSOQCSFSi3V7Ft1EQmJqCpqtEtkkQCk7PJpnMMhYLDh07wf/zT89xs1Ng6IWIrs9kOctWa0DLccEbolhJWgOHk1mfYTPEdV1c1yWfz2MYBk899RSbm5u49gTZbpv9RYtaowGhTz6RpD/sUhu6DBMaE5kMr7zyyridMZPJ0Gg0qPVcFDvL3IGjsPQW58+fR0rJzMwMzWaT4XBIFEU8lXOhLdgMUvSlQcoyOJUe8lTO4Y033qDdbnOyXKaZ3EJPuUzPp9i+e4tCqsDTTz9Np9Nh9c/f5HaQpyMSONFICBOARcBmlEMPI46oVSSS7ShNT+rovs+RTMSZgoIMynQ6HXqBwraXwRQ+hgxA0UklEsihx7KTIIpMLMXHJEBVNBIyREpJO1LooxMKFV0G+FIQAuX/P3t/Gh3Xmad5Yr+7xr4HENh3ECABAlwAUqL2zMpKKSurKrOq3F1V2e7xuNtzxjMenzm2Z7o982GO/cHunjOe8Qfbx57TM2NXV1bXkpVVlZVKZSlTqZ0iQRIkAIIgAGLfEfseceMu/gAhJIiUREmkRKbu7xwdAWDEvTdu3HjjfZ/7/J+/S6A14CdeNHj6my9iprc4c+YMcCDwvvvuu7z11ls8++yznDx5kng8zp07d2hra2Nubo5XVqpM7ptEfM76/OynN7bY3dll3J+nVCohiiLd3d2cOHGiPsbIsszU1BRnzpxhcHCQ2dlZRFFkcnKSM2fOfK65kiAIBAIBAoEAvb29mKZJOp1md3eX+fl5ZFkmEokQjUbxeDwPXCBzOBz09fXR29tLIpFgdnYWgI6OjiNdXm1sbGw+ykfXkD6/HwBNNx/IGvJRW0t/uKmPzaONLYg9AjwOLVoPeVzCAm1sbL4+fHgM9coW6WQch6oS9TnZrKl0DZ58ZMbQ+6VUKrG2tkY2m6W5uZkzZ858pvD5T+Oww+Pp06fv2+GxsLBAY2MjwWDwnv/+0cluwO8nFApRrRns5qvUJCdutxPTNDFNE13X6z+Hw2H8fj+JRIKFhQVcLhfhcBhZlllaWqKpqYlr167d13GWSiV+/vOf09TURKFQIJvN1hskBINB4vE4r7/+OoZhkMtVcCku/LJMUHKQKOkYAvC+t8kpGFQtGUkS0A0DBImwAwygpBkUyianGyT+8GwbIb+HYrHI+vo6/+3fTTCZsAg6JUIOgXxFYHUvRUA9yCDLmzKOSpUTXoMTahaPp5kLFy7wxhtvUKlUaG1tJZFI0NnZSWNHH381fxlTEAl5VMplk2KxhCG58DoVTg2209kU5sqVK5RKpYPX63CzaDbyxq5GRTd4+f/1KsHqPsfEKrqgsLu8R0vER3sseJDVVanw3QaJjv7j5GoWtVySreVtbs3sU6lU0DSNZDJJyCXT2uTD4XDg6e3F7/dz6dIlRMWJKCvUdJm8eSBEyhj4qRCWDrpQ7hNgQMoxLOzTa6WoWDJOQcdV0dnb/uD9y5tOaoi4qZE2nZRNFcEUwXQgigIyFfxSDdM4yJXz+3xYlSq6YdHml7iVk6jWLBTBpC3gpNFpgOLCKRdJ724QcIiUy2VcLheKovDcc88xMzPDL37xC86fP09zczPBYJBbt25RQWU+XUHUyyi6hVbWMIpFrGqNSysm/QMiJ/r66O/vv+tzFI1GKRaLzM/PMzg4yNDQEDMzM7hcrroo9kUdnqIoEolEiEQiwIHAl0wmWV1dpVgs4nA4iEajNDQ04HA8uDFQEIR6qXmlUmFjY4M7d+7Q0NBAe3v7A92XjY3NrwcPcw35OK2lbR49bEHsEeBxcl3ZrVptbGweNQ7H0Ca/g1w6SSwWA0HA84DuOn5ZWJbF3t4em5ubyLJMZ2cng4ODD9R1YVkWy8vL5PN5xsfH73tBvre3R7VaPSgh+xiOTHY9KoIgoKgqWa1KwO2kry32qe/DwMBAPbNobW2Nvb09BgYGGBoauq/zcFhS9vrrr1OpVOjo6KCxsZFiscjW1hb7+/v13KhqtUpPT5ib+WXmywpRl0zUI7GVKlKqgCiCLCtomsVBuISIKkK3HzRkcqUyv9euEZZrrNyZ5/b7Dq9s1WQ+oyDpZWp5DUMUUUUBt6BgmBL/9Q+e5uLF9zh/6gRLszcIBLrJ5/O8+eabOJ1OMpkMHo8HTdNQVZV3fvkzGi2J5bKfoMtPgy/IfiZPqWpxNgDVXIJLqws4HI66g+fvbmeZK2oEnRLHWiPc2dhmX/exJbmRFZWKbuHPOWgtFXip34XDcXDO9zeWKBQKGIaBaZq0tLTg8/nY2dmhVquRy+VYWFjA6z3obClJEm63m7+7nWOTKB6ximbIiCLoJggCKIqEZEFN8WJIbjyWiWyauPTKPd9Dp6CjYJC2XFRQUAQQMdAQqVkiWCIaEg7xwBlWLBapCioYOr1yHjUQ4HZGIOAQaA+o6JJKMl/hyY4Aic0Vmk+eZG1tjcHBQeBA3BkZGSESiXDp0iX6+/sZGhpidHSUi7OrxDObNLplsrkcAgcilFuGkuxi6Mxphtvv7lp6SGdnJ7Ozs2xubtLW1sbJkye5ceMGXq/3gYliH0ZRFJqammhqagKoNxCYm5ujWq3i9XqJRqNEIpEHJrA7nU76+/vp6+sjHo8zMzODJEl0dHR8Yhm0jY3N14uHuYZ8nNbSNo8etiD2CPC4ua7sVq02NjaPEodj6HY8RWskcLAK59EdQz9KpVJhfX2dZPJAzBsZGfnULo+fB8MwmJmZwe/3Mzo6et8L1WKxyOrqKuPj45/4uI9OdmuGRaJQ/cyTXUEQiEQi9XJJURSZmJigtbWVlpaWe+a+HXbE29/fx+fzMTY2xs7ODrquMzs7SyAQoL+/H7/fj2mavPnmmzQ0NOD1evlml4Nhbw/vLSeJp3NE/W5aG5zc3ExiiQqCrmFYIIkCURfITg+GoPBMTOWF0QYymQxPPfUUpmmSSCS4eGsVcTtNRAERB9FIlFw+TzqTJVO1+Kuf/AxJr5JMxojFYuzs7FAqlahUKgQCAZ599lkuX75MW1sba2tryLLMv/dUD/+wXGRiNUNBduB2uWkS9xjxQK12UBIcDAYJBoPMr+2wbVh4JA2rXGR9LYsK5EQf27qTZrFKULWoVCosSB6shRznAgcimK7rNDc3EwgE2Nraorm5mWQySTKZxOv10t7eTi6Xo1KpkEql2N7eJlHQ2NI7kawSqlUji4JggUOSKJsq1ZpGDRHBKCIoBQRVwOfz0dDQQGdnJzdu3CCVSmEYBoqioAARvcSe6UPCBMtAM0UMLFxCFRPIGgpuLBxolC2VtOFAFgzeSssosohHAVl1EC/qNIQ9/N65Fp5tFtnfsbhx4wZtbW3sZUtkykY936W1tZXf+I3f4J133iGRSNDW1kYuvo9TFkjkSvhVcDideL1eNBQ8CDSFPJ96PZ84cYJr167hdrsJh8OMjo5y/fp1/H7/QxHFPozT6aStrY22tra6eJhIJJiamkLXdYLBYD2g/4vmKQqCQGNjI42NjZTLZdbX11lYWCAWi9HW1vZQxjQbG5vHi4e1hnzc1tL3w1fdHODrhC2IPQI8bq4rlyrxz5/u4XunWu2wQBsbm6+cBp+D0SYnP0tk8OsCgm4+0mMoHDiZ4vE4GxsbCIJAR0cH/f39D81NUalUuHHjRr2l+f1yKKKNjo7e14L5w5PdZNmkyWN9rsmuruvMzc3Vw/sNw2Bzc5OJiQnC4TCdnZ2oqko8HmdzcxPTNGlra2NkZITd3d26G6ylpYVjx44dOa+CIJBKpegaPMnE9ByNoSjfPd9Nv5rBUtsZ7u/Eo8r8tz+ZYCZhsBXXqFrg93rQixksBF4abmZAjCPLMrlcjpmZGYrFIg0NDZwdGiC8ME0iXmCwqw2X200wFKJkbqOlS1wtu6lUNeZnKrSrZZpLcQSjRrVapVAosL29jWVZ9A6dwlDcbN6Zo7+nE5e6SYu2heiSSG6vIYklUvvgc3Xi9/uZn58/uK40hVLNSdTroVqVEY0amqZR1QUkEVTBxNR1Ak4nhZrOalnldNjJQE8LnZ2d6LpOrVajUCiwsLCAoih4PB4kSULTNAqFAtVqtR7IXxNVdEScZhVRsHALGkXBjSQIWAKUBQeCpHLCV8FfE1BVFUVRiMfjhEIhfu/3fo9KpcKvfvUrtra2ME2TTqXAshFBR6TMQYMDAYuCJWEhoho6eVyA6/2MNxHDgqwuYOgCuiUQtCzOtYj8i98Zpj1yIFwFfW52E2l+eG2H7M2rWJJaD17+4/EOdF2nsbGR+fl5VldXaWtrYziqcnkbTEXGxKRkiOS02n2PLYIgcOrUKa5cucLo6Chut5vTp09z7do1gsEgk5OTdBwbOiLOPQwEQag7+7q6ug5y17JZ4vE4S0tLCIJAOBwmGo3i9/u/0Fjkcrnqbs+9vT2mp6eRZZmOjg5CodBd27YXfjY2Xw8e1hrycVtLfxKPWnOArwN2l8lHBLtFq42Njc3nwzAM3r54mQWzkYnV9CM9hn60S1tHR8dDz9tJp9Pcvn2bkZERPJ5Pd7Qcchii39bWRjQa/Uz7TBSqvHHpGs8/cfZzTUSnp6dpaWm5a7+WZbGxscH09DTVarUe8J1Kpdjd3UVRFFpbW4lGo5RKJRYXFzl9+vSRbSwsr/HDy6ssZgXi6RzNDWF6vDr/9Mkeejrb6o+rVqu8c2UKy+FlY3GWp55+mr9/9XWePX+Gwa5W3nnnHWq1GpVKhZdeegmfz1df2P93f/Ur7hRVulsb6hPzW9tZioUiDaqGS7JAcVMyRJ7qcOHfuVZ3xW3s7LFgNLJedWIIEhIGxwLwbItIIZPC7XZTq9VQFIXd3V0sy8LpdBKLxajVamznNP7dVoCKJSEAgnXQqbFgyigiNIl5PE4FQRApV2uUBQdPOXc43RMjEAhQLpfJZDLvl5T2oCgK+/v7eDwempubuXbtGj09PaTTaTKZDI0dvfxwEWpalYHOFqZmbpI2VDTFj6brNEhlznf4eKFDZW5mikgkQiaTwbKsumNvcHAQwzAoFovcuXOHzUSWd7QOsqZKFfX9jpEiVSQsDvLJLAQMREQsnNTQkdCRUAULpyJgWiJ+xeC3hmL8737rVP1z9v/45S3+4uICXsXiRG832bLGTrrA2QaB3x8Oo+s61WqV/f194vE4x4dHmatFuLySolCpoVdLjLX7+d9+5wxux/3fVy6VSkxNTTE+Po4syxiGwbuXr3JxX+DKWhbF5cXjkL+yhY9hGAeNGBIJcrkciqIQiURoaGh4IM1ISqUS6+vrZDKZumusZgn2ws/GxuaB8Ouylj7s1h32OO4S9h6lRnu/TtiC2COG3aLVxsbG5rMxNzdHOBwmFos9kmPoYSbW+vo6hmHQ0dFBQ0PDl5Kts7Gxwd7eHqdOnfrMmUErKyuYpklvb+/n2vfExATj4+Of+XXu7OyQTqc5ceJE/W+6rrO9vc3Ozg4ul4uWlhay2exB8HmlwrFjx+7Z/XJ6evpIZ05d1/k//flbzOZUBK1Y7/ZYReF3znTcNdm8efMmzc3NvPrqq3R1dbGysgLAyMgI1WoVj8fDxsYGTz33Qn1hnymUSCfi9HR1YFhQrZlIosBuroLTKBB2Suzu7SJLMmUOhKnfDMbxqwLVapXbVgs3kgJuySDid1M1RXKaRa+SZViJ4/f7qVar1Go13G43hmFgGAZutxvLsnhrT+Zq3kvNkpDQERDeF4tE/EKVbt+BgKbVauiSk3Klym83Zgm65LpI1draSjab5cKFC7hcLiYmJtB1nXK5zPb2NuVyGVEUee655+jr6+N/eHeFv3pvkY6mCCoGc0urFA2J891hfrNdJL27gc/nQ5Ikent7mZqaolqtIkkSiqJQq9WIRqMMDAzQ0dHBT37yE15eNXgnoSJYJjIGZVRMBCxAfL+DpQlYiCjoWIhYgAD4FNAtaHaLuFWJ320pEnLJOAJR/seb1QO3WWKHcDiM0+mkaIjUdJ3/6GyAkFuhWq3S2dmJaZpMTk7S2tpKW99x0iWdsEehmk2wubnJ0NAQPp/vvq/tdDrN0tISZ8+eRRAE/vs37/CjiSUafE5ks4bq8ZMq1x6Jhc9hI4V4PF5vQnCYP/ZFRHzTNOsuzleWq1yLmzT6Xfhdqr3ws7Gx+cI8ivPA+yWer/IvfzyNKApHjj1RqGKZFv/q90ceu9f0OGCXTD4AHqTV227RamNjY3P/ZLNZKpXKQZA+j9YYWqvV2NzcZHd3l3A4zODgIC6X69Of+ACwLItbt24hSVJ98f1ZSKVSpNPpu9xVn4VDF8xnEeIqlQpra2ucO3euLiRubGygaRrNzc10dnayu7vL8vIysViMb3/724iiyObmJteuXSMajdbLKQH6+/uZnZ2lc+AkqaLGysoyq2UHEZ+DUjJD1TJobwiS14V7dqJqb2/n8uXLWJbF3NwcPT09pFIphoeHWV9fJ5/PYxjGB+3eXQpWMU3Q5yVdrvFsfwMvDjWTKWn8319bxGEYVIp5XE4XqsOBX1HZSBZp7j6GWkqwmymxnffQGpUpp/epFAu4XC40TLYNL8O+CsFgkGq1Ws8e0zQNy7IOwvAVD8vFMAFKaIJMyVIRZQWHKKIYBk5FQfZ68LodJHJFyrpAj1fHpx6U03k8Ho4fP47H4+HGjRssLCyQyWS4c+cOxWKRvr4+fD4f1WqVsbEx+vv7gYNS2Y31dbZqFinNJBAI0lrcoiW1jr9vjJwkEY/HuXDhAqqqMjw8zM7ODk6nk3Q6TblcZmNjg+3tbV566SVOnjxJkhWuZyqYgkpFq3F491Z6/6dDYczAQkdEfP93CwFTkFAkaIsFqRgSF154mma3xWvX5sgWs7itCl6fD5/Ph6ZpuGWB/apAslRjuL/riGDt9/t57733yOUuc/78eVwuJ/gOXJM3b94kFArR09NzX5+xUChEU1MTt27dorGjj0vLSaJeB0Yxgz8UolbJE3b5uLSU5Jm+KCB8ZYs6VVVpbm6mubkZgHK5TCKR4NatW2iahs/nIxqN1rvB3i+iKNLS0oLii7A8cx23WEaqlVB9TrsrnI2NzRfmUZoHflbs5gBfDbYg9gWwa3xtbGxsvjoORYovIto8aA5zedbW1tA0jba2Ns6fP/+FA6s/C5qm1YPDW1paPvPzK5UK8/Pzn8vd9WEURUHTtPteLFuWxczMDL29vSwtLZFIJAiFQjQ2NtYD3A9dRB8VFru6uujs7CQejzM9PY2iKHR3dyM73fzDao3561epmFDMFyigctrnpmCZqKpKPp9HVFTSmkAiXyXolNjd3WV3dxdBEHA4HDzzzDO88cYbeL1e1tfXgYOcpFQqRbps8N5ekrBbpZKJo0oCimxS1cq8ObNKr7lNTTco5TQSpSI++UDQ0apVsvkyDklmZ2WBkEtGl7ykCyXCTgGH00GpVMKyLGRZoWRIVFHqzQAURaGxsZFYLMbq6upBN8HtDLog4RF1oi4olPMIkgMsk4Il0uUUEJQga4kcTllkOGDwdJMbt8NPY2Mjoihy/fp1AoFA3RHmdruRZZne3l52d3dRVZVyuYzf7yefz+P1enGpEr/V56bnxAjJgsbO6gJ6QWJmJsvVq1dpbm6mVCpRk5wsrMcZ6GolFKpw6tQpVldX2djYYH19nVKpxE9/+lMaGxs5OzhM18YKyVSajoiHhaRGjYMyUDhwgkmSjGmYB78JAoZlIiJgWBCQTARRxi2JhD0KpWKGajaBjIEpO8A6yG2TZZlMWSfkc/HCk2N3LTZ8Ph8vvPACly9f5s0332RsbIxoNIrT6eTs2bNsbGwwMTHB8PDwfZUkt7S0sLm5yZVfvsF+WqAx4KZQc6EZFqZhoOUybFZk/qufzCIIwiMzt3S5XLS3t9Pe3l4XYOPxOOvr65imWQ/oDwaD9zXepYoaVQNaGsKo8gePtxd+NjY2X1d+HZsDPA7YgtgXoH5H2OOgJegiV6nVw/xsq7eNjY3Nw2V5eZnW1taHnsF1P+i6ztbWFjs7O/j9fvr6+j5TXteDIpfLMTs7y4kTJ+plgp8F0zSZmpri5MmTn7nE8qMclsLd736vXr1KOp1GlmVCoRChUIhMJgNAZ2fnp5amfbjLXaFQYGVlhR/NJLm2b+EUanhVCcXvZXO3wM3tDFHLoqv7IFx8K5lDKxZ497WfM60eiBadnZ3Iskw+n+f27dsYhlEv0XzvvfcwDIOdnR3WEiW2CxJOo4SIiepQD4QMWSBVEZG9IY43ejmV3uSN5RrucJByLkWhYiC6XDzZ6qTfI9HU1ES0rZuJn8xRLZc51t3K3t4eLpeLmuSkqtXoCEu4XC50Xcfn87G/v4+qqmiaRktLC3MrW8hWmFBjC+GAh914inwmgSEoNAZ9jLm30EpxUjWNqNNJCJlYdJjR0VHy+TzXr1+nXC4Tj8cRRZGXXnqJlZUVGtp7WNtN4g9EUYwKTz75JHBQkpvP55EkiWQyyTG9zLFYACPlIqWXOXnyJDMzM9xZXWdNbuPNny8d3P1eF+jxGnT3l+nv76ejowNFUdjb26NYLLK+vs7GxgZtcju7lkQum8WvuIjXBAQEHIKBCdSMg4ms8L5rrIaIKFoEZQOpViZRqPJct4/piXfJZDKEfT7GO71c2zNBMqjUapTLBlVkvtHX8LECjKIoPPXUU8zOznL58mWOHTtGX19fvSFGNBpldnaWaDRKV1fXPYXkXC5XP18NDQ3sl0xKZpkb20UELBxlnajbQTxTIKvV6Ix4aPA5H8m5pSAcdAr1+Xz09PRgmiaZTIZEIsGdO3cQRZFwOFwXb+91PuyFn42Njc1Rfp2aAzxO2ILY5ySer/LeUpKw5wNbpm31trGxsflyKBaLpFIpxsbGvtLjyOVyrK2tUSqVaG1tZXx8HEn6alwcOzs7bGxscObMmc8tEt66dYuOjg68Xu8XPh5VVT9VEMtms2xsbJBIJCgWi/T29pLJZCgUCrS2tjIwMPC5XGper5emrmNsXLtBwFnBKJbJl2uEQiH8sslWqojoAnFrm1xZI6/BuWaF0yf6EEWRYrHIrVu3AAgEAhiGQX9/Pzdv3qSrqwtVVRkdHeXSpUvsZZfwOGQs00NvayPpTIZwOEyqVKPRbTF2chCzlOXFPi+yLLNckNjTJVRVZCQC3x0M4Pc0kcvlSG6t8mRPmJ/N7LK4vktrLMLyxjbOQJSXTjbTJR6Ifnt7ewiCQLFYxOl0IssyFy9exCOJDEQUpnM6S7kCtRrUzAASJk/GnIw0d7Ozs8O3TpxgfX2d5eVlXnvtNS5evEg0GmV4eJjx8XFmZmbY3Nzkz/7yr9E7z/HWfJJiFRTB4DtnBxlrCKOIFj09BwKNruu89dZb7OzsMD8/z9bWFrVajb6+PhobG5m4U2Kj6sZBmoagF0mSeG+ziPfSKv/piyM4HA76+/tRFIXt7W1yuRz5fJ6oNcfJUA+rJRdaRcclSmimhWmBIosIuoYgqTiEGi0+hXyxhENVUR0OqsUK4eIarq0q6XCQ/v5+YrEYvvUtJLHEcsGJZgqEVYnRJicjrhw3btygu7v7nmKyIAgMDw8TCoWYnp4mnU5z5swZZFnG7XYzNjbG2toaV65cYXh4GLfbTbVarXc89fl8tLW11bf9qy3Il+bRdPA4FSwTVlJlarpIxGFSTm6hBvsei7nloQAWDoeBg+vhMDexUCigqirRaJRoNFp3d9oLPxsbG5u7+XC37p1MGbcqfa5u3Tb3jy2IfU7sGl8bGxubrwbLspidnWVoaOihBdN/UjbkoTNoa2sLj8dDV1fXZwrWftBYlsXi4mI91+nzlmdubm4iSVI9M+iL8nEOMU3T2NraqrufJElib2+P3t5eIpHI5xbBPkqqqFGuWbREgkgRP/Pz8zidTo43O5lcTWGKEiVTpSEa5HvHGu9ZkmZZFplMhrW1NZaXlw9yugyDjY0N2tvb2dzcJOSSOdcQ5K2VHOmKQbVmkCp9sLCX9TILq6ucO3sWv+c2e9kS5YE2Vm5N0dEYJuB1c/z4cXK5HDdv3mTEk6c2GOXthT3W4lkEUWI4UGNAjLOzs0NXVxdnzpwhnU4zPT2NYRjMzc0hSRLnzp0jvWZQTVUwBYlarYYgCkiCxMrKCpHhg+eFQiG2t7cZGxujVCpRLpepVqtcuXKF9957D8uykCSJVamVq5NruDBwSia+YANvreZRHQ6eilRpb28HDvLiAoEA/f39SJLEpUuXWNzY5dZWhsXdHHGiOMUaqqFRyufoam9Dqzq4vJLmzsYe5cw+q6urVKtVYrEY7e3tzM3NoWkaztQaLbJMLeDE43Lyyp6bvZoTzbCQBIF2V40fnG4gv32H9oEgqztJUiWNSJMDq5Ij5I8hSRLb29sAjA4f58lz7nsGLxeLRVZWVpifn6ezs5PGxsa7rsXW1lZ8Ph9Xrlzh7bffZmxsDJ/PhyAIdHV1EYlEePfdd7Esi3A4TFtbG+fOnUMUReL5KvO7eSzL4p3FfbrDTnbTBUq6QNW0MA0L07SIquBxe+Awx+wxm1vKslx3a8JBx9ZEIsHCwgLlchmPx0M0GuUfnT4o6bYXfjY2NjYHuFSJf/50D9871frYNgd43LAFsc+JbfW2sbGx+WrY3NwkEok8lJLET8qGNGsHge/5fJ7m5mbOnj37hcsKPysfFep0XWdqaopoNMqxY8c+93ZzuRw7OzsP1HF3mCEGB8JSPB5nc3OTWq2G0+lEkiR0XSebzfLcc8/R1NT0wPYNR7+ngw6RSCTC6uoqlsNLR8jFf/GdE/j8gU+cbAqCUC/f1HWdUqlEMpkkmUwiCAKqqpLL5RgPVQmHu3j3TpxMFRo9Fi8NN/O94Shzc7OMjY0hSRKpVIqQ18tQSwvFrUUKhUL9Ovb7/YyPjzM9Pc1oJUH/sIPZO2v09DYiG/tcOP/7XLsm1HPhLMtiZ2eH69evoygKL7zwAnvZMklB5lijiFbMkS2UaIk1kCtrJCoBfvTTf6C3tZFkMkkoFKqXKgaDQaampujv72dlZYWbN29S0AVmd3RUs4ZkVTENAbOUxZCqvHqjgNpw0NDikM3NTarVKvlSlZ8vFbiVsKhRoFqJoKsemuQKNaOKKIosLS/h8viJZ0pMTN+i1SsSjUYJBAKcOHEC0zRZX1+npaUFj8fD9vY2mpZluqhgig4apRJYForDSdmQyMkBarkkadnCquQIChYRr4+u4TGy2Swul4tisUg8Hqe5uRm3233P4GWPx8Pw8DC1Wq0ugjY3N9PW1nbks+73+3nmmWe4evUqFy9erOeHTS+sspct0tvahUcyyefzBINBKrrJDy+t8Nb8LtliGV03SFUtjkedDDb5KGs6hiAhygo3d8t4Qj6aox+4NB/3uaXD4aC1tZXW1lYsy6JUKpFIJNiZv8VJpcbQSQ+4fPS0NBALuL/qw7WxsbH5ynmcmwM8btiC2OfEtnrb2NjYfPkcliCdP3/+oWz/rmzIssbfTa6zsb7B7w0F6ezsJBgMPpR9fxL3EurG2v0cV5KcPDFAJBL53Nuu1WrMzs5y5syZB+q4UxSFZDLJ7du369lglmXVRabm5ua6uPSgxTAAvwoDQYFfLiTJuWSiAT9yqUY8V+b5Fomz/a2f6fX29PRw+/ZtEokEHR0dnD9/njt37rC5uYkiWoyoCdp7DEylgbGTA/hVgcnJSU6fPo2iKGQyGTKZDKOjoywuLgIHbqO1tTXa29spl8vs7OwcuLqEA/Hp2dF+dnd3kSSJdDqNKIqYpkkul+MXv/hFXZg7efIko6OjXFncIjG1SHvEx1pqj4jPhUuVUWSZLQOCkXZKpTSLi4sEg0GuXr1az4AaHBzk4sWLhEIhnn/+eVbSVf7+F0tE/S7y2TSSeOA406oamugmWzXqYeqBQABRFBkaGuK/+9kUyzUJn7eCVswiOx3s6RJZyUfIoaMbOmiQLqWQFZXRwV6Od7eRSCQolUoAlEolDMPA7/fjcrlQVZW1vRTbO15kq4yKBgJ4FAVdUvn7iXmeFA0cxSIejweHw0FLSwuZTAbDMHjmmWewLIuFhQUmJiYIBAKcOnXqY52diqLQ19dHT08POzs7XLt2Db/fT1dXV73kT1VVTp8+zcWLF3n5H37BohVjnyCaKeHezvBkb4QX+zp44403eH0bricg6JKJuiSSRZN81WA1W2OsO0rkfbEtUajS4DMpVA0Sheqv5dxSEAQ8Hg8ej4fOzk4syyKXy5FIJFhfmGXNsgiFQjQ0NBAIBO7rM/ogu73b2NjY2Hy9sAWxL4Bd42tjY2Pz5XLr1i2OHz/+UEolP5wNGXRKFPJZTE3DrypsaC5aewcJfkWLrY8KdYlckb+5ugJj3Tz/BcQwy7KYmpri+PHjD6w5ga7r7OzssLy8TDqdpqmpCUEQiEQitLS04HQ6gYNulisrK5w7d+6B7BcOxL3DDpGSJPGPzrQQCoW4vJIiXTGQZIUzjVXaKisYhnHfDr9cLsfi4iKWZREIBPB4PAduM8vC6/Vy/vx5JEni8uXLuN1w5+Z1tra2GB4exrKsgyD+67MIoVa2kjlSqRQej4e+vj7efvttXn/9daLRKE1NTXR3dyOKIslkksuXL6MoCsVikevXryN7Q7zy3hTby/MI1TKWZTEyMkJjYyOSJNHRGIJahfnlJAGnA4uDsr10WcdVqPD02VE27txifX2dsbEx0uk0AwMDpFIpJiYmMAyD3d1d9vb2KFsKYa8H0zTw+/z1rpKYEkalilk+OCeH2VHJZJKFvTy/mMsR8LjwOwV2UmVaYwHKGYO9nIYv6EPWy+QqNUSXnxbiRDxqvczXsg7C8ZeWlg7EN02jp6cHWZZJGU4cORBKaQTroKukXtMxtDwlQ0EIB1CUg5LmcrnM008/jSzLXLlyhddee41vfOMbDA0N0dvby/T0NK+99hodHR2cPHkSRVHu+b6LokhraystLS2k02nm5uawLAuPx0M+n0eWZU6ePMlUJczkzA5BZ46mSIB0vsBfvpdifU3lmY4Qs4k4imkQdrvweDzEGlTK62n2chX2CzWiXrEufP3emVYUSfzazC0FQSAQCBAIBOjt7cU0TdLpNLu7u8zPzyPLMpFIhGg0isfjOTL2293ebWyMSsHpAADXEElEQVRsbGy+KLYg9gWwa3xtbGxsvjz29vZwOp2fq3vi/ZAsVMkWy3iFGtmKiNfrJRgMohnWV5rfc6SJi0cln8/jMDU6Y2GubeRIFKqf+7gWFxdpbGz8wq43y7JIp9NsbGyQyWTqjQVcLhfDw8N3hfRblsXMzAzDw8NfuAmBYRjs7++zs7ODYRg0Nzdz+vTputj1v2qH759pY3krTq2QIurtYGJigjfeeINvfOMbn5i5ZhhGPfdodHQUTdMOwu+TSfL5PC6XC5fLhWEYKIqCKIoMDw8zOTnJM888g2EY3Lh5ix9eWmPX8lOuFZCsXbrdGmeDFVKpFA0NDTgcDkZGRo7sOxKJ8M1vfpO33nqL3USa16/vsVkrYAgyMg6aBI3/+NvPk07sk06nuXLlCoIg0O2pcbUg0dXUTrWQYWUnQcWSeaq/AUkv8c1vfpMf/vCH/PKXv+TMmTO4XC5SqRTHjh2ju7sbTdP4t//23+J0OmkSTW4VVYRqDX9IpiY5MSSFF4+3ckJys76+jizLJDI5frpQYLFksFuRyBo6mZKBw4JkMklQVqgoTlSXBwEPPrVI1Ejx7V4/b7zxBmNjY/XPdbVaZX9/H9M06e3tJZ1Ok81m6WyK4NnKYkoh9GIa0zQxLRPNUlFFC6lWoli08DW0kixWyVZNoorA+Pg4v/rVr5iYmODkyZP4/X7OnTtHPp/n2rVrvPzyywwPD9Pb2/upQrssy+RyOQqFAoqi0NbWhqG4mdwqEnIrGIU0G4U0DlVFMSSubWo0OWqYooJPhlQ6Tej94PkTTX403aRaM+4Svlyq9LWdW4riQXnzoeu1VquRTCZZXV2lWCzicDjqAf1/dnXb7vZuY2NjY/OFsAWxB4Bd42tjY2PzcNF1neXl5QfqJjqkVquxsbHBytoOMiay20fE/0HDlFxF+0rzez7cxOXQQROJRD6XUPfh0iKzdJAB9UWyxyqVChsbG2xvb2OaJpIkEY1GaW1txev1MjU1dc+OlSsrKzQ0NHzuZgSWZZFMJtna2qJSqRCLxRgaGvpYl1vU62A5u83Y6CgrKyv09PSQTqeZmJjg/Pnz9xRC9vf3WVpaoqenh1gsBoDb7UYQBDY3NzFNk+eff57r169jGAa6riNJEjMzM7S3txOLxbAsi3/z9hIregCPbCFrefIVnZs1H5Ik87sdHXR1dfGrX/2K4vvlfh/G4XDwG7/xG/zyf/oHZrJZvIqG32FSqBnseVr4k4srvNitYpomTz75JLOzs5wNVnC7w2xqNfKGgiIY9DuK/G++/RRmrcK1a9d45plnePPNN5mdnaVWq9HT00NbWxuWZXHr1i0aGxtRVZU/GmhguhTgx+/OkCgZSKJIp5XibMBJQ7gDURTxeDz85VSC+ZIHlyygilCp1ihXIaR4cQgaNUuiwWHyncY8La1t+NQA6R2dra0tnn/+eaanp1F8YdyhRraStykUCgQCAWKxGLdu3SKXy2FlszSJcMf04fVHMaoFChWdom4x4C7hkGCyFGRnWcQUfSz89XTdLXTixAmy2Szz8/P1TDCfz8fzzz9PIpHgypUr3L59m/Hx8fp7DQelm5ubmwfZb6EQvb29eDweSqVSPb9tbjvDbsJHQDZwuVyIooiu67RGwqRqEt5YFNZWKOKg0f/BZ6FYM+hr8PKfvziIZXGX8GXPLQ9QFIWmpqZ6WXWlUiGRSHDpxiw/u5ZClSW8koIqi49FR04bGxsbm0cLWxCzsbGxsXnkuX37dr2D3YMil8vVO9u1tbXx4vMX2FJWeOXmDoIoPjL5PR9t4uLz+w+O/zMIdR8tLXJI0OEo8y9+/+nPfDymabK3t1dvMHDo6GhrayMSidTFJcuyMAzjrufncgclg2fPnv1M+7Usi2w2y9bWFvl8nkgkQn9/P273p4dwJxIJ/H4/DoeDbDbL2NgYExMTJJNJZmdnGR4erj+2Wq1y69YtHA4H4+PjR8oqTdOkr6+P+fl5KpUKuq5jmmY9cL9QKNDd3U0sFiOTyfDm5etMrBXxez20hH1sagWi4RCiy89yIsHsnXUkvUStVuOdd97hueeeq5eUHu7v4uRNbsZrNAe9lLNxqqZIY0OMoiGwkJf4Ly88R3xzhcXFRba3t2mNNfBHzz7La+9O4Aq2oRfSUBUoF7I0NDRQKBQYGBggGo2SzWZZWlqqv/5DN5wgCHzjG984yChbX0ds13j6N84Q8aoYxUw9921lZYWyJbNaVnEKJeRaBY/gICMoyIKAJjvI1XRqgsW5JoE2v0wlvkpWlikUCkiSxJvvvkex6RS/vLaBZuZQBIvRJjcv9Ed44/o812YX8CnQ1tbGf/WDp/h///Imkxt58rqM3+/hyQYFdX2JqUIjd6puXKJGU8SFZZl1t9A/e6qbtbU1zp49y9LSEtPT0wwNDdUF3BdffJGNjQ0uXryI1+uls7OTTCaDqqq0tbXR19dHJpNhY2OD/f39es5bKBRiPNrC629sYkgqfr8Hn99PpapxdXmfnAbr8SyFGtQMne2iRWstQ6PXQaZc46XhZo7FvroOtY8jTqeTtrY2inIAx8xNGj0KlvXBOPO4deS0sbGxsflqsQUxGxsbG5tHmlQqhWmaRKPRL7wt0zTZ2dlhc3MTj8dDd3f3EZfSo5gN+SCauHw4g6zZ72B9N85M2c2/u7px36VF2WyW1dVVdnd3AQiHwwwPD9fzqz7KvVxXhmF85gD/QqHA1tYWqVSKYDBIe3s7Pp/vvp9vWRZLS0ucPn267uJyOBz09vayt7fH6uoqHo+Hrq4u1tfX2d3dZXBw8J6luZZlEYlEkCSp3uXRsqx6YwJJkigUCly+fBlBEKhYMt5QhJagG1UWcbvcBPx+BFllV5DxNbRwsiPCwMAAP/nJT3j32jSpUo3WsJ+2hgBLS0sspyo4vH4UrYDpdFF1hFjMmFRrFXRD4P/28hS/26uwu7WOZVn89m//NoqiMNTbQTweZyu9x0svvcTVq1dRFIXGxkaWlpaQJInGxkYUReHVV1/l5MmT7O/vo/qjVJwWmYpBU1MTqqoyOTlJR0DG5XKAN8bW1tZBZllHB7qnAZfPJOp3U8pnEcvlg/fNkClWdVyyxFijyP/sdDMuVWJnZ4d8Po+u6+TzeW5bzcyuL9MYcCOV8+SrFm/suPjV6jZOoYYqRDnfHeaJriYuv/s2zzcF+MMnTpGtmCzMXOMf/+43+ZtXNN6eKuGggmrpeFxOPJKJ5HHU3ULd3d2srq4yODjI3t4eV65cYWRkpC6out1uOjs72dzc5L333qO3t5empibW1ta4fv06pmmiqiqRSISGhgai0SiFQoHbt2/znbO9vLaYoopIJZlmIV4mpYk4RChWdZyqgmFAVTdZTZSoaCb/eLz91zYX7Mvg4EaBTMmAqPcDUfxx78hpY2NjY/PlYgtiNjY2NjaPLKZpMj8/z9jY2BfaTrlcZm1tjUwmQ1NTE2fPnr1noPqjmg35RYS6j2aQJZNJ2htDFHThU0uLNE1jY2ODpaUlNE0jEAhw4sQJmpqaPjaI/JOYm5ujt7f3UwP8K5UK29vbxONx3G43ra2tHDt27HM1U4jH44RCIVRVrf8M0NzczM7ODn19fUxPT3Pnzh36+vo4d+7cJ+5nYWGB8fFx3njjjXqp6OXLl9na2uLpp5+mra0NURS5efMmzz95llf3ZuvuvkPx5WDRLuN7f82uur0sCG38ZDKH4vRgLSSImouci2iEglFK2RRej4eS7CdZMvA4HRhmBVGUuRnXMIppOitb9Pf3Mzs7SyAQwO/3s7y8TGNjI7IsMzIywuuvv86xY8eYmZmhsbERr9dLoVAA4JVXXyPfNMrmeoXdRIFXtmZ4sjfCH4w2EYlEuHHjBkNDQ/j9fk6cOMHU1NRB2Hy5hFuVqFSgsbGRarWKmkxgODxk8iV+r8PCWU0wNblDMBikpaWF/v5+RFHk4uRN/na6iEsxqGYTmKZJ2XKRyZdxqDJhqYIoq7yzXkSrrfMvfvepI8K4nlxncnISd6gRTyBLk2wQ39lGr9XI5XJEGpvqbqFjsUZWV1ep1WrEYjF8Ph9Xr15FVQ/ehEgkQkdHB4FAgLW1NRYWFpicnKSzs7PeuODDJa1ra2skEgnGx8c5ZQqojjXenNthL1+jbEo0eATKmolTlTC1CiGPBwSBtpAbt3KQEWYHv39+7G7vNjY2NjYPAlsQs7GxsbF5ZFlcXKSrq+tziS+HOVNra2sIgkBnZycDAwP3Jao8avk9X0So+3AGGYDH48HpdCLq5j1LiyzLYn9/n/n5+Xo3xN7eXtra2r5QJ8q9vT3gQDS5F7VajZ2dHXZ3d1EUhdbWVrq6uj4x9P7TsCyL5eXlennmYakfHDjYBgcHee211xAEAV3XCQQCn3h9lEqlepi6JEns7+/XQ/VHR0c5ceIEuq5z5coVTp06hcvlOrJoF0yLYqlGvmZxvs2LVz7IhPuziXVm8yrVQoJGWSBTqbKEn55AhKC+wbnOIO9t6+wVdCQRNA10w6LRI+CyqqxWVI4FojzxxBP4fD5yuRyJRIJqtcrCwkK9QUStVmNycpK+vj4KhQLRaBSfz8f8/DzragdTS1l6WlSiLhFRFHjl5g6VSoWw5MLbeoxLN2Y5fbyPhoYGmpqaKBQKiIbBk70R/vbaGoKgIZs6uuQmV9K40OHlP/4nL/DWW28RCoWYnZ2lXC4zMzNDsVgkrslYooKKfpDDZgmUOAjJ1zWNsl4l7IVwQxMp2QXOo+WF586d40//9E8ZOfc00aBFLpuhtbUVy7Iol8vsp3O4HU4iXvWg4UB3d/187O7u4vP5SCaTVKtVUqkUuq7j8XhoaWnh1KlTqKrK5cuXmZyc5Mknn8Tj8WCaJjdv3sTpdNadjpVCgVPOFMNjAVJSJ//jxXU8MkxvZVEFC4/PD6JIqaoTcisUq4Zd0vcAeBQdvTY2NjY2jxe2IGbzlfDhYGd7QmhjY3Mv8vk8hULhM4e+67rOxsYGe3t7hMNhhoaGjuQyPc58HqHuoxlkTteBMPbR0qJCocDCwkK9c2B3dzdnz569K+j9s5CpmNzeyeFXBdbu0RTBMAz29vbY3t4GDlxbZ86cuad77/Owu7tLNBqtC6rZbJaBgQHgwDl2586deglmPB5nenqas2fP3hX2b5om+/v7LCwsMDQ0RCaT4ZlnnuGnP/0puq7T3NyMz+fDsiympqYYGBjA9f55/vCiPVHU8TlFXhpu5oU2hWq1Wnfw+RQQqJHPpulsayNRqPKr2S2e+61BvtXfy3/9yk3+6tomFlCr6fglHblUpKaqpKtQ9Ai8/PLLRCIRVFUlnU7T1NTE0tISuVyOra0t1tfX8Xq9vPvuuzQ3N7O6ukoqlaKxo4/bczUCDoXc/hZOp5Nmt4Jhmvz4+g5OSyCQXMCtSFzcnuMH50r0dR9066zVavzg3HFy2SzXNgvs5KsE3E6ebLD4/kgDN27cwOl0YlkWTU1NZLNZ3G43siwj1gTUzTKlmoUDcPsDWHkBTBMBC4ckEAyGEDDYTaS4OnObC0Pd9ZJZRVEIh8Pk41s82dvBy9NVEoUsQbeKJjhIZ0v85nEPEY/K/v4+m5ubLC4u0tnZSalUYnt7G0VR6h1Cn3322buaQHzzm98kmUzyzjvvABAIBBgcHKSpqYlarcbCwgLVapUTJ07gcrm4tbyBWS2S10Sc6sF2RUmiUjOQJQHNsOySvgfEo+rotbGxsbF5fLAFMZsjPGyh6qPBzm5VqneBsksHbGxsDjnsdjc6OnrfZXL5fJ7V1VXK5TJtbW2cO3fuC7mLfh04HNNH2wK8uRgHjpYWfft4jNT2GpfeX9S3t7fz/PPPf6pT6tM4HOt/PlVAvn0TvVzkhaFWThngFK2DbKutrXr52sjISL107UFhWRarq6uMj48D1PPDNE1jbm4ORVEYHx9HkiSuXLnC8ePHmZmZYWpqirGxsXqJ5c7ODtVqlWq1SmtrK0899RRwICAKgkBOs7i5keLJMy3Mz8/T0NBAOByuH8eHF+3T88u0RgP0tjaysrLC6tY++9UN1rcTBFUTn89Xz2qTFAeqO4QmOrg9O82ImuJaUMHt9aLUirgdCuWyk2RRIxpUOdHrIx/fpqfnIBNO0zSi0Wj9XFcqFU6ePEk0GmVqaore3l4uXbqEz+fDVNwgVzCrBZxeD8VikTt37pARfOwWDDq8Is1+J3nNYCYt8ReTW/wj/UAEevXVV1FEiz8608RYwy6Tszs0h0wiXgd6pUS5XD4Q3RobuXDhArdv36a1tRVJkrh16xZDu3EmdgUQQKrpWKjolkhAqOByyOzv76FLLnz+AG0NQdbX1ykUCgSDQRyBKEUlQDld5DfPuYAWfjmts5cv4hBhrDWAu7TDn/5onUa/i3K5jGEYrK2tcebMGc6fP1/vHJrL5ZiZmWFgYODI+wcH5ZTPPvss7733Xl28LRaL7O/v09/fTzAYZGNjg93dXWKxGC+e6uYf5vbwmTrJgoammxiWSdTroFCt2SV9D5hHzdFrY2NjY/P4IFiHPdxtvtZ8WULVv3lnuR7s/NG8h/sNdraxsfn1Z2VlBVEU6ez85NIX0zTZ3d1lc3MTl8tFV1fXXe6eryN3dZWURRRJQDetA6eKZdAiFzjpztEaa+DEiRM0NjZ+IRHswxyO9YpRxeuQKesWBV1gPCbzG+0iDQ0NtLS01F1UD4OtrS2q1WpdINrf32dpaaleKhkMBuuPLRaL3Lp1i56eHiYmJigWi7S2thKLxWhubsYwDC7dmCVd1nnhyTH8qsC7l6/yD3cKXFyKo7i8xCIhTkRl/ve/c/7I96ZpmuRyObLZLIuLiwAEg0EURSGfz5OvwZ8uWLhdToxiBkmSSCaT5KoWgiTynz3XytNjo+zu7vLTxSJvruQQtRKqYJCvGBR1+KOnj/PPn+lhcnISh8NRF+ba2trqDiZVVXn++edZW1sjl8tx+fJlWltbKRaL4PTz4y0XulZFrJVQVZVCqcKa5sKyLAb8Ft0dByWziUIV3TD5nx8T0IsZEokEXq8XU/Wyny0haAUGu1q5cOFCvdlCPp/n5Zdf5lvf+hYul4tXXnmF7u5uhoeH+fHf/ZSfLWTZrHmQnR5SpRqVmkmDWiMW9FK1RPazJY65ijwbM+jo6MDh8fOrtQo3dkpkixVcishQVOG/+EfPEU/n+PvXLzK1r7FeAGQHMiYjMQf/6XfHaIk1cPXqVUZHR+8qAdZ1nenpaYLBIN3d3fXPw9raGvF4nNHRUTKZDO+88w6pVIqhoSF8Ph+FQoH29naam5sRBIGyZvDDiTXeXUywkihS0HS8DpnuqIen+qL2TUAbGxubrwl2VdSjjy2I2QBfjlAVz1f5lz+eRhSFIwNColDFMi3+1e+P2AOFjY1NPWNofHz8YwWaSqXC2toaqVSKpqYm2tvbH1iZ3a8D9xrT49kyfV6NDiFFLODm/KkhOjo6HriL7sNjPeUc+Xwej8dDyZRwOJz8N//4zEMf6y3L4tKlS5w7d67e+fH111+ns7OTkydPHrmudF1nf3+f6elpBEEgHA6jaRqqqnL27FnKNYN//aN32NBc7KdyNEWCtCpFQtEG3l7KkNhaxeeUcQciGIqb3zgW5rePeclkMpRKJURRxOfzEQwGyefz+P1+YrEYq6urTE5O8u1vf5t/d32Pv7m6ilnOomIgu3wUDZHTEYsnQmVkWcbr9TJyZowfXlrlV7e2KGkGernIHzw7wr93oReXKpFKpQ7cZZLE0NAQc3NzXLt2jaamJnZ3d/nOd75TL51samrCNE2ampqYnZ1lN3CCn93cQdSKCLUKeVNmJWMQVnWiUhlBEFEVBUuQKFgK/+F4hJ6Ii/XtXX61ppEQQ+wmM6iixW+N9fHvP3PsiOhz48aNehOC4eFhEokEfr+fn/zkJweB9ZEm0qUaywtzaI3HWas4yRbKuFSJfr/FMXEP0TKoVqvctlpY1gO4RQPJ0CjWTFKlGt1ShnOBAtcKfpb1IC5Bp7+rDc2S2Exkea7Hz7/83jlSqRQ7OzsMDQ3d89pZWVkhk8kwPDzM7du3cTqdtLa2Mj8/j8PhoLGxkfX1de7cuYOmaVy4cIGenp4j11U8X2U5XsDioGzZsrAXRDY2NjZfE+yqqMcHe/Vgc7QD2fsTtcP/f1oHss/CR4OdD/E7lXsGO9vY2Hz9sCyL2dlZTpw4cZcYZlkWqVSKtbU1ADo6Oj5358FfZz48poddMslkglw+T00XWLO8/B/+yUs0hbyfvqHPyeFY3+RTyRQMmpqacLlcaIb1pY31m5ubtLS0ADA/P0+hUCAUCtXFMF3X2dvbY3d3F9M0aWxs5Pnnn+fGjRuMjIywtLRUdwv9Yl3nWtyiJSITcYnkczmumjK1/STtIScVBXStgk+BfK3EG7erPNvZS29vLy6X68j1Wa1W0XWdqakpVFWltbWVQqFAn7VLrLrFnhhE9UcJuJ281BvhN7tdbG+sEY1GefPNNw8y9RSF3/snT/LDv/4JsU43L53tZD1VIuJViYRC/PKXv+S73/0um5ubWJbFhQsXSCaTNDc3Mzc3x9TUFM3NzbS0tCDLMtvb21iWxflojXynl8urGvFCGQGNoEPCZdbo6uzCsiwSySRlQyIa8NPT0oigFXhnByZ2NILuFKpewh2I8NpiCtWxVr+hls/nSSaT+Hw+mpub2draIp1Oc+XKFaLRKG19x1nejrOzushgeyNeb5X/cHyMsiWztXyb8ZODFAoFrl27Rjxf4fa6ia7vIUoWiqoiGjpOBOJiCGeskfVMBVkAyaxRzGVpbm4GAtzYLnD5xiznRk+wvLxMpVK5K19QEAR6enrY2dnhxz/+MWfOnME0TW7dukUkEiGRSLC/v8/x48c5e/YshUKBt99+m5mZGS5cuIA3GLYXQTY2NjZfc/5sYr1+Y7Il6CJXqdUb7NhVUY8WtiBm86UJVR8Ndj7ko8HONjY2X1+2t7cJBAJHgq11XWdzc5Pd3V1CoRDHjx9/qKV2jzPlcpnbK1vsJtK4qZDHwuFwEAwE8JqwX9R559o0Hf4Pvv5FUURVVVRVRVGUj/3//TrJDsf6zf0kHY0h1PfL0nIV7aGN9R8uSQi7FTY3N+nt7WViYoKuri56e3u5du0am5ub9W6XsViMkydPHskuO3HiBLOzs5w+fZqrV69S0AXevL1DS1MTUY/K8l4Br8dDsaSxk9OJOU062tvRdZ2GhgZC74t+ijeM2+2+6ziLxSIbGxscO3aMUqnE6urqgXsun+HFbgfnn3uKiqUccRIFfR5u3rxJb28vuVyOarXKu7/6Oc0+hSsJgTd/PF0XXkabnHR7/SwuLqIoCidPnmRjY4NcLsf58+f5kz/5E6LRKM8880w98yqRSAAHDQi+/61v8TsjTRQMkYXpa9wsuHhzyWJtL0Vvewu+aDP5VJ6wniCxpXFsZIyEVCUWsBC0AmW9RtAlky6X+PnkEsOeElQLSJLEE088weTkJLlcDkEQKBaLmKLClZyXv7ucIpEpoQgtPO9o5smQSWJrhfHxcfobz3L9+nVGRkb4/ve/z8vvXsfa2kTRSlQNA0VRcDqcSLJFAQf/sFZlT3OgyjKYEmgyrlwev8dLseoiq1ncvn2b3t5eFhcXOXny5F3vUzqdZmVlhZMnTzI5OUljYyMu10H56OnTp490vfV6vbz00kvs7e3x7rvvcjHpYLHioyHgshdBNjY2Nl9DvgyziV2K+eCwBTGbL02oavA5jrSf/2hppv1htrH5eqNpGuvr65w/fx74ICS/VCrZIfn3wLIsCoUCqVSKVCpFuVw+CDAv1XApMl5vwxEnWKJQpclp8fwTR8vTDcOgVqtRq9XQNK3+X6FQqP+tVqthmuaR/UuS9LFCWpta4popkdMs/JL50Mb6e5Uk9AcsRj0l9vf3GR0dJZFI8Pbbb1OpVGhtbWV0dPSIoPFh/H4/Ho+H3d1dRkZG+P/8+U+wpACCXgEOSuVUVcXpF9guJTFkF4GAk3Q6DYLwsaKfZR2IMJenbmEobrypPMN9nSys7zK1uk81W+B3vv0NotHwPY/p+PHj/PjHP+bpp5+msbGRv/qrv2LOaubqnkZ3S4WWkI9cpcZPp7Y436LidaV4/vnnKRQKbG1tUSqV+Iu/+AvK5TIOh4NCoUBHRwfDw8OoqsqlS5eYm5tjYWGBxsZGYrEYnc88gfTmO+QaBDYqDmbXdmiLNfD9sS7+eLyD9ZU7/PjlV9lJ+Gn0qrhDMWq1GtFoFL9usrKXYX5tm+H2CIZh8Itf/IJUKsXp06dpbm5meXmZqYKH20UFxczgkwx84RhvrxUxLTf/wbMdXLp0ia6uLjweD3/913+NLMukSjW0koIoSPR3tKIoCpqmsZspki5qVJwORMFAFEwcLhfxooley9MZU3CrEmdO9JPb32J7e5tqtcr6foayKdUXFWtra6ytrWFZB80fOjo6sCwLj8dDd3f3x45BsViMp775En/xp5eo5VNYihfV0/BQHPc2NjY2Nkd5lASih2k2sUsxHzy2IGbzpQpVH24/v5Mp41YlXhpurv/dxsbm68utW7cYHBxkb2+PjY0NnE4nXV1d+P3+r/rQHglM0ySTyZBKpchkMhiGgc/nq5cAqqpKd3c3sViM7MVVXrm5g1yofuqYLkkSkiTdVTr2SViWVRfSDgUzTdOoVCrcuXOH8VAVSXQxvZthJ27hkAWGGxQGxDjXr2c/1ol2+LMsy/dVCnukJCHgZC+d42fraWoDUSKBErdv3yYWi9HY2Ehra+uRIP2Po7+/n8uXL1Or1RgZ6OGNTIpMsYrLUcb7fsOGXKFK1OugqNVIlSS0mkGiUL3rHBuGwcbGBu9evspUwcNKKYQhyLybr6Bfm2YvnkdSnbiUIObtHD84F7rnhPbOnTsMDAxQKBS4ceMGo+ef5t2rWXyOXWr5JCmjjCAIKEaV6T2LLvJ4rlyhJrmY20xhlit0xsL09fUhSRLRaPTIuXjiiSfI5/OUSiX6+/tJJBJsbW3hczs4Zt7hj154nrXdJFYlyffPn0ZRZAYHBwk1d/D2n1xkJ5Em5JJpampC13U2dpKoksS3nnmSYmqXVCrFc889Rz6fJ5VK8fbbbyO4/KyUDGSzgmJWkUQZ1awi1Sq8u1hg0JFHL6aZm5sjEAgQiUS4c+cOlUqF4YZebhUcbCfzhP0uKoZAGRVB1PAJNWpCjYrgRRRFFMEiVbWw9jL84YW+9zsS9nD7zgo/ubHP3HvXkJ0e3KpEm1pi2JHBqYg0NjbS399PKBRCEAT29vaYmJhgZGTknu4/gHSphqA4GegJUCrkiMfjBINBOxrCxsbG5iHxKApED9NsYpdiPnhsQcwG+PKEqg+3n08WHg0V38bG5qtna2uLRCJBuVwmFovdVZb0daRWq5FOp0mlUvUys0AgQDgcJhaLsb29TSqVIhqNcvLkySOC1sMe0wVBQJZlZFk+Ur6aTCZxOp381oULfFcQSBSqR8Z6y7LQdf2IG61Wq1EsFslkMvXfa7XaXftTFOWIcJavwZtzO/hVEY9osLezh6ZpRDxO5jMW/8GxIWJBD3CQKRYIBO7rtYmiSG9vL2+//Tbf//73mUxO8YvbSUjmaTShikSqWOX3TrehyAKXlpIkyyZNHouXhpv54/EO0uk0Gxsb7O3tUavVSIUGWUpncTtMPKrI7f0suzmNgCIy3OBEF5X6hPafPdVdPz/VapWtrS2WlpZobm5mbW2NnZ0d1rI1NuN+XJaGZjgo5koY1TJejwfD4SLcHmMi52RyI8/ato6Mg+93dOPSUpwfG+HGjRucO3euLjpKksQzzzzDX/7lX7K1tcWxY8fo7Owkk8lQrVaZn7pKS0sLWb3Myy+/zHe+8x1UVSUWcPPi6R7+/voGqUyc4toayC5Ed4Cn2z2sLdyku7ubvr4+BEHA5/Nx7do12tra2CyYGEKB5oiHQjZDQ0MDXq+XmpEgXtIpWzLnz5whm81Sq9WYm5tDURT6+/sZ8/g4LbTwzuI+8XSOxpCP0/0BJu7sE/MpDHm9LMYL7Ocq6IaJCZyMOXmp/4MOtO/sWkzGQdBKtAfcrG1ts1CzoC/If/7tcTwez5HrIhaL4fP5mJ6epqenh8bGxruuncNFUF4ziIbDGLpOJpMhp1m4PR47GsLGxsbmAfMoCkQPy2zyZeV+f92wBTEb4MsXqg7u0NofWBubrzOHIfkrKyusrKzwzDPPEIvFvrYh+ZVKpV7+WCwWkWWZUChEU1MTAwMDmKbJzs4Oy8vLOBwO2tvbP7apwFdx86FUKrGwsHCkO+hHx/oPC1sf57K5F5Zl3eVGW8vkyFc0oi6JYlEnEAxSLpUIRRrYyVbIVExiHGTQSZL0ma6rnZ0d2traSCaT/K+/NUKlMsHNeI2V/QwNQX9dXHSpEt871cobl67xxKljlNP7TE1ewe/3U6lUaGlpIdrew1//zU3CHgcuaqxubJEuq8hAVYfdvX18bgeVGvz123HM5cv4VaHeNbVYLOJwOFAUhXQ6zfe+9z12M0Xee3ObRFHFsBSK5QqSqJCvijTINd5c2Gc2DW7BIKSCJrn5m+tbbLc4ePYpB5FIhJ2dnXrjAQCPx8OJEye4evUq7e3tuFwuarUaJ06cIJvNsr+/z5NPPsmlS5f4i7/4CwYGBmhqauL3R2OUikV+cmkfKdCAXikSLa1zLtzLE088ceS8FwoF9vf3OXXqFLdffwe3GqKomThdTmRZJpfLUdItWhujdMYUWlpa2NraYmpqit7eXn77t3+bd999F0Gw+Kfn2/j+mTZ2UgW2V+bp7g6xtFcgmcsSDvgZbPLTFfGwly0jSyL/4neGWLk9Q3NDhGSxxntLSZojPhRdZm1licaGRpo9QbZqAmVLxvPRiwJwu92Mj48zOztLKpViYGDgyOu71yLIcvooV4ocU0rk49tEPF1f2zHOxsbG5kHyMAWiL1qC+TBuTNoN6h4OtiBmcwRbqLKxsXnY6LrO1tYWOzs7BINBJEniueeeu6fj4tcVy7IoFoskk0lSqRSapuF0OgmHw3R3d+N2uxEEAcuyyGQyzMzMUC6XaWlp4cyZM3Wx5NP4ssb0w86Jo6Oj931snwVBEOrOsENEd5DIjQyCKBD1Oshls/j9fnJV/UhJQjqdJhy+O5vr44jH40iSxNmzZ7ly5Qrj4+P8H3/vCV57ZwJ3uINcfIvnx1pwqhK6rlPJxLHSW8Q3XLS1tdHY2MitW7fo7e2lsbGR+d08Jc2g2e8gn81hChImAi5VpFozsUQJURTxKJCpCQhOPz7PgUMwHo8TCoVIpVJMTk7idru5cuUKAOWCzG5BQhGryJjURJm4ZqLoBbIZE0PXkSQLI9zKbraCZlb4aUEj8toC//ypbqavXyUWiyFJH5SUDA0NkUqlePPNN3nxxRcpl8u4XC66urq4ePEit2/f5qWXXuLSpUvkcjmi0Shvv/4arbUa32nIEG0LMdDRzbHOFi5dusTf/M3f8OSTT9Lc3IxlWbz11lucPn2aX7x9mWRBoy+kcHW7RNAp0ehwUbZkDFnk6WMxKtkFfv7zZVKpFN/+9rcpFArMzs4yMDCA1+tlcnKSM2fOEO2IcLzlHDdu3GCoQeEXSYHddIGwz02hqlM1TL5xPEZTyIve3s7Kygq6p7G+qFBlB75UilqtRtilspOtfOKiQpIkRkZG2NjY4OrVq4yMjOBwfPDYey2Cfvt0O3883kFib5tLly7R19dHQ0PD/X8AbGxsbGzu4mEIRA+qBPNh3Ji0G9Q9HGxBzMbGxsbmS6FQKLC6ukqxWKS1tZVz586Ry+VYXV39tRfDTNMkl8uRTCZJp9MYhoHX6yUcDnPixIkjC2qAarXK5uZmPYOop6fnSOfNRwnLspiamuLYsWOfyfX1RfmwG8cyTbRiGYfPQap0tCQhmUzS3Nx8X9s0DOMgA218HFmW6evrY35+nqGhIZ574gxXr15l/OQg7777LqFQCF3XaWpqorOzk+HhYTY3N0kmk5w5c6b+noY9Ki5ZYH0vQcgp0RAJkRINarqFSo1jva24HAqJQhW/afH974wQditcvXqVF154Ab/fz89+9jMAvvOd7yAIAvF8lb/PTZJa30ezBExLRpFEAi4RlzNIsVjEJ0NJ8rKdKqNKAk7JoobAP8zu4nHIfLevm6WlJY4dO1Z//YfXZDweZ35+HkEQ8Pv9iKLI+Pg47777LpOTk1y4cIGXX36Zixcv8uyzz7KyskK5XKY/5oNqgbW1NUZHRxEEgYmJCURRJBKJgKzy49kMF5dNnJ4mxGKF7uYohVKZZNnE0DSe6fLRbW6x975b8p/+039KMBhkbW2Na9euMTw8jCRJDA8PMzk5ydmzZ1FVlbNnz2Jdn6JYdDGzX6JqCHfdkW9tbeXKlSs0+6NHFhXt7wtlG3sJ/P7AfS0q2tvbCQQCTE5OHuSphULAJy+COjo6aGlpYXFxkbW1NY4fP35XaaaNjY2Nzf3xMASiB12C+SBvTNoN6h4OtiD2BXlYHS0epU4ZNjY2Np8Xy7LqIfkOh4POzs56lpNpmszNzXH27Nmv+CgfPLqu1/O/stksQD3/q7Oz854uKtM02d/fZ3NzE1EUaWtro6en55Evr1pYWCAajR4IHl8yh0LHL6fX0QUHqsVdJQnZbJaBgYH72t78/Dy9vb3196ehoYHt7W3S6TSqquJyufjlL39Jb28vpVKJCxcuIIoiyWTyQGhpbubs2bNH3jOPZNAiF9nSLFRFpDkSxFvKslkqEXWJSJJ0VyD/zMxMXXBJpVJsb2/zgx/8oL7dzXiGeCpDq9ugoaERU5CRMCgWC6wkcrgcDkTVSTqnIwsWlq5jSAoup0SDz8mlpSS/O9pCZn297gI7pLm5mcbGRiYmJujq6qKjowMAh8PB2NgYb7/9Nn//93/P8ePHmZqa4uLFiwwPD9PU1FRv7FCpVNjb2yMej9PU1EQ8Huett95iSe7gZlbF73AQcYtolouiLjAQdfBHz/Qy+e4bHGtUmZtbpLW1lbGxMarVKpqmsb29zYULF5ienubUqVP4fD6GhobqopiiKIydHiXou0PXjVnOXjhDY8B9V8nu8ePHWVhYuGtRIboD7GWKnGn33/e8y+/3MzY2xvT0NOFwmK6uro8tFz5ElmWOHz9OsVjk9u3buFwujh07dmRMsOeANjY2Np/OgxaIHoeMLrtB3YPHFsQ+Jw+ro8Wj2CnDxsbG5l580qKtWq2yvr5OIpEgFotx6tSpu0Lyl5aW6OjoOFIG97hSrVbr+V+FQgFJkgiFQsRiMfr7+xFF8WOfm8/n2djYIJfL0djYyMjIyGNzTra3t6nVavctOD1oXKrEPz3XRoe5R0f/EFHfURHis+SHHQbIf9itqOs6Pp+PV199lWPHjuGJNNFywkelWqC/v53p6Wmam5vZ2Njg9OnTtLe337XNubk5Xur3EQgIzKctdrIVWoMuglINp8tz14R2bW0NVVXrrrY33niDgYGBuvsuk8mweHMSpywiCB4ioQA1TSOdLpAr1+hpbWK8p4FXbu5SM/N4HCLlioEginT4nES9DnYyZVLFGoODg9y+fZvTp0/Xj7mlpaUuMh2WOB6ei42NDdxuN7lcjqWlJb75zW8yNTXFrVu3eOmll1hcXKS7uxuHw4HX6yWbzVIoFIjH4/QOneIfrmZxCDUcFhg1iZ6OTuK5CvPxApt35nAKOjdv3uTZZ5/l+PHj6LrOxMQETqeT48ePEwwGqVQqLC4ucuzYMfx+P8ePH+fatWt1Uay/v59cLsf23DUGXnjhrvfZ5/Ph8Xj4VswJNNcXFR6vj6cDAp3aBtvbbUfy1T4JRVE4c+YMy8vL3Lhxg5MnT95X2bDH4+Hs2bPE43GuXLlCa2srkVgz/+7Khj0HtLGxsblPHqRA9DhkdNkN6h48tiD2OXlYHS0exU4ZNjY2Nh/m44T7Px7voFLMsba2hmEYdHR01LvLfZRCoUA2m6Wvr+8reAVfDMuyKJVKpFIpkskk1WoVh8NRd395PJ5PFWBqtRrb29vs7u7idrvp6Ojg+PHjj7wb7MNks1k2NzcZGxv7So9jaWmJMyf6iUb9d/3b/eaHmaZZF4YsyyKRSLC5uYmu6zQ3N3Pm3BP86Poud6Y3KWkGplZmbFtnxJNjb2+PkydPkq8JzO/m65PTvb09VldXkSSJjtYWnnqitd5106fA9so8XYMnj0xok8kkyWSyLkItLy9TKpUYGhoCYHd3l3fffZehwUF+K+Tiz96dZ3ZtD68MFd1AcPp47ngzfzTWTiaTZSORo1wDl8tJg0elr9FLpvxBKYnf60CWZVKpVP08ybKMw+GgqakJh8PB1atX6erqYnV1FY/Hg9PppLe3l3w+j6ZpPPvss/z5n/85N2/epFqtMjc3RzabJRwO09vby9LSEm1tbWzkDETVSYNbplIqUK1qlMtlyvk0VUPm1tIGQaHEH/7hH9ZdpIedTCuVCsFgEICuri5u3rzJ9vY2LS0tBAIBBgcH6yKeLMuMjo7y1ltvcfXqVU6dOnVXSXJ/fz8TExP8+0+eO7Ko2Fu7gyi2cOXKFZ5++un7dj0KgkBvb2/dKTg8PIzP5/v0J3LgQoxGo6ytrfGvf/QOUymRxqDHngPa2NjY3AcPUiB6nDK67NzvB4ctiH0OHpad8nGwadrY2NjcJdyXqvzt1TXW19b4ozNNR9ws98KyLGZnZxkZGXksBCDLsshms6RSKdLpNLqu4/F4CIfDDA4O4nQ673s7yWSSjY0NdF2npaWFsbGxI6HmjwvVapVbt25x9uzZT3S/fRnHkc/nOX78+D3//X7zw5aXlwmHwywvL5PJZIhGowwODtZLCf/NO8u8sZylJeyjJegmlbf46fQW+okY3znWxP/v8gaLWTDFg8n0YEjkqZiFBHR2dhKLxYAPJrALCwt0dnYemdCWSiUWFxcZGxtDEAQ0TWNiYoK+vj78fj/z8/PcuHGDb3zjG7j9Qf7+59MUKjWSVQHDNGn0O/n9kWYuNJpMX7/K/2K8FYfDwc9v7dIUcONTLDLlu0tJBgYGmJyc5Pz58/XPY3t7O5ubm8RiMZaXl0kkEkQiEUKhECMjIwBcv36dO3fuYJomkUiE+fl5DMPg+PHjDA4OIgjCgZttcZGmpiaUTJyQ10O+XKIhGMSyLLZ3tqmgolXLeMImP/jDHxwZO0qlEqZpAgf5boeflaGhIa5evYrb7SYYDBIMBunv7687xWRZJhqNEo1GuXbtGqOjo0eyuiRJoqenh8XFRQYHB+vnwtfXV+9oeeXKFS5cuIDff7fQ+nFEIhHOnDnD1NQUra2ttLa23tfzBEHAE2lmU9vDYeWhnEP1NthzQBsbG5v75EEIRHZG19eTr24W+xhzaKf0O4+W//idCiXNIFnQHqnt2tjY2DwoPirclws5zEqekEtmS/fQ0N7zqcHq6+vrNDY2HsktepQwDINEIsHCwgITExNcuXKF3d1dvF4vIyMjnD9/nuHhYVpaWu5LDCuVSiwsLHDp0iVSqRSDg4OMj4/T2tr6WIphpmly48YNhoeHv/LSzjt37nyiyzD7fufJj0PTNObm5rh+/TqVSoWmpiaeeOIJ+vv769fn4TXf3hBE1Ipo5SJSrURPSwNX1rO8tmVxcb1EoVTB45DI5vK8tpji53fy9PX11cWwQ0zTJJlMEo1G63+7V5fON954oy4aT0xMcPv2bb73ve+hqir/+kfv8PZKlvagg+NhkcGYD9ky2dvZIuTz8MQTT9DR0cF/8q0TPNOq4JAldnMalmndVUqiqiqxWIzNzc363wKBAPPz8yxtxUmbTvI16nligiCg6zrhcJjd3V3eeOMNQqEQf/AHf4DP5+PmzZv113nx4kWi0Sh7e3v0tcXocFYpGiLZqonHH6BsKiTzVYYbVEYHjo4dlmUxMzPDyMhI3aF2iCAInDp1irm5OSqVCgDhcJi+vj4mJyfrDtV0Os3p06eZnp4mnU4feR9isRjFYpF8Pl//m8PhwO/309jYSCAQ4Nq1a5RKpY+9fu6Fw+FgfHycXC7HzMxMXdA7vJbmd/MkCtUjz7Esi8X1HfbTWTyqeOSateeANjY2Nl8ePzjXyUvDzVimxU6mfM/vTZtfL2yH2OfgYdkpHyebpo2NzdeTj+YreL1eArKMppv3la9QqVTY3d3l3LlzX9YhfyqaptXzv/L5PJIkEQwGiUaj9PX1fS4HlGEY7O7usr29jaIotLe309/f/1g44j6JQ5Giq6vrvkvCHhblcplyufyxJZG7mSJbBYtk8eg1aZom8Xiczc1NTNMkk8nw0ksvfWwXzw9f85roYWtrC6/Hg2BYFGsmP722hC6o7BcqrGXjqLKEQzRZKXmwHHdvc2dnh5aWlvq1cNilc2BgoC7Cra2tUalUaG9v50c/+hE9PT28+OKLLCwskCxqbGhuWiISRiGNJElYloYn6GJLV1F84fq23Q6ZPz7bjOgOMHtnjWfOjdzz89nZ2cnly5dpbm4mk8kwOXWTV9d0VkoevMEIeqrAXGmePz5fplLIYRgGzc3NfPe73+WXv/wlhUIBVVV55plneP3115mcnERRFDRNI5/P8+STT7K+vk63sYXc1MGu5WdhM45kGZyKWvyz5wdYXrh9pHRzcfEgWN/tduNyubh8+TIdHR31LEJFURgZGeHGjRuMj48jSRKRSATLspicnOTMmTPk8/m6QHX9+nXa2tqOOAZPnDjBzMwM4+Pj9XPW29vLtWvXOHPmDJcuXWJycpJz5859JvH3MLx/d3f3wOU3OMTf3ozfVWb+B6NNxHe3SCQSKN4wDaEAsiyhOuw5oI2Nza8fj0PDEDuj6+uHLYh9Dh6WndK2adrY2DzqfFS4l953s9zvom12dpYTJ058ZcKQZVmUy+W6AFYul1FVlXA4THt7O16v93Mf22Fp5cbGBqVSiaampns2E3icWVlZwePx3OV6+ir4OHfYYcbdm3M75CsaL29O82RvhO8OBknu7ZDP52lsbGR4eJjd3V2i0ejHimHw0WveS39/P8VCgZ10AYeikNUEihUdRRRxyAI106Rgimxkq/cUiDc3N490Vp2fn6ehoaEuBNVqNS5evMjZs2d59dVXicVidHZ2cv36dY4dO4aqKRRvTOPVSkiShM/nw+F0fqwo3dLSwsbGBq1e8WPnEaIo0t7eziuvvIIsy1zKuFk1gihyhUaPzHbJ4t2NEqXSbf7PP3j+yPmKRqPUajWuX79O1+BJLH8Ts0vrVDJxvF4vo+ef4efvTVFK79Hd2sQLve28NXEd9ViE3/6NbzI/fY315TucP3+etbU1kskkkUiEYrFIf38/cCAw9fX1cefOnSPlsR6Ph773yxxPnz6NIAhEo1Esy+L69evEYjF2dnZobW3l7NmzzMzMUKlU6O7uBsDlchEOh9na2qKtrQ04ENoikQjpdJpTp04xPT3N5OQkY2Nj9xWW/2Gamprw+Xz867+d4EZSIBby0hJwksyX+KtLd9jc3OQ/+sZgPWvxqeyyPQe0sbH5teNxbBpnZ3R9fbAFsc/Jw2p5ardStbGxeZT5IsL9zs4OXq/3S3UWWZZFPp8nmUySTqep1Wq43W7C4fCRsrgvQrVaZWtri/39ffx+/yPhnnoY7O/vk8vlGB0d/aoPhVKphKZp9aD1D3OYcaeaGq1BF5liib+8mGR3189/8q2hemD7/boV73XNV0UHOGA4qvDybAJBANHUARG/x0W+YlCo6nxUW02n0/h8vrqwsrW1VS/vO+RXv/oVXq+X1dVVotFovWTv/Pnz5PN5tufnMWtlLI+HaOgDYerjRGmfz0c+n/9Yp6NlWSwvL3Pr1i0KhQIj557iL97cwilUwCijaxV6WhtY3UsRF11cvnGLbzz1gaNKURSa2jr4/759h/mJ9yhqOoIu4ddkIjUvf/43M5Q0E8kKc9wS2Etc4ZlzY5w8ebKex1cul8nlcpw6dYrV1VV+8Ytf8Fu/9VtHxOloNMrKygqVSuVIqXI0GqVYLLKwsFDvdtrQ0IBlWaysrADQ2tqKKIqMjIywsLDArVu36k0senp6uHz5MrFYrC5ed3d3c+XKFZ544gm6urrY3d3lxo0bnDlz5jM7RkumzFbNg0vMI1TyZIoGboeDtmiAzaqI6A7UX6c9B7Sxsfl1xG4aZ/MoYwtin5OHZad82DbNx8GqamNj82jzeRZttVqN1dVVzp8//1CPzTAMMpkMqVSKTCaDaZr4/X7C4TBtbW0PzK1lWRb7+/v13KXW1lbOnTv3lQbMP0wKhQLLy8tHSsu+ShYXF+vuoQ9zmPcVcilouRxa2aLB58HtdrNahJr0gZDyWdyK97zmTzYz3hnm3dUiubKGXjPwOJxUdQvTsvA5ZCzr6HZWV1frok0mk2F7e5uxsbH6d3N8c5m1tbX6a6tUKpw5cwZFUbh27RqqqnJu5DhLxh6v3NxBLlQ/VZQWBIFQKMTt1W1u7+SI+j64651Kpbh06RIAo6Oj5HI5fnXxCntpP15VItzYTNDvRRBFmiNBVvczyN4Y8/PzDA4OUqvVUBSFNzZ1JuMmbtnCqZfIVWrc0hswdyyaXRpBVSBTqjEZF+k5fyCGwYGw6XA46o6wVCpFLpfjiSeeYGZmhv7+/iNZa8eOHWNhYaEe7H9IZ2cnN2/eZGtrqx5k39jYiGmaXLp0iXw+j8/nQxAEBgYGWF9f5/r164yOjiJJEseOHeP27dv145JlmVgsxvb2Nu3t7WQyGURRZHp6mtHR0c/0GUgVNUo1k7ZYFNEyDsRQQbino88u1bGxsfl1w24aZ/OoYwtiX5CHZad80Nt9HK2qNjY2jyafZ9E2NzfHwMDAJwpGn0ew1zSNdDpdX0iLokgwGCQcDtPT0/PAQ+sLhQIbGxtks1kaGhoYHh7G4fj1nsjVajVmZmY4ffr0I9EEoFAo1IXOj5IqauQrGk69RENDQz2LSfqI+PBZ3Yofd83H81W6ox62syKpnEm5ZiJLAhGvSmvAdcStValUME0Tt9tNpVJhbm6OEyOn+B/eXeG9pSTxdI5SNsVwYxPNFY2nnzjH1NQUy8vLBAIBhoeH686ozyJKlzSd1zZNXpku4V69idepcK4zSC87pON7tLe3I4oiiUSCcGMTO1KMvVKRrbyBtwCxCvQ3eKmYAi5FpDXip5KNs7Ozg6IoGIqH95aStET81PIpdot5sqaboiViWbBVNMiUNLrCTgLRJqZ3KyQKVaJeB6VSiVqtRkNDA62trbz22ms0NzfT2dlJa2srt27dIpFIMDAwgCAIBAIBDMOgUCjcVeZ62HnS4/HUnYNNTU0cP36ct99+m5deeqkuZHV0dOB0Orl69SqnT58mEomwublJJpOpP7ezs5OJiQmam5sZGhriypUrBAIBbt269ZnKvj9PPqxdqmNjY/PrwkezZw/xO5X7yp61sXnY2ILY14Svwqpqu9FsbH69ud9FWyKRQBTFjw0//yyC/WH+VzKZpFwuoygK4XCY1tZWBgcHH4p7Sdd1tre32dnZweVy0dHR8dD29ahhWRY3btzg+PHj99VR88vg49xhALJRRi8XkYOBjw0m/yJuxY9e8w0+B0/3R3nl5g5uSyIaDlAzLQrVGk/1R488dm1tjc7OTgzDYGpqipMnT/Ln13f46dQWslFBrhYQRIHbJRcdZpSG+XkKhQJPPfXUXc7GzyJK/9nEOq8tphCwaHBLJHJ5/vTNLc40CPzx2Q5aWlpobm5GlmX+zdvL3EpbOAWdnAkVTWdpL0sik8cpwqmIyZuv/pRgMFj/XOcED6t7XkKqhVYpkdYd5CwFCwswsYCC4KAgeml3O44sgIrFIqVSiYGBAWq1GqqqUq1WsSwLWZYZGRlha2uLiYkJRkZGcLlcdTfXh3PY4IPOk1evXuXUqVP1cuhjx46xurrK9evX6zljcOAgczgcXLt2jdHRUQYHB7l+/Trnz59HEAQkSarnr3V2djI6Osr169cJhUIsLy/T29t7X9eMnQ9rY2PzdcZuGmfzqGMLYl8Dvmyrqu1Gs7GxOcQwDBYXFxkfH//Yx3ysYG/BPz7VUA/A1zStHoLd19eHy+V6aKKUZVmkUik2NjbQNI2WlhbGxsYeCYfUl8mtW7doaWm5Z1bXV8GhC/BeIfh7e3skt9Z48XQ3/zC3jyTdu5xwenr6U92Kn4VDV9bPJ5cpVHW8Dvkut5ZpmqTTafr7+5menqanp4eSKfP67BZirYRT0MlUy3Q3N5Mq1biynuOPnjyLLN/+xDLfTxOl69//bpWKJnJn4TaiKOKUXdxOi1y9OY9nbg6Agi7wk10fogCNooGFQMV0YpoC2YrBuU6V3z0eplZxIcsyQ0NDXLp0iXNDQ1y9mqFUKGAgYbn8SJUqNdMEIBzwohsWO5kSAbeKx+moL4CKxSKmaeJyubhy5Qrj4+PE43FWVlbo6Tm4Wdfa2kowGGRqaoru7m5isRgOh4N0Ok0oFDryeg87T05NTdVD8AVBoL+/n1wux/T0NCMjI/VxIxAIMDo6ytTUFMePH6e5uZnV1dV66H57ezuXL1+mra0Np9PJsWPHWFtbQ1VVNjY2aG9v/0zXiJ0NZmNj83XDvilg86hjC2JfA75sq6odnGhjY3PoEE1tr9HT0/Ox3dk+LNhH3ApVTUM1qkhahVcmlzjhLtLT2sDQ0BCq+mDuIn6Se7VcLrO5uXlQPhYOc+zYMdxu9wPZ7+PGxsYGoijWM5keBRYXF490GTxkdXWVTCbD2NgYJ3ULUZLuKT58mlvx83Do1jruKuAJx2hrCN51XW1vb9PS0sLy8jJ+v59gMMhP35kklSsSVE2qWpVQMIjD6aQzEGInW2E7ncfj8Xzu46pWq9xaWmcvmSHsFPB6vWSz2QP3k2iSNyR0yUUk4iQQCJAynTiLeZr9Dpyqgi+ZxOX1UzEgW9H5gye76W/wYFkW09PT+P1+nMEGVncS9IfdvLZbIRgIkk1oGJaIhYgAJPMVQh4npiixtpfmnzzdXz8/2Wy27riKxWJ4vV48Hg+Tk5NHyhc9Hg/nzp2rl1D29vYyMzNzz0w7j8dTFx4PHWFtbW1MT08Ti8WYmZnh5MmT9ee53W7Gxsa4fv06HR0drK2t0dLSQk47mD/5os2srR2MY5FIhGw2i2VZJBIJVFW9r46rdjaYjY3N1xn7poDNo4wtiH0N+DKtqnZwoo3N15sPO0RzpSqWXuHF025+EDJwqRKWZVGtVikUChSLRW5upthLpIm4RBIlgXKlgixJOCWJVMViL1ci4ExSLBZxOBw4HA6cTicOhwNFUT6TQ+zj3Kt/NNZONhVna2sLWZZpa2ujr6/va1ES+XGkUin29vbuKkv7KslkMqiqekSgtCyLubk5JEmqh527VI6ID4fkylWWP8Wt+EVo9LuIBlVCH/mOsyyLra0t2tvbSafTtLa28uabb2JUDERTI1e26GltxuvxgCCQKFRxqxIOs/qZupValkUmk2F/f79+rsLeEI0hP5IsEfQ6CIZCaNUq6/tpPLLMS98YRKqVSCQSmMkc1DT2UlUCDhFJksgk41gOLy5ZxiOaCIKAx+Ph+PAo//3rt7i578AQJDLJTaJ+N+vpErolIwoifoeEhUBZ04kXKrgUkRcGQkTTt6nVeslUTOb38pwOx6ik0/VrTRAERkZGuHr1KmNjY3WHnCiKDA8Ps7Ozw9TUFG63m/39/XsKUpFIhEKhUO88qaoqkiQRiUQwTZObN28yPDx8pFPm2NgYU1NTKE4P/+pvJlivOuvjRKtc4D9rasXndtDd3c2NGzdoa2tjdXUVVVXvcqp9HHY2mI2NzdcR+6aAzaOMLYh9Dfgyrap2cKKNzdebukPUpaDqBQTVzY+vrLC5ucl3eg4yqBwOB16vF6/Xy3BfF7E7OqIoHBkbEoUqMa/FhdMn8MgHItqhkJZIJKhWq2jaB2KHIAioqnpEMPvwz5Ik3eVeTeVL/PXlZdbX1vhfPtXNqVOnHlgXyseZcrnM/Pz8I9NR8pA7d+4wPDxc//0wi6uhoeGepWtuVeJv78TrAqiplXmyJ8wpU3gokx+Hw0G1Wj3yt3i+yupOnKIhsr6+jtPp5K233kKSJPRymSd7O7get6igoBoWuYpW/26WjQo+X/Mn7lPTNOLxOPv7+2iaRiAQIBaLcezYsfp7d2HPPPr9XwNTcXOh00tya5X29nbOnTuHIAgkvUv8dHoLQwEVA122SGZLPN8XRKwV2d+voOs6P5pJ8cZSBq8qIJkallFjJ1PG5fTiU1Xy5RoOWcTlUMiVRaq6wW90OzkpbqKoPv7l//QqGbWB1S2dn22u8JunuhmqmfVYBUVRGBwcrDdz+PB12NzcTCAQYGpqiuvXr/Ptb3+bREG7y/XZ2dnJ7OxsvfOkJ9LE29N3ODs8gGVZ9XD8Dz/31KlT/F9/fIm3VnK0NYi0BL3kKjWuJwX+n/8wxb/8/sF5OnnyJFeuXGF4eJjZ2VmGhoY+k3hpY2Nj83XEvilg8yhiC2JfE74sq6odnGhj8/XBsiwqlUrd7bURz/LKtX1EATTDRDANQm4FRVZYr0j0nBi950TokwT7WPCgZOxemVEfPRZN06hUKkfEs8Ofk8Uar1zPIwqAWCVTNHEoCk0hD9u6jDfabIthHDQQmJqaYmRk5GPLXL8KUqkULperHuxfrVa5ceMGPT09NDQ03PM5HxZAo26JvYrB22sFvBNrD6V8X1XVukj7YTfiXiKNaOl0uTRO+Qo0hAL4fD6+/e1vY4kKP5xYu+d38+z09buue8uyyOfz7O3tkUqlkGWZhoaGT2x6cM/v/5MH+3DIAqurq1y+fJn+/n7+yRNdCKLApfdFxHDYy/MnnAwqSdxuN/l8nt1Midm4RtijYpVzlCtlGv0BMrrCdsVk2G+StCBd0cnUdEQRAg6J3z7VzsnWYf71305wabtCyLWLV9TweSP84nYcWZaPvC+hUIhUKsXa2hpdXV1HXpPb7eb8+fP84ldv8l/+yS/ZswKUauZdmaUnTpzg3ctX+fFshhs7JXYSKZrnqzzZG+FMSOH/8teXWC7KdSfYybYAizmJxoCbUmoPNew7GLMsi4n1NLvpAk0hbz3w/+bNm4yOjnLjxo0jQf42NjY2NjY2jwePzmzX5qHyZVlV7eBEG5tfP3Rdp1gsUigU6uKXrusAOJ3OutvL3+BGdZdoCbpRJYFcLke5VMLvCxAv6h/rEH0Qgr0gCHVX2L2Y382jzs/SEnShiAePRxDQdNN2r77PYTZUX1/fF8quehjcuXOH0dFRAAqFAjMzMwwNDeH3++/5+I+W72czGbqbo6TK+kMr33c4HOTzeeADMS7gkKCSoaTDzZoHn6+J54d66O7urruePu672TCMAyeZrpNIJNjf36dUKuH3+4nFYvT29t5XY4BP+/7v6emhra2NxcVFKqur/NHpwbseu7u7y+7uLqOjozj3CsjzN1HKaQxVJRAI4PZ4UMs1NotF8jWTrqBCs25RNUzKmoGAQXxtgZ8v1phNgE8BvZDG5fPREvGTKFTv+b709PRw7do1QqEQgUDgyOsSRZF1tYPLO/MEnWnaGiOUDI5klgqCwK1qkL+/sUR7Y4i2oJuKVuWVmztcdqtsJ0v4VIGI3026UORvJhIUNYNOt47T4SCXy+EPBPC7VNIFJzduL/HikwfXocfjobOzs35t3rhxg7Nnzz6wrEMbGxsbGxubh48tiH3N+DKsqnZwoo3N44dlWZTL5brgVSgUKJfLAEiSVA+7Pgy+vpd7SMhXcaty3SHqDwTQdZ21nSSCJBF03nvx/mUI9rZ79dNZXFwkHA4TjUa/6kM5QiKRwOfz4XA4SKVSLCwscPr06Y91RMHd5fuB98PZ/U7hoQmghw6xeL7KxTsJ3IJJOZ1EwmSgs429bInNmgd/Y+tdpagf/m62LItcLkc6nebKlSsIgkA0GqWvr+8LNXj4pO9/VVUZGhqiUChw+/btekfFQ9dkU1MTlUqF+fl5Iq3dqIJJslRj+FhPXZT7/7P3Z8GN7fl94Pk9B/tKEAAJ7vuSXJNbMrPKVyVbKkm+ttW+drU9PX1jImYiaualp+eho8Ouh4mQJqYjpqLb7ph56YgZ18RER88t93gpWS5JV1JLllSuUmVySWZyyeS+kyAJgASI9WA788ALJNdMbgAOgO8n4kZWkSDwx37O7/9bkmoJNZVALBEH9BZUGbQ4jSWQ/GZD7B9/1oalgyD+8OQsMK1VCWflz7EYrHrttc9Lpp/Y9PQ0nj17duFzxxOU8HLjGHV2C8waIBwMwGg0wm7SZYNrsgy82vSjocoGxIIwV1RA8vkgpzV45Q6gySIAsTj8iQg0ajXsBjWCSRFmRxXqbAZ4PB4kk0mcxlKoMBmgSkYhSVI28F5TUwO/3w+fz4fe3l7MzMyU5TRaIiKiYsWAGD06Nk4kUq5kMpnN9MoEv1KpFADAYDBks71cLhcMBsOdekhdnyGaQkprwK+0WLD+fhZyaytcLte115vLgD2zVz/O7XZDkiR0dnYWeikXyLKMtbU1DA8PY39/H/v7+xgbG/tkOWchAqAajQYejwfLRyEc+QJocJhgqWzAzu4ukqkUmmqqcHAqXRuMS6VS8Pl8ODo6QigUgiiKqKysxMjISF6DK2azGaOjo/D5fJienobL5UJzczNEUURLSwvev3+PvbX3aDZIONTocRxJXHgv/aOxJkRDQbza9CMsGa5siF1+XhwOBzweD2S95cbnRavVoqurK1uemPnsyAQ9nSY9YuEgqqqqEAgEkJSiiIqG7ECFbGDUoMbBwQGkeByy1oQ0BFhNRjjNNgiiCINeD2caONk8hicUh1ajgslSgS23DymNAZ8P1GK014bV1VX09fVl19fd3Y2pqSlYrVa0t7djZmYGIyMjt8reIyIiosJiQIxyho0TiW7HE5SuNIR+CFmWEYlELpQ5xmIxAIBarc5me9XW1sJkMj1qr6iPZYhqVWelb7u7u+jt7X1Qtstjr62cnZ6eYmdnB2NjY4pqou8JSljdcUPUmbG9vY1oNIrR0dFbrTFfAVBZlnFycoLd3V1EIhFEIhGMPx/BH2wvQUolkTg+RoXVCo1ajeA3faoyQZ9IJIKjoyN4vV7IsgyHw4GWlhaYTCbs7e1BEISCZRo5HA7Y7Xbs7e3h1atXaP0mkG21WjE5OYl/MNgBiyWKZb985b1k0Krw19NzSKr16G1vuvBYX/e8yHoLdo9O8J8+77jxeXE4HPD5fNjd3c0OULCbtNAgBbcviPa6KkAQUGGzIXIcRDwYhCYdg8VivRCAa2puRiwaxc5xGHptEhq9ASbzh75fpxEJrU4ThptsmN0NwBNLQVSr8a1mU/a+ra+vIxqNZvuFCYKAp0+fYnp6GqOjo6ivr8fc3BwGBwcV9X4iIiKiqwRZluVCL4KIqBydb7ydaep8viH0pyQSiSvZXul0GoIgXMj2MplM0Ov1eT0584akGzNEw+Ew3r17h4qKCrS3t+f9pP9jays3kiRhenoaY2Njiul9dP59ceA9hlGrxmijBf+nz0dg1N0+eBuNp7IN6zPvrxd3eH99TDAYxN7eHvx+PyorK1FfXw+z2YyJiQk8e/YM/+3vT+E/rB6jsaoSFQYttg48SKkM+FsdFfhuowqnp6cwGo2orq6G0+m8EpR+//496uvrb+yRlk+pVApra2vY2NiAxWLB8+fP8cd//Mfo7e2Fparu2veSLMuYmppCV1fXld5f1z0vT2sNGK2I4VvjNwc8ZVnG5OQkenp6YLFYsL+/j//nX61g7kQFh0V/Iej5G0+qMGY+hdPpxJ/tpPH1wtlwBateg9NoHFsHPjTXOHAciX/4+bmA6fc/a8t+TtgMKqy/e5st2QwEAtja2sLg4OCF9QUCASwvL2NsbAzb29uIRCLo6el5/CeEiIiIHg0DYkREBfKjn69np+Bdd0IGAOl0+kq2lyRJAM5KtDIBr8y/xdK7RpZlHBwcYHNzE+3t7aiuri70kspOOp3OBhiUEHjJyLwvjEIaicgpNCYrImnxwvviLh4rABqLxbC3twePxwOz2Yz6+nrYbLYLAZxf/OIX0Gg0qLBX4T+6Zfxi1YNAOAo5IaFJF8X3/1YPmutrYbFYPhqgnpycxOjoqGLK7jY2NrJTLYGzwQayLGNkZOTGTM94PI6pqSmMjo5eO+zi8vOyvb2NcDj80SBSLBbDzMwMqqqqEIlE0NHdix9Pbl8b9NRrRKyvr+PI58dc1IqJTX/2Mh1WGd8bacBf7cRuFTD1eDzweDzo7e0FALx+/RpPnjy5ct93d3cRCoXw5MkTLC0tQaPRoK3t8SeaEhGVq8euqiBiQIyIqAA8QQk/+MksRFGA06xDOpVCIpHA0WkMiWQS/4enJli0Z+U4RqPxQraXTqcrmVKcZDKJlZUVRKNR9PT0ZMuQKLdkWcbc3Byqq6tRU1NT6OVknX9fpMN+WK1W6A0GeEMS5LSMH35vMK8HwIlEAgcHB3C73dBoNKivr4fT6bw2UOV2u/Gzn/0M3/72txGJROD3+xFJq6A22dDRUAPv3gbq6+tht9s/ebsTExMYHx/PxV26s5WVFSQSCfT09EAQBAQCAfzJn/wJnjx5gmAwiGfPnt2YXXh6eorFxUWMjY3dKri3sLCAiooKNDQ0XPt7WZYxMTEBv9+P3/zN38x+Dn4s6HlycoLFxUVUN7YhqTbCYdbCrJbx7t07jIyM3DpgOjMzg/b2dlitVgSDQaytrWFoaOjK5ebm5lBVVQWXy4X5+XlUVlbeeH+IiOh2HlpVQXQT9hAjIiqAy1PwItEo0qkULAY1fBERde3d6Kmt+MS1FD+1Wo2enh4Eg0HMzc3Bbrejra1NMZkxpWpzcxMGg0FRwTDg4vtCa3Zlf27Va3I2HfKydDqNo6Mj7O/vI51Oo6amBiMjIzf22otEIpiYmEAkEoFKpUIwGERtbS26urouBK4d5m68fv0az58//2hAO5lMKiLTU5ZlLC4uZt+jmTUbDAZ0dHTA5XLB5/PhP/yH/4Dvfve71z4+VqsVDQ0NeP/+/YVG9Dfp7e3F9PQ0TCYTKisrL/wunU5jdnYWNTU1qKiowP7+Purr6wF8vGdpZWUlxsbGMDc3h4qKCjhcbRAEAel0GpIk3brfaU9PD96+fYvx8XFYLBYAZ6Wzmf+d0dfXh8nJSZjNZvT392NmZgZarZZZsERED/Djie1sVUWdzYDTWCLbj/I+2eNEGTzjICIqgPPT1oCz6W7WigokBQ2sBi2qLPoCrzC/LBYLnj17BoPBgFevXsHn8xV6SSXL4/HA7/ejo6Oj0Eu54vL7IiOX0yGBs+CPz+fD7OwsJicnEY1G0dfXh7GxMTQ0NFwI9siyjEAggJWVFfzFX/wF/vAP/xBOpxO//uu/jt7eXjQ1NaGysvJK0Eur1aK2thbb29sfXUsoFLoSZMk3WZaxsLAAnU6Hzs7OC/clEAjAZrPB5XLhV3/1V+FyufB7v/d78Hg8115XXV0dRFHE7u7uJ28306B+cXER0Wg0+/NkMonp6WnU1NSgubkZ3d3d2N3dRTgcvtX90Wg0GB4ehkqlwvT0NCRJQmNjI2aXN7F0EIQ3JH3yOvR6PaqqqrCzswMA6OrqwsrKypXLiaKIp0+fYm5uDqlUCk+fPsXm5ib8fv+t1kpERBd5ghJ+ueaD3XS2gaFVi3CadbCbdHi55rvVZzjRTZghVqJYX/1wfAwpl/I1Ba+YCIKA+vp6VFdXY3l5Gdvb2+jp6YFeX17BwVwKh8NYW1vDs2fPFFl2m+/3RTAYxO7uLvx+P+x2O9rb22Eyma5cLplMwuPx4OjoCNFoFFarFZFIBBUVFfjss8+g0WgAnAW94vH4jbfX1NSEV69eoba29sYyw+uyjvJJlmXMzs6isrISTU1NV36feayAs+DP8PAwKisr8fr1a9jtdvT09MBsNl/4mydPnmBqagpmsxk2m+2jt6/RaDA4OIi3b99ibGwMqVQKMzMz6Orqyt6uIAjZy4yPj98qo1QQBLS0tMBut+OvJ6YxF7HgL9+7oTOdDW64TelNa2srXr16hZqaGhiNRmg0Gvj9/iv3Sa/Xo6urC7OzsxgeHsbw8DCmp6fR399/5bEhIqKPu1xVkZHP7HEqXewhVmJYX/1wfAwpX3I5Ba8UBAKBs94/1dVoaWlRZACnmCQSCUxNTWFoaEjRvdpy/b6IRqPY39/PNsdvaGhARUXFhdeXLMsIh8M4PDyEz+eDKIqoqqpCVVUVgLM+UfX19aivr7/wdzs7O1Cr1aitrb3x9o+Pj7G3t4eBgYFrf7+wsIDm5uaCBE7S6TTevHmDmpoa1NXVXXuZ6elpPH369EqJZKbXWCwWg1arRVdX14WgXyKRyA4LuK7J/mVerxcrKyuQZRkDAwPXBgkPDw/h8XjQ399/p/v5//qrVfxkcgMGMYm6KjviUF0ZaHKTk5MT7OzsYHBwELFYDPPz8xgbG7v2suvr65BlGe3t7dmBAMPDwwzyExHdweW+uxmF6i9KpYUZYiWG9dUPx8eQ8sWgVeH7n7Xhi6H6R5mCV2oqKiowPj6OnZ0dvHr1Ct3d3Vf6CtHtyLKMt2/f4smTJ4oOhgG5eV8kEgm43W4cHBxkm+O3tbVdCGalUin4fD4cHh4iHA7DbDZng7EqlQqyLGNvby8bzLpuuqJWq0UsFvvoWux2O7a3txEIBFBRcbVPYCQSuTZLLddSqRRev36NpqYmuFyuGy+XTCav7RfW0dGBubk51NTUQK/XZ6dBtrS0QBRFaDQaDAwM4M2bN3j27Nkns7rUajVOTk7Q2Nh4Y8acy+WC1+vFwcHBrfvheYISXm2eoNFVCU0ying0DOc3gc6Xaz58MVT/0ddbZWUldnZ2cHx8DLvdDoPBkP3fl7W2tuLNmzfwer1wOp0YHBzEzMwMxsbGslmFRET0cayqoFxiD7ESwvrqh+NjSIXgNOvQXWPhF/o1BEFAU1MTRkZGsLOzg9nZ2Y+WpNH13r9/j5qamqIKKD70fZFOp+F2uzE9PY23b99CpVJhZGQEw8PDqK6uhiAIiEQi2NzcxNTUFF6/fo1QKITW1la8ePEC/f39qK6uhkqlQiKRwJs3bxCNRjE+Pn5tMAwAdDodJOnT3xM9PT1YXFzE5SR9T1DCViABXzi/r/FMj67W1taPBsPi8fiNpZ6CIGBgYAD7+/uQZRnj4+PQ6XR49eoV3G43ZFmGxWJBU1MT3r1799H1HB0dYXl5Gb/xG78BQRCwv79/42V7enqwubmJSCRyq/uaKb2x6jUwGI1QfxOYsuo1iMRT8IU+/dg/efIES0tLkGUZHR0dWF1dvfJcAh8ek8wkXZPJhJ6eHrx+/RqpVOpW6yUiIuDL8WZ83l8LOS3D7Y9CTsv4vL8WX443F3ppVOSYIVZCWF/9cHwMiZRJq9VicHAQfr8fr1+/Rm1tLZqamlhGeQuZZuYNDQ0FXknuybKM4+Nj7O7uIhaLweVyob+/P1uil06n4fV6cXR0hNPTUxiNRlRXV2NoaOjGKZKZ0r0nT558MqD4qR5iGTqdDlVVVdjd3UVjY2O2VP+vVz04Og7j673ZvJXqx+NxvH79+lYZmH6//9qstgxBEDA0NISpqSn09fWhvr4eNTU1WF9fx8TEBLq7u1FbW4tAIIDt7W00NTVd6de5s7MDj8eD0dFRqFQq9PX1YWpqCkaj8dr+Y6IoYnBwEHNzc7fKPDs/uMGsSkP9zUTPuwxu0Gq1aGhowMbGBtra2lBRUQGPx3PtJEm1Wn2h35nNZkNbWxvevHmDkZERfoYREd2CEqoq2F+6NDEgVkLOH+Sdf5PmejqXUt3nQ4uPIZGy2Ww2PH/+HFtbW9kT7E816S5nJycnODg4wOjoaKGXklOnp6fY29tDIBCA3W5HR0dHtuwwFothe3sbHo8HqVQKdrsdDQ0NsFgsHw1GpNNpLC0tIZFI4NmzZzcGzM7TarW3yhADgJaWlmyD9h9P7ODreTcsGgF1FXrIopCXUv1MX6u+vj5YrdZPXj4QCMDhcHz0Mmq1GsPDw3j9+nW2X1ZnZyckScLS0hI2NzfR1dWF17ML+P3FIN64I9lecZ1WGb/RZsDI8HD2uTkfZLup/5bRaERjYyOWlpbQ09Pz0fWdL72JCmlYDGp4Q9KdS28aGhowOTmJ2tpatLW1YXp6GlVVVde+pkwmE1pbWzE/P4/BwUFUVVUhHo9jbm4OAwMDDIoREd2S06zLezCK/aVLGwNiJYT11Wce8qHFx5BI+TLT4mpra/H+/XuoVCo8efKEPXm+kdkMMKnT2FlZxNjYWEmecEejUezt7cHr9cJisaC+vh5PnjwB8KHxeSAQgE6nQ3V1NQYHB2/9GgkGg9nm9h9rkH+ZWq2+dSmcKIro7OzEq7fv8cs1CXaTDma1DEADvf7su+Y2Pa3uKxKJ4O3btxgYGLh1A/9AIIDW1tZPXk6n02X7hY2NjUGtVkOn02FwcDD72P7lbgJ/tbaD5ho76ir02PMc4z8ey3A6nRi99Hq93H9Mpbr6XV5XVwefz4ejo6NrM7XOy5TY/NnsNrxhoMKovnPpjSAI6Onpwfv37zEyMgKn04mDg4MbXy8ulwt+vz+bGVdfX494PI6lpaXs65aIiJSH/aVLGwNiJSZzMPdyzQe3PwqjVlV29dUP/dDiY0hUHHQ6HYaGhuDz+TA1NYXGxsYrU//KyfnNgLCURDwSxHcHmzAkiyiVUGE8Hofb7cbh4SG0Wi3q6+vR3t6ORCKBo6MjbGxsIB6Pw2azoaamBt3d3Xd6PciyjK2tLXg8HgwNDeV8GqDT6YR3fh3BaByNDgu06g/lfrks1Q+FQpibm8PTp09v7Id2nVQqdatMOQAwm83o6urCzMwMRkdHs6WMFosFLU8GsfRmChYNEPV7IEgGVFsNsECDl+s+fDF8NQhoNpvR3t6O2dlZDA0NXfu89vb2YnJyElar9aPPXab0plt3CmtVHeqdFfd6jC0WCwwGA46OjtDS0oLJyUnU1NTc+Jrr6urC9PQ0rFYrbDYbWltbsbi4iI2NjVsFGomI6G4eWuZ4ub80gOy/udy0ovxhQKzEKKG+upAe40Or3B9DomLjcDjw/PlzbGxsYHJyEj09PTdOpStl2c0AoxaGdBQ6swl/vuyDVqst6h3MVCqFo6OjbLP2uro6jIyMIBwO4+joCOvr69BoNKiurkZvb2+2X9hdSZKEubk52O32vGbVjfZ14396O4HTqA5Oy4cgTq5K9U9PT7GwsHBj+eFNJEm6saH+Tex2e/ZxHRwczD6mx+E4klChtaEGgRNfttRVIwsfDQJWVVUhFAphdXUVnZ2dV36vUqnQ39+P2dlZPHv27JPPoUFIYqDJ+cm+Yx/T2dmJyclJOBwO1NTUYG9v78Z+fYIgYHBwEFNTUxgbG4NWq0V3dzfm5uawv7+Purq6e6+DiIg+eKwyR/aXLn2cMlmiynVq3fnpUefdZXpURrk+hkTFSBRFtLe3Y3BwEKurq3j37h2SyWShl5U35zcDDEISJoMOdQ5r0U7IlWUZXq8Xb9++xdTUFCRJwpMnT1BfXw+Px4Pp6Wns7+/DbrdjfHwcIyMjaGhouHcw7OjoCK9fv0ZXVxfa2toeFAwTBOHaiYM3aayqwLPmChychOENSYgn09meVi/aHY/6HXRycoJ3795hdHT0ztlvgUDgXv36amtrYbVasby8nP1Ztl+nlERVdTUcDge2trdxEop+MgjY0tKCaDQKt9t97e/NZjPq6uou3N5N0un0g4JhwFmZbGtrK1ZXV9Hc3IydnR2k0+kbL6/VatHf34+3b99CluXsJEq32w2v1/ugtRAR0ZnMJqEoCqizGSB+05vzq4mtO13P+f7S57G/dOlgQEyBPEEJSwfBojuBUQJ+aBGVN71ej+HhYVRVVWFycjKbVVTqzm8GmEymbIbcfTYDCkWWZQQCAbx79w6vXr3C8fExamtr4XQ6cXR0hPfv3yMej6OrqwvPnz9HT08PHA7Hg4JXqVQK8/Pz8Hg8GB8fv1Vj+U/RaDRIJBKfvuA5/+XfHsaQI410Kp2zcfI+nw/Ly8vZzKS7CgQCH50w+TGtra1IpVLY3t4G8KFf53FYgjckwWipgKC3YGPfg8Ea/UeDgIIgoL+/Hzs7Ozg9Pb32Mg0NDYjFYnkLMNXU1CAYDCISiaC+vh47OzsfvbzVakVtbS0WFxcBfBgcsLa2hkAgkI8lExHlhBLOYy9XDGnVIpxm3b02CS9/Xz1k00oJjw1dxZJJBeEEi4djU3wiAs5KqxwOB9bW1jA1NYWenp5bNw4vRsU8ITcSiWBvbw8+nw8mkwlGoxFGoxHHx8dIJBKorq5Ga2vrgzN5LssE39rb2z/ZhP0udDrdncsLTXoN/svf7Mfa7hHsdc2PXqp/dHSEra0tjI2NXduQ/jYCgQDa2u5fetvT04M3b95Ar9ejurr6Sr9Ok8mM33Dp0ZLcxfZ2BZqamm68LlEUMTQ0hOnpaYyMjFybGdjX14fJyUlYLJZrf//YgfLe3l4sLCxgdHQUr169QkNDw0cf64aGBszPz8PtdkNttuM4HEdzVx/evTsrL42k1Q/qe0NElE9KOo997DLHh/aXVtJjQ1cxIKYgnGDxONgUn4iAD1P8IpEI3r17B7PZjM7OznsHBJSs2DYDMs3xDw4OIAgCtFotVCoVYrFYtnm6yWTKyW3Lsoz19XX4/f4bgykPodVqEY/fPSOvuroaOzs7qDcLMD/i87W/vw+3232hsf19pFKpB713BEHA06dPMTU1Ba1WC5vNdqVfp8OkxcLCAra2thAMBtHb23tjBmCm9DAzefLyfVOr1ejr68Ps7Oy1PeGSyeSjTqY1Go2orKyE2+1GU1MTtra2PhlAbOnown/7b3+BbUkPKSXAqFXhaX0l/v2/fYWDlAmxpMwTJyIqCko6j33sTcKH9pdW0mNDVwlyOdSSFAFPUMIPfjILURQuvMG8IQlyWsYPvzeouBMapfOGJDbFJ6Ksw8NDrK+vo62tDS6Xq9DLeXTReApfTWzh5bkdyBcFOJG+aaJTKpXC4eEhdnd3EY1GoVKpIIoizGYzXC4XHA7HrScY3lc0GsXc3BxcLheamppy0jh/d3cXoijeq0F6JBLBwsLCozX139nZgc/nw9OnTx90fZIk4f379xgaGnrwmhKJBKampm6ccCnLMubm5pBKpZBKpTA0NPTR18Xh4SEODg4uNO0/b2trC4lEAh0dHRd+HgwGsbu7i56engffp4x0Oo1Xr15hdHQUr1+/xtjY2EfX/qOfr+MP3+5DlYygqaYKwXgK790ByGkZtYY0GmuqEIqnsoFtnjgRkRJdOI81aYFvPosLeR77o5+vZ4NQlzcJ8/lZynN85WOGmEIoeYLFQ8fVForTrCuq9RJRbrlcLjidTqysrGBnZwe9vb3XnpAXq0JPyL22JKDNgd9qN+Jg7ywwI4oiLBYL6uvr4XK5YDab8zbNcX9/H9vb2+jv789p+axWq0UkErnX3xqNRlitVhweHqKmpuZB69jY2EAoFHpwMAx4WP+wyzQaDZ4+fYq3b99idHT0Smlppsn8zMwMKioqMDk5eWPwDDh7X4dCIayvr6O9vf3K75uamvDmzRscHx/Dbrdnfy5J0qNnB4qiiK6uLiwvL6OlpQUbGxvXTsMEPvS4cVr1sKh1OPYeIS2qEYrEIUOGzqSC3+dBTW0tgNtPyiYiyidZlrF16IMvEIRVncZBMJ393CrkeaxSKoaUfI5PZxgQUwgl9n9hvTMRlRqVSoUnT54gFAphYWEBNpsN7e3tj96fqpAKtRmQLQkwauE0iHAf+/HVzw7w7p2A/2y4GoODg6iqqnrUMrXbSCaTWFhYgE6nw/j4eM6fa51Oh5OTk3v/fUdHByYmJlBVVXWvEkVZlrG6uopkMon+/v5HCTgGAgE4nc4HX0+G0WhEb28vZmZmMDY2huNI8sLGW6bJ/PT0NBobGzE7O4uurq4LAa3z2traMDs7i8PDwyvZn5kA2+Tk5IUAXCwWu/OkzdtwOBzY2dmBXq/PZqdd95o/f5KkVYtQqVTQmyugOT6BDMBktUJIRAHwxImIcu8uCRCJRAJerxdHR0eIRqNIa02wGnTQ6bSQo6eALAOCUNDz2EJvEmYo8RyfLmJATCGU2P8lV/XOxZpxRkSlw2w2Y2xsDG63G69evUJHRweqqqoKvSxFkWUZyWTyVv8dncbwR1MeSLEovMcSDAYD7GYzLCYnAmo12noLUxJwcnKCxcVFdHV1weFw5OU279tDLEOlUqG1tRVra2vo6uq609/KsozFxUWo1Wo8efLk0bLvHtpQ/zoVFRWobWzG//Vf/wI7kuHajbeRkRFMTU2ho6MDGxsbCIfDaGxsvHJd54NeRqMxO2U1Q61Wo6enB3NzcxgZGYEgCIjFYqisrHzU+5TR09ODmZkZtLW1YW1tDU+ePLlymQsnSSYtBEGAXquGDEAAgHQSesNZRgFPnIgoV26TACHLMoLBII6OjnB8fAyVSgWn04muri4Yvvmcehc5K1FUJWRYEqkL5d6FPN8rdMWQEs/x6SIGxBREKamdwNVxtQCy/943bZ8ZZ0SkJIIgoK6uDtXV1VheXsbOzg56enqyB3fFGLyXZRmpVArJZBKJRAKpVAqJROKTAa3r2okKggC1Wn3jf3q9Hul0Gpubm5hd3sVpWIMGhwVVjqZs36R4Ml2QzBZZlrG8vIxIJIKxsbG8ZqVptVpI0sNGqrtcLuzs7CASidy6rFeWZczPz8NsNqO1tfVBt3/ZQxvq3+RP1yOYOkjBrAmivtpxZeNNpToLik1PT6O3txd7e3t49+4denp6rgT7MpMnX79+fW0pps1mg91ux8bGBixV9Vg6DGLA6kAuwqQ6nQ61tbUIh8M4PT29tjzzwklSOo1kGghLSeg1IiAL8AejMLkc8IYknjgRUc7clACRTqXw213mbBaY1WpFdXU12trars20zpyv/uH0BlaPgqg06jjU7BtKOsenqxgQUxClpHYCual35oQNIlIitVqN3t5eBINBzM7OwlRhxy+PRLzcOM5L8P58EOs2/2UCXel0+sp1ZYJYKpUKGo3m2iDWdcGt88GFTwUCk8kkVlZWsLKyAkmS0NTUhL/33V/Fmz/dgCgKF5qIFyKzJRKJYG5uDvX19eju7s7b7WaoVKprn5u7EAQBPT09eP/+PUZHRz95eVmW8fbtW9jtdjQ1NT3oti/LRa8t4MPGW63DAl1aglqQr91402g0GB4exuvXr/H06VP4fL7s/77csF6n06Gvrw9v3rzB2NjYlZO26roG/He/90tshA/gC4RRvZLAtzucOXlvNzU14dWrV2hpacHq6ir6+vquXCZzMvTXq174Ymm4jDK+fN4MyMCfvd3A4anEEyciypnLCRCJeBzaVAxiPIo/fbuJcVcbus9lgX2MjLONNUEUIMsy5Py0By0KSjrHp6sYEFOgQqd2Ao9f75yLjDMiosdksVgwPj6O//4PZ/An745Q57Cizma6NngvyzLS6fSVINWnsrFuCmKpVKobM7GMRiPUajU0Gs2FQNdjN6P/WBavTi1gY2MDi4uLCIVCqK+vx2effXahp9O32k8LWhIgyzJ2d3exv7+PgYGBoh+YYDabYTQacXR0hOrq6hsvl06n8ebNG9TU1NxrsuWnPGZD/fMu9tD68Pq4buNNp9NhaGgIb968wfDwMEwm043N9q1WKxobGzE/P4+BgYEL75N/ObmD2WMRYiIEu06ASiXmbGMuE9Tc2NhAMplENBq9clKZOUn6Oz1VmJpfwrdHB+A06+D3+zFik2CraeKJExHlzOUECCkeh1anQ1ONGe5ADAZb9a2CYcCHxAe9RoU6kx4SVEx8uEQJ5/h0FQNidK3HrnfmhA0iKgbeUBzvfCk0uexQJ6M4OthHIpGAlAB+7xfH0B/MwaoVIIpnTbDPB6jO/5v5L5ORpdVqs78XRfHCf5mAGIC8TVy8zpUs3mgc/256C0tLSxg1BVBdXY2xsTFUV1dfu85ClgTE43HMz89ng5qFfBwfU2dnJyYnJ+F0Oq8tUUmlUnj9+jWam5s/GjR7CL/fn5P+enfdeDMYDBgcHMTMzAxGR0ezkyqfPHlypRdYbW0tQqEQNjc3s+WjmY05h0UPq0YPj8eT8425iooKqNVq2Gw2rKysYHBw8NrL2U0atFR+CHx5PB50NNbA4bBce3kiosdw+XM4M4HZG5LulABxPvHBJJ4d51j0THyg4sCAGN3oMU9uOGGDiIrBxawVA2w2G2QA8UQK+4Eo+kc70O40ZjPEPvZf5jKSJJ1NYbrh9+f/f6aX13UBnet+JwjCtQG2yz+7LgB3/nIn0ST+49IBrFoRJjEF35EPkUgEKkGL/YQF/+Tv/i1UWT4+ka9QJQFerxcrKyvo6emBzWbL+e3dhiCclYw8NDCnVqvR1NSE9fV1dHR0XPhdIpHA69ev0dHRkdOBAaenp2hvb3/0673PxpvJZEJfXx9ev36NsbExPHv2LJsd19DQcOGyHR0dePPmDTweD6qqqq5Mdayvr8/ebi435rq7uzE1NQWNRoNwOAyTyXTlMul0+kLA8+TkJCePORHReY+VAHGXjF8ipWFAjG70mCc3nLBBRMXgcvBeEEUIAEKJBCx6LRqqbLAo5PMqE0C7KcD2sQBcpgdZ5r9NbxSBcAxOoxrRaBomkwm1tbWIp2S4/VEchxOfDIhl5KskIJ1OY3FxEclkEs+ePbvST6qQMpMmH6P3Vl1dHSYnJ7HrDSCcFOEwa2HVCnj9+jW6u7tzNikxI1cN9YH7bbxZrVZ0d3dnm+ePjo7i/fv3WFxcRHd3dzYIKQgCBgcHMTk5eTb19PLG3DeXy/XGnEajQVNTE3w+H1ZWVjA0NHTlMucDYslkMhuoJiLKtcdIgGDiAxUz5Rw9kmI91skNJ2wQkdIVU/BeEIRsltdDWaokON5HIIgC7BcOZuOKPJgNBoNYWFhAc3MzamtrC72cK3Q63aM1o48mUpg8NeMv/uUkNHoz9GoBDZow/o+fD+c8GJarhvoZ9914q6ysRGtrK2ZmZjAyMoLe3l5sb29fabavUqkwNDSEmZkZjI2NFey9XVdXh/39fciyjGAwCIvlYimkLMvZ9/Hx8fGF3nxERLn0GAkQxXTsRHSZIF83a50oh7whiRM2iEixovEUvprYwstzzeVf5HDKpFL86Ofr2R5ilw9mldIQV5ZlbG1twePxYGBgAHr97bLW8m19fR1WqxVOp/PB15V5XjQpCTaTDt5AGEmVAX9vqD7nz8vh4SGi0ShaWlpyejv35Xa7cXBwgKGhIQiCkM3Cevr06YVG0H6/HysrK+gbHMaPJ7cL8t4OhUKYnZ2FVqvF2NjYhd8FAgEcHBygu7sbCwsLaGpquhI0IyJSsnI9dqLix4AYERHRNcoteK/0g1lJkjA3Nwe73Y7W1lZFN87f29sDgGyfqvvyBCX84CezEEUBdqMGPq8XlXY7/LEU5LSMH35vMKevzeXlZVRVVeU8E+0hdnZ24Pf70d/fD0EQEA6HMTs7e6Wn3P7+Po6Pj9Hf31+w9/bS0hKOjo4wMDBwYW0nJyfwer3o7OzEq1evSmowBBGVl3I7dqLix5JJKlqeoITjMD9wiSg3ym08dqGa4t/G4eEh1tfX0dfXB6vVWujlfJJWq0UoFHrw9ZxvVCyKIqq+mSRp1Yt5aVScq4b6j6mxsRHJZBKLi4vo6emByWTC2NgY3rx5g7q6umxQsq6uDsFgEFtbW2hubi7Ia7ujowOHh4dYXFzEixcvsj/P9BCLxWLQ6XQMhhFR0Sq3YycqfgyIUdGJxJP48cQ2fnkui+FbCspiICIqZko6mE2lUnj37h1EUcT4+HjOmrs/Np1OB5/P9+DrKXSj4lw21H9Mra2tWF5exurqKjo6OqDRaDA2NoZ3794hFAqhq6sLgiCgq6sLMzMzMJlMj1LOelcqlQpPnjzBmzdv4PP5stNBMwExr9dbkHURERGVK46woaLz44ltfD3vhigK3+yaC/h63o2vJrYKvTQiInokgUAAExMTcLlc6OvrK4rATEZmyuRDZRoVH4cleEMS4sk0vCEJx2EJL9odOQ1cxmIxxfZou05nZyckScLW1tmxgCAI6Ovrg16vx8zMDFKpVHby5OrqKsLhcEHWWV1dDZvNhoWFBWS6lmSa6nu9XlRVVRVkXUREROWIAbEi4wlKWDoIwhuSCr2UgvAEJfxyzQe76SyDQasW4TTrYDfp8HLNV7aPCxFRqZBlGaurq1hdXcXIyAiqvykTLCaPFRADziY0f95fCzktw+2PQk7LeZnQHAgEUFFRkdPbeEyCIKC3txeBQCDbww0Ampub0dTUhMnJScRiMajVajx9+hSzs7NIJpMFWevg4CACgQCOjo4AnGWIAbmf6klEREQXsWSySLBM8Mz5firnWfWavPRTISKi3IlGo5ibm4PL5cLIyEjR9lISRTEb5HioQvV28/v9cLlcOb+dxyQIAgYGBjAzMwO1Wp1dv9PpzGaKZZrtZ0oXR0dH8/460+v16O7uxszMDH7rt34L6XQasVgMZrM5r+sgIiIqd8wQKxIsEzxzvp/Kefnqp0JERLmxv7+Pt2/fore3F83NzUUbDMsVp1mH7hpL3jZ9gsEgLBZLXm7rMQmCgKGhIWxvb1/o42Y2mzE2NoaVlRXs7++jsrISNTU1eP/+PYD8Z+B3dnYimUxiZ2cH3pCE+Z1jqEy2vNw2ERERnWGGWBG4XCYIIPvvyzUfvhiqL5usqEw/la/n3QDOMsNOYwkchyV83l9bNo8DEVGxujwhOJlMYmFhATqdDuPj4xBF7tUpQbE01L+OKIoYGRnB1NQUVCoVbDYbAECj0WB0dBQLCwsIhULo7OyE5ziAf/YHr/Hel8prBr4gCBgcGcM//3cv4RHtOD4No25XhW93SGWX/U9ERFQoDIgVAZYJXpTpm/JyzQe3PwqjVpWXfipERHR/15X+D9UaMWAIYKC3Oztxr1RkyiaLMcC36w3gMKaCNyQV7fGFSqXCyMgIpqen0dfXl812E0URAwMD2NzcxJs3bzB5asH/sriL2koL6mwmnMYS2U2373/WltM1/ul6BPOnWlRoI7DrAZVKzNttExEREQNiRaHQY9eVplD9VIiI6P4ypf92kw51FXq4vSf4g9kT6J614m+WWDAM+NBYv5gmNWaCln/1bh8hKYnf35wt6n6lGo0Gw8PDeP36NZ4+fQqj0Zj9XUtLC2LQ4o9/toB6RwWEeBga0ZS3DPxM9n9LjQO+vU0Ync6yzf4nIiqky5nrVF6Kb9uyDBVy7LqS5bufChER3U+29N+ohS4t4dh7BKdFjyaXHROb/pKcEKzT6SBJxXW/MkHLVDKBhkpjSfQr1el0GBoawtu3bxGLxS78TtBboNYZkYwEYauogPBNNp9Vr0EknoIv9DiTQq+Tyf63mfQwGo04PT3N220TEdHZJtCPfr6OH/xkFr/70wX80387ix/9fB3ReKrQS6M8YkCsSBRq7DoREdFDeYJReP1BnBzuI3B6CrvDAZPZXNIn/5kMsWJxvl+pWQOYDGd9S+0mHV6u+Yo6aGkwGDA4OIiZmZkLz4ndpIXFoIXWXIFAIIBIJAIgPxn457P/GxoaEI6EkYjHyzb7n4go3zi0jgCWTBYNlgkSEVGxicViWFtbw+t3q5ATGljs1aixm7O/L+WT/2LLEDvfr9QfSV/IliqFfqUmkwl9fX14/fo1xsbGoFarLwzqsRutCIZC8JxGIUGDzwdyO6jn4pAgHXQGM96t78Jgc+b8tomIyh2H1lEGA2JFxmnW8c1JRESK5vf7sb6+Dp/PB1EU8ZvfeQFpJYyv591Qh6SymBCs1WqzZXDFIJOxFIhIUJ+bLllKQUur1Yru7m68fv0ao6OjUKlUFwb1REU9kI7jqT2N/2y0IefrOX/botkOyXOAv1WvZ/Y/EZUEJffm4tA6ymBAjIiIiB5MlmW43W5sb29DlmUkEgl0d3ejsbERgiDgy8qznhzlMiG42DLEMhlL//71Nqw6EeZkuiSDlpWVlWhtbcXMzAxGRkauzcBPRwJYmJ3B0NAQdLqz+52LE7vLt/2Xf/JTOOIbEOV+AMU3xICICLh+qrTSBrRwaB1lCLIsy4VeBBERERWnRCKB7e1tHB0dwWKxIBQKobKyEu3t7VCrr+67eUNSWZT+S5KExcVFPH36tNBLubVoPIX/xx9O450viaQswqhV4YXCTmIei9vtxsHBAYaGhiAIwpXfB4NBzM/Po7XzCf5g8SQvJ3bv3r3DxMQE+vv7MTY29qjXTUSULz/6+Xp2qvTljPDvf9ZW6OVlFcs6KbeYIUZERER3FgqFsLGxgWg0ipqaGhgMBqRSKQwNDUGv19/4d+VS+l9sTfWBs4yl7zaK+N/9rafwx9IlHbSsra1FMpnE3NwcBgYGIAjCxSwwiwWjo6P4b/71LzDjE1BTaUadzYDTWOKbvl949BOmTDmn1+vF0dERqqurH/X6iYhyrZh6c50vWy+HzHW6HgNiRDmm5Pp5IqK7kGUZXq8XW1tb0Gg0aGxshMfjweHhIbq6ulBRUVHoJSqGIAgoxiT8VCoFl80EV6EXkgeNjY1IJpN4M/cO06eGK1lgv9Vbg72kGQYhAE0yCq1al9MTO5VKhZaWFng8HiwtLcFut1+bZUlEpFTF1JuLQ+sIYECMKGeKoX6eiOg2UqkUdnd3sb+/D4fDgf7+fhweHmJpaQnt7e3o6uq6tuyMiksikYBWW159U1pbW/F/+8kr/HzrAHXOimwW2B/NubF5cIyj4wAqtRcDm7k8sRscHMQf/dEfQRRFLC4uor+//1Gvn4gol4qxN1e5ZK7T9cRCL4CUzxOUsHQQhDdUPM2BleDHE9v4et4NURRQZzNAFAV8Pe/GVxNbhV4aEdGtxGIxLC4uYnJyEiqVCs+fP4fNZsObN28AAC9evEB1dTWDYSUiEAjAarUWehl55QlKWA2KsGgFaFMxxGMRIHoKVSKCxYMgrGYjdNZKVNhs2b/J5Ymd1WqF1WpFNBpFNBrFycnJo98GEVGuZAa0HIcleEMS4sk0vCEJx2EJL9odDDyR4jBDjG7EDKf7K6b6eSKiy/x+PzY2NpBOp9HS0oLu7m4Eg0FMT0/DYrFgbGyMpVy3oFKpkEqloFIVx3dmIBAou7LXTHlPfZUd8VgEoiDA7nDAmgbc/ij66yswtX0MAFeaLufqe7yvrw9v375FZWUlFhcX8fz5c4gi97CJqDiwNxcVEx7N0o0yGU52ky7njWSV6r79v4qpfp6ICDjrD+Z2u7GzswOz2Yzu7m4YjUbEYjHMzs5ClmX09/fDYDB8+soIwIfG+sXymAUCATQ1NRV6GXmVLe+RknBaLNmfn0YkGLUq/G++1Ywqqy6vJ3aNjY2YmZlBLBZDdXU1VldX0dXVlbPbIyJ6TOzNRcWEATG6VrlnOD00O64Y6+eJqDwlEglsb2/j6OgINTU1GB0dhVqtRjKZxPLyMvx+P7q7u8suc+gx6HQ6SJJUNAGxRCIBjUZT6GXkVaa8J7PhdzkLrKHSmPcTO1EU0dzcDL/fj9PTU8iyjGAwCMu5gB0RkdKxNxcVA+Zf07UyGU5W/cUDY6teg0g8BV+ouEbJ39VD+3+xfp6IlC4UCmFubg4zMzMwmUx48eIFWltboVKpsL29jcnJSVRUVODZs2cMht1TJkOsGCSTybItg/1yvBmf99dCTstw+6OQ0/KVLDCnWYfuGkvevr+7urqywbDGxka8e/euKKeWEhERKVl5HvnQJ5VzhtNjZcexfp6IlEaWZXi9XmxtbUGj0aC1tfVCE3WPx4O1tTXU1NSwb9EjyGSIFYPT09Oya6ifocTyHpPJhIqKCqhUKuzs7MDlcmFrawstLS0FXRcREVEpYUCMrvWpEoJCHyjm0mP1/1LiATYRladUKoW9vT3s7+/DbrdjYGAAOt2Hz6NgMIilpSWYTCaMjo6WXdlcrmi1WkQikUIv41bKsaH+ZUor7+nq6sLMzAyamppgNpuxtrYGl8tVNCW4RERESseAGN2oXDOcHjs7TmkH2ERUPmKxGDY3N+H3+9HQ0IDx8fELWV+SJGFpaQnpdBq9vb0wGo0FXG3pKaYMsUAggLq6ukIvg86pqamBSqWCXq/H2toaenp6sLCwgNHRUQiCUOjlERERFT0GxOhG5ZrhVM7ZcURUXG6ahOv3+7GxsYF0Oo2WlhZ0d3dfOIFOpVJYX1/HyckJurq6YLPZCrD60ldMPcTi8fiFrEEqPFEU0dTUhO3tbdTX1yMWi8FqtcLtdjN4SURE9AgYEKNPKscMp3LNjiOi4nDtJNw2B369WYcj9y7MZjO6u7uvZHzJsozd3V3s7u6itbUVHR0dzDTJIY1Gg0QiUehlfFI6nWa/OIVqaWnB9vY2rFYr1tfX8ezZM0xMTMDpdEKrLd1+rkRERPkgyBxZQ3Qjb0gqq+w4IioOP/r5Or6ed8Nu0sGsVeHw5BTe0yi+2+3Af/V3h6+dFuj1erG6ugqXy4Xm5mYGQPJkYmIC4+PjhV7GR/n9fhweHqK7u7vQS6FrvHz5EgDgdDphMpmg1+uxvb2Np0+fFnhlRESfdlM2O5ESMEOM6CPKMTuOiJTt/CRcbSqGoF9CtcUMk9GIZT/gj6XgNH/4es80zDcajWyYT9diQ31la2lpwfz8PJ48eYL379/jxYsX2Nvbg9frhdPpLPTyiIiudW02e7sDX443w6BVFXp5RAAAbg8TFRlPUMLSQRDeUHE0aiaix5WZhGvVaxCPx+FwOGAwGmE1aBGJp+ALnfWskiQJc3NzWF1dRW9vL3p7exkMKxClJ+Ofnp7CarUWehl0A5fLBZVKhe3tbdTW1mJvbw9PnjzBysoKUqlUoZdHRHStH09s4+t5N0RRQJ3NAFEU8PW8G19NbBV6aURZzBCjgmH67N1wl4WIgIuTcPUGA/yhCESNLvu5YDOosLKyguPjY3R1daGysrLQSy5rarUaqVTq2jJWpYhEIjAYDIVeBt1AEATU19djb28PXV1dmJmZQV1dHdra2rC8vIyenp5CL5GI6ILz2eyZ87zMvy/XfPhiqJ7nf6QIyj06o5LFwM79ZHZZ7CYd6mwGnMYS2UmY3/+srcCrI6J8yUzC/YPZfYRiCZyEYkgLKqTTMgZqDHg/+wbdHWyYrxSZSZNKDYjJsgxBEPhaUbjGxkbs7+9jd3cXDQ0N2N7eRktLC/b391nySkSKk8lmr7Nd3Gyx6jVw+6PwheIMiJEisGSS8o7ps3d3eZdFqxbhNOtgN+nwcs3H8kmiMvPleDOqzDp4gnHEU2loVIBVI8MXSWBFdqG2tpYBDoXQ6XSQJOV+RodCIZjN5kIvgz7BYDDAbDbD7Xajrq4ObrcbqVQKvb29eP/+veLLcomovJzPZj/vNJaAUauCw8wpuaQMDIhRXjGwcz/newadZ9VrLvQMIqLyEJKSSMvAUFMlhmpN6HNo8O3uWjQ4K/Bq45ifpQqSyRBTKmYXFY+mpiaIooiDgwO0tLRgc3MTOp0O9fX1WF9fL/TyiIiyMtnsx2EJ3pCEeDINb0jCcVjCi3YHs8NIMRgQo7xiYOd+uMtCROdlPkudZh0aXA7UVDshiiI/SxVI6RliDIgVj6qqKgDAzs4OXC4XPB4PEokEGhoacHx8jHA4XOAVEhF98OV4Mz7vr4WcluH2RyGnZXzeX4svx5sLvTSiLGU2tKCSdT6wc35ngIGdj8vssmR6hln1GpzGEjgOS/i8v5a7LERlhp+lxUOr1SIUChV6GTcKh8MwmUyFXgbdgiAIqK6uhs/nQyAQQFtbG9bX19Hd3Y2+vj4sLCxgbGyM5dJEpAgGrQrf/6wNXwzVwxfiIDVSJmaIUZYnKGHpIJjTUptiTp/Nx+PzMdxlIaKMYv4sLTdKzhDL9J1iAKV4NDU1QZZlbGxsoKqqCn6/H/F4HEajEXa7Hbu7u4VeIhHRBU6zDt01Fh6bkCIxQ4zyPvUxE8B5ueaD2x+FUatSdGBHKVMxuctCROcV22dpuVJyD7FIJAKj0VjoZdAd6HQ66HQ6JBIJRKNRdHR0YHV1Fb29vWhra8OrV69QXV0NnY7HB0RERJ8iyBxLU/Z+9PN1fD3vht2ku1KK9/3P2nJ2u96QVBSBnUI9PkREt1Esn6XlSpZlTE5OYnx8vNBLuWJ/fx+pVAqNjY2FXgrdgdfrxfb2NvR6PXp7ezE5OYmBgQHo9XoEAgGsr69jeHi40MskIiJSPJZMlrlCTn187PTZXJQ0ciomESkdSxGUTcnliGyoX5wcDgckScLp6SmSySQ6OzuxsrICAKioqIBer8fh4WGBV0lERKR8LJksc5lJZXU2w4WfW/UauP1R+EJxxZ9k5bKksRQeHyIiouuEQiGYzeZCL4PuKNNcPxKJYHd3Fy0tLVhfX88OSOjq6sLExAQcDgfUah7qExER3YQZYmXu/KSy84ppUtmPJ7bx9bwboiigzmaAKAr4et6Nrya2HnzdpfD4EBFR4SmtQ4Usy0in0xBFHgoWo8bGRsRiMbjdbsiyfCFLTKVSobOzE4uLiwVeJRERkbLxKKjMFfukslyXNBb740NERIWn0WiQTCYLvYwLYrEYDAbDpy9IiqTVaqFWq2G1WnF0dASLxQIACAaDAACn04l0Oo3j4+NCLpOI6E5y0QKH6GMYECN8Od6Mz/trIadluP1RyGm5aCaVZUoarXrNhZ9b9RpE4in4Qg+f7FXMjw8RERWeEidNsn9Y8WtqaoIsy9je3gYAdHV1YXl5Ofv7np4eLC0tIZ1OF2qJRES3Eokn8aOfr+MHP5nF7/50Af/0387iRz9fRzSeKvTSqMSxsQDBoFXh+5+14Yuh+qKbVHa+pPH8mh+zpLGYHx8iIio8nU4HSZJgMpkKvZSsQCAAl8tV6GXQA9jtdiwvL8NoNOL09BRWqxVarRZ+vx82mw0ajQYtLS1YWVlBd3d3oZdLRHSjTAscu0mHOpsBp7EEvp53AwC+/1lbgVdHpYwZYpRVjJPK8lnSWIyPDxERFZ4SM8SCwWC2zI6KkyAIcLlcMJvN2NjYAIALvcQAoKamBqFQKFtKSUSkNLlugUP0MQyIUdFjSSMRESlZJkNMSVKpFFSqh01ipsJrbGzE8fExEokEJEmCXq+HyWSCz+eDJyhh+TCEmpZOvHv3TnGDHYiIgPy0wCG6CUsmqeixpJGIiJRMq9Xi9PS00MvIkiQJOh2/J0uBRqOBRqOB3W7H1tYWurq6UNfUiv/+919iL2lGJJ6CUavCk0o1rKvr6Olshyco4TjM4yUiUoZ8tMAhugkDYlQynGYdD+wKiAfYRETXU1qGGBvql5bm5ma43W4EAgGk02n8mzduTB2l4TRLqLNZcBpL4OVeDMeBLTTtJjG1fZoNlH2r3YEvx5th0DJbkIgKI9MCJ9MzzKrX4DSWwHFYwuf9tTyvoJxiQIyIHiQST+LHE9v45ZqPB9hERNdQWg+x09NTVFZWFnoZ9EhsNhuWlpagq6jCH04u4a+WT1FvtyAVCUAtmLMnkxNHEib2N9FWa2fTaiJSlEyrm5drPrj9URi1KrbAobxgQIyIHoRTYYiIPk6lUiGZTBZ6GQDOsnnntr34trO20EuhRxJNpPAfj9R489aNA38Yx3ERtRYtTMkwRFGEDEBKCfCE4mixqmEQktCq9dlA2cs1H74YqmcWBhEVDFvgUKEwIEZE93Z5KgwAHmATEV0iCEKhl3Ahm/fAG8Af7LxjNm+J+PHENn65G4MqGUNdhR7Hh1FsHYdRZ9ajo7oa6VQKK24/0uk0IIVweBRCm9kM4Kw0ye2PwheK8/uaiAqOLXAo3zhlkojujVNhiIiKQyabV4CMKpMGoijg63k3vprYKvTS6AEyG1MOix52owZ6MQ2zmAAgYPc0geVtNzbcXoTiSdi0AvQmM1pbWrN/z6bVRERUzhgQI6J7Oz8V5jweYBMRXSQIAmRZLshtn8/mtWoFmPRnpSh2kw4v13zwhpTT8J/u5vzGlMViwbE/AJVKBQgC4mkRK34ZXknEaJWAL4bqkNIY4IskEE+m4Q1JOA5LeNHuYEYGERGVJQbEiOjeMlNhjsMSvCGJB9hERDfQaDRIJBKfvmAOnA+aaLVamM+VyzGbt7id35jSaLWI6+04lgC1kEaFXoUGiwqinERrcxP+i9/ox+f9tZDTMtz+KOS0zKbVRArkCUpYOghys4IoD9hDjOgWPEEJx2E2eLwOp8IQEX1aZtKkVpv/zNnzQROnWYdMRzNm8xa/zMbU1/NuSMkUPOEkNGo1YjEJDjPQXWNHKCViYtOPfziaZNNqIgXj5Hai/GNAjOgj+MX0aZwKQ0T0aTqdDpIkZbOz8ul80AQ4yww7jSVwHJbweX8tP7OLXGYD6s/eHSIST8KkVaGhthId1WZoNWqIyfSFxvlsWk2kTJzcTpR/LJkk+ojMF5MoCqizGdiE+COcZh26ayw8yCYiukYmQ6xQvhxvZrlcicpsTP1f/pM+9NZa0VVjRW+9DVrN2b43MwGJlO/y5HatWmSvR6I8YIYY0Q0ufzEByP77cs2HL4bqGfwhIqJb0el0iEQiBbt9ZvOWvu4aK77b68LX8254QyIzAYmKSKbXY53NcOHnVr3mQoYnET0uZogR3eB8E+Lz2ISYiIjuqtAZYhnM5i1tzAQkKk6c3E5UGMwQI7rB5SbEGfxiIiKiu9JqtZAklrxQbjETkKg4sdcjUWEwQ4wKTqmjhTNfTMdhCd6QhHgyDW9IwnFYwot2B7+YiIjo1nQ6nSIyxKg8MBOQqPgww5Mo/wRZluVCL4LKUzFMcIzGU/hqYgsvz63xhcLWSERExeFPf/YSzV19zNohIqIbeUMSMzyJ8oQBMSqYH/18PTta+HJasNJGC/OLiYiI7iuzAfT19Bq0RosiN4CIiIgeyhOUcBzmORMVD/YQo4IotgmOTrNOUeshIqLi8eOJbXw974YoAHU2A05jiWyfGKVtABEREd1VMVT+EF2HPcSoIDjBkYiIysH5DaAKnQitSoDTrIPdpMPLNZ/i+mcSERHdVXbjRxRQZzNAFAV8Pe/GVxNbhV4a0UcxIEYFwdHCRERUDrIbQDo1/H4/QuEwAG4AERFRabhc+aNVi9z4oaLBgBgVBCc4FpZSJ3sSEZWazAaQ9zQMUaWC/+QEADeAiIioNLDyh4oZe4hRwWRGCL9c88Htj8KoVXG0cI6xvp+IKL+qLDp8q82Bf/XLZVToTYin4nAfBxFMyPi8v5YbQEREVNTOV/6c/07jxg8VAwbEqGAMWhW+/1kbvhiq5wTHPMnU99tNOjZ2JiLKk19r0sLnc+CNO4KTWBLJUBifj7ZzA4iISGE4JfHuMpU/mXMKq16D01gCx2GJGz+keAyIUcFxgmN+FNtkTyKiUiDLMg73d/Bf//Y41vaO8LNXr9HXXovOJiszc4mIFIJVFA/Dyh8qVgyIEZWJTH1/nc1w4edWvQZufxS+UJwBMSKiR7azs4O6ujqoVCrUVppRpU1itL8LMzMzcDgcEASh0EskIip7pVhFkc9sN1b+ULFiQIzoAYoprZr1/URE+ZVKpbC3t4fnz58DAHQ6HWRZhiAIsNls8Hg8qK6uLvAqiYjKW6lVURQy242VP1RsGBAjuodiTKtmfT8RUX5tbW2hqakJong21Fuj0UCtViMUCqGtrQ3T09OoqqpilhgRUQGVWhVFKWa7EeWKWOgFEBWjzBeNKAqosxkgigK+nnfjq4mtQi/to74cb8bn/bWQ0zLc/ijktMz6fiKiHEgmkzg6OkJdXV32Z4IgQKfTIRQKQaPRwOFw4PDwsICrJCKi81UU5+WrisITlLB0EIQ3JD3KdZ3PdtOqRTjNOthNOrxc8z3KbRCVEmaIEd1RMadVs76fiCg/1tbW0NbWdiX7KxMQA4DW1lZMTU3B5XIxS4yIqEAKVUWRi4qTUst2I8o1ZohRyXnMXZbrZL5orHrNhZ9b9RpE4in4QvGc3O5jcpp16K6x8AuRiCgHJEmC3+9HVVXVld+dD4ip1WpUV1fD7Xbne4lERHROIaooclFxUuhsN6JiwwwxKhn56utVqOb0xdTAn4ionK2urqKzs/ParC+dTodwOJz9/83NzZiYmEBtbS2zxIiICiTfVRS5qjhhz2Ciu2FAjEpGvhpI5vuLphgb+BMRlatoNIpoNAq73X7t7/V6PYLBINLpNERRhEqlQm1tLfb29tDQ0JDn1RIR0Xn5mpKYy9LGTFbbyzUf3P4ojFoVewYT3YABMSoJ+e7rlc8vGk6KuRmz5ohIaVZWVtDZ2Xnj77VaLTQaDcLhMCwWCwCgqakJr169Ql1dXXYiJRERla5cVpywZzDR7TEgRiUh3w0k8/VFU8wN/HOJWXNEpEShUAipVAoVFRU3Xkan00Gj0SAUCmUDYqIoor6+Hjs7O2hu5g4+EVGpy0fFSb6y3YiKGbchqSQUqoFkrpvTK72Bf64HGNwkF01IiYgeanl5GV1dXR+9jE6ng0qlyjbWz2hoaMD+/j7S6XQul0hERApRiEb+RHQRM8SoJJRqA8lCNfD/lEJmaDFrjoiUKBAIQK1Ww2QyffRyOp0OoigiGAxe+LkoimhsbMTW1hZaW1tzuVQiIlIAljYSFR4zxKhklOIuSybQdxyW4A1JiCfT8IYkHIclvGh3FOxLs5AZWkrPmiOi8nSb7DDgLCCWSqWQSCSu/K6+vh4HBwdIpVK5WCIRESlQritOiOhmzBCjklGquyxKmxRT6AwtpWbNEVH58vl8MJlM0Ov1n7ysRqNBIpGAIAiQZRmCIGR/JwgCmpubsbm5ifb29lwumYiIiKjsMSBGJafUGkgqLdCX7wEGl5VqeSwRFSdZlrG6uorh4eFbXT4TADMYDIhGozAajRd+X1tbi1evXqG5uRlqNQ/TiIiIiHKFJZNERUIp6dSFGmBwXimWxxJRcTo6OkJlZSW02rt99pnN5iuN9YGzgFlLSws2NjYea4lEREREdA1uPRLRnSghQ0tpWXNEVJ5kWcb6+jqePXt2p78TBAEmkwmhUAjV1dVXfu9yubC1tYVEIgGNRnPNNRAREd2dJyjhOMxjZ6IMBsSI6M6U0tes1Mpjiai47O3tweVy3bm0UaPRQKfT4eDg4NrfC4KA1tZWrK+vo7u7+zGWSkREZayQE+KJlIwBMSK6M6VkaHGXi4gKJZ1OY2dnB8+fP7/z3+p0OgiCgFgsduNlqqqqsLGxgXg8fudyTCIiovMyE+LtJh3qbAacxhLZao/vf9ZW4NURFQ4DYkR0b4XK0OIuFxEV2vb2NhoaGiCKd2/HqtPpIEkSZFm+8TKCIKCjowNra2vo6el5yFKJiKiMFXpCPJGSsak+ERWdzC6XKAqosxkgigK+nnfjq4mtQi+NiMpAKpWC2+1GQ0PDvf4+ExDTarWIx+M3Xs7hcCAYDEKSpPsulYiIylxmQrxVf7EnpVWvQSSegi908/cQUaljQIyIisrlXS6tWoTTrIPdpMPLNR+8IZ44ElFubWxsoKWlBYIg3OvvdTod4vH4jZMmz+vs7MTq6uq9boeIiEgJE+KJlIoBMaIbeIISlg6CDLAoTLHscvH1Q1SaEokEvF4vampq7n0dmQyx2wTEKisrEY1GEY1G7317RERUvjIT4o/DErwhCfFkGt6QhOOwhBftDpZLUlljDzGiS9ifStnO73Kd/wJXyi4XXz9EpW1tbQ3t7e33zg4DPgTELBYLtrY+Xerd0dGB1dVVDAwM3Ps2iYiofCllQjyR0jAgRnQJp7AoW2aXK/OcWPUanMYSOA5L+Ly/tuC7XHz9EJUuSZJwenqK7u7uB12PRqNBPB6H0WhEJBL55OVtNhvW1tYQiURgNBofdNtERFR+lDIhnkhpWDJJdA77UxWHL8eb8Xl/LeS0DLc/CjktK2KX6/LrR07G4TBq+PohKhErKyvo7Ox8UHYYgOzfi6KIdDp9q7/p7OzEysrKg26XiIjKm9OsQ3eNhcEwom8wQ4zonEx/qjqb4cLPrXoN3P4ofKE4v0AUQKm7XJdfP+l0Gh6vF4KoRjCt4euHqIhFIhFIkoTKyspHvV6VSoVUKgWV6uMl1VarFel0GqFQCGaz+VHXQERERFSOmCFGdA6nsNxPoRrIK22X6/Lrx2A0orqqCklRg5QUgXtzGaenpwVeJRHdx/LyMrq6uh7t+gRBgCzLt2qsn8EsMSIqJxxQRES5xgwxonOU3p9KadhA/qKbXj/BhIzPR9rwbMCFjY0NRKNRNDc3o6qq6kLplSco4TisnIw3IjoTDAYBABaL5dGuM9NHLBMQq6io+OTfmM1miKKIYDD4qGshIlKSfB1f8riLiBgQI7qEU1hujw3kr/rY68egVaG/vx/xeBxbW1tYW1tDfX09Kqtc+P9N7zGwSKRQy8vL6OnpedTr1Ol02YDY4eHhrf+us7MTi4uLGBkZedT1EBEpRa6PL7mhS0QZgizLcqEXQaRE3pCkqP5USuMJSvjBT2YhisKFx8cbkiCnZfzwe4Nl/bjd5vWTTqext7eHf/GzVUwdpVFnt6DCqLuQlViugUUipTg5OcHe3h76+/sf9XrX19dhtVphs9nw9u1bjI6O3vpvZ2dn0dzcfKusMiKiYpKP48sf/Xw9G3C7XA3C4y6i8sIeYkQ3UFp/KqXJNJC36jUXfm7VaxCJp+ALxQuyLqX0m7jN60cUReht1dhLmlFl0SMV9iMSDHCyKZGCZCZLPjadTgdJkqBWq5FMJu/0t+wlRkSlKtfHl5woT0TnsWSSiO7lfAP580GfQg0gKNb098yBn9OggT+aAr7pKcbJpkSF5/F4YLVaodM9/ntQp9Nlh2xkGuyf7yn4MQaDAXq9HicnJ48+9ZKIqJByfXzJifJEdB4zxIjoXjIN5I/DErwhCfFkGt6QhOOwhBftjrwfTGT6TYiigDqbAaIo4Ot5N76a2MrrOu7KbtJCjRQOjgMwGgwwm0wAONmUqNBkWcba2hra29tzcv2ZDDEAMBqNiEQid/r7zs5OrK6u5mJpREQFk+vjS06UJ6LzGBAjonv7crwZn/fXQk7LcPujkNNyQQYQFHP6uxTwoN2SQkpjhC+SQBpiQQOLRHTm4OAATqcTGo3m0xe+h/MBscykybv+vclkgs/ny8XyiIgKJpfHl0rb0CWiwmLJJBHdm0Grwvc/a8MXQ/UFHUBQLOnv58d7O0xarK2tIRaL4Z/8g2/jf/zFGv5iYRfuQIyTTYkKTJZlbG5uYnx8PGe3odFokEicZShYLBb4/X64XK47XUdHRwdmZmZgt9tvXW5JRKR0uT6+5ER5IspgQIwoR84HP5QQjMklp1lX0PuotH5ml13X36zVmMAXA0709fVBEAT8nXY9vtPcCbWpsixeM0RKtru7i9raWqhUues/mOkbBpxliO3s7Nz5OrRaLWw2G7xeL6qqqh57iUREBZWr40ulbOgSUeExIEb0yIq1uXsxy6S/fz3vBoArI7QLfZCT6W9mN+lQa9Vh98iHl34VamrV6H9yltXh9XoxODiYs/IsIrqddDqN3d1dPH/+PG+3qdVqcRiIYukgeOcTs7a2Nrx+/RpOp5NZYkREd1DoDV0iKjwGxIge2fngR53NgNNYIhuo+f5nbQVeXelSavr75f5mR4eHqK00IyKr8XLNhy+G6pFKp7FxIqFZSsPJeBhRQW1tbaGxsRGimJ82q2EpgX85uYOv34SgXZ6HUau+0yaKRqOB3W7H0dHRnUsuiYiIiMoZA2JEj+hy8ANA9t9M8IM7Ubmh1PT3y/3NrBUVOA0EoNLqEEio8C9+to73+ycIhGP4em+W2YREBZRMJnFwcIAXL17k5fa0Wi3+v7/cxJ8uHkGtElFj0SGUSN95E6W1tRVTU1Oorq5mlhgRERHRLXHKJNEjygQ/rPqLaT5WvQaReAq+ULxAKysfTrMO3TUWRQTDgKvjvfV6PaqrqxFNAnveAP5q0Y1EXEKTwwxRFPD1vBtfTWwVeNVE5WljYwOtra15CypF0ir89Zr3bBPFoocgp+41IVetVqOqqgoHBwc5XjERERFR6WBAjOgRXQ5+ZCiluXsueYISlg6Ctz6BKxfXjvcOxxFIAEajEU6TGkYxBbNRf68TYSJ6HPF4HMfHx3ktOzyIAJ5gDOl4DP6TEwQCAUCW77WJ0tLSgs3NzWyjfiIiIiL6OJZMEj0ipTd3zwUOEfi06/qbvWhz4O1OAK5KI7TqD3sTVr0Gbn8UvlC8JF8vREq1urqKjo6OvGSHhaUE/j8/W8bXb3exFUhj3y+i2qRBt8OMI48Hgt5y500UlUqFmpoa7O3toaGhIYerJyIiIioNDIgRPTKlNnfPFQ4R+LTr+pvJMvCDn8ziNJa4EPgqh2xCIqWJxWIIh8NwOBw5v529vT38jy+3MXmYgsNiQlyWsB+M4zAiQneagNNgxN7hMb4Ybb5zULy5uRmvXr1CfX09e4kRERERfQIDYkSPTKnN3XOBQwTu5vJ473LLJiRSquXlZXR2dubkutPpNA4PD7G3twdRFGGodGEvaUZDlQCjmIIUPIGxqhJ7pxJ2j6OorK/APxxvQ6/mGEdHR6iurr71bYmiiLq6Ouzs7KCpqSkn94eISpMnKOE4XNrHrURElzEgRpQjl4MfpejyBMUMlv3dTrllExIpUTgcRjKZhM1me7TrlGUZgUAAOzs7iEQicLlcGBwchEajwdTKHjz+U9jUaYh6LWxWC0SVjPrKSuwFYvgv/mYHnrc5kEqlMDc3h3A4jJaWlltnfDU2NuLVq1doaGiAKLJVLBF9HFtf5A+DjkTKw4AYEd3b+SECLPu7u3LKJiRSqsfMDpMkCbu7u/B4PLBarWhpaYHRaITX68X79+8Ri8Ug68yoNBug1Whgt+gBnJVS7hwdw2o0ob3aDOCsJ9jTp0+xurqK+fl59PX13SrAJYoiGhoasL29jZaWlke5X0RUutj6IvcYdCRSLm4dEtG9XTtBMSThOCzhRbuj4MGdYpl86TTr0F1jKfjjRVRuTk9PIYoiLBbLva8jnU7D7XZjamoKCwsLMJvNGB4ehsViwfLyMqanpxEOh9HZ2Ynnz5/jxVAffqW7BseRePZzM5QUkNaa0F0pYHf1PeLxs+mSgiCgs7MTTqcT09PTSCQSn1jNmYaGBrjdbqRSqXvfLyIqfZdbX2jVYtlOvM7lMWMm6CiKAupsBoiigK/n3fhqYuvRb4uI7oYZYkT0IEos++NOHBHdxvLyMvr6+u71t5mSyHA4jOrqarS3t+Pk5ARbW1vY29tDdXU1BgYGoNVezZS97nPz7wycfW7GwqeYnp5GS0sLamtrAQC1tbUwGAyYmprC4OAgTCbTR9cmCAKam5uxtbWFtjZmeBDR9dj6IvfHjOy3S6RsDIgRlbmH9jNQYtkf0/+J6FOOj49hMBhgMBg+feFvSJKEvb09HB0dwWKxoKKiAmq1GoeHhwgGg6ipqUFLS8snSxs/9rlp0Fbi+fPnWF5ehtvtRl9fH3Q6HWw2G4aHh/HmzRt0dnZ+ciJmbW0tXr16haamJqjVPNwjoqty2fqiWPpl5fqYkUFHImXjERJRmXrsHTGlDBE4vxNnVqUhIs2dOCK6QJZlrKysYGho6JOXTafTODo6wt7eHmRZhtFohMFgQDAYhEqlQk1NDbq7u2/d9P68mz43RVHEkydPcHp6ipmZGTQ0NKC+vh56vR5jY2OYnZ1FOBz+6CRJQRDQ0tKCzc1NdHR03HltRFT6Mq0vHnPidTFl6ecje4v9domUjT3EiMpUqfYzyOzEWfUaqDUaHB8fIxIOw6rXIBJPwReKF3qJRFRgHo8HNpsNOt3NJzqnp6dYWFjAL3/5S2xvbyOVSiGdTkOv16OjowMvXrzAkydPYLPZ7hUMuw2r1Yrnz58jFothenoa0WgUarUaw8PDiEQiePfuHWRZvvHvXS4XvF7vrXuPEVH5+XK8GZ/310JOy3D7o5DT8oNaXxTT8eX5Y8bzHvOYUen9donKHTPEiMpQKfczuLwTV11VBX8ggAN/GGazhTtxRGXME5TgC0nYWl7Fr3372ZXfx+Nx7O3tYXd3F4lEAiqVCmazGdXV1XC5XNf2A8s1QRDQ0dGBUCiE2dlZuFwuNDc348mTJ9jd3cX09DSGhoauLYsUBAFtbW3Y2NhAV1dX3tdORMr3mK0viu34Ml/ZW0rst0tEZxgQIypDpdzP4Lr0/6TagJicRpc6BHUyCqA47xsR3c/5Ep6TUAQ6EdhR7eDL8Wbo1AKOjo6wuroKv98PrVaLqqoq1NXVwel0QqVSRomP2WzG+Pg4Njc3MTk5ib6+PjQ0NMBoNGJychJPnz6F0Wi88ndVVVXY2NhAPB4vSECPiIrDY7S+KLbjy1yUjF5Hif12iegMA2JEZajU+xlctxP328ON+EdDtVhbfg+r1Yr29nYIglA0TV+J6P6yTZONWpgRh85YgZ++2cXW5iZGTAFotVo0Njair68vpyWQDyUIAlpbW+FyubCwsAC73Y62tjY8ffoUb9++xZMnT1BZWXnlb9rb27G+vo4nT54UaOVEVA6K8fgyn9lbSum3S0QfCPLHmk8QkSLkImjzo5+vZ6fqXN4RK5VJjN6QdGUnTpZlbG9vY2vvAO/ilZjcCii+6SsR3Z8nKOEHP5mFKApQJyIIhUJIJBIIpQQYDSb8d/94GM0ue6GXeWeyLGNnZwf7+/vo6+uDwWDAmzdvUFtbi/r6+iuXn5iYwNOnTz/aN42I6KGK9fjyumNGIip9DIgRKVguJ/VE4yl8NbGFl+eu+0UZBYT+hz9/j383vYWaSjOqbJaiOWAjortZOgji//x7b2AWEohFQ5DTZ5Mi04IKJ5KM/+pXm9DfaIfJZIJOp1NsdthNYrEYFhYWYLFY0N7ejsXFRajVanR1dV24L8fHx3C73ejr6yvgaomo1JX78SURFRcGxIgULB+7bOW4I5bNGBEAdTIKq9UKUaWCNyRBTsv44fcGy+axICpVqVQKbrcbC2s7+H/PRWE2GeGymbK/955GEU+m8E/+Zj10chyRSASxWAzAWZmh0WiE0WiEyWTK/m9RVOZwblmW4Xa7sb29jSdPniAQCMDn8+Hp06cXeqBNTU2hv78fer2+gKslonJQjseXRFR82EOMSKHyNamnHPsZnG/6qlV/ODFUatNXIrq9aDSKra0t+P1+1NXV4bufjWNHtYWv591QhaQPmwvRBD7vr0Vfe9OV60in04hGowiHwwiHw/B4PIhGo0in0wAAvV5/IVhmMpmunfKYL4IgoK6uDg6HA+/evYNer0d9fT0mJycxNDSUDYB1dHRgZWUFAwMDBVsrEZWHcjy+JKLiw4AYkUIV26SeYlKMTV+J6GayLMPn82FrawuiKKK5uRnd3d3ZksG7Nk0WRREmkwkmk+nK72RZhiRJCIfDiEQicLvdiEQiSCaTAACNRnMlWKbVavNSiqnT6TA8PIzDw0Osr6+jsbERMzMz6O3tRUVFBWw2G9bW1hCJRK6dSElEVGgcdkRE+cSSSSKFOt8I+vwBAcv6HkexNn0log+SySR2dnZwcHAAh8OBpqamj5YD5qOEJ5FIZINlmX8lSQJwFmi7HCwzGAw5CZYlEgm8f/8e6XQasVgMzc3NqK2txenpKTY3NzE4OPjot0lEdF+57JtLRHQTBsSIFIxBm9xh01ei4hUMBrG5uYloNIqGhgbU1NQotr/XealUCpFI5EKwLBqNInMoZjAYrvQte2gpptfrxfLyMmRZhsvlQnt7O968eYOurq5rM+CIiAqBx7xEVAgMiBEpGIM2ucemr0TFQZZlHBwcYGdnB3q9Hi0tLbBarYVe1qORZRmxWOxKdlkqlQJwVop5PrPMZDJBo9HcKrssmUxicXERBwcHsNvtaG9vx8bGBurbe1iaREQFx6oIIioUBsSIHigfvQ4YtCGiciVJEra3t+H1euFyudDY2AiNRlPoZeWVLMvXlmLG43EAgEqlujIV87pSzOPjY0xNTSEhC5jwabEZ1SKeFliaREQFtXQQxO/+dOGbYUcfsn3jyTTc/ih+57f70F1jKeAKiahUsak+0T3ls9cBJ/VQrimtia3S1kP5Jcsy/H4/Njc3kUql0NzcjI6Ojrw0plciQRCg1Wqh1WpRWVl55feZUsxwOIzT01O43W7EYjHIsgxBEC6UYo6Pj+P//sez+Mu1EzjMOrQ21OI0lsDX824AYGkSEeUdhx0RUaEwIEZ0Tz+e2M72OqizGXhCQUVJaU1sc7EeBteKRyqVwv7+Pvb29lBRUYHu7m5OQ7wFlUoFi8UCi+VqBoUsy4hGo9msstXdI8wfSrCbtEgEj3HiVcFVUwPgbArnF0P1fJ8QUV5VWXT4Vrsjexx9uYcYP5OIKFcYECO6B09Qwi/XfLCbPmRuZf7lCUVhMfhxN0oL7D7mepQW7KObRSIRbG5u4vT0FPX19Xj27BlUKj5Hj0EQhGw5JQDEdEFojGE02wxQoQara2uQJAmu2nocnJ6V5/Ozk4jy7cvxZgBnx9FufxRGrQqf99dmf05ElAsMiBHdw3E4jkg8hTqb4cLPrXoN3P4oTygKgMGPu1NaYPf8emx6FVLJOKwaAUmtiF8sH+HzHieqrQaIoghRFD9ZPqe0YB9dJMsyPB4Ptre3oVar0dzcjJ6enrIti8yXy6VJnZ2d2NrcxPv1TVRXuViaREQFYdCq8P3P2vDFUD375hJR3jAgRnQP7HWgPAx+3J3SArvn15NOJSBJEmRZhiqVxlE4idcLy2iwqJBOp5FOp3F5JowgCNmf+WNpfD0ThEoQIKjigMbCLE6FSCQS2NnZweHhIZxOJwYGBqDT8bnIl+tKk8zOWnj2PTCF9+HdtcP55EmBV0lE5Yp9c4konxgQI7oH9jpQFqVlOhULpQV2L69H+02QxBuSUK2V8a2RgVs/j0sHQWiX5lFr1UOjErJZR8ziLJzT01Nsbm5CkiQ0NDTgxYsXzAYrkOtKk/7T5x3oEA6wtLQEt9uN73znOyxbJfoG2zGUFj6fRJTBgBjRPbHXgXIoLdOpWCgtsPuY6zkLrqkRjKcUEewrV+l0GgcHB9jZ2YHRaERra+u1jd8pv24qTQoG6zA/Pw9BEPD7v//7+LVf+zXYbLZCL5eoYNiOobTw+SSiywT5cs0JEd2JNySx10GBeYISfvCTWYiicOE58IYkyGkZP/zeIJ+bG0TjKXw1sYWX5w4OXxTw4PAx1/Ojn69ny2gvB9dYRptbsVgM29vb8Pl8qK2tRUNDA9Rq7sEVA7/fj6WlJdTU1ODVq1cYHh5GR0cHs/moLPF7pLTw+SSiyxgQI6KSwIOch1FaYPcx1qO0YF+pk2UZJycn2NzchCzLaG5uhsPhYCClCPl8PmxsbKC7uxt/+Zd/CZvNhm9961sMalJZ4WZbaeHzSUTX4ZENEZUElrA+jNKa2D7GejixKj+SyST29vbgdrths9nQ09MDg8Hw6T8kxXI4HEgmk1hdXcXnn3+OV69e4euvv8Z3vvMdVFRUFHp5RHnBdgylhc8nEV2HATEiyrtcNDNl8INuorRgX6kIh8PY3NxEKBRCfX09xsfHIYpioZdFj8TlciGRSODdu3f41re+hfX1dfz5n/85nj59ira2Nmb+UclT2uAZehg+n0R0HQbEiChv8tHMlMEPotyRZRlHR0fY3t6GVqtFc3Mzm66XsIaGBiSTSSwtLaGnpwd2ux2/+MUvcHh4iGfPnkGj0RR6iUQ5o7TBM/QwfD6J6DrsIUZEecM+X5QLHJ+ee/F4HDs7Ozg6OkJVVRWampqg1XI3vVysrKxAEAR0dHRAkiT88pe/RCwWw4sXLxgQpZLGXpSlhc8nEV3GgBhRiVFqcOB8M1OHUQMpHoder2czU7o3jk/PvUAggM3NTcTjcTQ1NaG6upqlcmVIlmW8e/cOZrMZzc3NkGUZ8/Pz2NzcRE9PD9rb2/m6oJKmhMEzSj2+K0ZKeD6JSBlYMklUIpQeHDjfzFQQBJwGAtBqtWxmCh7k3tePJ7azGYd1NgNOY4lsKQQzDu8vnU5jf38fe3t7MJvNaG9vh9lsLvSyqIAEQUBvby9mZ2ehVqtRX1+PgYEBOBwOvHnzBl6vFyMjI8wapJJVyHYMSj++uwulHO+wvQYRZTAgRlQilB4cuNzM1FpRgdNAAEmNUVHNTPN5sFZKB7n55glK+OWaD3bTh4PazL8v13z4YqieB7t3FI1GsbW1Bb/fj9raWoyOjkKt5mECnREEAYODg5iZmYFGo0F1dTXq6upgtVoxMTGBn/3sZxgZGYHdbi/0UolKSq6P7/Jx3MPjHSJSKh7pEpWAYggOXG1mqoUvFEBCJeLvPq0r+PoKcbCm1CCmUnZwP4bj0x+HLMvw+XzY2tqCIAhobm5Gd3c3y9/oWoIgYGhoCNPT01CpVHA4HDCbzfjOd76DN2/eYGpqCi0tLejs7ORriOgR5PL4Lp/HPUo93iEiYkCMqAQUS3Dgy/FmAGcHcW5/FAaTCf3mVPbnhZTvg7ULB7mms+y4Qgcxi2kHl+PTHyaZTGJ3dxcHBweorKxEX18f9Hp9oZdFRUAURQwPD2N6ehpqtRoVFRVQq9UYHR3F+vo6NjY2cHx8jOHhYeh0hf/eISpmuTy+y9dxTzFs2hJR+RILvQAierjzwYHzlBYcMGhV+P5nbfjh9wbxO7/dh3/+vxrB339iQSIWhicoYekgCG9Iyvu6Lh+sadUinGYd7CYdXq75crKmzEGuVa9BMpXC0dER4pIEq16DSDwFXyj+6Lf5KZmDY1EUUGczQBQFfD3vxlcTW3lfy6dkMg6PwxK8IQnxZBrekITjsIQX7Q4eXN8gFAphfn4e09PT0Gg0GB8fR3d3N4NhdCdqtRojIyN49+4dQqEQgLPssfb2dgwPDyMajeLly5fwer0FXilRccvV8V0+j3vOH++cV8jjHSKiDGaIEZWAq+WIGpzGEjgOS/i8v1ZxwYHzzUyFlnb88z+cxm7cWLCspEJk2F3OcHI6nTg5OUEwARhN5rwHMYtxB/dyxqFRq8Ln/bWKyDhUElmWcXh4iJ2dHeh0OrS0tMBqtRZ6WVTkNBoNRkZG8Pr1awwNDcFgOPv8dDgc+Na3voWZmRksLCygpqYGXV1dLKEkuodcHd/l87iHGd1EpGQMiBGViGINDvze3BEm3AlUVyRQZzMVpK9EIQ7WrjvIlfUWhKIhdKiCUCejAD6sJdd9vYql7Pa8TMbhF0P1HJ9+DUmSsLOzA4/HA5fLhaGhIWg0mk//IdEt6XQ6PH36FG/evMHIyEi2RFKv1+P58+d4//49PB4P/H4/nj59ykxEyqti6Id5G7k4vsvncU+xbdoSUXkRZFmWC70IIno83pD0KMGBfBxIeoISfvCTWQiQIUghOKuqAJzdBzkt44ffG8zbgdKPfr6e7aVx+WAtV4G5aDyFrya28PJcz64X7Q78o6FarK8swmAwoKGlDf/z1G7O+3plngtRFC485oV4Luhh/H4/Njc3kUwm0dTUhKqqKmbnUE4Fg0EsLCxgdHT0StB1b28P6+vrEAQBXV1dqK6uLtAqqVwUUz/Mu3is47uMfB733HS8U+zPCREVPwbEiOiCfB5ILh0E8bs/XUCdzQCtSgC+OWmPJ9Nw+6P4nd/uQ3eN5VFv8yaFPFi76SDX7Xbjf/jz95j1q1FdYcz5AWshgoL0OFKpFPb397G/vw+LxYLm5maYTKZCL4vKiN/vx9LSEsbGxqBSXfzMPD09xdzcHFQqFSoqKtDd3Q1RZBtbyg1+l91OIY57HjuoR0T0UAyIEdEF+TyQVGJWkpIO1jxBCf/037xBKBREtdUAyzd9n3L1+HAHt/hEIhFsbW0hEAigrq4O9fX1V4IRRPni9XqxubmJkZGRKwGvRCKB2dnZ7P8eHByE0WgsxDKphCnxuELplHTcQ0SUb+whRkRZ+W6srsS+Eucb/hfacTiOaFJGY00V1MKHvYtc9fViT67iIMsyvF4vtre3IYoimpub8eTJE5ZFUsE5nU4kk0m8ffsWQ0NDF16TmSb8q6urODk5wZs3b9DW1oaampoCrphKTTH2wyw0JR33EBHlGwNiRJRViAPJYh0GkA+FmszEg2NlSiQS2N3dxcHBARwOB/r7+7NNzImUoqamBslkEnNzcxgYGLgQFBMEAZ2dnfB4PFhZWcH+/j68Xi96e3tZQkmP4rG/N0ulMT8REV2PATEiyipEAIZZSTdTYgYd5V8wGMTm5iai0SgaGxvx/PlzBg9I0RoaGpBIJLC4uIienp4rv6+qqoLJZMLs7Cz0ej0mJiYwMDDAvnf0YI/1vVmqjfmJiOgi9hAjogvYjFZZ2NerPKXTaRweHmJnZwcGgwEtLS2wWPIzYILosSwvL0MURXR0dFz7+1QqhYWFBQBn/fAaGxtRX1+fzyVSCXqM700eCxERlQcGxIjoAgZglKmUmt6WewnKx+6/JEnY2tqCz+dDTU0NGhoaoNFoCrRSooeRZRnv3r2D2WxGc/PNZfDb29s4PDyEXq+HLMvo6+vjcAh6sPt+b7IxPxFR+WBAjIiuVUoBGFKGYi1BeawA3k33/z9/1oRY+BRbW1tIp9NoamqC0+lkk3wqCbIsY3Z2Fk6n86PZX36/H+/fv4fL5cLh4SH6+/sRg7asg+dUGEsHQfzuTxdQZzNAq/5Qnh5PpuH2R/E7v92H7hpm7BIRlQIGxIiIKC+KrQTlsQN4V+5/RIL7JIyRKuB/PVKD5uZmGI3GHNwTosKSZRkzMzNoaGhAdXX1jZeLx+N4+/Yt9OYK/JuZfawGRaRFbdEEz6k0MEPscZR7NjgRFQc21SciopzzBCX8cs0Hu+nDBMvMvy/XfPhiqF5xB8w/ntjOBrDqbAacxhLZRs13DeBl779RC70cRyQYQTqRQKVBi72kDlWNbTAalXX/iR6LIAgYGhrC9PQ0VCoVHA7HtZfTarUYGxvDD39vAq/cCVQa1NDLEYgw3/u9R3RXHGjzMMWaDU5E5YljqoiIKOeOw3FE4ilY9Rf7YVn1GkTiKfhC8QKt7HqXA3hatQinWQe7SYeXaz54Q9KtryudTmN+dRNbewfYXV/B1tYW9AYDqqurUW23IhpPK+7+Ez02URQxPDyM1dVVBAKBGy/nDcWxGhRRbTVAK8dhMRmQjp7CqhXv/N4rJ56ghKWDIB+fR/LleDM+76+FnJbh9kchp2V83l+LL8dv7oVHZzKbSaIooM5mgCgK+Hreja8mtgq9NCKiK5ghRkQEpvbnmt10VvZ0GktceHxPYwkYtSo4zNoCru6qTACvzma48HOrXgO3PwpfKP7R10kymcTa2hpWV1dxeHgIXzgOtVCDqvom1DsrspdT6v0nygW1Wo3h4WFMT09jYGAAZrP5ymfvcTiOQCgKIyQYDQZEIhHo9XqEImH4U2p4gxI/o89hNk5uGLQqfP+zNnwxVM9+qndQjNngRFTeGBAjorLGk4n8KLYSlPsE8GKxGJaXl7GxsQFJkqBWqxEKhdDQ0IB/OD6OyvljfD3vhjckKf7+0wcMlj8urVaL4eFh/HLyNRaTDkxtn2Y/e4frTWhVnUKFFHTmCtisBkCWEQwGEU7K0KjS2N9YQrtz6E7TV0v5OXzM0m66ymnWldxrJpceuplERJRvDIgRUVnjyUT+ZEpNXq754PZHYdSqFFuCctsAXjAYxOLiIra3tyHLMhoaGlBVVYWdnR3Y7Xb81m/9FkwmEwDgy/GzqWTFcP+JwfJc0uv1WEw48HtTG2hy2eEya7DvPcEfePz4+6NN+I2n1WfvPVGEVa+BJOqQVKUxaEuiyqLH5OQkent7YbPZPno7pf4cMhuntBVjILfYssGJiDhlkojKFidJFYY3JBVFCUo0nsJXE1t4ee5k+kWbA7/VbsTW+ircbjc0Gg1aWlrQ2tqKxcVFbGxsoKmpCaOjo9Dprr9v+bj/xXgipTTFNhW1mGQ+e9OpJNKRACAIcDqdOI3LkNMyfvc/6cMfLxxcfO+1O/CfP2vCwd42Dg8PAQAulwutra0QBOHa2yn153DpIIjf/ekC6mwGaNUf2gLHk2m4/VH8zm/3obvGUsAV0n0UeyC31N93RFRamCFGRGWLqf2FUSwlKJkeMn//aR3er+/g2L0NaX8dbwMmtLe34/nz54jH45iensYf//Efo6OjA9/73vc+WcqVy/tf7CdSSsHMm9z68NlrhMqiRzgchv/kBIJKg9OUGmEpdWP/pra2NrhcLszNzeHo6AjHx8cYHByEVnsx8+TCc2jSIp1Ol9xzyGyc0lTsmevFlA1ORMSAGBGVLZ5M0E2SySS2trawsrKCQCCAyspKPO3uRFPT34RKpYLf78df/dVf4eTkBD09Pfgbf+NvQKUqfMCp2E+klILB8ty6/NlrraiA1WrFwUkYkCLYWp6HLlaLuro6OK/JcIqk1bA19yJ8fICw7wAvX75Ef38/7HZ79jKH/jBOghFY1SkcRWRYLRboDYaSeg6LrTcjfVopBOM5kICIigkDYkRUtpR8MsGSt/yLx+NYWVnB+vo6otEoqqur8fTpU9TU1EAQBMiyDI/Hg+npaUSjUfT39+PXf/3XbyzXyrdSOJFSCgbLc+umz97TRBqfD7Xiu99uweHhIWZnZyGKIurr61FVVYVYMn0lA3K0oQo9mmO8fv0aLpcLRqMRPp8PwQRg0AhQ682osn4IbJbac8hsnMdV6O/eUgrGF0s2OBGVNwbEiKisKe1kgiVv+RUKhbC8vIytrS0kk0nU19fj29/+NhwOR/YysixjZ2cHb968QTqdxtDQEJqamhQTCMsoxhOpQp983kTJwfJS8bHPXlEUUVtbi9raWkiShL29PWxsbODPdtOYPEiiusJ4lgEZjeNP3h3hpFaLQV0Us7OzqKqqwmeffQaj0YgN+ayXkfBNc/5SfA6ZjfM4lPLdy2A8EVF+sak+ERGU0+idzWjv7zbBFVmWcXJygqWlJezt7UGlUqG5uRmdnZ2wWC6WZqXTaayvr2N+fh4ajQbDw8Oora1VXCAso5iGRCjl5PNjrh2qoLA1loLbfvZ6ghL+63/1GlIsCpMqjUQiAbVajZishkanxz/7x8PQI4HXr18jEolgbGwM5go7n8M8U2qQ+1OU9N2rpLUQEZU6BsSIiBSimAIaSvKp4Iosyzg4OMDS0hKOjo5gMBjQ1taGjo4OnMZx5eQtmUxieXkZi4uLMJvNGBkZgdPpLPC9vJ1iOZEqlnUCygmWl7sLExVVAsLhMKKxGBLJNIJpNf6bfziEntoKyLKM5eVlvH//Hh0dHejr64MvHOdzmGPFEOS+idK+exmMJyLKH5ZMEpFiFetO830VY8mbElzXSP6P5tw49h1j2HiCk5MTWCwWdHR04LPPPoNarUYknsT/dOnk7VlzBZ4ag3DvbqGyshK/9mu/BpvNVui7dydKKwG+TrH1OmMfHGW4XEpmMpthMpvhOY1CF41iZ+Udkl4zGhoa0NXVhbq6OvziF7/A/v4+fuVXfuXa5vz0eIp5oIfSvntZBktElD8MiBGR4hTzTvNDKLF3iNKDkueDK3aDGqenAUSCQUiRJH4R0eJv/70n+PVfP+tJdN75k7dqkxp7nmP8zzsH2Gky4p9+8V1YrdYC3aOHKYYTKaWdfFJxuKmv20k0gc/7G/Gbn7UhGAxid3cXS0tLcDgc+M53voPV1VX89Kc/xbe+9S00NjYCUP7nWrEptiD3ZUr87gUYjCciygcGxIhIcYp5p/khlNTIWwlByductO56/Dg89sOQiuI4FT+bSFdXjxqNDlvHEbgTBtRFLp7kZE7erBoRyeAxjmIx2EwmWCuqcSiLiIvFfwKi5BMppZ583hWDKvn3qQxIi8WCnp4eyLIMr9eLpaUlJJNJDAwMYGJiAktrm9gQ6/By4zgnn2vl+poo9iC3kr57iYgovxgQIyJFKfad5odSSslbIYOSHwvG6TUifD4flpaWsL+/j0haBb1ogbmiCgYhCVmWEQxHsBOO4TgSx7/4+Tr+1bTmwt+vbLuxd+iFVZWEzWpGVVUVVGo14sl0UZy8FbtiP/lUQrC4XN02A1IQBFRVVaGqqgrJZBL7+/toaGjAV1NuvDk+RntdNepsxkf7XCv310QpBLmV8t1LRET5xYAYESlKse80P5QSSt4KHZS8EoyLxvHvpjaxsryMEVMAZrMZbW1tePbsGbRaLRLfNGiPphJw2SuwtHuC/UAETQ4Tmh2ms5PeOTf8fj8GNB54Q3FYjSZYLQ44rB9eZ8V08lbsivnks1wzWJXkLhmQarUaTU1NMFS6EFyQUW09xbF7C/HgWb8x4OGfa+X+mij2IDegjO9eIiLKPwbEiEhRSmGn+TEUsuStkEHJTDCuQqeCKh7GttuHtJyGIOiwETbi+9/9VVRXGCCKIpLJJNLpNP7RUC1CsQT+bHYbEV8Ep3EZdTY96vVJqJCGNhVDMhTAX73341d+sxW/3teF6MuzE1hBFIvy5K3YFevJZ6GDxXR/x+E4Yimguc4FFaqwsrKC5eVlVFQ6EBX19/5c42viTDEHuc9Tcrk5ERE9PgbEiEhRSmGnudjlMyiZTCZxenqa/W/pMIgDbxg1Fi2gEqDX6yHLMnSCiNNECmt7h0hH9UilUkin04jEk/h6OYi5IwmhcBRiPAUhlUadXoNYNIrNrRB0Wi1cDiuCKTVsrkao1eqSOXkrdsV28lnuGazXKZa+WZc/17q6urC9vY0jfxCiKoq9tfeoMXSjoqLiTtfL18SZYg1yExFReWNAjIgUh8GKwspFUFKWZUQikWzgKxgMIpVKQaPRwGKxwGq1wuVyoaZFwB/vz0EUBVSadai02wEAh/4wEJOgTsZwehqH0+lEdXU1/uXMIWb9YdgrbTCpAeiteL19jLVjCe0VKlRVVcFkMsEbjsOkkrPBPJ680X0wg/WDYuubdd3nmtHuwqHbizGHCEghvH37Fnq9Hq2trXA6nRAE4ZPXy9fERcUW5CYiovLGgBgRKQ6DFYVxPtPjIUHJRCKBYDCYDX5Fo1EIggCj0ZgNfHV0dEClunrSbDDg2mBcQErh84FGfPezNqRSKXi9XkwvrOCPpo6g1ahhVmlwIqeQCnlhElIIpdRQWezQ6PXwhuM3BvNK/eStWLJ3igUzWD8oxr5Z132u/aPnHWhO7kCtVsNut+P09BT7+/tYW1tDQ0MD6urqIIrijdfJ1wQREVHxEmRZlgu9CCKicqWEgMXHMj3C8eSNQcnbZH1ZrVbo9fpbZVpkROMpfDWxhZfn1vPimsyTpYMgfven86gyqpFKxhHw+2Gz2aDSGfF2148aqx6iINz496Ws2LJ3isltX5+lzBOU8IOfzEIUhQufC96QBDkt44ffG1R0IMgbki58roVCIbx+/RqCIKC9vR1utxt6vR4GgwGHh4eoqqpCc3MzNBrNtdfH1wQREVFxYkCMiKgAlBSw+NE3UxrtJt2V7IZMpkcikcgGvc5nfZlMJlitVlgsFlgslmuzvu7r8knrZZ86Kf8nf/sJZBlwmLWQZRQ88JhPt3lO6WE+9fosZWfB6AXU2QzQqj9kT8WTabj9UfzOb/ehu8ZSwBXeXSAQwLt37yCKIurr66HVarG2tobm5mYIgoDt7e3shFuDwXDtdZTza4JupoSNLyIiuh5LJomICkAp5UZXJqTJMmw6EbFoGn/2dgut8MKskaHRaLIZXzU1NdDpdHfK+rqPT5UzfqpUqctlUVTgMV849e5mj3liWurlth9Tin2zKioq0N3djdXVVZycnECj0eDZs2dYW1tDMBhEf38/4vE43r9/D0EQ0NbWdqUBvxJeEwy+KOcxKMfvHyKiYsOAGBFRnikpYHF5Qtrx8fFZ5pdWjeOUBrVtXeits+VlLffxqV5nSgk85hOn3l3FE9PHVap9s+x2O1paWrCzswOz2YzZ2Vk8ffoUsVgM79+/h8Viyf7/9fV1RKNRtLS0oKqqKucbBJ/C17jyHoNy/P4hIio2DIgREeWZkgIWlzM97A4HgLPSH6tRRrX1+tIgpfjYAAYlBR7zqRSzdx6KJ6aPr1SnAVdXVyOVSuHw8BCNjY2YnJzE0NAQRkdHcXBwgImJCbS3t2NgYADxeBxbW1tYX19HfX096uvrP9qAP5f4GlfWY1Cu3z9ERMWmMN/aRERl7HzA4rxCBCwymR7HYQnekIR4Mg1vSMJxWMKLdkfRHLA7zTp011gurDcTeLTqLzbCtuo1iMRT8IXi+V5mXpTKc/pYLp+YatXiWfDXpMPLNR+8IanQSyxKmWD0D783iN/57T788HuD+P5nbSWRjVRbWwuHw4GDgwP09fVhZmYGp6enqK2txbNnz+Dz+TA9PY1kMonOzk48e/YMsizj1atXWF1dRSKR+PSNPCK+xpX3GJTr9w8RUbFhQIyIKM+UFrD4crwZn/fXQk7LcPujkNNySWR6KCnwmG+l+pzeB09Mc+u6YHQpaGxshNlsxu7uLkZHR7G0tISDgwOo1Wr09PSgu7sb8/PzWF1dhSAIaGpqwosXL2CxWDAzM4OFhQVEIpG8rJWvceU9BuX8/UNEVExYMklEVABKKjf6WNlhMSvVPke3odTntBDNrllCSvfV2tqK5eVlbG9vY2xsDHNzcwiHw2hra4PZbMazZ8+wv7+PV69eobOzE06nEy6XCy6XCycnJ1hcXLyxAf9j4mtceY9BOX//EBEVE0GWZbnQiyAiKlfekKSogEWpicZT+GpiCy/PNVl+UWaNppWg0M2uf/Tz9WxvocsnpuXSX4nuR5ZlvH//HkajEc3NzVhfX0c4HEZ/f3+2X1gymcTS0hLi8Th6enqg1+uzfx+JRLC+vo5IJJLTBvx8jSvvMeD3DxGR8jEgRkREJa9YAo+FyKDKh0KfqPLEtDgo9fUvyzLm5uYAvQU6qxOpiB8hrxtDQ0PQaj9kHp2enuL9+/eoqqpCa2vrhcBXpgG/1+tFQ0PDozfg52tcuY9BsXz/EBGVIwbEiIiICqzQGVS55AlK+MFPZiGKwoWTQW9IgpyW8cPvDebtJJEnpsqk9Nd/JJ7Ej19t44/frEMWNbCZjRiuN6Ff58fY8CDMZnP2srIsY3d3F7u7u+ju7obdbr9wXalUCnt7e9jb20NVVRWam5uh0Wgu3+S98TXOx4CIiG6PTfWJiIgK7McT2/h63g1RFFBnM0AUBXw978ZXE1uFXtqDKanZdak2gC92Sn/9/3hiG18vuFFZUQGLmEQymcBfrJ5gMeXE/Pw8vF5v9rKCIKCxsRFjY2PY29vD27dvIUkfJhweR5KIaivR0T/8yQb8nqCEpYPgnSYk8jXOx4CIiG6PTfWJiKgoKLWc6qE8QQm/XPPBbtJl71fm35drPnwxVF/U91dpza5JWZT++r+8PtmkhdfrhVVjxNRWAP/gi0Fsry0iEomgqakp+3cajQYDAwPw+/2YmZlBhaMavzgAfrl+OQtuFFIkiKWlJQBnjfy1RrOiM+aIiIhKBQNiREQFUKrBnVxQejnVQ2UyqOpshgs/t+o1cPuj8IXiRf0a4bS13CmFzxGlv/4vr08QRTidTuwfHCGsMsAfTWF4eBhLS0t49+4denp6LvQOs9lseP78Of7ZT1/jf1n0oN5RgTqbEaexRPY98f3P2jA8PJxtwP+vZ72Y8QIumwl1NsOVyxIREdHjYECMiCiPiim4o5ST7Uw5ld2kK8mTw3LIoPpyvBnAWcaP2x+FUavC5/212Z/T3RTT58inKP31f936BFGE1lyB0IkfeiEBQRDw5MkT7O7uYnp6GkNDQ1CrPxxie0NxLJ6k0eSyQwp4kdKLcJqNAC5mwRmNRriaO7EzGYZZE4U6FYNWrVdUxhwREVEpYUCMiCiPiiG4o6STbaWXUz2GcsigMmhV+P5nbfhiqL4oml0rJRh8k2L4HLktpb/+b1rfSTSB3xxqwf76EqqtQzAYDGhoaIDRaMTk5CSGhs5+5glKeL11gkA0Abs2hUQiAZ1en72uy1lwx+E4okkZdc5KaNUfWv0qJWOOiIiolDAgRkSUJ8US3FHSybbSy6keS7lkUDnNOkU/X0oKBt+kWD5H7kLpr/+PrU9ONuDNmzcYGRmBTqeD3W7H06dP8XJqBvPRCrxxRxCIJrB2eIo9MYXRFhdE8SzQdV0WnNIz5oiIiEoJA2JERHlSDMEdpZ1sl8vJYbFlUJUqJQWDb1IMnyN3pfTX/0fXpzWiv78fMzMzGB0dhUajgdFoxEq6Cn/wdh3VFUbYNcChFvCEgWWfhAG94cYsuExG2r9/s49wLIFKkw7xVFoxGXNERESlRPz0RYiI6DGcD+6cp6TgTuZk26rXXPi5Va9BJJ6CLxTP63oyJ4fHYQnekIR4Mg1vSMJxWMKLdkfJnRw6zTp011hK7n4Vg8vBYK1ahNOsg92kw8s1H7whqdBLBFAcnyP3pfTX/03rs1gs6OnpwevXr5FMJuEJSni16UeTyw5tWoKANNptajTajTgOx7HlC0NOy9dmwUXiSSRSaUQTKbw/COKv17zY9Ibx3Sc1ismYIyIiKhXMECMiyhOl98oBlJmRpfRyKioNxZJ5VQyfI+WooqICHR0dmJmZgbmhC5F4CnaTFscpAUYVYDWb4HAaseUL43//WRtGWyqvfa5+PLGNP3t/iBanCV0uC47DcYTjCWjUgmLKdvNB6X38iIioNDAgRkSUR0oP7ijxZFvp5VRUGpQYDL6J0j9HypXD4UAqlcLU+0V4QxJWjoJIJhJQi3E0OtWoTMdhM2huDIZdV7JuNWjgDUlF2x/uroqhjx8REZUOBsSIiPKoGII7Sj3ZVnpD9mLDDIyLlBgMvkkxfI6Uq+rqaixN7SMQOkE8BYiyDLVGi3VvCJaQGv/bb7fe+FwVS5ZiLhVDHz8iIiodDIgRERWAkoM7PNkubczAuJlSg8E3UcLnCAOrF3mCEuY9CXRUGbHn9SMiayDLgFYlwqBR42/31tz4t8WUpZgLShvqQkREpY8BMSIiupYSTrbp8TED42YMBt8eA6vXy2Z5OSrgsuohqzSIJ9MQBAEn4TjC8dSNf1tMWYq5wAw5IiLKN06ZJCIiKhPFMkmx0JQ+7VAJMoFVURRQZzNAFAV8Pe/GVxNbhV5aQZ3P8tIbDDBo1agwahFPpW+V5fXleDM+76+FnJbh9kdvnEZZikp5gioRESkTM8SIiIjKBDMw6DGwtO1mD83yKucsxXLPkCMiovxjhhgREVGZYAYGPYZMYNWq11z4uVWvQSSegi8UL9DKlOExsrzKNUuxnDPkiIgo/5ghRkREisEG3bnFDAx6DOXe/P1TyjnL66H42BERUT4xIEZERAWnlAbd+QrIFTLwV2yTFEl5lBJYVXoAnYNJ7o+PHRER5YMgy7Jc6EUQEVF5+9HP17OTDy+fXOdj8mG+AnJKCfwBgDckMQPjESk9OPPYovEUvprYwstzr+UXeXotK+l9RERERMWLATEiIiooT1DCD34yC1EULgQSvCEJclrGD783mPMAQ74CcoUO/NHjK/fgTCECq3wfXVRuwVgiIqLHwpJJIiIqqEJPPszXxDxO5itNP57YzgZn6mwGnMYS2VLCcgjO5Lu0je+jD8o9GEtERPRQnDJJREQFVejJh/mamMfJfKXncnBGqxbhNOtgN+nwcs0Hb0jKyW0uHQRzct3FgO+jDzLBWFEUUGczQBQFfD3vxlcTW4VeGhERUVFghhgRERVUoRt052tiHifzlZ58ZjcyG+hMId9HSipNZKYcERHRwzEgRkREBVfIyYf5CsgVOvCXS0oKFORTPoMz5V6amVGI95ESg5GFLjUnIiIqBQyIERFRwRm0Knz/szZ8MVRfkMmH+QrIFTLwlwtKDBTkU76CM8wGuijf7yMlBiOZcUpERPRwnDJJRET0jXxNzCvEZL5cyNe0PyVnoEXjKXw1sYWX54KCLx45KLh0EMTv/nQBdTYDtOoP7V/jyTTc/ih+57f70F1jeZTbKib5eB8pYQruTThtk4iI6GGYIUZERPSNfE3My/dkvlzIR9ZSMWSg5SO7kdlA18vH+0jJpYmllnFKRESUbwyIERER0Z3lI1CgxFK1m+QyOFPK/eeUTsnByEKXmhMRERU78dMXISIiIvr/t3f3PG2dUQDHD07ApElQFFKpNANSGLIwZEJUyhdgy86aj9WVfAW+QAeLmaUDkejiSjEWauy61wm+nagc1ErNy+W+nN9vQWKA57E8/XXuPZ9aDgXLvlUouDmBtna3F08e9OPx/X4Mzi5iNCm+6u+3zeHedhzsbkW5KGN4OYtyUZoGugXXMXI8LWI0KWL+cRGjSRHjaRH7O5uNCFBPHvTj+Q8PG3EWAGgTE2IAwGeremqpyY+q1cE0UH08mggA3SSIAQBfpMpQ0ORH1erUhffPtY0YCQDdZMskAHRAnZsYq9r2Z4seAABVEcQAoMXasInxS83mV3F0ch6DpbvtV3S3OoNiHbLdFwDgJkEMAFoswxRVVRNoEd0Oiv8m230BAP6LLZMA0FJZNjFWuUXvzclvcXw6jF5vJX58dC96vZU4Ph3G0cn5N/9fTdDE+757X8Svv7/vzPcVAGgHL9UHgJayifHr3AyKEfHPz8HZRbx68bRTn1/T7mtaDQCokwkxAGip5U2My7JvYvy/roPixvrqJ7/fWF+NP+dXcTGZ13SyajTtvk2cVgMA8hDEAKClvn/Yj592NmM8LWI0KWL+cRGjSRHjaRH7O5udmm6qQrag2KT7ZnncFwBoLkEMAFrscG87Dna3olyUMbycRbko42B3Kw73tus+WuNlC4pNum/TptUAgHy8QwwAWuze2p14/fJZvHrxtLJNjF12HQ4HZxcxvJzFd2t3Oh0Um3Lf5Wm15e9rV6fzAIDmWSnLsqz7EAAAdRpNilRBsQn3/fmXt3F8OozH9/uxsb4af/z1IcbTIg52t+L1y2e1nAkAyEMQAwDg1s3mV3F0ch6DpS2T+7ZMAgC3RBADAKA2TZhWAwDyEcQAAAAASMWWSQAAAABSEcQAAAAASEUQAwAAACAVQQwAAACAVAQxAAAAAFIRxAAAAABIRRADAAAAIBVBDAAAAIBUBDEAAAAAUhHEAAAAAEhFEAMAAAAgFUEMAAAAgFQEMQAAAABSEcQAAAAASEUQAwAAACAVQQwAAACAVAQxAAAAAFIRxAAAAABIRRADAAAAIBVBDAAAAIBUBDEAAAAAUhHEAAAAAEhFEAMAAAAgFUEMAAAAgFQEMQAAAABSEcQAAAAASEUQAwAAACAVQQwAAACAVAQxAAAAAFIRxAAAAABIRRADAAAAIBVBDAAAAIBUBDEAAAAAUhHEAAAAAEhFEAMAAAAgFUEMAAAAgFQEMQAAAABSEcQAAAAASEUQAwAAACAVQQwAAACAVAQxAAAAAFIRxAAAAABIRRADAAAAIBVBDAAAAIBUBDEAAAAAUhHEAAAAAEjlb7Pe9lZbvEjRAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_similar_papers(paper_title, graph, top_n=5):\n",
        "    if paper_title not in graph:\n",
        "        return \"Статья не найдена в графе.\"\n",
        "\n",
        "    # Собираем всех соседей и вес связи с ними\n",
        "    neighbors = []\n",
        "    for neighbor in graph.neighbors(paper_title):\n",
        "        weight = graph[paper_title][neighbor]['weight']\n",
        "        neighbors.append((neighbor, weight))\n",
        "\n",
        "    # Сортируем по убыванию веса связи\n",
        "    neighbors_sorted = sorted(neighbors, key=lambda x: x[1], reverse=True)\n",
        "    return neighbors_sorted[:top_n]\n",
        "\n",
        "# Пример использования\n",
        "input_paper = df.iloc[3]['title']\n",
        "similar = find_similar_papers(input_paper, G_papers)\n",
        "for paper, weight in similar:\n",
        "    print(f\"Статья: {paper} | Общих ключевых слов: {weight}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00npDEwcMXMI",
        "outputId": "c9ac7d7f-445d-4bd4-b71d-0eae84118c6e"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Статья: Logical methods of object recognition on satellite images using spatial\n",
            "  constraints | Общих ключевых слов: 1\n",
            "Статья: Visual Vocabulary Learning and Its Application to 3D and Mobile Visual\n",
            "  Search | Общих ключевых слов: 1\n",
            "Статья: Synthesising Dynamic Textures using Convolutional Neural Networks | Общих ключевых слов: 1\n",
            "Статья: The Effect of Top-Down Attention in Occluded Object Recognition | Общих ключевых слов: 1\n",
            "Статья: The use of Octree in point cloud analysis with application to cultural\n",
            "  heritage | Общих ключевых слов: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Количество статей (узлов): {G_papers.number_of_nodes()}\")\n",
        "print(f\"Количество связей (ребер): {G_papers.number_of_edges()}\")\n",
        "print(f\"Плотность графа: {nx.density(G_papers):.3f}\")\n",
        "print(f\"Средняя степень связности: {sum(dict(G_papers.degree()).values()) / G_papers.number_of_nodes():.1f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpX8VBxvMXGu",
        "outputId": "9214473f-75aa-4843-c31e-9e3e1e6c7506"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Количество статей (узлов): 1000\n",
            "Количество связей (ребер): 5489\n",
            "Плотность графа: 0.011\n",
            "Средняя степень связности: 11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Метрики графа и качества кластеризации\n",
        "\n",
        "Реализованы:\n",
        "- Coverage разбиения,\n",
        "- Conductance для сообщества,\n",
        "- Средняя conductance,\n",
        "- Silhouette на спектральных эмбеддингах.\n",
        "\n",
        "Эти метрики помогут оценить качество найденных сообществ.\n"
      ],
      "metadata": {
        "id": "eHnzcQuxrEs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import SpectralEmbedding\n",
        "from sklearn.metrics import silhouette_score\n",
        "import numpy as np\n",
        "\n",
        "# --- Метрики графа и кластеризации ---\n",
        "def coverage_of_partition(G, communities):\n",
        "    m = G.number_of_edges()\n",
        "    if m == 0:\n",
        "        return 0.0\n",
        "    intra = 0\n",
        "    for comm in communities:\n",
        "        intra += G.subgraph(comm).number_of_edges()\n",
        "    return intra / m\n",
        "\n",
        "def conductance_of_community(G, community):\n",
        "    comm = set(community)\n",
        "    cut_edges = sum(\n",
        "        G[u][v].get(\"weight\", 1.0)\n",
        "        for u in comm for v in G.neighbors(u)\n",
        "        if v not in comm\n",
        "    )\n",
        "    vol_comm = sum(G.degree(u, weight=\"weight\") for u in comm)\n",
        "    vol_rest = sum(G.degree(u, weight=\"weight\") for u in G if u not in comm)\n",
        "    denom = min(vol_comm, vol_rest)\n",
        "    return cut_edges / denom if denom > 0 else 0.0\n",
        "\n",
        "def average_conductance(G, communities):\n",
        "    return np.mean([conductance_of_community(G, c) for c in communities])\n",
        "\n",
        "def silhouette_on_embeddings(G, communities, n_components=16):\n",
        "    nodes = list(G.nodes())\n",
        "    A = nx.to_scipy_sparse_array(G, nodelist=nodes, weight=\"weight\", format='csr')\n",
        "    A.indices = A.indices.astype(np.int32)\n",
        "    A.indptr = A.indptr.astype(np.int32)\n",
        "    embed = SpectralEmbedding(n_components=n_components, affinity='precomputed').fit_transform(A)\n",
        "    label_map = {}\n",
        "    for cid, comm in enumerate(communities):\n",
        "        for n in comm:\n",
        "            label_map[n] = cid\n",
        "    labels = np.array([label_map.get(n, -1) for n in nodes])\n",
        "    mask = labels >= 0\n",
        "    if len(set(labels[mask])) < 2:\n",
        "        return float(\"nan\")\n",
        "    return silhouette_score(embed[mask], labels[mask])\n",
        "\n",
        "\n",
        "# communities_all = полный список сообществ по всему графу ключевых слов\n",
        "communities_all = nx.community.louvain_communities(G, resolution=0.9)\n",
        "\n",
        "print(\"=== Метрики для графа ключевых слов ===\")\n",
        "print(\"Модульность:\", nx.community.modularity(G, communities_all))\n",
        "print(\"Coverage:\", coverage_of_partition(G, communities_all))\n",
        "print(\"Средняя conductance:\", average_conductance(G, communities_all))\n",
        "print(\"Silhouette (по спектральным эмбеддингам):\", silhouette_on_embeddings(G, communities_all))\n",
        "print()\n",
        "print(\"=== Общие метрики графа ===\")\n",
        "print(\"Число узлов:\", G.number_of_nodes())\n",
        "print(\"Число рёбер:\", G.number_of_edges())\n",
        "print(\"Плотность графа:\", nx.density(G))\n",
        "print(\"Средний clustering coefficient:\", nx.average_clustering(G))\n",
        "if nx.is_connected(G):\n",
        "    print(\"Средняя длина пути:\", nx.average_shortest_path_length(G))\n",
        "else:\n",
        "    print(\"Средняя длина пути: граф несвязный\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xALVN6prlaGC",
        "outputId": "ed770f67-8ccb-45d3-935b-6221b4106cc7"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Метрики для графа ключевых слов ===\n",
            "Модульность: 0.8887559039271096\n",
            "Coverage: 0.9157878648937796\n",
            "Средняя conductance: 0.008172248894385158\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/manifold/_spectral_embedding.py:329: UserWarning:\n",
            "\n",
            "Graph is not fully connected, spectral embedding may not work as expected.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette (по спектральным эмбеддингам): 0.3423463262398279\n",
            "\n",
            "=== Общие метрики графа ===\n",
            "Число узлов: 3085\n",
            "Число рёбер: 6543\n",
            "Плотность графа: 0.001375426470495494\n",
            "Средний clustering coefficient: 0.9073333147598779\n",
            "Средняя длина пути: граф несвязный\n"
          ]
        }
      ]
    }
  ]
}