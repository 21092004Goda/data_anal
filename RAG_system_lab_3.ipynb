{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOs5oCopmzJfKM+ry6wWzCG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/21092004Goda/data_anal/blob/main/RAG_system_lab_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGNKXplsC9M-",
        "outputId": "f4824b42-e68c-43da-9825-c44ab0b2d4bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.28.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.187.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.43.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.12.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.72.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (2.32.4)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv\n",
        "import pandas as pd\n",
        "\n",
        "def fetch_arxiv_ml_articles(max_results=10):\n",
        "\n",
        "    client = arxiv.Client()\n",
        "\n",
        "    search = arxiv.Search(\n",
        "        query=\"cat:cs.LG\",\n",
        "        max_results=max_results,\n",
        "        sort_by=arxiv.SortCriterion.SubmittedDate,\n",
        "        sort_order=arxiv.SortOrder.Descending\n",
        "    )\n",
        "\n",
        "    articles_data = []\n",
        "\n",
        "    for result in client.results(search):\n",
        "        article_info = {\n",
        "            'title': result.title,\n",
        "            'authors': [author.name for author in result.authors],\n",
        "            'abstract': result.summary,\n",
        "            'pdf_url': result.pdf_url,\n",
        "            'published': result.published.date()\n",
        "        }\n",
        "        articles_data.append(article_info)\n",
        "\n",
        "    df = pd.DataFrame(articles_data)\n",
        "    return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df_articles = fetch_arxiv_ml_articles(max_results=15)\n",
        "\n",
        "    print(df_articles)\n",
        "\n",
        "    df_articles.to_csv('arxiv_ml_articles.csv', index=False, encoding='utf-8')\n",
        "    print(\"\\n–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª 'arxiv_ml_articles.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arHYjMRN_Fad",
        "outputId": "07bb8fa7-f02e-4ca2-e770-98cb45c876b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                title  \\\n",
            "0   TraceGen: World Modeling in 3D Trace Space Ena...   \n",
            "1   ToolOrchestra: Elevating Intelligence via Effi...   \n",
            "2   Matrix: Peer-to-Peer Multi-Agent Synthetic Dat...   \n",
            "3   Agentic Learner with Grow-and-Refine Multimoda...   \n",
            "4   On Evolution-Based Models for Experimentation ...   \n",
            "5   DSD: A Distributed Speculative Decoding Soluti...   \n",
            "6   Through the telecom lens: Are all training sam...   \n",
            "7   Escaping the Verifier: Learning to Reason via ...   \n",
            "8               EvilGenie: A Reward Hacking Benchmark   \n",
            "9   Continual Error Correction on Low-Resource Dev...   \n",
            "10  Aligning LLMs Toward Multi-Turn Conversational...   \n",
            "11  Mechanisms of Non-Monotonic Scaling in Vision ...   \n",
            "12  Scale-Agnostic Kolmogorov-Arnold Geometry in N...   \n",
            "13        On the Origin of Algorithmic Progress in AI   \n",
            "14  Beyond URLs: Metadata Diversity and Position f...   \n",
            "\n",
            "                                              authors  \\\n",
            "0   [Seungjae Lee, Yoonkyo Jung, Inkook Chun, Yao-...   \n",
            "1   [Hongjin Su, Shizhe Diao, Ximing Lu, Mingjie L...   \n",
            "2   [Dong Wang, Yang Li, Ansong Ni, Ching-Feng Yeh...   \n",
            "3   [Weihao Bo, Shan Zhang, Yanpeng Sun, Jingjing ...   \n",
            "4                     [Sadegh Shirani, Mohsen Bayati]   \n",
            "5   [Fengze Yu, Leshu Li, Brad McDanel, Saiqian Zh...   \n",
            "6   [Shruti Bothe, Illyyne Saffar, Aurelie Boisbun...   \n",
            "7                         [Locke Cai, Ivan Provilkov]   \n",
            "8   [Jonathan Gabor, Jayson Lynch, Jonathan Rosenf...   \n",
            "9   [Kirill Paramonov, Mete Ozay, Aristeidis Mysta...   \n",
            "10  [Daniel R. Jiang, Jalaj Bhandari, Yukai Yang, ...   \n",
            "11                 [Anantha Padmanaban Krishna Kumar]   \n",
            "12  [Mathew Vanherreweghe, Michael H. Freedman, Ke...   \n",
            "13  [Hans Gundlach, Alex Fogelson, Jayson Lynch, A...   \n",
            "14  [Dongyang Fan, Diba Hashemi, Sai Praneeth Kari...   \n",
            "\n",
            "                                             abstract  \\\n",
            "0   Learning new robot tasks on new platforms and ...   \n",
            "1   Large language models are powerful generalists...   \n",
            "2   Synthetic data has become increasingly importa...   \n",
            "3   MLLMs exhibit strong reasoning on isolated que...   \n",
            "4   Causal effect estimation in networked systems ...   \n",
            "5   Large language model (LLM) inference often suf...   \n",
            "6   The rise of AI in telecommunications, from opt...   \n",
            "7   Training Large Language Models (LLMs) to reaso...   \n",
            "8   We introduce EvilGenie, a benchmark for reward...   \n",
            "9   The proliferation of AI models in everyday dev...   \n",
            "10  Optimizing large language models (LLMs) for mu...   \n",
            "11  Deeper Vision Transformers often perform worse...   \n",
            "12  Recent work by Freedman and Mulligan demonstra...   \n",
            "13  Algorithms have been estimated to increase AI ...   \n",
            "14  Incorporating metadata in Large Language Model...   \n",
            "\n",
            "                               pdf_url   published  \n",
            "0   https://arxiv.org/pdf/2511.21690v1  2025-11-26  \n",
            "1   https://arxiv.org/pdf/2511.21689v1  2025-11-26  \n",
            "2   https://arxiv.org/pdf/2511.21686v1  2025-11-26  \n",
            "3   https://arxiv.org/pdf/2511.21678v1  2025-11-26  \n",
            "4   https://arxiv.org/pdf/2511.21675v1  2025-11-26  \n",
            "5   https://arxiv.org/pdf/2511.21669v1  2025-11-26  \n",
            "6   https://arxiv.org/pdf/2511.21668v1  2025-11-26  \n",
            "7   https://arxiv.org/pdf/2511.21667v1  2025-11-26  \n",
            "8   https://arxiv.org/pdf/2511.21654v1  2025-11-26  \n",
            "9   https://arxiv.org/pdf/2511.21652v1  2025-11-26  \n",
            "10  https://arxiv.org/pdf/2511.21638v1  2025-11-26  \n",
            "11  https://arxiv.org/pdf/2511.21635v1  2025-11-26  \n",
            "12  https://arxiv.org/pdf/2511.21626v1  2025-11-26  \n",
            "13  https://arxiv.org/pdf/2511.21622v1  2025-11-26  \n",
            "14  https://arxiv.org/pdf/2511.21613v1  2025-11-26  \n",
            "\n",
            "–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª 'arxiv_ml_articles.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from typing import List, Dict\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
        "df_articles = pd.read_csv('arxiv_ml_articles.csv')\n",
        "\n",
        "# –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–ª–∏–Ω—É –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π\n",
        "def analyze_abstracts_length(df):\n",
        "    word_counts = []\n",
        "    for abstract in df['abstract']:\n",
        "        words = re.findall(r'\\b\\w+\\b', abstract)\n",
        "        word_counts.append(len(words))\n",
        "\n",
        "    df['word_count'] = word_counts\n",
        "    return word_counts\n",
        "\n",
        "word_counts = analyze_abstracts_length(df_articles)\n",
        "\n",
        "print(\"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–ª–∏–Ω–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π (–≤ —Å–ª–æ–≤–∞—Ö):\")\n",
        "print(f\"–ú–∏–Ω–∏–º—É–º: {min(word_counts)}\")\n",
        "print(f\"–ú–∞–∫—Å–∏–º—É–º: {max(word_counts)}\")\n",
        "print(f\"–°—Ä–µ–¥–Ω–µ–µ: {sum(word_counts)/len(word_counts):.1f}\")\n",
        "print(f\"–ú–µ–¥–∏–∞–Ω–∞: {sorted(word_counts)[len(word_counts)//2]}\")\n",
        "print(\"\\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω:\")\n",
        "for count in word_counts:\n",
        "    print(f\"- {count} —Å–ª–æ–≤\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ajIILCm_FiR",
        "outputId": "aef7708b-d79f-40f3-8f4f-1b5479891bc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–ª–∏–Ω–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π (–≤ —Å–ª–æ–≤–∞—Ö):\n",
            "–ú–∏–Ω–∏–º—É–º: 131\n",
            "–ú–∞–∫—Å–∏–º—É–º: 250\n",
            "–°—Ä–µ–¥–Ω–µ–µ: 188.5\n",
            "–ú–µ–¥–∏–∞–Ω–∞: 185\n",
            "\n",
            "–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω:\n",
            "- 224 —Å–ª–æ–≤\n",
            "- 217 —Å–ª–æ–≤\n",
            "- 201 —Å–ª–æ–≤\n",
            "- 221 —Å–ª–æ–≤\n",
            "- 250 —Å–ª–æ–≤\n",
            "- 136 —Å–ª–æ–≤\n",
            "- 166 —Å–ª–æ–≤\n",
            "- 177 —Å–ª–æ–≤\n",
            "- 171 —Å–ª–æ–≤\n",
            "- 185 —Å–ª–æ–≤\n",
            "- 201 —Å–ª–æ–≤\n",
            "- 182 —Å–ª–æ–≤\n",
            "- 131 —Å–ª–æ–≤\n",
            "- 201 —Å–ª–æ–≤\n",
            "- 165 —Å–ª–æ–≤\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_into_chunks(text: str, chunk_size: int = 150, overlap: int = 15) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º.\n",
        "\n",
        "    Args:\n",
        "        text (str): –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç\n",
        "        chunk_size (int): –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–ª–æ–≤–∞—Ö\n",
        "        overlap (int): –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏ –≤ —Å–ª–æ–≤–∞—Ö\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏\n",
        "    \"\"\"\n",
        "    # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Å–ª–æ–≤–∞\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "\n",
        "    chunks = []\n",
        "    start = 0\n",
        "\n",
        "    while start < len(words):\n",
        "        end = start + chunk_size\n",
        "        chunk_words = words[start:end]\n",
        "        chunk_text = ' '.join(chunk_words)\n",
        "\n",
        "        chunk_info = {\n",
        "            'text': chunk_text,\n",
        "            'start_word': start,\n",
        "            'end_word': min(end, len(words)),\n",
        "            'total_words': len(chunk_words),\n",
        "            'is_complete': end >= len(words)\n",
        "        }\n",
        "        chunks.append(chunk_info)\n",
        "\n",
        "        # –ï—Å–ª–∏ –¥–æ—à–ª–∏ –¥–æ –∫–æ–Ω—Ü–∞ —Ç–µ–∫—Å—Ç–∞\n",
        "        if end >= len(words):\n",
        "            break\n",
        "\n",
        "        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º —Å—Ç–∞—Ä—Ç–æ–≤—É—é –ø–æ–∑–∏—Ü–∏—é —Å —É—á–µ—Ç–æ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è\n",
        "        start += chunk_size - overlap\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def create_chunked_dataset(df, chunk_size=150, overlap=15):\n",
        "    \"\"\"\n",
        "    –°–æ–∑–¥–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç —Å —á–∞–Ω–∫–∞–º–∏ –¥–ª—è –≤—Å–µ—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame —Å–æ —Å—Ç–∞—Ç—å—è–º–∏\n",
        "        chunk_size: –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–ª–æ–≤–∞—Ö\n",
        "        overlap: –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏ –≤ —Å–ª–æ–≤–∞—Ö\n",
        "\n",
        "    Returns:\n",
        "        DataFrame —Å —á–∞–Ω–∫–∞–º–∏\n",
        "    \"\"\"\n",
        "    chunked_data = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        chunks = split_into_chunks(row['abstract'], chunk_size, overlap)\n",
        "\n",
        "        for chunk_idx, chunk in enumerate(chunks):\n",
        "            chunk_record = {\n",
        "                'original_article_id': idx,\n",
        "                'title': row['title'],\n",
        "                'chunk_id': chunk_idx,\n",
        "                'chunk_text': chunk['text'],\n",
        "                'start_word': chunk['start_word'],\n",
        "                'end_word': chunk['end_word'],\n",
        "                'total_words_in_chunk': chunk['total_words'],\n",
        "                'is_final_chunk': chunk['is_complete'],\n",
        "                'total_words_in_abstract': len(re.findall(r'\\b\\w+\\b', row['abstract']))\n",
        "            }\n",
        "            chunked_data.append(chunk_record)\n",
        "\n",
        "    return pd.DataFrame(chunked_data)\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏\n",
        "chunk_size = 150\n",
        "overlap = 15\n",
        "\n",
        "df_chunks = create_chunked_dataset(df_articles, chunk_size, overlap)\n",
        "\n",
        "print(f\"–ò—Å—Ö–æ–¥–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π: {len(df_articles)}\")\n",
        "print(f\"–ü–æ–ª—É—á–µ–Ω–æ —á–∞–Ω–∫–æ–≤: {len(df_chunks)}\")\n",
        "print(f\"–°—Ä–µ–¥–Ω–µ–µ —á–∞–Ω–∫–æ–≤ –Ω–∞ —Å—Ç–∞—Ç—å—é: {len(df_chunks)/len(df_articles):.1f}\")\n",
        "\n",
        "# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã —á–∞–Ω–∫–æ–≤\n",
        "print(\"\\n–ü—Ä–∏–º–µ—Ä—ã —á–∞–Ω–∫–æ–≤:\")\n",
        "for i, (_, chunk) in enumerate(df_chunks.head(3).iterrows()):\n",
        "    print(f\"\\n--- –ß–∞–Ω–∫ {i+1} (—Å—Ç–∞—Ç—å—è '{chunk['title'][:50]}...') ---\")\n",
        "    print(f\"–°–ª–æ–≤–∞: {chunk['start_word']}-{chunk['end_word']}\")\n",
        "    print(f\"–¢–µ–∫—Å—Ç: {chunk['chunk_text'][:200]}...\")\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
        "df_chunks.to_csv('arxiv_ml_articles_chunks.csv', index=False, encoding='utf-8')\n",
        "print(f\"\\n–ß–∞–Ω–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'arxiv_ml_articles_chunks.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebdpkQ1b_Fl2",
        "outputId": "31b0b463-1fca-4286-a408-7ddf6b51f3cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–ò—Å—Ö–æ–¥–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π: 15\n",
            "–ü–æ–ª—É—á–µ–Ω–æ —á–∞–Ω–∫–æ–≤: 28\n",
            "–°—Ä–µ–¥–Ω–µ–µ —á–∞–Ω–∫–æ–≤ –Ω–∞ —Å—Ç–∞—Ç—å—é: 1.9\n",
            "\n",
            "–ü—Ä–∏–º–µ—Ä—ã —á–∞–Ω–∫–æ–≤:\n",
            "\n",
            "--- –ß–∞–Ω–∫ 1 (—Å—Ç–∞—Ç—å—è 'TraceGen: World Modeling in 3D Trace Space Enables...') ---\n",
            "–°–ª–æ–≤–∞: 0-150\n",
            "–¢–µ–∫—Å—Ç: Learning new robot tasks on new platforms and in new scenes from only a handful of demonstrations remains challenging While videos of other embodiments humans and different robots are abundant differe...\n",
            "\n",
            "--- –ß–∞–Ω–∫ 2 (—Å—Ç–∞—Ç—å—è 'TraceGen: World Modeling in 3D Trace Space Enables...') ---\n",
            "–°–ª–æ–≤–∞: 135-224\n",
            "–¢–µ–∫—Å—Ç: triplets Pretraining on this corpus produces a transferable 3D motion prior that adapts efficiently with just five target robot videos TraceGen attains 80 success across four tasks while offering 50 6...\n",
            "\n",
            "--- –ß–∞–Ω–∫ 3 (—Å—Ç–∞—Ç—å—è 'ToolOrchestra: Elevating Intelligence via Efficien...') ---\n",
            "–°–ª–æ–≤–∞: 0-150\n",
            "–¢–µ–∫—Å—Ç: Large language models are powerful generalists yet solving deep and complex problems such as those of the Humanity s Last Exam HLE remains both conceptually challenging and computationally expensive W...\n",
            "\n",
            "–ß–∞–Ω–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'arxiv_ml_articles_chunks.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ —Å FAISS\n",
        "import pandas as pd\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º —á–∞–Ω–∫–∏\n",
        "df_chunks = pd.read_csv('arxiv_ml_articles_chunks.csv')\n",
        "\n",
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤\n",
        "texts = df_chunks['chunk_text'].tolist()\n",
        "embeddings = model.encode(texts, normalize_embeddings=True)\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º FAISS –∏–Ω–¥–µ–∫—Å\n",
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(embeddings.astype('float32'))\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ\n",
        "faiss.write_index(index, 'faiss_arxiv_ml.index')\n",
        "\n",
        "print(\"‚úÖ –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\")\n",
        "print(f\"‚Ä¢ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {len(df_chunks)}\")\n",
        "print(f\"‚Ä¢ –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤: {dimension}\")\n",
        "print(f\"‚Ä¢ –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: faiss_arxiv_ml.index\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ywq-pY0lAslj",
        "outputId": "dd79f2e4-7b68-4ac4-873b-3cb73d9115fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\n",
            "‚Ä¢ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —á–∞–Ω–∫–æ–≤: 28\n",
            "‚Ä¢ –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤: 384\n",
            "‚Ä¢ –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: faiss_arxiv_ml.index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø—Ä–∏–º–µ—Ä —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ–∏—Å–∫–∞ –∏ –æ—Ç–≤–µ—Ç–∞\n",
        "def enhanced_search_and_answer(question, top_k=3):\n",
        "    \"\"\"–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫: –Ω–∞—Ö–æ–¥–∏—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –∏ –¥–∞–µ—Ç —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π –æ—Ç–≤–µ—Ç.\"\"\"\n",
        "\n",
        "    # 1. –í–ï–ö–¢–û–†–ù–´–ô –ü–û–ò–°–ö: –ò—Å–ø–æ–ª—å–∑—É–µ–º FAISS –¥–ª—è –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤\n",
        "    query_embedding = model.encode([question], normalize_embeddings=True).astype('float32')\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "\n",
        "    # 2. –°–ë–û–† –ö–û–ù–¢–ï–ö–°–¢–ê: –ì–æ—Ç–æ–≤–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è LLM\n",
        "    context = \"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –Ω–∞—É—á–Ω—ã–µ —Å—Ç–∞—Ç—å–∏:\\n\"\n",
        "    article_titles = []\n",
        "\n",
        "    for i, idx in enumerate(indices[0]):\n",
        "        chunk = df_chunks.iloc[idx]\n",
        "        context += f\"\\n[{i+1}] –ó–∞–≥–æ–ª–æ–≤–æ–∫: {chunk['title']}\\n\"\n",
        "        context += f\"   –§—Ä–∞–≥–º–µ–Ω—Ç: {chunk['chunk_text']}\\n\"\n",
        "        article_titles.append(chunk['title'])\n",
        "\n",
        "    # 3. –ì–ï–ù–ï–†–ê–¶–ò–Ø –û–¢–í–ï–¢–ê: –ò—Å–ø–æ–ª—å–∑—É–µ–º LLM –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏\n",
        "    prompt = f\"\"\"\n",
        "    –¢—ã ‚Äî –Ω–∞—É—á–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–∏–≤–µ–¥–µ–Ω–Ω—ã–π –Ω–∏–∂–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç, —á—Ç–æ–±—ã –¥–∞—Ç—å —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å.\n",
        "\n",
        "    {context}\n",
        "\n",
        "    –í–æ–ø—Ä–æ—Å: {question}\n",
        "\n",
        "    –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∫ –æ—Ç–≤–µ—Ç—É:\n",
        "    - –ë—É–¥—å —Ç–æ—á–Ω—ã–º –∏ –æ—Å–Ω–æ–≤—ã–≤–∞–π—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ.\n",
        "    - –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞, —Ç–∞–∫ –∏ —Å–∫–∞–∂–∏.\n",
        "    - –í—ã–¥–µ–ª–∏ –æ—Å–Ω–æ–≤–Ω—ã–µ –∏–¥–µ–∏ –∏, –µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ, —É–∫–∞–∂–∏, –∏–∑ –∫–∞–∫–∏—Ö —Å—Ç–∞—Ç–µ–π –æ–Ω–∏ –≤–∑—è—Ç—ã.\n",
        "    - –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–µ–∑–µ–Ω –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è –≤ –æ–±–ª–∞—Å—Ç–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.\n",
        "    \"\"\"\n",
        "\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text, article_titles  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ç–≤–µ—Ç –∏ —Å–ø–∏—Å–æ–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π\n",
        "\n",
        "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞\n",
        "question = \"–ö–∞–∫–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –≤ –æ–±–ª–∞—Å—Ç–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π?\"\n",
        "detailed_answer, used_articles = enhanced_search_and_answer(question)\n",
        "\n",
        "print(f\"‚ùì –í–æ–ø—Ä–æ—Å: {question}\")\n",
        "print(f\"üí° –û—Ç–≤–µ—Ç: {detailed_answer}\")\n",
        "print(f\"üìö –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏: {used_articles}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "Y8kuyDUHAsin",
        "outputId": "e36a8807-9549-4984-c420-771b095eb9b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'SentenceTransformer' object has no attribute 'generate_content'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2845974299.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"–ö–∞–∫–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –≤ –æ–±–ª–∞—Å—Ç–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mdetailed_answer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mused_articles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menhanced_search_and_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"‚ùì –í–æ–ø—Ä–æ—Å: {question}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2845974299.py\u001b[0m in \u001b[0;36menhanced_search_and_answer\u001b[0;34m(question, top_k)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \"\"\"\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marticle_titles\u001b[0m  \u001b[0;31m# –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ç–≤–µ—Ç –∏ —Å–ø–∏—Å–æ–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1962\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1964\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1965\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1966\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'SentenceTransformer' object has no attribute 'generate_content'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lSvszPrcAscA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Exui2pOXAsZU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}