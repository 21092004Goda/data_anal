{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "authorship_tag": "ABX9TyOEsEe5FYgJDuiol0Mce0rr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/21092004Goda/data_anal/blob/main/RAG_system_lab_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Установка зависимостей**"
      ],
      "metadata": {
        "id": "-WQ2x_qGzJ6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install --quiet langchain_huggingface\n",
        "!pip install --quiet sentence-transformers\n",
        "!pip install --quiet faiss-cpu\n",
        "!pip install --quiet arxiv"
      ],
      "metadata": {
        "id": "1aaJge2dPQmA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Классы**"
      ],
      "metadata": {
        "id": "NU25Ct7qzUR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Извлечение статей**"
      ],
      "metadata": {
        "id": "idkNE3MUzUFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv\n",
        "import time\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "class ArxivTopicFetcher:\n",
        "\n",
        "    def __init__(self, max_results_per_request: int = 100, delay_seconds: float = 3.0):\n",
        "        self.client = arxiv.Client(\n",
        "            page_size=max_results_per_request,\n",
        "            delay_seconds=delay_seconds,\n",
        "            num_retries=3\n",
        "        )\n",
        "        self.common_categories = {\n",
        "            'machine_learning': 'cs.LG',\n",
        "            'artificial_intelligence': 'cs.AI',\n",
        "            'computer_vision': 'cs.CV',\n",
        "            'nlp': 'cs.CL',\n",
        "            'robotics': 'cs.RO',\n",
        "            'databases': 'cs.DB',\n",
        "            'security': 'cs.CR',\n",
        "            'networks': 'cs.NI',\n",
        "            'algorithms': 'cs.DS',\n",
        "            'hci': 'cs.HC'\n",
        "        }\n",
        "\n",
        "    def _get_sort_criterion(self, sort_by: str) -> arxiv.SortCriterion:\n",
        "        sort_map = {\n",
        "            'relevance': arxiv.SortCriterion.Relevance,\n",
        "            'lastUpdatedDate': arxiv.SortCriterion.LastUpdatedDate,\n",
        "            'submittedDate': arxiv.SortCriterion.SubmittedDate\n",
        "        }\n",
        "        return sort_map.get(sort_by, arxiv.SortCriterion.SubmittedDate)\n",
        "\n",
        "    def build_query(self, category: str) -> str:\n",
        "        if category in self.common_categories:\n",
        "            category = self.common_categories[category]\n",
        "        return f\"cat:{category}\"\n",
        "\n",
        "    def fetch_articles_paged(self, query: str, total_results: int = 2000,\n",
        "                            sort_by: str = 'submittedDate',\n",
        "                            sort_order: str = 'descending',\n",
        "                            batch_size: int = 500) -> List[Dict[str, Any]]:\n",
        "\n",
        "        batch_size = min(batch_size, 2000)\n",
        "\n",
        "        all_articles = []\n",
        "\n",
        "        for start_index in range(0, total_results, batch_size):\n",
        "            print(f\"Загружаю статьи с {start_index} по {start_index + batch_size - 1}...\")\n",
        "\n",
        "            sort_order_obj = (arxiv.SortOrder.Descending if sort_order == 'descending'\n",
        "                             else arxiv.SortOrder.Ascending)\n",
        "\n",
        "            search = arxiv.Search(\n",
        "                query=query,\n",
        "                max_results=batch_size,\n",
        "                start=start_index,\n",
        "                sort_by=self._get_sort_criterion(sort_by),\n",
        "                sort_order=sort_order_obj\n",
        "            )\n",
        "\n",
        "            try:\n",
        "                batch_articles = []\n",
        "                for result in self.client.results(search):\n",
        "                    batch_articles.append({\n",
        "                        'arxiv_id': result.entry_id.split('/')[-1],\n",
        "                        'title': result.title,\n",
        "                        'authors': [a.name for a in result.authors],\n",
        "                        'abstract': result.summary.replace('\\n', ' '),\n",
        "                        'published': result.published.date() if result.published else None,\n",
        "                        'categories': result.categories,\n",
        "                        'pdf_url': result.pdf_url\n",
        "                    })\n",
        "\n",
        "                all_articles.extend(batch_articles)\n",
        "                print(f\"Получено статей в пачке: {len(batch_articles)}. Всего: {len(all_articles)}\")\n",
        "\n",
        "                if len(batch_articles) < batch_size:\n",
        "                    print(\"Достигнут конец списка результатов.\")\n",
        "                    break\n",
        "\n",
        "                time.sleep(1.0)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Ошибка при загрузке пачки (start={start_index}): {e}\")\n",
        "                break\n",
        "\n",
        "        print(f\"\\n✅ Загрузка завершена. Всего получено статей: {len(all_articles)}\")\n",
        "        return all_articles\n",
        "\n",
        "    def fetch_articles(self, query: str, max_results: int = 50,\n",
        "                      sort_by: str = 'submittedDate',\n",
        "                      sort_order: str = 'descending') -> List[Dict[str, Any]]:\n",
        "\n",
        "        return self.fetch_articles_paged(\n",
        "            query=query,\n",
        "            total_results=max_results,\n",
        "            sort_by=sort_by,\n",
        "            sort_order=sort_order,\n",
        "            batch_size=min(max_results, 1000)\n",
        "        )\n",
        "\n",
        "    def fetch_by_category(self, category: str, max_results: int = 50, **kwargs):\n",
        "        query = self.build_query(category)\n",
        "        return self.fetch_articles_paged(\n",
        "            query=query,\n",
        "            total_results=max_results,\n",
        "            batch_size=min(max_results, 1000),\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "    def print_summary(self, articles: List[Dict[str, Any]], n: int = 5):\n",
        "        if not articles:\n",
        "            print(\"Пусто.\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n=== Короткий обзор ===\")\n",
        "        for idx, a in enumerate(articles[:n]):\n",
        "            print(f\"\\n{idx+1}. {a['title']}\")\n",
        "            print(\"   Авторы:\", \", \".join(a['authors'][:3]) + (\" и др.\" if len(a['authors']) > 3 else \"\"))\n",
        "            print(\"   Дата:\", a['published'])\n",
        "            print(\"   ID:\", a['arxiv_id'])\n",
        "            print(\"   Категории:\", \", \".join(a['categories']))\n",
        "            print(\"   Абстракт:\", a['abstract'][:200], \"...\")"
      ],
      "metadata": {
        "id": "0dWifRB5PQqU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Разбиение на чанки**"
      ],
      "metadata": {
        "id": "GvxMHJY7zcsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class TextChunker:\n",
        "\n",
        "    def __init__(self, chunk_size: int = 500, overlap: int = 50):\n",
        "        self.chunk_size = chunk_size\n",
        "        self.overlap = overlap\n",
        "\n",
        "    def chunk_text(self, text: str):\n",
        "        if not text or not isinstance(text, str):\n",
        "            return []\n",
        "\n",
        "        words = text.split()\n",
        "        chunks = []\n",
        "        start = 0\n",
        "        while start < len(words):\n",
        "            end = start + self.chunk_size\n",
        "            chunk_words = words[start:end]\n",
        "            if not chunk_words:\n",
        "                break\n",
        "            chunks.append(\" \".join(chunk_words))\n",
        "            start = end - self.overlap\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def chunk_many(self, texts):\n",
        "        return [self.chunk_text(t) for t in texts]\n",
        "\n",
        "    def to_dataframe(self, articles):\n",
        "        rows = []\n",
        "        for a in articles:\n",
        "            chunks = self.chunk_text(a[\"abstract\"])\n",
        "            rows.append({\n",
        "                \"id\": a[\"arxiv_id\"],\n",
        "                \"title\": a[\"title\"],\n",
        "                \"authors\": \", \".join(a[\"authors\"]),\n",
        "                \"published\": a[\"published\"],\n",
        "                \"categories\": \", \".join(a[\"categories\"]),\n",
        "                \"pdf_url\": a[\"pdf_url\"],\n",
        "                \"abstract\": a[\"abstract\"],\n",
        "                \"chunks\": chunks\n",
        "            })\n",
        "        return pd.DataFrame(rows)\n",
        "\n",
        "    def chunk_statistics(self, df: pd.DataFrame, plot: bool = False):\n",
        "        chunk_counts = df[\"chunks\"].apply(len)\n",
        "        stats = {\n",
        "            \"Total articles\": len(df),\n",
        "            \"Total chunks\": int(chunk_counts.sum()),\n",
        "            \"Min chunks per article\": int(chunk_counts.min()),\n",
        "            \"Max chunks per article\": int(chunk_counts.max()),\n",
        "            \"Mean chunks per article\": float(chunk_counts.mean()),\n",
        "            \"Median chunks per article\": float(chunk_counts.median())\n",
        "        }\n",
        "\n",
        "        print(\"\\n=== Chunking Statistics ===\")\n",
        "        for k, v in stats.items():\n",
        "            print(f\"{k}: {v}\")\n",
        "\n",
        "        if plot:\n",
        "            plt.figure(figsize=(8,4))\n",
        "            plt.hist(chunk_counts, bins=range(1, chunk_counts.max()+2), alpha=0.7, color='skyblue', edgecolor='black')\n",
        "            plt.title(\"Distribution of Chunks per Article\")\n",
        "            plt.xlabel(\"Number of Chunks\")\n",
        "            plt.ylabel(\"Number of Articles\")\n",
        "            plt.xticks(range(1, chunk_counts.max()+2))\n",
        "            plt.show()\n",
        "\n",
        "        return stats\n"
      ],
      "metadata": {
        "id": "VT4P807BPQwE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Векторизация текста**"
      ],
      "metadata": {
        "id": "69lWFQ18zg-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import faiss\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "class ArxivVectorPipeline:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        device=\"cpu\",\n",
        "        normalize=False\n",
        "    ):\n",
        "        self.embeddings_model = HuggingFaceEmbeddings(\n",
        "            model_name=model_name,\n",
        "            model_kwargs={\"device\": device},\n",
        "            encode_kwargs={\"normalize_embeddings\": normalize}\n",
        "        )\n",
        "\n",
        "        self.index = None\n",
        "        self.embedding_dim = None\n",
        "        self.chunks_df = None\n",
        "        self.embeddings = None\n",
        "\n",
        "    def _flatten_chunks(self, df: pd.DataFrame):\n",
        "        rows = []\n",
        "        for _, row in df.iterrows():\n",
        "            base = {\n",
        "                \"id\": row[\"id\"],\n",
        "                \"title\": row[\"title\"],\n",
        "                \"authors\": row[\"authors\"],\n",
        "                \"published\": row[\"published\"],\n",
        "                \"categories\": row[\"categories\"],\n",
        "                \"pdf_url\": row[\"pdf_url\"],\n",
        "                \"abstract\": row[\"abstract\"]\n",
        "            }\n",
        "\n",
        "            for idx, ch in enumerate(row[\"chunks\"]):\n",
        "                rows.append({\n",
        "                    **base,\n",
        "                    \"chunk_id\": idx,\n",
        "                    \"text_chunk\": ch\n",
        "                })\n",
        "\n",
        "        return pd.DataFrame(rows)\n",
        "\n",
        "    def _embed(self, texts):\n",
        "        if not texts:\n",
        "            return np.array([])\n",
        "\n",
        "        emb = self.embeddings_model.embed_documents(texts)\n",
        "        return np.array(emb, dtype=np.float32)\n",
        "\n",
        "    def build(self, df: pd.DataFrame):\n",
        "        self.chunks_df = self._flatten_chunks(df)\n",
        "        texts = self.chunks_df[\"text_chunk\"].tolist()\n",
        "        self.embeddings = self._embed(texts)\n",
        "        self.embedding_dim = self.embeddings.shape[1]\n",
        "        self.index = faiss.IndexFlatL2(self.embedding_dim)\n",
        "        self.index.add(self.embeddings)\n",
        "        return self\n",
        "\n",
        "    def search(self, query: str, top_k: int = 5):\n",
        "        if self.index is None:\n",
        "            raise ValueError(\"Индекс пуст. Сначала вызови build().\")\n",
        "\n",
        "        q_emb = self._embed([query])\n",
        "        distances, indices = self.index.search(q_emb, top_k)\n",
        "\n",
        "        results = []\n",
        "        for dist, idx in zip(distances[0], indices[0]):\n",
        "            row = self.chunks_df.iloc[int(idx)]\n",
        "            results.append({\n",
        "                \"distance\": float(dist),\n",
        "                \"chunk_id\": int(row[\"chunk_id\"]),\n",
        "                \"text_chunk\": row[\"text_chunk\"],\n",
        "                \"article\": {\n",
        "                    \"id\": row[\"id\"],\n",
        "                    \"title\": row[\"title\"],\n",
        "                    \"authors\": row[\"authors\"],\n",
        "                    \"categories\": row[\"categories\"],\n",
        "                    \"published\": row[\"published\"],\n",
        "                    \"abstract\": row[\"abstract\"],\n",
        "                    \"pdf_url\": row[\"pdf_url\"]\n",
        "                }\n",
        "            })\n",
        "\n",
        "        return results\n"
      ],
      "metadata": {
        "id": "bsaQNGRtPQyr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Интеграция с LLM**"
      ],
      "metadata": {
        "id": "rRIYZB8gzrJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "\n",
        "\n",
        "class LLMClient:\n",
        "\n",
        "    def __init__(self, api_key: str, model: str = \"gemini-2.5-flash\"):\n",
        "        self.client = genai.Client(api_key=api_key)\n",
        "        self.model = model\n",
        "\n",
        "    def ask(self, prompt: str) -> str:\n",
        "\n",
        "        response = self.client.models.generate_content(\n",
        "            model=self.model,\n",
        "            contents=f'\"role\": \"user\", \"content\": \"{prompt}\"'\n",
        "        )\n",
        "        return response.text\n"
      ],
      "metadata": {
        "id": "obZ9tsKwR_Gj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **RAG-QA по корпусу статей**"
      ],
      "metadata": {
        "id": "-Qmj-uBqz6mz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ArxivQA:\n",
        "\n",
        "    def __init__(self, vector_pipeline, llm_client, top_k=5):\n",
        "        self.vec = vector_pipeline\n",
        "        self.llm = llm_client\n",
        "        self.top_k = top_k\n",
        "\n",
        "    def answer(self, query: str) -> str:\n",
        "        hits = self.vec.search(query, top_k=self.top_k)\n",
        "        context = \"\\n\\n\".join(chunk[\"text_chunk\"] for chunk in hits)\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "You are a smart assistant. Here is the context from scientific articles:\n",
        "\n",
        "{context}\n",
        "\n",
        "Now answer the user's question:\n",
        "{query}\n",
        "\n",
        "Answer clearly and concisely.\n",
        "        \"\"\"\n",
        "\n",
        "        return self.llm.ask(prompt)\n"
      ],
      "metadata": {
        "id": "DG_1vx-RR_DZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **поиск и RAG**"
      ],
      "metadata": {
        "id": "nu0QiYiU0PO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class SearchEngine:\n",
        "\n",
        "    def __init__(self, vector_pipeline, llm_client):\n",
        "        self.vec = vector_pipeline\n",
        "        self.llm = llm_client\n",
        "        self._build_spell_corpus()\n",
        "\n",
        "    def _build_spell_corpus(self):\n",
        "        words = set()\n",
        "        for txt in self.vec.chunks_df[\"text_chunk\"]:\n",
        "            for w in txt.lower().split():\n",
        "                if w.isalpha():\n",
        "                    words.add(w)\n",
        "        self.corpus_words = list(words)\n",
        "\n",
        "    def vector_search(self, query: str, top_k: int = 5):\n",
        "        return self.vec.search(query, top_k)\n",
        "\n",
        "    def build_prompt(self, query: str, context: str, template=None):\n",
        "        if template is None:\n",
        "            template = \"\"\"\n",
        "User query: \"{query}\"\n",
        "\n",
        "Use ONLY this context:\n",
        "-----------------\n",
        "{context}\n",
        "-----------------\n",
        "\n",
        "Answer clearly and factually.\n",
        "\"\"\"\n",
        "        return template.format(query=query, context=context)\n",
        "\n",
        "    def ask_rag(self, query: str, template=None, top_k=5):\n",
        "        hits = self.vector_search(query, top_k)\n",
        "        context = \"\\n\\n\".join(h[\"text_chunk\"] for h in hits)\n",
        "        prompt = self.build_prompt(corrected, context, template)\n",
        "        return self.llm.ask(prompt)\n",
        "\n",
        "    def ask_vanilla(self, query: str):\n",
        "        return self.llm.ask(query)\n"
      ],
      "metadata": {
        "id": "nMrzMo14R-_D"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Проверка выполнение**"
      ],
      "metadata": {
        "id": "wNloiQVc0Tgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fetcher = ArxivTopicFetcher()\n",
        "articles = fetcher.fetch_by_category(\"machine_learning\", max_results=2000)\n",
        "fetcher.print_summary(articles, n=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvTz9eRAPQ4q",
        "outputId": "7bd1e3a4-326b-4007-e906-a3bcb0c6f184"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Запрос: cat:cs.LG\n",
            "Получено статей: 2000\n",
            "\n",
            "=== Короткий обзор ===\n",
            "\n",
            "1. ThetaEvolve: Test-time Learning on Open Problems\n",
            "   Авторы: Yiping Wang, Shao-Rong Su, Zhiyuan Zeng и др.\n",
            "   Дата: 2025-11-28\n",
            "   ID: 2511.23473v1\n",
            "   Категории: cs.LG, cs.CL\n",
            "   Абстракт: Recent advances in large language models (LLMs) have enabled breakthroughs in mathematical discovery, exemplified by AlphaEvolve, a closed-source system that evolves programs to improve bounds on open ...\n",
            "\n",
            "2. SmallWorlds: Assessing Dynamics Understanding of World Models in Isolated Environments\n",
            "   Авторы: Xinyi Li, Zaishuo Xia, Weyl Lu и др.\n",
            "   Дата: 2025-11-28\n",
            "   ID: 2511.23465v1\n",
            "   Категории: cs.LG\n",
            "   Абстракт: Current world models lack a unified and controlled setting for systematic evaluation, making it difficult to assess whether they truly capture the underlying rules that govern environment dynamics. In ...\n",
            "\n",
            "3. The Price of Progress: Algorithmic Efficiency and the Falling Cost of AI Inference\n",
            "   Авторы: Hans Gundlach, Jayson Lynch, Matthias Mertens и др.\n",
            "   Дата: 2025-11-28\n",
            "   ID: 2511.23455v1\n",
            "   Категории: cs.LG, cs.AI, cs.CY\n",
            "   Абстракт: Language models have seen enormous progress on advanced benchmarks in recent years, but much of this progress has only been possible by using more costly models. Benchmarks may therefore present a war ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunker = TextChunker(chunk_size=150, overlap=15)\n",
        "\n",
        "df = chunker.to_dataframe(articles)\n",
        "\n",
        "print(df.head())\n",
        "print(df[\"chunks\"].iloc[0][:2])"
      ],
      "metadata": {
        "id": "U19D0qcYQo5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stats = chunker.chunk_statistics(df)\n",
        "print(stats)"
      ],
      "metadata": {
        "id": "W7z2WBZLc6bP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Full pipeline\n",
        "pipeline = ArxivVectorPipeline(device=\"cpu\")\n",
        "pipeline.build(df)\n",
        "\n",
        "results = pipeline.search(\"reinforcement learning for robots\", top_k=5)\n",
        "\n",
        "for r in results:\n",
        "    print(\"\\n---\")\n",
        "    print(\"Distance:\", r[\"distance\"])\n",
        "    print(\"Chunk:\", r[\"text_chunk\"][:200], \"…\")\n",
        "    print(\"Article title:\", r[\"article\"][\"title\"])\n",
        "    print(\"ID:\", r[\"article\"][\"id\"])"
      ],
      "metadata": {
        "id": "L-7zLkBLQo1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = LLMClient(api_key=\"AIzaSyAQDkTa1BshAi4NMnDTasTZgbmijWdBA8w\")\n",
        "\n",
        "summary = llm.ask(\n",
        "    f\"Сделай короткий хардкорный конспект статьи: {articles[0]['abstract']}\"\n",
        ")\n",
        "\n",
        "print(summary)\n"
      ],
      "metadata": {
        "id": "VFCKtkQSQouD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_model = LLMClient(api_key=\"AIzaSyAQDkTa1BshAi4NMnDTasTZgbmijWdBA8w\")\n",
        "\n",
        "qa = ArxivQA(pipeline, llm_model)\n",
        "\n",
        "ans = qa.answer(\"How do data augmentation techniques improve the generalization of machine learning models?\")\n",
        "print(ans)\n"
      ],
      "metadata": {
        "id": "VME_WNhlUIX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search = SearchEngine(pipeline, llm_model)\n",
        "\n",
        "resp = search.ask_rag(\"reinforment learnig for robtos\")\n",
        "print(resp)"
      ],
      "metadata": {
        "id": "xP0vhDixUIVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q = \"What are advancements in model-based RL?\"\n",
        "\n",
        "print(\"=== Vanilla LLM ===\")\n",
        "print(search.ask_vanilla(q))\n",
        "\n",
        "print(\"\\n=== RAG ===\")\n",
        "print(search.ask_rag(q))\n"
      ],
      "metadata": {
        "id": "e2-Wh57nUIRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3MFVUurMUILy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_1 = \"\"\"\n",
        "Provide a scientific explanation grounded strictly in the supplied corpus.\n",
        "\n",
        "Query: \"{query}\"\n",
        "\n",
        "Base your answer ONLY on the information in:\n",
        "{context}\n",
        "\n",
        "If details are absent in the corpus, respond that no relevant evidence is present.\n",
        "\"\"\"\n",
        "print(search.ask_rag(\"how transformer-based models compress high-dimensional embeddings\", template=prompt_1))\n"
      ],
      "metadata": {
        "id": "h_oE3pdOYBEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_2 = \"\"\"\n",
        "Your task is to extract factual statements from the given context\n",
        "and use only those facts to answer the user’s question.\n",
        "\n",
        "Question: \"{query}\"\n",
        "\n",
        "Relevant extracted facts must come solely from:\n",
        "{context}\n",
        "\n",
        "Do not infer or extend beyond what is explicitly stated.\n",
        "\"\"\"\n",
        "print(search.ask_rag(\"why normalization of embeddings affects similarity search accuracy\", template=prompt_2))\n"
      ],
      "metadata": {
        "id": "WeWcvra4YBBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_3 = \"\"\"\n",
        "Explain the answer with simple analogies suitable for a newcomer,\n",
        "but rely strictly on data from the context.\n",
        "\n",
        "Question: \"{query}\"\n",
        "\n",
        "Context to use:\n",
        "{context}\n",
        "\n",
        "If the context lacks information needed for the answer,\n",
        "state that the corpus does not cover this topic.\n",
        "\"\"\"\n",
        "print(search.ask_rag(\"trade-offs between large pretrained models and lightweight fine-tuned models\", template=prompt_3))\n"
      ],
      "metadata": {
        "id": "VbGEpSTWYA-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_4 = \"\"\"\n",
        "Compose a brief analytical report (3–4 sentences)\n",
        "based exclusively on the provided material.\n",
        "\n",
        "Question: \"{query}\"\n",
        "\n",
        "Use ONLY the content below:\n",
        "{context}\n",
        "\n",
        "Avoid adding external knowledge or assumptions.\n",
        "\"\"\"\n",
        "print(search.ask_rag(\"how synthetic data generation impacts model generalization\", template=prompt_4))\n"
      ],
      "metadata": {
        "id": "wR5cFNHlYA7k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}