{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "authorship_tag": "ABX9TyP6AHkeSgQrmivuHD0C5Tb2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/21092004Goda/data_anal/blob/main/RAG_system_lab_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Установка зависимостей**"
      ],
      "metadata": {
        "id": "-WQ2x_qGzJ6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install --quiet langchain_huggingface\n",
        "!pip install --quiet sentence-transformers\n",
        "!pip install --quiet faiss-cpu\n",
        "!pip install --quiet arxiv"
      ],
      "metadata": {
        "id": "1aaJge2dPQmA"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Классы**"
      ],
      "metadata": {
        "id": "NU25Ct7qzUR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Извлечение статей**"
      ],
      "metadata": {
        "id": "idkNE3MUzUFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv\n",
        "import time\n",
        "import math\n",
        "from typing import List, Dict, Any, Iterator\n",
        "\n",
        "class ArxivTopicFetcher:\n",
        "\n",
        "    def __init__(self, max_results_per_request: int = 100, delay_seconds: float = 3.0):\n",
        "        self.client = arxiv.Client(\n",
        "            page_size=max_results_per_request,\n",
        "            delay_seconds=delay_seconds,\n",
        "            num_retries=3\n",
        "        )\n",
        "        self.common_categories = {\n",
        "            'machine_learning': 'cs.LG',\n",
        "            'artificial_intelligence': 'cs.AI',\n",
        "            'computer_vision': 'cs.CV',\n",
        "            'nlp': 'cs.CL',\n",
        "            'robotics': 'cs.RO',\n",
        "            'databases': 'cs.DB',\n",
        "            'security': 'cs.CR',\n",
        "            'networks': 'cs.NI',\n",
        "            'algorithms': 'cs.DS',\n",
        "            'hci': 'cs.HC'\n",
        "        }\n",
        "\n",
        "    def _get_sort_criterion(self, sort_by: str) -> arxiv.SortCriterion:\n",
        "        sort_map = {\n",
        "            'relevance': arxiv.SortCriterion.Relevance,\n",
        "            'lastUpdatedDate': arxiv.SortCriterion.LastUpdatedDate,\n",
        "            'submittedDate': arxiv.SortCriterion.SubmittedDate\n",
        "        }\n",
        "        return sort_map.get(sort_by, arxiv.SortCriterion.SubmittedDate)\n",
        "\n",
        "    def build_query(self, category: str) -> str:\n",
        "        if category in self.common_categories:\n",
        "            category = self.common_categories[category]\n",
        "        return f\"cat:{category}\"\n",
        "\n",
        "    def fetch_articles_paged(self, query: str, total_results: int = 2000,\n",
        "                            sort_by: str = 'submittedDate',\n",
        "                            sort_order: str = 'descending',\n",
        "                            batch_size: int = 500) -> List[Dict[str, Any]]:\n",
        "\n",
        "        batch_size = min(batch_size, 2000)\n",
        "\n",
        "        all_articles = []\n",
        "        total_fetched = 0\n",
        "\n",
        "        iterations = math.ceil(total_results / batch_size) if batch_size > 0 else 0\n",
        "\n",
        "        for iteration in range(iterations):\n",
        "            remaining = total_results - total_fetched\n",
        "            current_batch = min(batch_size, remaining)\n",
        "\n",
        "            if current_batch <= 0:\n",
        "                break\n",
        "\n",
        "            start_num = total_fetched\n",
        "            end_num = start_num + current_batch - 1\n",
        "            print(f\"Загружаю статьи {start_num}-{end_num} (пачка {iteration + 1}/{iterations})...\")\n",
        "\n",
        "            sort_order_obj = (arxiv.SortOrder.Descending if sort_order == 'descending'\n",
        "                             else arxiv.SortOrder.Ascending)\n",
        "\n",
        "            try:\n",
        "                search = arxiv.Search(\n",
        "                    query=query,\n",
        "                    max_results=current_batch,\n",
        "                    sort_by=self._get_sort_criterion(sort_by),\n",
        "                    sort_order=sort_order_obj\n",
        "                )\n",
        "\n",
        "                batch_articles = []\n",
        "                results = list(self.client.results(search))\n",
        "\n",
        "                for result in results:\n",
        "                    batch_articles.append({\n",
        "                        'arxiv_id': result.entry_id.split('/')[-1],\n",
        "                        'title': result.title,\n",
        "                        'authors': [a.name for a in result.authors],\n",
        "                        'abstract': result.summary.replace('\\n', ' '),\n",
        "                        'published': result.published.date() if result.published else None,\n",
        "                        'categories': result.categories,\n",
        "                        'pdf_url': result.pdf_url\n",
        "                    })\n",
        "\n",
        "                all_articles.extend(batch_articles)\n",
        "                total_fetched += len(batch_articles)\n",
        "\n",
        "                print(f\"Получено статей в пачке: {len(batch_articles)}. Всего: {total_fetched}\")\n",
        "\n",
        "                if len(batch_articles) < current_batch:\n",
        "                    print(\"Достигнут конец списка результатов.\")\n",
        "                    break\n",
        "\n",
        "                if iteration < iterations - 1 and len(batch_articles) > 0:\n",
        "                    time.sleep(2.0)\n",
        "                    print(f\"Пауза 2 секунды перед следующей пачкой...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Ошибка при загрузке пачки {iteration + 1}: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"\\n✅ Загрузка завершена. Всего получено статей: {total_fetched}\")\n",
        "        return all_articles\n",
        "\n",
        "    def fetch_articles_streaming(self, query: str, max_results: int = 1000,\n",
        "                                sort_by: str = 'submittedDate',\n",
        "                                sort_order: str = 'descending',\n",
        "                                batch_size: int = 100) -> Iterator[Dict[str, Any]]:\n",
        "\n",
        "        batch_size = min(batch_size, 2000)\n",
        "        fetched = 0\n",
        "\n",
        "        while fetched < max_results:\n",
        "            remaining = max_results - fetched\n",
        "            current_batch = min(batch_size, remaining)\n",
        "\n",
        "            if current_batch <= 0:\n",
        "                break\n",
        "\n",
        "            search = arxiv.Search(\n",
        "                query=query,\n",
        "                max_results=current_batch,\n",
        "                sort_by=self._get_sort_criterion(sort_by),\n",
        "                sort_order=(arxiv.SortOrder.Descending if sort_order == 'descending'\n",
        "                           else arxiv.SortOrder.Ascending)\n",
        "            )\n",
        "\n",
        "            try:\n",
        "                batch_count = 0\n",
        "                for result in self.client.results(search):\n",
        "                    article = {\n",
        "                        'arxiv_id': result.entry_id.split('/')[-1],\n",
        "                        'title': result.title,\n",
        "                        'authors': [a.name for a in result.authors],\n",
        "                        'abstract': result.summary.replace('\\n', ' '),\n",
        "                        'published': result.published.date() if result.published else None,\n",
        "                        'categories': result.categories,\n",
        "                        'pdf_url': result.pdf_url\n",
        "                    }\n",
        "                    yield article\n",
        "                    fetched += 1\n",
        "                    batch_count += 1\n",
        "\n",
        "                    if fetched >= max_results:\n",
        "                        break\n",
        "\n",
        "                print(f\"Загружено {batch_count} статей. Всего: {fetched}\")\n",
        "\n",
        "                if batch_count < current_batch:\n",
        "                    print(\"Достигнут конец списка результатов.\")\n",
        "                    break\n",
        "\n",
        "                if fetched < max_results:\n",
        "                    time.sleep(2.0)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Ошибка при загрузке: {e}\")\n",
        "                break\n",
        "\n",
        "        print(f\"Завершено. Всего загружено: {fetched}\")\n",
        "\n",
        "    def fetch_articles(self, query: str, max_results: int = 50,\n",
        "                      sort_by: str = 'submittedDate',\n",
        "                      sort_order: str = 'descending',\n",
        "                      **kwargs) -> List[Dict[str, Any]]:\n",
        "\n",
        "        batch_size = kwargs.get('batch_size', min(max_results, 100))\n",
        "\n",
        "        return self.fetch_articles_paged(\n",
        "            query=query,\n",
        "            total_results=max_results,\n",
        "            sort_by=sort_by,\n",
        "            sort_order=sort_order,\n",
        "            batch_size=batch_size\n",
        "        )\n",
        "\n",
        "    def fetch_by_category(self, category: str, max_results: int = 50,\n",
        "                         batch_size: int = 100, **kwargs) -> List[Dict[str, Any]]:\n",
        "\n",
        "        query = self.build_query(category)\n",
        "\n",
        "        sort_by = kwargs.get('sort_by', 'submittedDate')\n",
        "        sort_order = kwargs.get('sort_order', 'descending')\n",
        "\n",
        "        return self.fetch_articles_paged(\n",
        "            query=query,\n",
        "            total_results=max_results,\n",
        "            sort_by=sort_by,\n",
        "            sort_order=sort_order,\n",
        "            batch_size=batch_size\n",
        "        )\n",
        "\n",
        "    def print_summary(self, articles: List[Dict[str, Any]], n: int = 5):\n",
        "        if not articles:\n",
        "            print(\"Пусто.\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n=== Короткий обзор ===\")\n",
        "        for idx, a in enumerate(articles[:n]):\n",
        "            print(f\"\\n{idx+1}. {a['title']}\")\n",
        "            print(\"   Авторы:\", \", \".join(a['authors'][:3]) + (\" и др.\" if len(a['authors']) > 3 else \"\"))\n",
        "            print(\"   Дата:\", a['published'])\n",
        "            print(\"   ID:\", a['arxiv_id'])\n",
        "            print(\"   Категории:\", \", \".join(a['categories']))\n",
        "            print(\"   Абстракт:\", a['abstract'][:200], \"...\")"
      ],
      "metadata": {
        "id": "0dWifRB5PQqU"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Разбиение на чанки**"
      ],
      "metadata": {
        "id": "GvxMHJY7zcsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class TextChunker:\n",
        "\n",
        "    def __init__(self, chunk_size: int = 500, overlap: int = 50):\n",
        "        self.chunk_size = chunk_size\n",
        "        self.overlap = overlap\n",
        "\n",
        "    def chunk_text(self, text: str):\n",
        "        if not text or not isinstance(text, str):\n",
        "            return []\n",
        "\n",
        "        words = text.split()\n",
        "        chunks = []\n",
        "        start = 0\n",
        "        while start < len(words):\n",
        "            end = start + self.chunk_size\n",
        "            chunk_words = words[start:end]\n",
        "            if not chunk_words:\n",
        "                break\n",
        "            chunks.append(\" \".join(chunk_words))\n",
        "            start = end - self.overlap\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def chunk_many(self, texts):\n",
        "        return [self.chunk_text(t) for t in texts]\n",
        "\n",
        "    def to_dataframe(self, articles):\n",
        "        rows = []\n",
        "        for a in articles:\n",
        "            chunks = self.chunk_text(a[\"abstract\"])\n",
        "            rows.append({\n",
        "                \"id\": a[\"arxiv_id\"],\n",
        "                \"title\": a[\"title\"],\n",
        "                \"authors\": \", \".join(a[\"authors\"]),\n",
        "                \"published\": a[\"published\"],\n",
        "                \"categories\": \", \".join(a[\"categories\"]),\n",
        "                \"pdf_url\": a[\"pdf_url\"],\n",
        "                \"abstract\": a[\"abstract\"],\n",
        "                \"chunks\": chunks\n",
        "            })\n",
        "        return pd.DataFrame(rows)\n",
        "\n",
        "    def chunk_statistics(self, df: pd.DataFrame, plot: bool = False):\n",
        "        chunk_counts = df[\"chunks\"].apply(len)\n",
        "        stats = {\n",
        "            \"Total articles\": len(df),\n",
        "            \"Total chunks\": int(chunk_counts.sum()),\n",
        "            \"Min chunks per article\": int(chunk_counts.min()),\n",
        "            \"Max chunks per article\": int(chunk_counts.max()),\n",
        "            \"Mean chunks per article\": float(chunk_counts.mean()),\n",
        "            \"Median chunks per article\": float(chunk_counts.median())\n",
        "        }\n",
        "\n",
        "        print(\"\\n=== Chunking Statistics ===\")\n",
        "        for k, v in stats.items():\n",
        "            print(f\"{k}: {v}\")\n",
        "\n",
        "        if plot:\n",
        "            plt.figure(figsize=(8,4))\n",
        "            plt.hist(chunk_counts, bins=range(1, chunk_counts.max()+2), alpha=0.7, color='skyblue', edgecolor='black')\n",
        "            plt.title(\"Distribution of Chunks per Article\")\n",
        "            plt.xlabel(\"Number of Chunks\")\n",
        "            plt.ylabel(\"Number of Articles\")\n",
        "            plt.xticks(range(1, chunk_counts.max()+2))\n",
        "            plt.show()\n",
        "\n",
        "        return stats\n"
      ],
      "metadata": {
        "id": "VT4P807BPQwE"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Векторизация текста**"
      ],
      "metadata": {
        "id": "69lWFQ18zg-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import faiss\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "class ArxivVectorPipeline:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        device=\"cpu\",\n",
        "        normalize=False\n",
        "    ):\n",
        "        self.embeddings_model = HuggingFaceEmbeddings(\n",
        "            model_name=model_name,\n",
        "            model_kwargs={\"device\": device},\n",
        "            encode_kwargs={\"normalize_embeddings\": normalize}\n",
        "        )\n",
        "\n",
        "        self.index = None\n",
        "        self.embedding_dim = None\n",
        "        self.chunks_df = None\n",
        "        self.embeddings = None\n",
        "\n",
        "    def _flatten_chunks(self, df: pd.DataFrame):\n",
        "        rows = []\n",
        "        for _, row in df.iterrows():\n",
        "            base = {\n",
        "                \"id\": row[\"id\"],\n",
        "                \"title\": row[\"title\"],\n",
        "                \"authors\": row[\"authors\"],\n",
        "                \"published\": row[\"published\"],\n",
        "                \"categories\": row[\"categories\"],\n",
        "                \"pdf_url\": row[\"pdf_url\"],\n",
        "                \"abstract\": row[\"abstract\"]\n",
        "            }\n",
        "\n",
        "            for idx, ch in enumerate(row[\"chunks\"]):\n",
        "                rows.append({\n",
        "                    **base,\n",
        "                    \"chunk_id\": idx,\n",
        "                    \"text_chunk\": ch\n",
        "                })\n",
        "\n",
        "        return pd.DataFrame(rows)\n",
        "\n",
        "    def _embed(self, texts):\n",
        "        if not texts:\n",
        "            return np.array([])\n",
        "\n",
        "        emb = self.embeddings_model.embed_documents(texts)\n",
        "        return np.array(emb, dtype=np.float32)\n",
        "\n",
        "    def build(self, df: pd.DataFrame):\n",
        "        self.chunks_df = self._flatten_chunks(df)\n",
        "        texts = self.chunks_df[\"text_chunk\"].tolist()\n",
        "        self.embeddings = self._embed(texts)\n",
        "        self.embedding_dim = self.embeddings.shape[1]\n",
        "        self.index = faiss.IndexFlatL2(self.embedding_dim)\n",
        "        self.index.add(self.embeddings)\n",
        "        return self\n",
        "\n",
        "    def search(self, query: str, top_k: int = 5):\n",
        "        if self.index is None:\n",
        "            raise ValueError(\"Индекс пуст. Сначала вызови build().\")\n",
        "\n",
        "        q_emb = self._embed([query])\n",
        "        distances, indices = self.index.search(q_emb, top_k)\n",
        "\n",
        "        results = []\n",
        "        for dist, idx in zip(distances[0], indices[0]):\n",
        "            row = self.chunks_df.iloc[int(idx)]\n",
        "            results.append({\n",
        "                \"distance\": float(dist),\n",
        "                \"chunk_id\": int(row[\"chunk_id\"]),\n",
        "                \"text_chunk\": row[\"text_chunk\"],\n",
        "                \"article\": {\n",
        "                    \"id\": row[\"id\"],\n",
        "                    \"title\": row[\"title\"],\n",
        "                    \"authors\": row[\"authors\"],\n",
        "                    \"categories\": row[\"categories\"],\n",
        "                    \"published\": row[\"published\"],\n",
        "                    \"abstract\": row[\"abstract\"],\n",
        "                    \"pdf_url\": row[\"pdf_url\"]\n",
        "                }\n",
        "            })\n",
        "\n",
        "        return results\n"
      ],
      "metadata": {
        "id": "bsaQNGRtPQyr"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Интеграция с LLM**"
      ],
      "metadata": {
        "id": "rRIYZB8gzrJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "\n",
        "\n",
        "class LLMClient:\n",
        "\n",
        "    def __init__(self, api_key: str, model: str = \"gemini-2.5-flash\"):\n",
        "        self.client = genai.Client(api_key=api_key)\n",
        "        self.model = model\n",
        "\n",
        "    def ask(self, prompt: str) -> str:\n",
        "\n",
        "        response = self.client.models.generate_content(\n",
        "            model=self.model,\n",
        "            contents=f'\"role\": \"user\", \"content\": \"{prompt}\"'\n",
        "        )\n",
        "        return response.text\n"
      ],
      "metadata": {
        "id": "obZ9tsKwR_Gj"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **RAG-QA по корпусу статей**"
      ],
      "metadata": {
        "id": "-Qmj-uBqz6mz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ArxivQA:\n",
        "\n",
        "    def __init__(self, vector_pipeline, llm_client, top_k=5):\n",
        "        self.vec = vector_pipeline\n",
        "        self.llm = llm_client\n",
        "        self.top_k = top_k\n",
        "\n",
        "    def answer(self, query: str) -> str:\n",
        "        hits = self.vec.search(query, top_k=self.top_k)\n",
        "        context = \"\\n\\n\".join(chunk[\"text_chunk\"] for chunk in hits)\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "You are a smart assistant. Here is the context from scientific articles:\n",
        "\n",
        "{context}\n",
        "\n",
        "Now answer the user's question:\n",
        "{query}\n",
        "\n",
        "Answer clearly and concisely.\n",
        "        \"\"\"\n",
        "\n",
        "        return self.llm.ask(prompt)\n"
      ],
      "metadata": {
        "id": "DG_1vx-RR_DZ"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **поиск и RAG**"
      ],
      "metadata": {
        "id": "nu0QiYiU0PO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class SearchEngine:\n",
        "\n",
        "    def __init__(self, vector_pipeline, llm_client):\n",
        "        self.vec = vector_pipeline\n",
        "        self.llm = llm_client\n",
        "        self._build_spell_corpus()\n",
        "\n",
        "    def _build_spell_corpus(self):\n",
        "        words = set()\n",
        "        for txt in self.vec.chunks_df[\"text_chunk\"]:\n",
        "            for w in txt.lower().split():\n",
        "                if w.isalpha():\n",
        "                    words.add(w)\n",
        "        self.corpus_words = list(words)\n",
        "\n",
        "    def vector_search(self, query: str, top_k: int = 5):\n",
        "        return self.vec.search(query, top_k)\n",
        "\n",
        "    def build_prompt(self, query: str, context: str, template=None):\n",
        "        if template is None:\n",
        "            template = \"\"\"\n",
        "User query: \"{query}\"\n",
        "\n",
        "Use ONLY this context:\n",
        "-----------------\n",
        "{context}\n",
        "-----------------\n",
        "\n",
        "Answer clearly and factually.\n",
        "\"\"\"\n",
        "        return template.format(query=query, context=context)\n",
        "\n",
        "    def ask_rag(self, query: str, template=None, top_k=5):\n",
        "        hits = self.vector_search(query, top_k)\n",
        "        context = \"\\n\\n\".join(h[\"text_chunk\"] for h in hits)\n",
        "        prompt = self.build_prompt(query, context, template)\n",
        "        return self.llm.ask(prompt)\n",
        "\n",
        "    def ask_vanilla(self, query: str):\n",
        "        return self.llm.ask(query)\n"
      ],
      "metadata": {
        "id": "nMrzMo14R-_D"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Проверка выполнение**"
      ],
      "metadata": {
        "id": "wNloiQVc0Tgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fetcher = ArxivTopicFetcher()\n",
        "articles = fetcher.fetch_by_category(\n",
        "    category='machine_learning',\n",
        "    max_results=4000,\n",
        "    batch_size=1000\n",
        ")\n",
        "fetcher.print_summary(articles, n=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvTz9eRAPQ4q",
        "outputId": "ab39c7b8-f0e9-420b-d38e-124c07c27e07"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Загружаю статьи 0-999 (пачка 1/4)...\n",
            "Получено статей в пачке: 1000. Всего: 1000\n",
            "Пауза 2 секунды перед следующей пачкой...\n",
            "Загружаю статьи 1000-1999 (пачка 2/4)...\n",
            "Получено статей в пачке: 1000. Всего: 2000\n",
            "Пауза 2 секунды перед следующей пачкой...\n",
            "Загружаю статьи 2000-2999 (пачка 3/4)...\n",
            "Получено статей в пачке: 1000. Всего: 3000\n",
            "Пауза 2 секунды перед следующей пачкой...\n",
            "Загружаю статьи 3000-3999 (пачка 4/4)...\n",
            "Получено статей в пачке: 1000. Всего: 4000\n",
            "\n",
            "✅ Загрузка завершена. Всего получено статей: 4000\n",
            "\n",
            "=== Короткий обзор ===\n",
            "\n",
            "1. ThetaEvolve: Test-time Learning on Open Problems\n",
            "   Авторы: Yiping Wang, Shao-Rong Su, Zhiyuan Zeng и др.\n",
            "   Дата: 2025-11-28\n",
            "   ID: 2511.23473v1\n",
            "   Категории: cs.LG, cs.CL\n",
            "   Абстракт: Recent advances in large language models (LLMs) have enabled breakthroughs in mathematical discovery, exemplified by AlphaEvolve, a closed-source system that evolves programs to improve bounds on open ...\n",
            "\n",
            "2. SmallWorlds: Assessing Dynamics Understanding of World Models in Isolated Environments\n",
            "   Авторы: Xinyi Li, Zaishuo Xia, Weyl Lu и др.\n",
            "   Дата: 2025-11-28\n",
            "   ID: 2511.23465v1\n",
            "   Категории: cs.LG\n",
            "   Абстракт: Current world models lack a unified and controlled setting for systematic evaluation, making it difficult to assess whether they truly capture the underlying rules that govern environment dynamics. In ...\n",
            "\n",
            "3. The Price of Progress: Algorithmic Efficiency and the Falling Cost of AI Inference\n",
            "   Авторы: Hans Gundlach, Jayson Lynch, Matthias Mertens и др.\n",
            "   Дата: 2025-11-28\n",
            "   ID: 2511.23455v1\n",
            "   Категории: cs.LG, cs.AI, cs.CY\n",
            "   Абстракт: Language models have seen enormous progress on advanced benchmarks in recent years, but much of this progress has only been possible by using more costly models. Benchmarks may therefore present a war ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunker = TextChunker(chunk_size=150, overlap=15)\n",
        "\n",
        "df = chunker.to_dataframe(articles)\n",
        "\n",
        "print(df.head())\n",
        "print(df[\"chunks\"].iloc[0][:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U19D0qcYQo5O",
        "outputId": "54404a6d-2076-410d-c4ed-b1c2fbd0fc0c"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             id                                              title  \\\n",
            "0  2511.23473v1   ThetaEvolve: Test-time Learning on Open Problems   \n",
            "1  2511.23465v1  SmallWorlds: Assessing Dynamics Understanding ...   \n",
            "2  2511.23455v1  The Price of Progress: Algorithmic Efficiency ...   \n",
            "3  2511.23449v1  Physics-Informed Neural Networks for Thermophy...   \n",
            "4  2511.23443v1  Provable Benefits of Sinusoidal Activation for...   \n",
            "\n",
            "                                             authors   published  \\\n",
            "0  Yiping Wang, Shao-Rong Su, Zhiyuan Zeng, Eva X...  2025-11-28   \n",
            "1  Xinyi Li, Zaishuo Xia, Weyl Lu, Chenjie Hao, Y...  2025-11-28   \n",
            "2  Hans Gundlach, Jayson Lynch, Matthias Mertens,...  2025-11-28   \n",
            "3                         Ali Waseem, Malcolm Mielle  2025-11-28   \n",
            "4                         Tianlong Huang, Zhiyuan Li  2025-11-28   \n",
            "\n",
            "                   categories                             pdf_url  \\\n",
            "0                cs.LG, cs.CL  https://arxiv.org/pdf/2511.23473v1   \n",
            "1                       cs.LG  https://arxiv.org/pdf/2511.23465v1   \n",
            "2         cs.LG, cs.AI, cs.CY  https://arxiv.org/pdf/2511.23455v1   \n",
            "3  cs.LG, cs.AI, cs.CE, cs.CV  https://arxiv.org/pdf/2511.23449v1   \n",
            "4              cs.LG, stat.ML  https://arxiv.org/pdf/2511.23443v1   \n",
            "\n",
            "                                            abstract  \\\n",
            "0  Recent advances in large language models (LLMs...   \n",
            "1  Current world models lack a unified and contro...   \n",
            "2  Language models have seen enormous progress on...   \n",
            "3  Inverse heat problems refer to the estimation ...   \n",
            "4  This paper studies the role of activation func...   \n",
            "\n",
            "                                              chunks  \n",
            "0  [Recent advances in large language models (LLM...  \n",
            "1  [Current world models lack a unified and contr...  \n",
            "2  [Language models have seen enormous progress o...  \n",
            "3  [Inverse heat problems refer to the estimation...  \n",
            "4  [This paper studies the role of activation fun...  \n",
            "['Recent advances in large language models (LLMs) have enabled breakthroughs in mathematical discovery, exemplified by AlphaEvolve, a closed-source system that evolves programs to improve bounds on open problems. However, it relies on ensembles of frontier LLMs to achieve new bounds and is a pure inference system that models cannot internalize the evolving strategies. We introduce ThetaEvolve, an open-source framework that simplifies and extends AlphaEvolve to efficiently scale both in-context learning and Reinforcement Learning (RL) at test time, allowing models to continually learn from their experiences in improving open optimization problems. ThetaEvolve features a single LLM, a large program database for enhanced exploration, batch sampling for higher throughput, lazy penalties to discourage stagnant outputs, and optional reward shaping for stable training signals, etc. ThetaEvolve is the first evolving framework that enable a small open-source model, like DeepSeek-R1-0528-Qwen3-8B, to achieve new best-known bounds on open problems (circle packing and first auto-correlation inequality)', 'DeepSeek-R1-0528-Qwen3-8B, to achieve new best-known bounds on open problems (circle packing and first auto-correlation inequality) mentioned in AlphaEvolve. Besides, across two models and four open tasks, we find that ThetaEvolve with RL at test-time consistently outperforms inference-only baselines, and the model indeed learns evolving capabilities, as the RL-trained checkpoints demonstrate faster progress and better final performance on both trained target task and other unseen tasks. We release our code publicly: https://github.com/ypwang61/ThetaEvolve']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stats = chunker.chunk_statistics(df)\n",
        "print(stats)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7z2WBZLc6bP",
        "outputId": "579820aa-0810-4430-995b-19bd2a1b2ea6"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Chunking Statistics ===\n",
            "Total articles: 4000\n",
            "Total chunks: 7356\n",
            "Min chunks per article: 1\n",
            "Max chunks per article: 3\n",
            "Mean chunks per article: 1.839\n",
            "Median chunks per article: 2.0\n",
            "{'Total articles': 4000, 'Total chunks': 7356, 'Min chunks per article': 1, 'Max chunks per article': 3, 'Mean chunks per article': 1.839, 'Median chunks per article': 2.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = ArxivVectorPipeline(device=\"cpu\")\n",
        "pipeline.build(df)\n",
        "\n",
        "results = pipeline.search(\"reinforcement learning for robots\", top_k=5)\n",
        "\n",
        "for r in results:\n",
        "    print(\"\\n---\")\n",
        "    print(\"Distance:\", r[\"distance\"])\n",
        "    print(\"Chunk:\", r[\"text_chunk\"][:200], \"…\")\n",
        "    print(\"Article title:\", r[\"article\"][\"title\"])\n",
        "    print(\"ID:\", r[\"article\"][\"id\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-7zLkBLQo1i",
        "outputId": "8277641e-0f2f-436b-890a-f788d27db5bd"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---\n",
            "Distance: 0.9049038290977478\n",
            "Chunk: We propose a refinement of temporal-difference learning that enforces first-order Bellman consistency: the learned value function is trained to match not only the Bellman targets in value but also the …\n",
            "Article title: First-order Sobolev Reinforcement Learning\n",
            "ID: 2511.19165v1\n",
            "\n",
            "---\n",
            "Distance: 0.9049038290977478\n",
            "Chunk: We propose a refinement of temporal-difference learning that enforces first-order Bellman consistency: the learned value function is trained to match not only the Bellman targets in value but also the …\n",
            "Article title: First-order Sobolev Reinforcement Learning\n",
            "ID: 2511.19165v1\n",
            "\n",
            "---\n",
            "Distance: 0.9049038290977478\n",
            "Chunk: We propose a refinement of temporal-difference learning that enforces first-order Bellman consistency: the learned value function is trained to match not only the Bellman targets in value but also the …\n",
            "Article title: First-order Sobolev Reinforcement Learning\n",
            "ID: 2511.19165v1\n",
            "\n",
            "---\n",
            "Distance: 0.9049038290977478\n",
            "Chunk: We propose a refinement of temporal-difference learning that enforces first-order Bellman consistency: the learned value function is trained to match not only the Bellman targets in value but also the …\n",
            "Article title: First-order Sobolev Reinforcement Learning\n",
            "ID: 2511.19165v1\n",
            "\n",
            "---\n",
            "Distance: 0.9419611096382141\n",
            "Chunk: The field of Offline Reinforcement Learning (RL) aims to derive effective policies from pre-collected datasets without active environment interaction. While traditional offline RL algorithms like Cons …\n",
            "Article title: A Comparison Between Decision Transformers and Traditional Offline Reinforcement Learning Algorithms\n",
            "ID: 2511.16475v1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = LLMClient(api_key=\"AIzaSyAMrgl2EgMFzZsQvPruQAP3skf5vf6rx5c\")\n",
        "\n",
        "summary = llm.ask(\n",
        "    f\"Сделай короткий хардкорный конспект статьи: {articles[0]['abstract']}\"\n",
        ")\n",
        "\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFCKtkQSQouD",
        "outputId": "6ce9a526-2a9f-41ba-9e24-46fb47ddb872"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**ThetaEvolve: Хардкорный конспект**\n",
            "\n",
            "ThetaEvolve — открытый фреймворк, упрощающий и расширяющий AlphaEvolve для масштабирования in-context learning и RL-обучения *во время тестирования*, позволяя LLM непрерывно обучаться и интернализировать стратегии улучшения открытых оптимизационных задач.\n",
            "\n",
            "**Ключевые отличия от AlphaEvolve (закрытая, ансамбли LLM, только инференс):**\n",
            "\n",
            "*   **Модель:** Использует **одну LLM** (даже малую открытую, DeepSeek-R1-0528-Qwen3-8B).\n",
            "*   **Обучение:** **RL во время тестирования**, модель учится, а не просто выводит.\n",
            "*   **Особенности:** Большая база программ (эксплорация), батчевая выборка (пропускная способность), \"ленивые\" штрафы (от застоя), опциональное формирование награды (стабильность обучения).\n",
            "\n",
            "**Результаты:**\n",
            "\n",
            "*   **Новые границы:** Первая открытая система, позволившая *малой LLM* (DeepSeek-8B) достичь **новых наилучших известных границ** для проблем AlphaEvolve (упаковка кругов, неравенство первой автокорреляции).\n",
            "*   **Превосходство RL:** RL-обучение **последовательно превосходит** инференс-only подходы.\n",
            "*   **Обучение способностям:** RL-тренированные модели *действительно усваивают способности к эволюции*, демонстрируя более быстрый прогресс и лучшую производительность как на целевых, так и на **невиданных** задачах.\n",
            "\n",
            "**Доступность:** Код публично выпущен.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_model = LLMClient(api_key=\"AIzaSyAMrgl2EgMFzZsQvPruQAP3skf5vf6rx5c\")\n",
        "\n",
        "qa = ArxivQA(pipeline, llm_model)\n",
        "\n",
        "# Каковы новейшие теоретические основы, объясняющие, почему и когда глубокие нейронные сети хорошо обобщаются, несмотря на наличие миллионов параметров?\n",
        "ans = qa.answer(\"What are the latest theoretical frameworks explaining why and when deep neural networks generalize well despite having millions of parameters?\")\n",
        "print(ans)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VME_WNhlUIX6",
        "outputId": "dfbd10d2-0e76-43c3-da8a-de4e8b6019cc"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "While a unified theory is still lacking, one recent theoretical framework aims to explain why deep neural networks generalize well, especially in overparameterized settings:\n",
            "\n",
            "1.  **Linear Stability Framework:** This framework analyzes the behavior of optimization algorithms like SGD, random perturbations, and Sharpness-Aware Minimization (SAM).\n",
            "2.  **Coherence Measure:** Central to this framework is a coherence measure that quantifies how gradient curvature aligns across data points. This measure helps to reveal why certain flatter or simpler minima are stable and favored during training, which is linked to better generalization.\n",
            "\n",
            "This approach suggests that the dynamics of optimization algorithms inherently prefer solutions with specific curvature properties that promote generalization.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search = SearchEngine(pipeline, llm_model)\n",
        "\n",
        "resp = search.ask_rag(\"What environments were used to test the RL system?\")\n",
        "print(resp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xP0vhDixUIVT",
        "outputId": "b7fe7cc4-40f6-46f3-a70c-1d19bc45eab3"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The RL system was tested in classic control, Atari games, and MuJoCo environments.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q = \"In which areas of domain knowledge does the system leverage expertise to improve RL learning?\"\n",
        "\n",
        "print(\"=== Vanilla LLM ===\")\n",
        "print(search.ask_vanilla(q))\n",
        "\n",
        "print(\"\\n=== RAG ===\")\n",
        "print(search.ask_rag(q))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2-Wh57nUIRE",
        "outputId": "89edc3a3-6d45-47b3-a686-cce10826f75a"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Vanilla LLM ===\n",
            "Domain knowledge is leveraged in Reinforcement Learning (RL) across various areas to significantly improve learning efficiency, safety, robustness, and overall performance. Instead of starting from scratch (tabula rasa learning), incorporating human expertise or existing knowledge can guide the agent, prevent catastrophic failures, and shape the learning process.\n",
            "\n",
            "Here are the key areas where domain knowledge is leveraged in RL:\n",
            "\n",
            "1.  **Reward Function Design:**\n",
            "    *   **How:** Domain experts define and shape the reward signal to guide the agent towards desired behaviors and outcomes. This involves understanding the true objectives, intermediate milestones, and undesirable states.\n",
            "    *   **Leverage:**\n",
            "        *   **Goal Specification:** Clearly defining what \"success\" looks like (e.g., scoring points in a game, reaching a target without collision).\n",
            "        *   **Shaping Rewards:** Adding dense, well-structured reward signals for progress towards the goal, even if the ultimate reward is sparse. This prevents the agent from aimlessly exploring.\n",
            "        *   **Penalties:** Incorporating penalties for unsafe actions, undesirable states, or inefficiencies (e.g., collisions, excessive energy consumption).\n",
            "    *   **Example:** In a robot navigation task, domain knowledge dictates that reaching the target location gives a large positive reward, taking a shorter path gives a small positive reward, and colliding with obstacles gives a large negative reward.\n",
            "\n",
            "2.  **State Representation and Feature Engineering:**\n",
            "    *   **How:** Domain experts can identify the most relevant features or aspects of the environment that the agent needs to consider, and how to represent them effectively.\n",
            "    *   **Leverage:**\n",
            "        *   **Dimensionality Reduction:** Filtering out irrelevant information from raw observations to make the state space more manageable and accelerate learning.\n",
            "        *   **Meaningful Features:** Creating higher-level, interpretable features that capture essential aspects of the environment (e.g., \"distance to nearest obstacle\" instead of raw pixel values).\n",
            "        *   **Normalization/Scaling:** Pre-processing input data based on domain understanding to improve stability and performance of neural networks.\n",
            "    *   **Example:** In autonomous driving, instead of raw camera pixels, domain knowledge suggests features like \"lane deviation,\" \"speed of surrounding vehicles,\" and \"traffic light status\" are crucial for decision-making.\n",
            "\n",
            "3.  **Action Space Design and Constraints:**\n",
            "    *   **How:** Domain knowledge helps define the set of available actions and impose constraints based on physical limitations, safety protocols, or operational rules.\n",
            "    *   **Leverage:**\n",
            "        *   **Valid Actions:** Limiting the action space to physically possible or legally allowed actions (e.g., joint limits for a robot arm, valid moves in a game).\n",
            "        *   **Hierarchical Actions:** Decomposing complex actions into a hierarchy based on domain expertise, allowing the agent to learn high-level strategies first.\n",
            "        *   **Safety Constraints:** Preventing the agent from taking dangerous actions that could lead to system damage or harm.\n",
            "    *   **Example:** For controlling a power grid, domain knowledge dictates which generators can be turned on/off, what are the load limits, and what frequency fluctuations are acceptable.\n",
            "\n",
            "4.  **Initialization of Policies or Value Functions (Imitation Learning/Expert Demonstrations):**\n",
            "    *   **How:** Instead of random initialization, the agent can be pre-trained or initialized using expert demonstrations, human behavior, or existing control policies.\n",
            "    *   **Leverage:**\n",
            "        *   **Warm Start:** Providing a good starting point for learning, reducing the amount of exploration needed and improving sample efficiency.\n",
            "        *   **Guiding Exploration:** Initializing the policy with reasonable behaviors guides exploration towards promising parts of the state space.\n",
            "        *   **Safety (with expert data):** Learning from safe expert trajectories can help prevent the agent from exploring dangerous regions.\n",
            "    *   **Example:** A robot learning a manipulation task can be shown a series of human demonstrations, allowing it to start with a roughly correct policy rather than flailing randomly.\n",
            "\n",
            "5.  **Environment Modeling and Simulation:**\n",
            "    *   **How:** Domain knowledge is crucial for building realistic and effective simulation environments where the agent can train safely and efficiently.\n",
            "    *   **Leverage:**\n",
            "        *   **Physical Laws:** Incorporating known physics, dynamics, and environmental interactions (e.g., gravity, friction, fluid dynamics).\n",
            "        *   **System Dynamics:** Modeling the behavior of complex systems (e.g., chemical processes, economic markets, game rules).\n",
            "        *   **Realistic Feedback:** Ensuring the simulator provides accurate and timely feedback to the agent.\n",
            "    *   **Example:** Building a highly accurate simulator for autonomous vehicles requires extensive domain knowledge of vehicle dynamics, sensor models, traffic rules, and diverse road conditions.\n",
            "\n",
            "6.  **Curriculum Learning and Task Decomposition:**\n",
            "    *   **How:** Domain experts can design a learning curriculum, breaking down complex tasks into simpler sub-tasks and progressively increasing difficulty.\n",
            "    *   **Leverage:**\n",
            "        *   **Structured Learning:** Guiding the agent through a logical progression of skills, much like a human would learn.\n",
            "        *   **Addressing Sparsity:** Enabling the agent to learn foundational skills in easier environments before tackling the full, challenging task.\n",
            "    *   **Example:** For a robot learning to play soccer, domain knowledge suggests first learning to walk, then to kick a stationary ball, then a moving ball, before playing a full game.\n",
            "\n",
            "7.  **Safety Layers and Constraints (Safe RL):**\n",
            "    *   **How:** Domain knowledge is used to define safety critical states, actions, or performance metrics, and implement mechanisms to ensure the agent operates within safe boundaries.\n",
            "    *   **Leverage:**\n",
            "        *   **Hard Constraints:** Explicitly forbidding certain actions or transitions to dangerous states.\n",
            "        *   **Penalty Functions:** Heavily penalizing safety violations.\n",
            "        *   **Shielding:** Using a \"safety shield\" based on formal methods or human rules that overrides the agent's actions if they are deemed unsafe.\n",
            "    *   **Example:** In an industrial control system, domain knowledge defines temperature limits or pressure thresholds that the RL agent must never exceed, even during exploration.\n",
            "\n",
            "By integrating domain knowledge, RL systems can overcome some of their inherent challenges, such as sample inefficiency, difficulty with sparse rewards, and the risks associated with exploration in real-world environments. It allows for more focused learning and the development of policies that are not only high-performing but also safe and reliable.\n",
            "\n",
            "=== RAG ===\n",
            "The provided context states that the system \"demonstrates substantial gains when provided with domain knowledge,\" but it does not specify the particular areas of domain knowledge leveraged.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_1 = \"\"\"\n",
        "Provide a scientific explanation grounded strictly in the supplied corpus.\n",
        "\n",
        "Query: \"{query}\"\n",
        "\n",
        "Base your answer ONLY on the information in:\n",
        "{context}\n",
        "\n",
        "If details are absent in the corpus, respond that no relevant evidence is present.\n",
        "\"\"\"\n",
        "print(search.ask_rag(\"What advantages does the RL system demonstrate in classic control, Atari, and MuJoCo environments?\", template=prompt_1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_oE3pdOYBEC",
        "outputId": "42070c75-b0e2-47fd-e430-eb1aa344bda3"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In classic control, Atari games, and MuJoCo environments, the RL system demonstrates the following advantages:\n",
            "\n",
            "*   It outperforms all baselines on eight out of fifteen tasks when compared to seven widely-adopted RL algorithms (e.g., PPO, SAC, TRPO).\n",
            "*   It demonstrates substantial gains when provided with domain knowledge.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_2 = \"\"\"\n",
        "Your task is to extract factual statements from the given context\n",
        "and use only those facts to answer the user’s question.\n",
        "\n",
        "Question: \"{query}\"\n",
        "\n",
        "Relevant extracted facts must come solely from:\n",
        "{context}\n",
        "\n",
        "Do not infer or extend beyond what is explicitly stated.\n",
        "\"\"\"\n",
        "print(search.ask_rag(\"Which types of RL tasks are mentioned as test cases for the system?\", template=prompt_2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeWcvra4YBBU",
        "outputId": "02f7cc3e-6c53-4a68-b470-5034a2359bb4"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The types of RL tasks mentioned as test cases for the system are classic control, Atari games, and MuJoCo environments.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_3 = \"\"\"\n",
        "Explain the answer with simple analogies suitable for a newcomer,\n",
        "but rely strictly on data from the context.\n",
        "\n",
        "Question: \"{query}\"\n",
        "\n",
        "Context to use:\n",
        "{context}\n",
        "\n",
        "If the context lacks information needed for the answer,\n",
        "state that the corpus does not cover this topic.\n",
        "\"\"\"\n",
        "print(search.ask_rag(\"How does combining semantic information with numerical metrics affect the performance of the RL system?\", template=prompt_3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbGEpSTWYA-Q",
        "outputId": "f40b0fd6-777f-48b5-f362-06c09b891c12"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the context, combining semantic information with numerical metrics positively affects the performance of the RL system.\n",
            "\n",
            "Imagine you're trying to build a LEGO model:\n",
            "*   **Numerical metrics** are like the instruction manual that tells you exactly \"use 2x4 block here,\" \"use 1x1 stud there.\" It's precise, but sometimes doesn't tell you *why* a certain piece goes where it does or how it contributes to the overall structure.\n",
            "*   **Semantic information (or domain knowledge)** is like having an experienced LEGO builder guide you, saying things like, \"This section is the wing, so it needs to be light and stable,\" or \"This piece will make the model more balanced.\" It gives context and understanding beyond just the numbers.\n",
            "\n",
            "When you **combine both** (follow the precise instructions AND understand the purpose and function of each part), the RL system performs much better. Specifically, the text states it:\n",
            "\n",
            "*   **Outperforms other systems:** It \"outperforms all baselines on eight out of fifteen tasks\" and shows \"substantial gains.\" This means it achieves its goals more effectively than systems relying only on numerical rules.\n",
            "*   **Becomes more transparent:** You can understand *why* it makes certain decisions.\n",
            "*   **Becomes more generalizable:** It can apply what it learned to new, slightly different situations more easily.\n",
            "*   **Becomes more human-aligned:** Its actions are more in line with what a human would expect or prefer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_4 = \"\"\"\n",
        "Compose a brief analytical report (3–4 sentences)\n",
        "based exclusively on the provided material.\n",
        "\n",
        "Question: \"{query}\"\n",
        "\n",
        "Use ONLY the content below:\n",
        "{context}\n",
        "\n",
        "Avoid adding external knowledge or assumptions.\n",
        "\"\"\"\n",
        "print(search.ask_rag(\"What is the main goal of the SmallWorlds benchmark described in the paper?\", template=prompt_4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wR5cFNHlYA7k",
        "outputId": "3e559daa-da29-4e6f-c3fd-e833be67af12"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The SmallWorld Benchmark was introduced to address the lack of a unified and controlled setting for systematically evaluating world models and assessing their ability to capture underlying environment dynamics. Its main goal is to assess world model capability under isolated and precisely controlled dynamics, without relying on handcrafted reward signals. Using this benchmark, experiments reveal how effectively models capture environment structure and how their predictions deteriorate over extended rollouts. This highlights the strengths and limitations of current modeling paradigms and offers insights for future improvement in representation learning and dynamics modeling.\n"
          ]
        }
      ]
    }
  ]
}